Introduction DOI: 10.4324/9780429269035-1 that linguists may have a hard time imagining, and corpus linguistics also has this kind of explorative data-driven facet. In regard to targeted elicitations, it underscores how difficult it can be for speakers to imagine usages of some structure out of contextas is the case for out-of-contexts elicitations and judgements -but that the relevant forms may come up promptly once the relevant context has been brought up. It is in this way that the corpus linguistic approach bears great potential not only for the study of language use but also for the demarcation of possible structures.

Corpus linguistics and usage-oriented linguistics

The core concern of corpus linguistics is with patterns of language use and their variation. Language use involves numerous decision-taking processes whereby users choose between alternative ways of expressing the same thing during test production and recipients choose between different ways of interpreting the structures they perceive. The more specific concern of corpus linguistics is to account for these decisions by systematically investigating related variants and conditions on their choice. For instance, whether a copular verb like is is realised in its full form or appears as a clitic 's is subject to numerous factors, and corpus linguists seek to identify these and relate them to one another in modelling the variation at hand (cf.

A major concern with language use is shared by a range of sub-disciplines in linguistics. One of these is sociolinguistics. Sociolinguistics is concerned with the variability of language use and seeks to correlate these with the social features of language users and their interlocutors. For instance, the choice between the two variants of the copula, is and 's, in spoken discourse is related to the preceding and the following words, speech rate, and other aspects of discourse context. But it is also influenced by demographic characteristics of speakers and their audiences, the social and physical setting, and other general aspects of the communicative situation. We will turn to the role of corpus linguistics in sociolinguistics in Chapter 9.

Other areas of linguistics where details of language use are of central concern are psycho-and neurolinguistics. These fields are interested in how language is processed, for example, how language users encode and decode discourse and what structures pose particular problems, reflected in processing delays. For the most part, these fields of linguistics target processing during perception and deploy various methods of measuring aspects of processing, for example, neurological EEG measures of processing delays (N400, P600) (cf.

An area related to sociolinguistics, psycholinguistics, and acquisition research is that of language change and language evolution. A major idea here is that experience with linguistic behaviour shapes aspects of (abstract) linguistic representation. This is true for acquired language not only in an individual (L1 or L2) learner but also throughout communities, so that linguistic representations (phonologies, grammars, lexica) change over time. These changes can then lead to increasing differences across groups of speakers, leading to diversification into distinct dialects and eventually also distinct languages. Corpus linguists' concern with variation is key to this programme, labelled language variation and change within variationist sociolinguistics after William Labov, since variation is a necessary condition of change. In a broader perspective, corpus-based research then also links to linguistic typology where researchers have over recent decades become more and more interested in explaining the greater likelihood and distribution of certain systems

Alongside the relatively narrow field of academic linguistic research, corpora can serve various other purposes; for instance, records of linguistic performance can be relevant in anthropological studies of cultural aspects or in studies of political discourse in a society. Moreover, in some contexts the texts compiled in a corpus for originally academic purposes can form the basis for publications with somewhat different purposes, for instance, where texts are used in educational material for a language community to help support their language in the light of language endangerment

RUNDOWN OF THIS BOOK

The first half of this textbook focuses on the basics of corpus linguistics (Chapter 2) and types of corpora, including what is relevant for assessing the make-up of a corpus (Chapter 3). We give examples of corpus linguistic research in Chapter 4, showing that the corpus linguistic approach is possible for many levels of linguistic analysis and diverse languages. The second part of the book focuses on some of the tasks you might do in conducting your own corpus study including querying a corpus and evaluating the results (Chapter 5), and composing and building your own corpus (Chapter 6). In this vein, we also introduce various corpus annotation schemata which give various kinds of information that cannot be read from the corpus text itself (

Each chapter includes a few exercises and some include recommended further reading to help you engage with and think more deeply about the issues presented. We hope this textbook provides a good first foundation for corpus linguistics and generates your excitement about this large and diverse field.

NOTE

1. We will see that probably any variation of expression comes with specific meaning differences, which can be semantic or pragmatic, and which may be very nuanced. Yet, there do seem to be endless possibilities to express the same core meanings through different expressions; Chapter 4 will outline a range of examples that will make this point clearer.

WHAT IS A CORPUS AND WHAT IS IT GOOD FOR?

In linguistics, a corpus is a collection of texts that serves as the empirical basis for the study of natural languages. A text is any instance of recorded language use that can be treated as a discrete unit: a newspaper article, a recorded university lecture, or dinner table conversation. You can see from these examples that 'text' in this sense is broader than a document or what you produce when typing in a text editor. Moreover, the texts in a corpus must be machine-readable so that they can be collated and investigated with the help of computers. Corpora are not defined in terms of specific properties of texts, so texts can be of any type and may exhibit a large range of diverse properties (cf. Chapter 3 on corpus composition).

In linguistics we distinguish between linguistic knowledge and language use. For instance, speakers of a given language know what sounds distinguish words, what words or constructions mean, or on what occasion to use which expression. Language use refers to what speakers do with language and how they act in a given society by using language. In contrast to linguistic knowledge, language use is directly observable and recordable, and the texts in a corpus essentially comprise such records of language use. The corpus-linguistic approach is the systematic study of language use represented by the texts in corpora, which targets both linguistic knowledge and language use. The insights into linguistic knowledge that we gain from corpora are highly relevant in lexicography (the compilation and writing of dictionaries and lexical databases), for the writing of descriptive grammars of languages, or in anthropological linguistics where one aim of researchers is to establish the connections between linguistic and cultural knowledge

Our definition of corpus and characterisation of corpus linguistics does not specify any properties of the texts included. Some corpus linguists may restrict definitions of corpora to 'authentic' texts, those produced in non-academic contexts

DEFINITION AND CHARACTERISTICS OF TEXTS IN CORPUS LINGUISTICS

What is a text in corpus linguistics?

In corpus linguistics, a text is a record of any stretch of language as produced on a specific occasion at some point in time, at some place, and as received or receivable at some place and time. Text production and reception take place in different modes, that is, texts can be written, signed, or spoken and are received accordingly, being read, seen, or heard. In this textbook, we attempt to counter-balance the traditional focus on written texts and refer to corpora of non-written language texts as much as possible. It is important to keep in mind that written language use is historically derivative of spoken language use in the sense that writing conventions developed after spoken language.

Basic concepts in corpus linguistics

In order to be machine-readable (and for easier consideration by human analysts), spoken and signed texts are considered in corpus linguistics in a written form. This means that spoken and signed texts are not immediately available for inclusion in a corpus, but are transcribed, that is, what is being said or signed is written down according to specific conventions, for example, the conventions of the International Phonetic Association (IPA), which are in turn based on specific writing systems. Linguistic transcriptions are more or less exact renditions of spoken texts and constitute one form of linguistic annotations which will be discussed in Chapter 7 on annotation. Transcription of signed texts also requires conventionalisation of annotation (cf. Auslan Corpus

Spoken language production is accompanied by gestures and facial expressions, and these can be captured in the form of video recordings of spoken texts so that these can be analysed by linguists as well. In modern corpus linguistics, transcriptions are linked to audio or audio-visual recordings of texts so that a spoken and signed language corpus consists of transcription text files as well as audio and/or video files.

We concentrate primarily on spoken and written corpora in this textbook but recommend

Both written and spoken texts consist of wordforms. Transcribed wordforms are identifiable as strings of letters between spaces or before punctuation. In this sense, texts are strings of words. Strings of words are how texts are represented prior to any further corpus annotation (cf. Stefanowitsch 2020:353). 2 But a string of words only forms a text as long as it represents content-forming abstract units, namely phrases, sentences, and utterances related to one another in such a way that language users interpret them coherently. A text is in this sense equivalent to discourse which

Exercise 2.1 A first foray into corpora

Let's look at the Corpus of Contemporary American English (COCA)

2.2.2

What is a word in a text?

In corpus linguistics a fundamental unit of reference is the wordform. In the following, we briefly explain what wordforms are and how this notion relates to that of word. Consider the following excerpt from Barack Obama's speech at the United States of Women summit that took place at the White House on 14 June 2016:

But our country is not just all about the Benjamins -it's about the Tubmans, too. We need all our young people to know that Clara Barton and Lucretia Mott and Sojourner Truth and Eleanor Roosevelt and Dorothy Height, those aren't just for Women's History Month. They're the authors of our history, women who shaped their destiny. They need to know that…That's the story that's still being written, today, by our modern-day heroes like Nancy Pelosi or Sonia Sotomayor or Billie Jean King or Laverne Cox or Sheryl Sandberg or Oprah Winfrey or Mikaila Ulmer or Michelle Obama, the countless ordinary people every day who are bringing us closer to our highest ideals. That's the story we're going to keep on telling, so our girls see that they, too, are America -confident and courageous and, in the words of Audre Lorde, 'deliberate and afraid of nothing. ' (Barack Obama, United State of Women summit, 14 June 2016)

A first distinction relevant here is that between wordform(s) and lexeme:

wordform -these are forms that are directly observable and reflect any inflexions appropriate for their context: 'go' , 'going' , 'goes' , 'went' , 'women' , 'woman' , 'women's' .

lexeme -the abstraction underlying inflectionally related groups of wordforms that share lexical meaning; often one wordform is used to represent a lexeme, for example, GO or WOMAN, as the head word in a dictionary. The all-caps writing is intended to indicate that this is the name of an abstract unit.

Treatment of clitic forms

Clitics are morphemes that have syntactic properties like a word, but that are not phonologically independent, meaning they adjoin to a previous or following word like an affix. Now try the Brown corpus

Basic concepts in corpus linguistics

In corpus linguistics we have to keep the lexeme-wordform distinction in mind when performing corpus queries: if we are interested in the use of any given lexeme in a corpus, we have to make sure that we are going to capture all its wordforms when searching for it. In the case of GO we may have to search for all four wordforms listed above. We will explain in Chapter 5 how you can design smart corpus queries that will do searches of alternate forms like this. Many modern corpora of well-studied languages, however, include the technical feature of lemmatisation, that is, the information about which wordforms belong to which lexemes

Lemmatisation is a layer of corpus annotation, and we will come back to this in Chapter 7. Moreover, while the term lemmatisation is derived from the term lemma which in corpus linguistics (but not necessarily in lexicography) is used fairly often as an equivalent of lexeme.

Types and tokens

A further notion central to corpus linguistics is the distinction between types and tokens, and related to this is the procedure of tokenisation. 3 Tokens are all the

Exercise 2.2 Lemma search

Go back to the COCA and the Brown corpus.

Try to find forms of the lexeme BE. How can you check whether your query returned all forms that belong to the lexeme? Check out different query options in COCA and Sketch Engine. What would you do in the case of the Brown corpus?

How do you find the clitic form 's in either corpus? (You can read 2.

first).

There is no general consensus on how clitic forms should be treated in corpora. For instance, the COCA treats all clitics as separate wordforms whereas the Brown corpus (and other corpora developed in that tradition) treat clitics plus their host as a wordform. This difference does not necessarily reflect the analytic view of corpus compilers, but can often be due to technical conditions. It is therefore important that we always ensure we understand how the text of a corpus is internally structured, and to critically relate these to our linguistic analyses and categories.

individual wordforms in a text. There are 145 word tokens in the Obama excerpt above if we take a space as the delimiter (separator) between wordforms. The three occurrences of that's are counted as three tokens, but as one type. Type frequencies are then lower than token frequencies, and the type/token ratio can reveal interesting insights into properties of texts, essentially telling us how lexically variable a text or corpus is. Counting word tokens in a text is far from trivial even in a language like English, where the question arises whether forms like It's or aren't should be counted as a single word token (taking the delimiter criterion, as we have done above) or two tokens. Different corpora follow different procedures here; for example, the Brown corpus follows the space-delimiter as we have done, counting aren't etc. as single tokens whereas the COCA treats such forms as two tokens (which means that in COCA you have to search for a two-word expression are n't in order to find instances of aren't). Identifying tokens correctly and consistently in a corpus is another aspect of pre-processing and annotation of corpora. In corpus linguistics the procedure identifying and marking the boundaries of word tokens is called tokenisation. Hence, as with lemmatisation, we have to check whether and according to which criteria any given corpus has been tokenised before we can perform informed queries (see also

Measures of frequency for types and tokens

One often sees three common measures of central tendency that can be derived from frequency: arithmetic mean (average), mode (most common value), and median (middle of the range); range is the difference between the minimum and maximum values of a set of measurements. Ranges are useful for understanding the distribution of frequency data, although often minimum and maximum values are provided rather than the mathematical range. They did not finish NEG-3pl.S-sleep-TERM-R:I:PFV sleeping 6 2 3 ti-ngai-tai-yo-ndop I will not be seeing NEG-1sg.S-see-2sg.P-IRR:D you and…

Basic concepts in corpus linguistics

Let's say we have a small corpus of Matukar Panau 4 that has been parsed and glossed. 5  From this, we can get some information about typical kinds of word structures. Although Matukar Panau is an agglutinating language, we see that by token frequency, most words are monomorphemic or bimorphemic in an 8,961-word annotated corpus. However, by type frequency, bimorphemic and 3-morpheme word types are most frequent. This means there are fewer kinds of monomorphemic words, but they occur much more often.

Textual and contextual properties of texts

Above we defined a text as a string of words that form a structured whole. An important notion in corpus linguistics is that of context. By context we refer to all those structures that are relevant for the interpretation of texts as a whole and the linguistic expressions therein, such as wordforms and morphemes. Scholars of discourse distinguish different types of context, all of which influence the production and comprehension of texts.

Linguistic, text-internal context

While wordforms constitute a text on the surface, they also form abstract units, namely phrases which in turn form more complex phrases and clauses and sentences. Wordforms themselves have internal structures, consisting of morphs and syllables, which in turn consist of individual sounds. And higher order phrase, clause and sentence structures are likewise paralleled by a layer of sound structure which form intonational units of different types. Linguistic context is thus multifold and can be viewed in relation to these different levels.

In a converse, bottom-up perspective, each level of representation (mentioned above) defines a structural context in which a given unit occurs. For instance, whether we identify a surface form like need as a verb or a noun form will depend on

Exercise 2.3 Mean, median mode and range

The mean token frequency for Matukar Panau morpheme counts is 1.65 morphemes and the mean type frequency is 1.30 morphemes. The median token frequency is 1 morpheme, the median type frequency is 2 morphemes. Now your turn. What are the modes, minimums, maximums, and ranges for token and type morpheme counts based on this sample? syntagmatic context -which words come before and after it: we need to versus a need for constructional context -what position it has, for instance, in the higher order construction.

If we were investigating the distribution of specific sounds, relevant contexts would be defined by other sounds occurring in the same syllable or (phonological) word, and so forth.

Of particular prominence in corpus linguistics is the contexts of words, and related to this the corpus linguistic concepts of collocations. The term collocation refers to the co-occurrence of multiple lexemes, often two lexemes, called a bigram and a cooccurrence of three lexemes is called a trigram. We can search for a specific lexeme in a corpus and determine its collocates, that is, a list of lexemes that co-occurs with it. A collocate refers to the "next door neighbour" of a lexeme, which may be on either side of the lexeme.

The lexeme of interest will often be referred to as w, the lexeme before a lexeme of interest will often be referred to as w-1 and the lexeme after it will often be referred to as w+1. You may also see L1 (one lexeme to the left of a lexeme of interest) or R1 (one lexeme to the right of a lexeme of interest).

It should be noted that we should generally be sceptical of an all too clearcut conception of linguistic levels, and it is corpus-linguistic research that has advanced our understanding of interactions between, for instance, syntax, morphology and phonology in the area of clitics (some examples of which we observed above) and affixation.

For instance, the copula verb is occurs in various sentences in Obama's speech, and with different words preceding or following it; where in some cases it is reduced in form to 's this may at least to some extent depend on the word that precedes it, and its various morphological and phonological properties. Moreover, the clitic forms a phonological word with whatever element precedes it, so that an account of their occurrence needs to refer not only to syntactic but also to morphological and phonological structure.

How structural contexts are defined depends not only on the type of linguistic unit under discussion but also on the linguistic (and often theoretical) conceptualisations a corpus linguist will bring to the playing field: for instance, some linguists think of syntactic structures in terms of dependencies between words whereas others rely primarily on aspects of constituency, and many (if not most today) combine these two notions (see

Basic concepts in corpus linguistics

Collocates can be ranked according to their token frequency. Collocational strength is particularly relevant in corpus-based studies of lexical relations, for example, where collocations point to semantic differences between lexemes that are often thought of as synonyms (cf.

Colligation is the co-occurrence of specific parts of speech (verbs, nouns, adjectives, etc.) with each other; for example, we can estimate that articles in English show different colligational patterns with nouns, adjectives, and adverbs.

Collostruction is the systematic occurrence of a lexeme in a specific grammatical construction, for example, specific lexical verbs in English occur more frequently in the double object construction as opposed to the indirect construction (cf. 4.2.5).

Language-external context

Given that the above excerpt from Obama's speech is literally "taken out of context", namely, the entire speech, we do not have here information available about its wider context. In this case, this would be whatever came before and after Obama's own speech, and this will still basically belong to the wider linguistic or language-internal context to which the structural context that we just discussed also belongs. It captures the observation that the whole of the text, comprised of linguistic

Exercise 2.4 Token labels

Your lexeme of interest is world. What is the w-1 and w+1 in the sentence FIFA Women's World Cup 2019 winner was the USA? How would you refer to FIFA using this kind of label? How would you refer to winner? List all the bigram tokens in the sentence above in alphabetical order. Hint: the number bigram tokens in a corpus/string will always be one less than the number of lexeme tokens.

Exercise 2.5 Collocate search

Many corpora provide a specific collocation query function where users can specify the orientation and distance of collocates of a search expression. Using COCA and the Brown corpus, find collocates for duckling (only in COCA) and farmer. What is the most common w+1? Include part-of-speech (PoS) filters to find the most common adjective and determiner w-1 and the most common verb and noun w+1 in each corpus.

structures, is in turn embedded in a larger chunk of text, also made up of linguistic structures. All texts also have language-external contexts, which comprise all physical, social, and cultural aspects of the situation in which texts are situated. For instance, Obama's speech took place at the White House, on 14 June 2016, in front of an audience of 5,000 people, with him on the stage speaking into a microphone so that his voice was louder than the cheering and the comments shouted out by the audience. Obama as the speaker comes with specific socially and culturally significant features: he was president of the United States at the time, he is from Hawaii, lived in Indonesia, Chicago, and elsewhere, is an adult male who natively speaks American English, is black, is a cis-gendered, straight male, among many other identifying aspects. Likewise, the members of the audience have specific social features, and it is by now well-established that the audience a language producer has in mind can bear on the way they speak, sign, or write (cf.

Situational contexts

Considerations pertaining to external features also give us a way of determining the boundaries of texts: we can -as a working hypothesis -assume that a text is a stretch of language use that can be characterised by a specific set of external features, for example, the same participants with the same roles. This essentially aligns with the definition of speech event in anthropological linguistics

Basic concepts in corpus linguistics to can shape our own linguistic behaviour. Likewise, relevant in this connection is the publication context of some written texts: for instance, newspaper articles occur next to other articles on the same page, and this may have an impact on how they are being received by a reader.

The central premise of corpus linguistics is that all of the language-internal andexternal contextual features are relevant to the way that people use language. In other words, the use of linguistic forms is always subject to certain conditions. In corpus linguistics, we are essentially interested in how various structures (sounds, words, constructions) are used in particular contexts characterised by internal and external features; how possible variation in usage and choices between alternative structures (the so-called variants of a variable) can be correlated with such features, and how the use of particular forms can then be explained in terms of specific (qualitative) mechanisms relating to such features (see 4.

CORPORA AS SAMPLES OF LANGUAGE USE

So far, we have described corpus linguistics as analysing the specific forms and their distribution in the texts in any given corpus or corpora.

Let's recall Obama's speech cited above: this is an example of a text produced in (American) English. While one could imagine there to be a specific interest in the linguistic properties of this particular speech, our interests as linguists are typically broader: we want to learn about the structure of (American) English and its use in general (i.e. the variable realisation of is as either is or 's). This brings us to the problem of representativeness: shouldn't we consider all the language use in a given language? Obviously, this is an impossible undertaking: nearly all spoken and signed texts in a given language, when not recorded, will disappear virtually upon their production (a property called ephemerality). Many written ones (emails, text messages, etc.) are not preserved either. Corpus linguists deal with a small fraction of all texts that have been produced in a language, so any corpus is always only a small sample drawn from the real-world population of texts (cf. 8.2.1). In other words, the specific texts included in a corpus are meant to stand for language use more generally, that is, represent it. We sample Obama's speech and many others that are also representative of the language population we are interested in. This is the crucial component of any corpus building or compilation project (see Chapter 6) or of carefully using the metadata of existing corpora.

Metadata is what distinguishes a corpus from a random collection of texts by giving it explicit structure: minimally any user of a corpus will know what amount of text data is contained in it, what characteristics texts have, for example, whether they are written for a newspaper or spoken during a conversation with friends or colleagues, who produced the text, and so on. This allows researchers to determine if the samples are representative of the population of interest.

CONCLUSION

To conclude, corpora are collections of texts that represent to some extent the use of a language. Texts consist of wordforms, and wordforms enter larger abstract structures.

Wordforms are immediately observable and can be searched for; larger structures can only be detected through linguistic analysis and searching for these requires further corpus annotation (Chapter 7). Wordforms and in turn the larger structures they form are embedded in text-internal contexts, and the text as a whole relates to situational context. A key concern in corpus linguistics is to explain the variable use of wordforms and other structures in dependence on both types of contextual factors.

FURTHER READING

The basic concepts explained in this chapter are explained in virtually any textbook on corpus linguistics.

NOTES

1. This includes not only constructed examples in some areas of theoretical linguistics but also self-reported structures by participants in surveys within some more recent strands of sociolinguistics and applied linguistics: in these cases, not only the example structures but also circumstances of use are imagined, approximately like 'under circumstances AB I would produce structures XY' . 2.

CORPUS CONTENT AND REPRESENTATIVENESS

Corpus size

The first question we can ask about the content of a corpus is how much text it contains: its size. Corpus size is typically reported as the number of wordform tokens. What size a given corpus has will depend to a major extent on the kinds of texts included and the resources required to compile these into structured collections. To get a better idea about corpus sizes, let's consider some specific examples of English language corpora, as listed in Table

The size of iWeb is outstanding in this list. This enormous size is achieved by (semi-) automatically compiling data from openly accessible websites, focusing on websites hosted in dominantly English-speaking countries and excluding certain websites with potentially offensive content. The current corpus (at time of writing) contains texts from 22,388,141 web pages from 94,391 websites. The smallest corpus in the list is CORE. The corpus contains 48,569 texts -which are equivalent to web pages herecomprising 52,933,543 wordform tokens. Despite the similarity between the two corpora in their web-based content, the striking difference in size is clearly related to a difference in corpus design: while iWeb includes just about any text from almost any website, CORE has been created with the idea of identifying as much and as clearly as possible the situational characteristics of all texts included which constitute the web registers mentioned in the title. Determining register properties for texts published on the web is a painstaking and resource-intensive undertaking, and this helps explain the size differences.

Corpus size is often directly dependent on limitations on resources and other more practical considerations, a point we will return to in Chapter 6 on corpus-building. Similar considerations apply to other mid-range corpora, for example, the Corpus of Contemporary American English (COCA)

Corpus linguists often create smaller sub-corpora of existing corpora for specific research purposes, and these may be much smaller yet. For instance, for our study of object pronoun use in the Oceanic language Vera' a we drew on an annotated sub-corpus of Vera' a with a total of 25,646 wordform tokens

Different issues apply to specific multilingual corpora where corpus size also depends on the number of individual languages (sub-)corpora included, and where quite elaborate procedures of corpus annotation (see Chapter 7, in particular, Section 7.3) will naturally limit the size of corpora. One prominent multilingual corpus is Universal Dependencies (UDs)

Counts of wordform tokens or corpus words is a standard measure to report corpus size, but it is not the only possible one. When discussing sizes of spoken-language corpora within documentary linguistics, the time length of primary audio and/or video data is often cited (see Thieberger 2006:7 on the corpus of Nafsan (formally South Efate)). This is obviously not immediately comparable to corpus sizes given in word tokens since time length does not relate directly to text size: texts may differ in the amount and length of pausing and the speech rate of speakers, so that texts of similar duration in time may exhibit very different numbers of wordform tokens. But the word token measure is likewise not without problems in a cross-linguistic context, since languages differ tremendously in their morphology: in some languages, words are complex and a sentence often consists only of few words, whereas in other languages words are internally relatively simple and sentences typically contain more words. This is obvious from the overview of Multi-CAST in Table

Corpus composition

The second major characteristic of corpora is their internal composition: what types of text are included and in what proportions? To categorise texts, we generally rely on the situational rather than the linguistic characteristics of texts. The inclusion of texts with different situational characteristics reflects the fact that language is used differently in different modes, in different contexts, and by different people, etc. The reason why composition is guided by these situational rather than linguistic characteristics is that the latter cannot be known before a corpus has been compiled and investigated: Corpus composition and corpus types you can select for texts based on the situational fact that they involve more than one speaker, but you cannot select texts with a particular proportion of first-and secondperson pronouns. One can expect interactional texts to contain more of these but it is impossible to know before analysis.

One means to represent different situationally defined text types evenly in a given corpus is to aim for balance. This has been done, for example, in COCA, which contains texts from different categories in roughly equal proportions. In COCA, there are six main 'sections' labelled 'tv/movies' , 'spoken' , 'fiction' , 'magazines' , 'news' , and 'academic' . These reflect rough distinctions of texts in terms of their external The rationale behind this kind of corpus composition is that each cross-category (section/year) has the same share of the overall corpus data and will thus contribute approximately equally to results from any corpus query. The major idea here is to avoid massive skewing in results by over-representing just a single or very few text types.

A somewhat different approach is taken in the famous Brown corpus of American English

The procedure described here may not seem very reliable: based on intuition how can one really estimate what the proportion of biographical texts might be among all written English texts of whatever year?

For some text types, however, matters are quite obvious: for instance, the relatively small amount of spoken texts even in large or super-large corpora of well-studied languages like English is quite clearly at odds with the reality of language use, and more recent corpus building projects have been aiming at including greater proportions of spoken texts, for example, the International Corpus of English (ICE) where 60% of texts are spoken.

Language documentation-based corpora are typically less varied in terms of text type. Depending somewhat on the region and specific projects' contexts, many LD corpora consist predominantly of spoken narrative texts, often monologues. In addition, procedural texts (how something is done or what it looks like) and performative texts like songs are common. Written texts are typically not part of LD corpora, given that most languages are only spoken (or signed). In some projects, like the ones on Teop and Vera'a, written texts have emerged alongside the documentary work in the form of edited stories or written encyclopaedic entries for a dictionary (cf.

Certain special corpora to be discussed in 3.3 are characterised by a fairly confined set of text types that researchers are particularly interested in, for instance, the oral interactions between plane pilots and air traffic controllers. Yet other corpora may

Corpus composition and corpus types

consist of texts that do not represent any text type attested in a population of texts in the given language, for instance, those collected as responses to certain stimuli (cf.

As with our conclusions on corpus size, we take all different types of corpus composition to be viable options in corpus linguistics. The adequacy of any composition is ultimately dependent on specific research agendas and goals. In this view, a corpus consisting entirely of traditional narratives from a specific indigenous 'orature' is as much a corpus as a super-varied one covering a wide range of situational characteristics, but they will be amenable to different research projects.

Authenticity, routines, and spontaneity

In addition to considerations of text type variability some corpus linguists consider further aspects of texts to be vital for the content of corpora. The first one is authenticity or naturalness which has often been considered a defining property of texts to be included in corpora

Authentic texts have speech routines and writing conventions, belonging more generally to communicative competence

Texts that are produced primarily for linguistic research purposes are designated as 'non-authentic' , such as texts elicited as part of linguistic experiments in response to stimuli. Two examples recurrently used in some strands of linguistic research are the Pear Film, a silent movie narrating a simple story of pear theft

The parameter of spontaneity or planning relates to whether texts are produced ad hoc and spontaneously or after some planning time. The turns in many conversations are often relatively spontaneous as participants will react to their interlocutors' contributions more or less immediately. In longer written text production, like a book, producers of texts may go through various rounds of pre-planning and rewriting a text before it is actually delivered. But this is not true for all written text, for example, text messages may be formulated fairly quickly and without much planning. Conversely, some spoken texts such as political or religious speeches involve a high degree of planning before their oral delivery. Basically then, both spoken and written texts can exhibit variable degrees of spontaneity/planning. Spontaneity differences are gradual, so that planning may take different amounts of time and various cycles of rehearsal.

In our view, none of the parameters of authenticity, routineness, and spontaneity constitutes a demarcation criterion for corpus text and/or corpus linguistics. As empirical linguists we are interested in investigating all instances of language use and their conditions, whatever their nature. The only type of text we do not consider corpus text is mere mentions of structures, for example, intuited example sentences. A corpus can also contain elicited texts, including even lists of elicited sentences, as long as all contextual information is preserved. The latter will allow corpus analysts to evaluate attestations of particular structures as being elicited, with such and such context, etc., and this may have specific implications for their linguistic analysis as well. Excluding such data from corpus linguistics will in a sense also deprive us of the possibility to thoroughly evaluate structures vis-à-vis certain production conditions, for example, the production of elicited texts not based on established routines. Discuss your initial observations with a partner, and vice versa, and refine your list. Do some of the expressions extend to other text types?

Exercise 3.2 Text data collection

Corpus composition and corpus types

It is worth reiterating that actual authenticity is pretty much unattainable for many types of text that are particularly "worthy" in this regard, as per

Representativeness

Representativeness refers to the identity in distribution of linguistic and situational features of language use found in the corpus (the sample) and real-life language use (the population). A corpus can be representative of all the possible linguistic features of a language (covering all possible structures that are part of language user's competence), or it can be representative of all the external or situational variables of different texts that are produced in a given language. Representativeness is essentially unattainable given that we can never know what the population really looks like (and unlike pollsters we never have anything like election results come in against which we could evaluate our sampling procedures). We, therefore, take a melioristic approach where we outline ways in which corpora can fare better or worse in regards to representativeness, focusing on the previously mentioned corpus parameters.

Representativeness and size

Generally, when it comes to size it is obvious from a representativeness point of view that bigger is better. But how much does size really matter? Given our definition of representativeness, the major issue with very small corpora is that they will hardly have a chance to achieve any reflection of a wider range of contexts of language use. This can become an issue particularly for small corpora from under-researched languages

It is conversely very clear that corpus size is not to be fetishised. We only need to consider the vast corpora like iWeb or corpora of the TenTen Family 3 to see this clearly: despite their massive sizes these corpora seem to not reflect well the large range of diverse instances of language use and, hence, fare relatively poorly in terms of representativeness.

Representativeness and composition

Approaching representativeness in terms of composition is by no means a trivial undertaking. We have seen above that the creators of the Brown corpus (and other subsequent corpora following the same design principles) sampled written English texts according to their intuitions about the proportions of different text types. This is obviously not particularly reliable.

Relating composition back to corpus size, it should be clear that composition takes precedence over size: tremendously expanding the size of a corpus may not help improve its representativeness very much, as is the case with iWeb. The fact that its metadata is very sparse here adds a further downside (cf.

A further issue is the potential imbalance between the production and reception of texts. In potentially all societies there are text varieties that are received by a large number of people but are produced by only a fraction of the societies' members.

Examples are speeches by political leaders or texts published in popular books or newspapers. This imbalanced situation contrasts with a more prototypical situation of language use in everyday conversations, where produced texts are not normally perceived by a larger public, and where producers and receivers shift roles regularly.

Given that both production and perception of previous texts can influence people's behaviour, the perception of texts is important for considerations of representativeness and balance during corpus building.

Given that the purpose of a corpus is to serve as an empirical basis for the study of human languages, it is inevitable that a corpus has to be treated as a representation of language use beyond the specific texts included in it. We do not share views here (occasionally uttered in corpus linguistics literature) that findings from any given corpus merely apply to this corpus and nothing beyond. Obviously, our motivation for building corpora in the first place and investigating them is to learn about language use in a given language more generally. Whether a corpus is adequate in terms of its representative depends on the research question(s) at hand.

Saturation

Related to the idea of corpus representativeness regarding internal linguistic features is that of saturation. Investigating saturation presupposes a dynamic perspective on corpus content and its coverage of linguistic forms available in a language variety. The idea of saturation is that in an incremental process of corpus building or processing

Corpus composition and corpus types

Corpus composition and corpus types 29 (e.g. by way of annotation), with a growing and ever more variable corpus, it becomes less and less likely to encounter new structures (words, wordforms, sounds, signs, clause structures, etc.). While there is no clear-cut limit, we can state that a corpus is more representative if it achieves higher saturation. One can test this by taking a corpus, establishing a catalogue of all structures from certain domains (sounds, words, sentence structures, etc.) attested, and then seeing to what extent the addition of any further text will expand the inventory of these structures. If the degree of expansion is high, then saturation is low (as is representativeness). If the degree of expansion is low, then the original corpus was already nearly saturated and hence reasonably representative. One important observation here is that saturation is typically reached to different degrees on different linguistic levels: usually, saturation is reached sooner for phonemes than some grammatical phenomena, whereas lexical saturation is probably unattainable.

As with representativeness, corpus size is a necessary condition for saturation but not a sufficient one. We also need to consider matters of composition, since some lexemes or constructions come up only in specific text types. This also means that -like representativeness -full saturation is not attainable but only approachable.

Text varieties: register, genre, and style

The central concern of corpus linguistics is the systematic study of language use and variation therein. An important dimension of variation comes from text varieties, which can be classified according to three parameters: register, genre, and style

Exercise 3.3 Ducklings again

Go back to the COCA, the Brown corpus, and also COHA (cf. Table

Exercise 3.4 Contraction in registers

Go back to the composition of COCA outlined in 3.1.2 and develop hypotheses as to the likelihood of certain contracted forms you can think of will occur in the different sections. Then perform the relevant queries (remember to check the description regarding tokenisation). How would you report your results?

As we have explained in Chapter 2, texts have different properties on two different dimensions, text-internal/linguistic and text-external/situational. Most relevant for corpus linguistic studies of variation in language use is the register dimension: this is characterised by the situational properties of texts. For instance, we can characterise texts in terms of their mode of production and reception, that is, spoken versus signed versus written, the participants (producers, recipients) involved (with their demographic properties and social relationships), or their communicative purpose, that is, whether they are meant to edify, inform, etc.

These differences in situational features impact the use of linguistic structures: spoken conversations contain more pronouns, including first and second person ones, since they involve the negotiation of interpersonal relations and the expressions of participants' viewpoints and evaluations. On the contrary, written academic prose is focussed on conveying dense information, excluding personal evaluations; hence, the rate of pronouns, in particular, first and second person ones, is quite low here and the use of full noun phrases (NPs) prevails instead. Also serving the transmission of complex information, academic prose is generally replete with complex NPs containing attributive adjectives and prepositional phrases. The much higher rate of complex NPs in written registers may also be due to considerations of mode and reception alone: a reader will have more time to process complex structures, and knowledge of this fact may carry over to considerations of text production. It is these kinds of interrelations between situational characteristics, linguistic features, and the functional connection between them that is the core concern of a register analysis. Note that register studies necessarily need to be comparative to be meaningful. Moreover, register analyses always start with an analysis of situational features and seek to determine the relative frequencies of a linguistic feature. Those features that are particularly frequent in a register are called pervasive linguistic features. The pervasiveness of linguistic features is then to be interpreted in terms of situational features.

Considerations of register distinctions are also relevant in the analysis of specific features in lesser-studied languages. For instance,

Corpus composition and corpus types the difference between objects that are expressed by a pronoun and those that are just zero/unexpressed/'dropped' in

The next dimension of text varieties is genre. Text genres are defined not by their situational features but by specific linguistic features that are conventionally used and not clearly motivated by communicative functions. A classic example is the genre of fairy tales in the European literary tradition: not only do these have a very clearly defined narrative structure but they also have conventionalised opening and closing formulas, as shown in

(3.1) Once upon a time (in a far-away land) there was …and they lived happily ever after. (3.2) Es war einmal … Und wenn sie nicht gestorben sind, dann leben sie noch heute.

Similarly in Vera'a a text variety called nelno vu' 'spirit voice' , that is, a customary story, is characterised as a genre by a certain narrative structure and the opening formula shown in

qōn ne vō-wal e ruwa mē-n gunu-ruod ay art.num card-one art.pers hum:du dat-cs spouse-3du 'One day there (were) two married people' .

JSU.001

Similar

genre markers are the opening of certain speeches or announcements (Ladies and gentleman, Sehr geehrte Damen und Hereren, e raga sul) or letters (and emails) (Dear …, Liebe/r …). You could object that these formulas still serve some communicative function, at the least by announcing the type of text they also prepare recipients for their task, that is, to stop talking when a speech begins! But the exact form of the expressions in question is purely conventional (although they may historically be motivated). These are genre markers: linguistic features of genres are not pervasive, rather they occur often only once, in specific positions in a text. Often text varieties with specific genre properties are recognised as such in speech communities and cultures and receive a name, as in the examples in

The style-related linguistic features of texts are similar to register-related ones in being pervasive. The major difference is that they are not motivated by communicative functions; they are due to the specific habits of language use of individual language users or groups thereof (e.g. the authors of a specific period).

Most relevant for general linguistic concerns is the register perspective on text varieties since this offers important insights into the functionality of linguistic structures in use. Even if we are not specifically interested in the genre and style features of texts, we need to keep in mind that genre and style features may play a role when we analyse the use of specific linguistic expressions, and we need to build these into our accounts and models as relevant.

LINKED DATA

In addition to the corpus text data, modern corpora are closely interlinked with two further types of data, namely raw data and metadata. Raw data are records of the originally produced and/or published data, capturing as much of its context as possible. Metadata are data about the corpus files and the structure of the corpus as a whole and the compilation process (including design decisions), as well as data about the situational characteristics of texts.

Raw data and primary data

We adopt the distinction between 'raw data' and 'primary data' from Nikolaus Himmelmann's work on language documentation. The term raw data refers to the recording of a speech event or a written text, including its paralinguistic properties (what else is happening or done by participants about the communicative event). In language documentation, raw data consists primarily of video and audio recordings of the spoken or signed text production. Video recordings capture all gestures and facial expressions and are, of course, the major type of raw data underlying sign language corpora

Exercise 3.5 Digital genres

In your first language, think of text varieties that you encounter in the form of digital writing (published or shared on websites, in social media channels, etc., or as part of private communication) and that may qualify as genres.

(1) What name does the genre have?

(2) Name possible conventional structures or genre markers.

(3) Think again: are you dealing with a genre or would it make more sense to approach the text variety from a register perspective?

Corpus composition and corpus types called corpus data, namely, the searchable text data of a corpus in a written form in digital format. For spoken texts this will be the transcription, often linked to the raw data through the so-called time-aligned transcription (cf.

Metadata

Another type of data linked to corpus data and raw data is metadata. Metadata is literally 'data about data' . In corpora, it encompasses properties of all data types, as well as data about the corpus as a whole and its creation process. Metadata capture properties of the (written) corpus text (text format, encoding, script, structure of annotations, etc.), properties of the original text, that is, the raw data linked to corpus data (audio and video formats, recording equipment, publication details, etc.), and aspects of the relationship between primary data and raw data (how do corpus text files and original publication files, in particular, video and audio files, link to each other? Is there time-alignment, and how is it encoded? etc.). In addition to these, metadata is also collected about the situational features of texts (cf. 2.2.4).

For instance, the Buckeye corpus

Metadata capturing situational features of texts is a central prerequisite for the basic representativeness of any corpus. Imagine a corpus that is overall fairly representative in terms of size and composition, but lacking metadata. How can we know it is representative since we lack this information for evaluating the results from our analyses? Metadata is also relevant to what kind of research we can do with a given corpus. For instance, to evaluate whether a language has been changing over time requires metadata not only on the time of text production but also on the demographic features of people like their age. Or one might find different frequencies of certain expressions (i.e. like first and second pronouns) in conversation versus in narratives where language is perhaps more formal or follows a certain structure. Or one may find different patterns in the speech of young speakers or women or younger women speakers. Without metadata, we cannot test whether differences between any of these categories are meaningful.

Metadata can be more or less detailed, and some details are easier to determine than others. For instance, a factor drastically lowering the representativeness of written corpora is the fact that we often do not know anything about specific circumstances of text production and editing before the written texts were published. This problem is even more drastic in the case of web corpora where -in contrast to newspaper texts or novels -we often know nothing at all about the authors.

CORPUS TYPES

Types of corpora are primarily differentiated with regard to the situational features covered so that we distinguish general corpora that intend to cover basically all situational features relevant for a given language, and special corpora that deliberately cover a subset of situational features. We will deal with these basic types in 3.3.1. In 3.3.2 we turn our attention to other variable features of corpora, for instance, what languages and/or varieties they are intended to cover, or how they are developed or developing over time. This differentiation of corpus types goes hand in hand with specific research goals as we will see.

3.3.1

Corpus types defined by situational features

General corpora

General corpora are those intended to be unrestricted in their coverage of situational features. They attempt to represent a language or variety as a whole and be appropriate for all kinds of linguistic research. One sub-type is a reference corpus: a general corpus taken to be a standard reference for corpus-based research on a given language. An example is DeReKo -Deutsches Referenzkorpus 'German Reference Corpus' , whose purpose is to make available a large corpus of German amenable to a large variety of research questions. Similar to this are national corpora, for example, the British National Corpus (BNC) (BNC Consortium 2007), the Turkish National Corpus (TNC) (TNC Team 2018), or the Russian National Corpus (

Special corpora

Special corpora are corpora that contain texts with a restricted set of situational features meant to represent only a limited range of communicative events. Such a focus in representativeness is motivated by a concomitant focused research agenda; for example, the communication during air traffic control (ATC) between pilots and air traffic controllers. The creation and use of special corpora like ATC corpora are typically motivated by specific research interests and goals, for example, to understand how verbal communication is conducted efficiently and exactly under great time pressure and embedded in other complex actions in ATC. Further scientific goals of an ATC corpus are applied ones, for example, the development of ATC-specific speech recognition systems (cf. the AIRBUS-ATC corpus;

Another prominent type of special corpus is learner corpora, which contain texts from learners of a language, typically at different stages of developing competence. Often these texts are from classroom contexts. These corpora then do not reflect the language use of a general language community, rather that of their emerging new L2

Corpus composition and corpus types

Corpus composition and corpus types 35 members. Typically, L2 texts produced by learners would not be included in a general corpus of the respective language, which in principle raises interesting questions (not to be discussed here), like who counts as a member of a language community? Several special corpora contain only specific types of text, which could also be included in a general corpus. For instance, the Corpus of Supreme Court Opinions (cf. the list in Table

Another type of special corpora defined by situational features is the experimentally elicited text corpora. Experimentally elicited texts are produced mostly or exclusively in the interest of some linguistic research programme and typically controlled in some way with a linguistic research question in mind. Examples are collections of Pear Film re-tellings

While special corpora are deliberately confined to a specific sub-population of texts in a given language, considerations of representativeness and saturation in terms of size and composition are still relevant within the specific text categories targeted. For instance, in learner corpora, we will still want to see learners of different stages, genders, age groups, L1 backgrounds, etc. represented. And again, the same sampling considerations of general corpora apply for special corpora: we sample either to achieve proportional equivalence or in order to reflect the population as closely as possible. Even for very restricted corpora in terms of text types, like ATC corpora, variability in situational features is relevant, and so these will have to contain text specimens produced by female and male pilots of different age groups, different linguistic backgrounds, and so forth. These considerations are relevant for related research goals, for example, an automatic speech recognition system needs to be trained on different articulation behaviours and voice qualities (which depend crucially on factors like sex and age). The same applies to artificial corpora of experimentally elicited texts: even where participants produce texts narrating the exact same content under the same experimental conditions, as with the Pear Film experiment, it is vital for the corpus to cover speakers with different demographic features, as the corpora are meant to represent the behavioural reaction to the stimulus characteristic of the language community as a whole.

For special corpora, it is more of an issue to determine clearly the exact limitations of the intended population of texts. This is relatively straightforward in the case of superspecialised ATC corpora: they include only those texts produced over the specific pilot-controller channel over shifting radio frequencies, which are in fact explicitly announced by controllers and confirmed by pilots. In other contexts, the situation is more intricate: an example is the work of Douglas Biber and colleagues who famously investigated English language use in US universities, their 'university language' . They describe the scope of the language use under investigation as "[…] the range of spoken and written registers that students encounter in U.S. universities and of the major academic disciplines […]"

Other parameters

There are a range of further parameters other than the situational features of the texts included that define different types of corpora. These aspects are related more closely to aspects of research design and interests as well as corpus creation and curation.

Static versus dynamic corpora

Static corpora are those that are finished when published, with content not altered thereafter; they remain exactly in the state in which they were created. Dynamic corpora grow over time as more and more texts are included (monitor corpora are a type of dynamic corpus). Both general and special corpora can be either static

Corpus composition and corpus types

or dynamic. Big reference corpora are typically monitor corpora. Since they attempt to achieve maximal representativeness for a language, they add new texts being produced with the flow of time, such as COCA mentioned above. Examples of static corpora are the corpora of the Brown family of corpora (cf.

3.3.1.3.2 Synchronic, diachronic, and historical corpora Synchronic corpora contain texts from a 'single point in time' , whereas diachronic corpora include texts recorded over a longer period of time. As a matter of convention, we would classify as diachronic only corpora that cover at least multiple decades of language use. The Brown family corpora would all be synchronic when considered individually. The Contemporary Historical Corpus of American English (COHA) is an example of a diachronic corpus. A crucial characteristic of a diachronic corpus is that the texts it contains are comparable across periods. This is because the purpose of a diachronic corpus is to enable comparison of language use across time spans, and that would hardly be possible if the texts included in each temporal section were vastly different, for example, personal diaries in some time spans and newspaper articles in others. The Brown/Frown combination, as well as their LOB/FLOB counterpart, could be considered diachronic corpora: here we have exactly the same selection of text varieties across two periods of time

Languages and/or dialects covered

Finally, we can classify corpora in terms of what languages they represent the usage of. For instance, we could in principle distinguish between corpora of European and African languages etc. (global language areas), or between those of Indo-European and Tibeto-Burman languages etc. (language families), and the like. While this is a fairly trivial reflection of groups of languages with little specific relevance for corpus linguistics, a grouping that is more important for our purposes is that between wellstudied and lesser-studied languages. This is relevant because it comes with various preconditions, and leads to different goals for corpus linguistic work. Well-studied languages often have already established large corpora, an established research tradition including descriptive and analytical work, etc. Lesser-studied languages typically lack these things, and here corpus linguistics often means to build corpora in the first instance, and these corpora will then often be relatively small, which then comes with various restrictions as to their usability in research. A further relevant distinction is that of minority language corpora, which relate to languages other than the official standard languages in big nation-states. A similar distinction is that between (standard) language and (regional or social) dialect corpora. Big reference corpora often target standard languages of big nation-states, but this need not be the case and there can also be big reference corpora of dialects or minority languages, for example, of Sorbian (a Slavic language of east Germany;

Monolingual versus multilingual corpora

Typically, reference corpora target a single language, often a standard and/or official language of a nation-state (German, Turkish, Russian, etc.). Such corpora are thus monolingual. There are also multilingual corpora that include texts from more than one language. An example we mentioned above was Multi-CAST

Research-oriented corpora

Some corpora are characterised by specific properties in terms of processing and corpus annotation (more in Chapter 7). One previously mentioned example is speech corpora, which are not only characterised by the content of their spoken texts -which is a more general characteristic of spoken corpora -but also by the

Corpus composition and corpus types

specific annotation for various phonetic properties. Typical morphosyntactic annotations are part-of-speech tagging (PoS tagging) which captures the word class and other morphological and syntactic properties of token wordforms in a corpus; such corpora can be loosely classified as tagged corpora. Corpora with so-called interlinear morphemic glossing which captures the meaning and/or grammatical functions of morpheme tokens in a corpus are sometimes labelled interlinearised corpora. Certain corpora containing specific syntactic annotations capturing either constituency or dependency relationships between wordforms and/or phrases are called treebanks. This annotation has sentence structures represented in a tree-like structure showing hierarchical dependencies, which is useful for testing assumptions of some theories of grammar.

Another type of corpus with special annotation is a speech or phonetic corpus.

One of the first speech corpora produced was the Texas Instruments/Massachusetts Institute of Technology (TIMIT)

A note on IPA versus ARPAbet and other phonetic alphabets

Phonetic corpus research necessitates that computers are involved in the production and reading of phones. However, the phonetic alphabet as represented by the International Phonetic Alphabet (IPA) is challenging for computers to read. It needs to be translated into another form, usually, one produced with ASCII characters. Vowels are usually represented by multiple letters, as are some consonants. Most of these phonetic alphabets also have characters to represent word and utterance boundaries. Speech corpora use various alphabets: ARPAbet (Advanced Research Projects Agency Alphabet) aka DARPA, MRPA (Machine Readable Phonemic Alphabet), TIMITBET (Texas Instruments/Massachusetts Institute of Technology Alphabet), SAMPA (Speech Assessment Methods Phonetic Alphabet), etc. Therefore, the metadata of a corpus will include a list of correspondences between the chosen phonetic alphabet and the IPA.

Categorisation of corpora in terms of specific annotations for certain linguistic information cross-cuts distinctions made above so that, for example, both mono-and multilingual corpora can be tagged or constitute treebanks. In practice, more specialised annotation is restricted to special corpora created for specific research purposes, but some larger corpora fall into these particular categories; for instance, COCA is a tagged corpus.

FURTHER READING

Biber (1993) is a paper discussing corpus composition and representativeness, and Gries and Berez (2017) a paper discussing various types of corpora and their properties. Biber and Conrad (

NOTES

1.

2. This figure represents the first 2,000 words in any given text, plus the number of words to complete a sentence started so that the text snippets included contain all complete sentences. 3.

LINGUISTIC STRUCTURES AND THEIR VARIANTS

Language use and contextualisation

Language use at all levels of representation is tremendously affected by a range of contextual features. As discussed in 2.2.4, corpora have external and internal features or contexts. Characteristics of the external (or situational) context are important for understanding variation. Language use can be conditioned by who is interacting to whom and when, how much interlocutors know about each other and what they are discussing, whether the language use is spontaneous speaking or signing or writing and what the genre, register and style of speaking, signing or writing are. Take for example written text found in a textbook and spoken conversational language use. We would expect to see different vocabulary choices, different average lengths of sentences, different kinds of structures and so on. Many conversations are full of spontaneous, unplanned language use, whereas the language in a textbook is produced more slowly, usually has gone through multiple drafts and is more carefully created. A conversation includes many situational clues to interpreting language such as eye gaze, gesture, facial expressions, tone of voice for spoken language, as well as means to establish common ground between conversational partners, all of which written text lacks, which should affect language choices. Academic writing also tends to be more formal than spontaneously produced spoken language (cf.

Much of the effect of context passes under our radar, especially when using natural language in spoken and signed contexts. We do not have to consciously think about the effect of context on meaning when it is congruent, but we notice when there is a mismatch. In corpora, we see trends of usage for particular contexts and when a usage has a mismatch for the context, there is often a reason why: emphasis, importance, a hesitation, a mistake, etc. However, until we actually look at multiple instances of usage in multiple contexts, it is difficult to know what is typical precisely because matches in context and meaning are processed unconsciously.

Choice of variants: possible and probable structures

When describing the characteristics of a language, a linguist might account for all of the possible ways of expressing a particular meaning, and a corpus linguist would also want to account for the probability of expressing a particular meaning, given the context. For example, when expressing a possessive relationship between two nouns in an NP, English has a genitive 's strategy (e.g. the woman's car) and a genitive prepositional of strategy (the car of the woman). For many English nouns, both strategies are possible, but the probable strategy will depend on the nouns involved, as well as other factors. A corpus study is an ideal way to find out which possessive

Exercise 4.1 Synonym context

Use the COCA, COHA, and Coronavirus corpora to compare ill and sick and their collocates. Are the collocates of the words similar or not? Are there any surprising collocates or usages? Under what conditions do these occur?

strategy is more probable and in what contexts, that is, what kinds of factors affect which strategy will be used. This particular problem of variability is well-studied, and we know that the genitive 's strategy is much more likely to be used:

• when the possessor is a person (i.e. it is animate): the boy's hat rather than the hat of the boy

• when the possessor has already been introduced or is known (also called 'given'): the university's student centre rather than the student centre of the university

• in speaking rather than writing, or in journalistic genres

The of strategy is much more likely to be used:

• when the possessor ends in a sibilant sound (fish, jazz, bass, garage): parts of a fish rather than a fish's parts

• when the possessor is expressed in a long or multi-word NP: the last paragraph of the first chapter rather than the first chapter's last paragraph

There are also many other context factors that each matter a little (cf.

Linguistic variation is the social fact that there are multiple means to express or encode content. Understanding the choice between possible variants is one of the main foci of corpus linguistics, especially variationist corpus linguistics. We do not expect people to behave completely randomly when they use language. Although people have many choices from moment to moment in their lives, they tend to follow routines or patterns. The choices they make are constrained by the people around them: to avoid offending them or to make them laugh, etc. When looking at linguistic behaviour, we know that each language has rules of use; some things are just wrong: *womans are going, *women is going, *women are go. But linguistic behaviour is also constrained beyond linguistic rules. When we research variant choice, we research the kinds of things that matter to people when making a choice (although often this choice is spontaneous and unconsidered). Regarding of and 's genitive alternation, the animacy of a possessor is the most important factor (?the hat of the boy), but phonological well-formedness (?the fish's parts) and information density (?the first chapter's last paragraph) also matter. These factors relate back to comprehension and processing: English users are much more likely to choose a genitive form that is easy to process and easy for an interlocutor to understand.

Corpus research of variant alternation is done at all linguistic levels: semantics, phonetics, phonology, morphology, syntax, and discourse. In the rest of Chapter 4, we will describe example studies from all of these levels as well as corpus studies of sign and gesture. Some kinds of research questions are easy to explore with a basic corpus. However, many kinds of questions require special kinds of corpora that have additional information or annotation, an issue we discuss further in Chapter 7.

Levels of linguistic representation

4.2 STRUCTURAL LEVELS OF VARIATION

Lexical semantics

Corpus linguistics started as an enterprise interested in words, their frequency, and their contexts. Even if a corpus is basic, simply a string of wordforms with no other annotation, it is easy to search for individual words since usually words are a kind of string of characters, separated by white space. There are many corpus studies that compare lexemes, especially near-synonyms and purported synonyms. Corpus searches of words that have similar meanings can show that synonyms can occur in quite different contexts. The differing contexts, then, should be taken into account as part of the definitions of words, as mentioned in Section 4.1.1 above.

Let's take a closer look at two studies that carefully examine the contexts of similar words. First, we will look at

"You shall know a word by the company it keeps"

The title of this section is an influential and oft-quoted insight from

• morphological features (tense, aspect, and voice)

• syntactic properties (transitivity, speech act [i.e., statement, question or command], main clause or subordinate clause)

• semantic characteristics of the subjects, objects, and complements co-occurring with run (i.e. human, animate, concrete, mass nouns, etc.)

• collocates in the same clause (collocates are the words before and after a token of interest, see more in Chapter 2)

• a paraphrase of the meaning of the word This coding results in data that can be investigated more deeply, both quantitatively and qualitatively. Gries has 252 distinct behavioural profiles across his 851 tokens. The wide diversity comes from the inclusion of different collocates. It should be immediately striking that there are many different behavioural profiles but also that there are not 851 distinct profiles, meaning there is consistency across the corpus in terms of not only the syntactic uses of run, but also the words it co-occurs with. In this set of data, Gries identified 56 senses of run. He then uses the behaviour profiles to address questions of similarity between these senses.

First, Gries finds that run with a meaning of 'fast pedestrian motion' is the most frequent sense, as well as the sense that occurs with the most different behaviour profiles. This supports its central meaning because it means that the meaning occurs across many different uses, rather than being restricted to a particular kind of use.

Gries also computes pairwise correlations (a statistic that can be used to assess strength of association) between each of the different senses using a matrix of the behaviour profiles associated with each sense. From this, he is able to identify the most similar and dissimilar senses. Most similar are the senses of 'fast pedestrian motion' and 'to escape' . Most dissimilar are the senses 'to overflow' (4.10) and 'to see

(4.10) their cups were already running over without us (4.11) He ran his eye along the roof copings

The pairwise correlations take all the behaviour profiles of two particular senses and compare them to each other, a pair at a time. In addition to many separate pairwise

Levels of linguistic representation

comparisons, similarity can also be quantitatively assessed using cluster algorithms

Finally, Gries flips the problem from polysemy to word sense disambiguation (an issue for computational linguistics and natural language processing). If one has a behavioural profile for a token, can one successfully predict which sense is being used? And are some elements of the behavioural profile more useful for sense disambiguation than others? For instance, Gries can successfully predict all cases of the sense 'fast pedestrian motion' by taking into account the combination of past tense, intransitive, followed by a to prepositional phrase, with a human subject noun. More data would be needed to fully specify which elements of the behaviour profile are most significant for word sense disambiguation, but in principle this would be possible, meaning that examinations of word profiles can be useful for both issues of semantic theory and practical applications.

Many corpus studies take a similar approach in looking at words or domains in the lexicon and comparing uses. For instance, Glynn (2014) expands upon

Synonyms and meaning clusters

We now turn from polysemy of one word to a look at near-synonyms (multiple lexemes with similar meanings).

For instance, she coded whether the cause was internal or external:

(4.12) Krawczak's results show that there is very little difference between American and British uses of ashamed, embarrassed, and humiliated. However, there are clear differences between the near-synonyms.

• Ashamed in the correspondence analysis plot is near both the atemporal and past levels of the cause time factor, and near the internal level of the cause type.

It is correlated with the violation of the social norm of emotional reaction, dubious social status, loss of social status because of financial reasons, bodily shame and failures.

• Embarrassed is also near the internal level of the cause type factor and atemporal level of the cause time factor but is closest to the present level of the cause time factor. It is correlated with causes such as the violation of social norms of politeness, insecurity, inadequacy, and violation of social norms of decency.

• Humiliated is nearer to the external level of the cause type factor and the past level of the cause time factor. It is closest to causes of mistreatment, social rejection, and failure. • The source of shame of loss of social status is in the middle of the correspondence analysis plot, showing that this level of the cause factor is shared by all the shame words.

The decision of the classification for each of these tokens from the corpora does require some subjective decision-making from the researcher, as is the case in many corpus studies. However, the overall results are robust and match well to the

Phonetics

Phonetic research is concerned with the physical, acoustic measurements of spoken language. Phonetic corpus research is concerned with the acoustic measurements of spoken language in context. Speech corpora are based on spoken language but necessitate detailed annotation including not only written transcription but transcription in phonetic alphabets and careful connections with the time course of speaking. This is usually made up of a text grid that divides the speech signal into characters that represent phones, which can be viewed or computer-processed with accompanying audio files. With this extra annotation, researchers can look at issues like word length, vowel acoustics, various phonetic realisations of words, phone realisation, speech rate, and so on. These kinds of corpora take a lot of work to produce, much more so if (portions of) the corpus have been manually inspected, checked for accuracy, and hand-corrected. Because of this, speech corpora tend to be smaller than corpora compiled for other kinds of research.

One large research area has been in the domain of speech reduction: when and how is speech reduced? And how do listeners understand vastly reduced speech segments? One well-known English speech corpus used for this line of research is the Buckeye corpus

Levels of linguistic representation

COCA, she shows that in addition to phonetic factors, duration is affected by how probable the word is in a specific context (quantified with conditional probability measures) and how probable the word normally is (quantified with informativity measures), given the particular construction it is a part of (modal, perfect, possessive). She finds that the construction with the highest average probability, the perfect construction, has the shortest durations of inflections of have and that durations are shorter when the following transitional probability is high, meaning both specific and average probabilities affect word durations (more discussion of frequency and probability measures can be found in Chapter 5). The results of this study indicate that rule-based processes affect not only word realisation but statistical tendencies built up from experience of language use affect word realisations as well. The results contribute to a body of work on the effects of frequency, probability, and informativity on word durations and word realisations. The results from these studies are used to inform theories of language organisation and language processing.

Another important research area using speech corpora has been automatic speech recognition (ASR), speech-to-text (STT), and text-to-speech (TTS) applications, that is, how can machines be trained to account for the variable productions by human speakers? Sociolinguistics is another area where researchers use speech corpora, because pronunciation, especially of vowels, often differs due to social dimensions in a language population.

Most phonetic corpora are of Standard American English speech varieties. For researchers doing work on other varieties of American English, other varieties of English, or almost any other language besides English, the recourse is to produce their own speech corpora. This has become a more viable option recently with various forced aligners available. Forced aligners match (or align) transcriptions to audio files at the phone level. They are called forced aligners because they 'force' a match between transcription and sound; they will not correct a transcription. The forced aligner will use an acoustic model of a language and a dictionary of words in a phonetic alphabet to match phone to the acoustic signal. Some forced aligners use acoustic models from English or other majority languages and some forced aligners create an acoustic

Exercise 4.2 Switchboard investigation

Take a look at the Switchboard-NXT

model of a language as part of the alignment process, such as the Montreal Forced Aligner

Phonology

Many phonological research questions will use phonemically transcribed corpora. Rather than basing analyses on acoustic measurements, like with speech corpora research, the analyses are based on the phonemic transcriptions. For some languages, the written words are similar enough to their phonemic representations that text corpora can be used to examine questions relating to phonology. Below we give an example of corpus research on Tagalog phonology

Tagalog is the official language of the Philippines and has over 70 million native and second-language speakers. It is an Austronesian language with infixes, such as -umthat marks realis aspect and infinitives (4.20).

(4.20) bago 'new' > bumago 'to change'

Whether or not speakers have the perceptual bias described by

Morphology

We now move away from sound to meaningful parts of words: morphemes. Let's look at two studies that use corpora to answer questions about morphology relating to the productivity of morphemes. Productive morphemes can be used with many words such as the -ness suffix for nouns deriving from adjectives in English (happiness, sadness, completeness). They can also be used to form novel words: 'she was overcome with upsetness' would probably be understandable to a reader. Non-productive morphemes can only be used with a smaller frozen subset of words, such as the -th suffix for nouns deriving from adjectives (warmth, length [from long + -th]). A novel word such as calmth would probably be confusing for most readers and older words such as dampth have fallen out of usage and have been replaced with dampness underscoring the productivity of -ness. We will look at two studies on morphological productivity:

Morphological productivity has impacts on both linguistic theory and computational tools. Some theories of grammar, such as early generative grammar, consider the lexicon to be split into two kinds of word creation. There are productive rules which are dynamic. Only the rule needs to be learned, not the words it generates. There are also unproductive rules that account for existing structure but are not used to create new words. The words generated through these rules need to be learned (cf.

in this emerging new theory, morphological productivity can be understood as resulting from a great many factors such as the individual language user's experience with the words of her language, her phenomenal memory capacities, her conversational skills, her command of the stylistic registers available in Levels of linguistic representation her language community, her knowledge of other languages, her communicative needs, her personal language habits and those of the people with which she interacts.

Corpora can be used to assess these factors, and all of these conditioning factors have been identified in corpora. Corpora show the probabilistic nature of morphological productivity, among other aspects of language use, and challenge traditional theories of the conception of language. Computational tools also need to take morphological productivity into account, as new words can always be created through productive word formation. If you wanted to create a computational tool to process language data, it needs to be able to process new data it has not encountered before. No training data or lexicon will be sufficient because people can always create new words, so productivity rules need to be built-in.

How can productivity be assessed? Morpheme categories with high membership (realised productivity), growing membership (expanding productivity), and potential for membership growth (potential productivity) are considered productive using different kinds of measures of productivity

Hapax legomena can be taken as a proxy for neologisms, although many will be old words that are simply too infrequent to occur in a corpus more than once. In any case, if a morpheme occurs often in the hapax legomena words, it is a hint that it can be used for new words. Potential productivity is important because if a morpheme already occurs in most of the words it can occur in, then it has little chance to grow the lexicon or be used on new words. Dutch has two suffixes used for creating agent nouns from verbs (as in English give > giver, bake > baker):

-er 'unmarked agentivising' (Dutch geef > geefer 'giver')

-ster 'female agentivising' (Dutch geef > geefster 'female giver').

There are many existing words in Dutch that already have the unmarked agentive suffix -er, and far fewer that have the female agentive suffix -ster. That means many new agent nouns could be created from verbs with the latter, but not the former. This

Hapax legomena are words that only occur once in a corpus. Hapax is a Greek word for once and legomena is the plural of legomenon meaning said, the participle of legein 'say' .

difference in expanding productivity of -er and -ster is due to a reluctance of Dutch speakers to explicitly mark gender for these kinds of nouns

Part of morpheme productivity is people's ability to segment the morphemes in a word, so that they can identify and then use a morpheme in additional words. Knowing sets of words with and without a particular morpheme helps this segmentability, as in happy and happiness. Knowing both of these forms help segment -ness from happiness. Morphemes are also more segmentable when they are in multimorphemic words that are more infrequent than their uninflected counterpart

Productive morphemes can enter a language from another language through borrowing. A commonly assumed pathway is an indirect strategy, where people borrow many words from another language with a particular morpheme and then use analogy to apply that morpheme to native words. For example, many French words with the adjective from verb deriving suffix -able were borrowed into English (profitable, honourable, deceivable) and -able was later used to derive adjectives from native verbs (knowable, speakable, workable).

Exercise 4.3 English past tense morphemes and productivity

The irregular past inflexion -t is associated with words of a certain phonological shape: sleep, weep, keep. But other and newer words of the shape (i.e. bleep) use the regular past tense inflexion -ed. Make a list of all the [eep] verbs you can think of and compare their past tense forms. Which past tense morpheme is likely to have higher realised, expanding, and potential productivity if you measured it in a corpus and why?

Levels of linguistic representation

An example of what is likely to be a direct borrowing comes from Zamboangueño Chavacano, a Spanish based creole from the Philippines with the prefix ika-. This language has cardinal numbers from Spanish (uno, dos, tres 'one, two, three'), but ordinal numbers with the prefix ika-(ika-uno, ika-dos, ika-tres 'first, second, third'). The prefix ika-is from two Visayan (larger family Austronesian) languages with which there was high contact during the 1800s. There are no complex loan words with ika-, only the hybrid forms, and there is no evidence that there were ever cardinal numbers from the Visayan languages in Zamboangueño Chavacano. Therefore, it is likely that ika-was borrowed directly, rather than as part of a set of complex words that all had replacements with hybrid forms.

A likely example of direct borrowing, but at a lower probability, comes from the noun classifier -ga used for 'plank shape' which was borrowed into Resígaro (Arawakan language of the Amazon) from Bora (Boran language of the Amazon). Many Resígaro speakers know Bora, despite the languages being unrelated. A Resígaro corpus

A likely example of a primarily indirect borrowing strategy comes from an agentivising noun deriving suffix -ero~-era borrowed into Northern Chinchay Quechua (Quechuan, Ecuador) from Spanish. An existing Quechuan corpus (cf. Bakker and Hekking 2012) of 80,000 words was used to identify 54 wordforms with -ero~-era, 47 of which were complex loan words with this suffix. This is a high ratio (47:54) of borrowed words with the morpheme to the overall number. Additionally, about 70% of the complex loan words have a simplex loan word correspondence, and 40% of these are infrequent relative to the simplex correspondence. Taken together, these distributions present a much stronger likelihood of indirect borrowing, even though most Northern Chinchay Quechua speakers probably also had knowledge of Spanish at the time of borrowing.

These two studies use a series of corpora to show and assess gradient phenomena in morphology: a cline of morphological productivity and a cline of directness in morphological borrowing. The clines are evidence against previous theories of distinctions in grammar or assumptions that a process does not exist. Corpora, then, are used to demonstrate that it is imperative to assess what actually happens in language use when constructing theories of language.

Syntax

In the sections above, we have described corpus studies that investigate phenomena at the word level or smaller. Searching for such phenomena requires some string information and some annotation. However, linguists also use corpora to look for phenomena at the constituent level in syntax. This can be difficult because string information is less useful at this level to identify tokens. Take for instance a category like subject or object. These could be any number of different nouns, NPs, or even zeros like in Spanish ø tengo la mejor madre del mundo 'I have the best mother in the world' .

One way to deal with this is special kinds of syntactic annotation (cf.

Afterwards, examples are coded for various factors. This is often hand-coding in combination with some automatisation. Below we describe a classic study that uses corpora to investigate the factors that influence the predictability of syntactic constituents: the order of recipients and themes in English

In

The authors test variables that are likely to affect the dative structures including:

• Accessibility of recipient versus theme (i.e. which is more accessible), often determined by seeing which is definite or expressed with a pronoun

• The length of the recipient versus theme

• The animacy of the recipient Interestingly, many of these variables pattern together (are correlated). For instance, pronouns are short, but they are also accessible, and often refer to animate referents. So, assessment of the contribution of these factors needs an appropriate kind of analysis.

The question of dative alternation and other structural alternations has been taken up in other varieties of English such as South Asian English varieties

Discourse

Discourse is the level of language use beyond the sentence, as there exist structures that only enfold over a text. Studies of discourse most often are of spontaneous spoken language, often conversations. As with many of the levels of usage we have described here, certain annotations help corpus linguists look for the particular kinds of phenomena relevant to their studies of discourse. For example, the Switchboard corpus has annotations of dialogue acts

There has been quite a bit of linguistic research on discourse in healthcare settings, the vast majority of it on English. This is a kind of interaction many people engage in and it can be high stakes, making it important that people understand each other. Much of this work falls under applied linguistics because it is an area where awareness of interactions can help address common problems that arise.

Levels of linguistic representation

data, particularly on which stages of the interaction featured which of these devices and to what purpose.

The health professionals were identified as using politeness strategies and language convergence. Additionally, language was used to establish relationships as part of health assessment, because this presumably leads to better outcomes and callers are more likely to give information and take healthcare advice. Researchers identified several recurrent themes:

• positive affirmation (OK, right) was used to establish that the health professional is accepting the patient's situation and concerns.

• modal verbs (may, can) were used for politeness and to avoid imposition on patients (cool baths may help itching)

• if was used both as part of the diagnostic dialogue (if you push on them…do they fade and come back again) but also as part of hypotheticals (if you are in pain…) which is part of politeness.

• or anything? was used as part of politeness (are you coughing or anything?) but is also helpful in diagnostics as it encourages patients to disambiguate or provide more information and may result in better information than just going through a checklist of symptoms.

• As part advising patients, the researchers identified strategies such as credentialing to give weight to advice but also to avoid creating an asymmetry between the caller and the health professional, allowing the caller to preserve face. So, rather than a directive coming from the health professional, they would instead give advice from manuals and books.

• Closing interactions, where the health professional has an active goal to assess if advice will be taken by the caller or not, feature multiple questions about next steps and reiterations and summaries from the conversation before allowing the call to end.

This study is a good example of how interesting and important information can quickly and easily be gleaned from even a small corpus. Keywords, a simple tool, were used as the initial assessment to build upon further. The corpus approach also helped see multiple real interactions (at least from the side of the health professionals) to assess the patterns in this institutional setting.

Exercise 4.4 Applied corpus linguistics

What benefits could a healthcare corpus have for patients and health practitioners? What about situations where people who use different languages interact? What would the corpus need to look like?

Let's turn now to a quantitative study within discourse analysis.

Factors relating to cognitive processing:

Because people in a conversation must listen and understand (mostly anyway) each other and respond quickly and appropriately, one could hypothesise that the gap between conversational turns could be shorter when everything is easier to understand (low-processing demands) and longer when there are difficult or complex things to understand (high-processing demands).

• T1/T2 turn duration -a longer turn from the speaker (T1) might be more complex, requiring more processing time before the next speaker can start, and a longer turn from the listener (T2) might require more planning, so turn duration is expected to increase FTO times.

• T1 speech rate -a faster rate of speech from T1 may lead to a shorter duration, meaning T1 turn duration and speech rate necessarily interact. Fast speech may Think about other fields where language use really makes a difference to people's lives like legal proceedings or air transportation. These are fields where corpora are used. Do a basic literature search to find how research questions have been mapped to corpora in one of these fields.

Levels of linguistic representation lead to a higher amount of speech for the interlocutor to understand in a shorter period of time, leading to a longer FTO for planning purposes. Finally, T1 speech rate might also interact with the kind of turn being taken, which may also have an effect on FTO times.

• T1 tree height -this variable was used as a proxy for complexity, by counting the number of nodes between the tree root and tip based on syntactic tree annotations and taking the largest height for the turn. More complex speech may be more difficult to process, leading to longer FTO times for planning.

• Frequency, surprisal, information density -frequency information was calculated based on Switchboard word frequency. The Surprisal measure was taken from

Factors relating to conversational sequential organisation:

A history of work on conversational analysis has shown that people are sensitive to social norms in conversations and tend to avoid both long gaps and interrupting each other. Additionally, people tend to follow patterns in conversation (as we saw above in

• T2/T1 initiating -this variable codes whether or not the turn was a question (or another kind of conversational pair beginning) that initiated an expected action from the interlocutor.

• T1/T2 responding -this variable codes whether or not the turn was the responding pair to a question or initiating a turn, showing the other side of the initiating variable. This obviously interacts with the previous variable as questions with preferred answers will likely have a short FTO because it is part of a routinised kind of sequence, but longer FTOs are expected if a listener has to work to form an appropriate response. Long FTOs may also signal that a listener does not want to comply with a request.

All of the factors that the authors hypothesise to have an effect are complicated, are expected to interact, and are expected to have non-linear effects on FTO duration.

Because of this, the authors choose to use a random forest analysis, a kind of quantitative analysis more robust to non-linear effects and interactions than linear regression (cf. 8.4.2). The authors ran analyses on just under 20,000 FTOs from the Switchboard corpus and then ranked the importance of the variables in relation to each other. The authors found that the most important variables were whether the T1 includes a responding action, T1 duration, T2 duration, and T1 speech rate. A random forest analysis provides a ranking of variables but follow-up tests need to be done to understand the directionality of effects. The authors found that FTOs were shorter when the T1 includes a responding action. This is reflecting that many T1 responses are followed also by T2 responses as both speakers show their agreement or assessment in turn (A: "that was funny, right?", B [T1]: "Oh yeah", A [T2]: "Yep"). The fastest mean FTOs were between T1 initiating and T2 responding actions, and the longest mean FTOs were between two initiating turns, showing perhaps the need for clarification.

Moving on to durations and rates, the authors found that both long T1/T2 and very short T1/T2 durations were associated with longer FTOs, and the authors take that to mean that turn duration is a covert proxy for the kind of turn with very short turns being backchannels and agreements and longer turns being statements, opinions, and questions. Faster T1 is also associated with longer FTOs, possibly reflecting a need for longer processing. Female participants were also found to have faster FTOs. Females overall had slower speech rates, so this factor may be interacting with T1 speech rate. Perhaps surprisingly, variables like frequency and surprisal were not ranked very highly in the random forest ranking, showing that these effects are small outside of a laboratory setting.

Overall the authors were able to take advantage of a well-annotated corpus and apply a fitting quantitative analysis to show that factors from both processing and conversational norms interact and have an effect on conversational interactions. Because of the many interactions and the many proxy effects, the study also provides the basis for future research looking at some categories in more detail, which may require additional annotation. Studies relating to conversational analysis are often nowhere near as large as this one, so we see the opportunity in utilising large corpus data to help bolster previous analyses and provide the springboard for future analyses.

SIGN AND GESTURE

Corpus building for the documentation and study of signed languages and gestures requires some special considerations. The primary data for such corpora are necessarily video-based. To make the data machine-readable, some kind of transcript is required. What that transcript might be can be very complicated, as the data must necessarily be annotated as multimodal: speech and gesture, manual signs, mouthing and gesture, right hand and left hand, manual signs and facial gestures and other combinations, and all of the above. A gesture-unit or a 'G-unit'

Levels of linguistic representation

(Australian signed language) corpus

Understanding manual gestures (as opposed to bodily gestures or facial gestures such as eyebrow-raising, which have received less attention) is a large area of study.

Manual gestures are present in both signed

Let's focus on a signed language study and take a closer look at

Exercise 4.5 Using a sign language corpus

Take a look at the Auslan corpus or British Sign Language Corpus

that influence the form that they take. The corpus consists of 20 retellings each of the fable The boy who cried wolf and the picture book Frog where are you?

The authors point out that the act of reference is about directing attention to something to establish it as a concept for further commentary and then using our semiotic repertoire to characterise it in some way. The repertoire of devices available will differ based on the mode of communication. For instance, pointing is available (to both signers and speakers) in face-to-face communication, but not in writing. In many studies of reference in spoken languages, researchers (including your textbook authors) have focused on distinctions between lexical NPs, pronouns, and zeros for referent expression. They tend to look at predictors such as salience, accessibility, referential distance, and animacy to explain the choice of one type of reference over another. In signed language studies of reference, researchers have looked at different strategies such as pronominal pointing, indicating signs, depicting signs, NPs, and enactment. They also focus on issues such as the grammatical role and cognitive accessibility to investigate which strategies are used.

The idea of cognitive accessibility is common to both signed and spoken language studies, that is, will the observer know what the language user is referring to when they use the referring expression? A zero or pronominal point is fine if the language user is referring to an established referent or something salient such as the signer/speaker or observer, but will be difficult to map onto a referent brought up for the first time. A 'heavier' and more informative referring expression such as a lexical NP (that big dog) can be used for something less salient.

The researchers used their corpora to quantitatively examine 4,699 referring expressions and investigated,

• the activation status of those referents: introduced, maintained, or reintroduced

• animacy of the referent: humans, animals, inanimate objects

• the form of the referring expression including, ¡¡ lexical manual sign ¡¡ mouthing (the mouth movements of speaking the referent in its spoken form)

¡¡ fingerspelling (signs for each letter of the written English form of the referent)

¡¡ partly lexical sign (i.e. maybe has a handshape or orientation like the fully lexical sign but another specification comes from how this form maps onto the dynamic signing space)

¡¡ pointing sign ¡¡ enactment (some bodily action that is conducted as if the signer were the referent)

¡¡ invisible surrogates (where a combination of actions create the impression of an entity in the signing space and the signer behaves as if that entity were present)

• phonological heaviness: operationalised as the number of strategies used in the composite utterance of a referring expression as the forms (above) can co-occur simultaneously, sequentially, or both

Composite utterances

In the researchers' study, they found phonological heaviness to range from 1 to 12, with very heavy references (8+ strategies) often a sign of disfluency or need for clarification. To assess phonological heaviness, the researchers modelled their data with mixed-effects linear regression. Their model includes a random effect for the study participants, as the researchers are looking for effects that hold beyond the variability or style contributions from particular signers (see Chapter 8 for more information on random/fixed effects and linear regression). Their model shows that referents are heaviest when first mentioned, less heavy when reintroduced and least heavy when maintained. This fits in with theoretical research that referents that are very salient (easily understood in the conversation) are expressed more minimally.

Their model also shows that human referents are expressed with fewer strategies, as human referents also tend to be very salient. Inanimate referents were more phonologically heavy and animal referents were the most phonologically heavy in their data, and this was particularly seen in the cases where animal referents were reintroduced. The kind of narrative did not show a significant contribution to phonological heaviness, showing that these patterns are robust across two (somewhat) different kinds of data.

Hodge et al. 's (2019) regression analysis of phonological

heaviness is what we classify as confirmatory statistics. The researchers had a clear hypothesis based on the theoretical literature and then tested if the hypothesis was confirmed for their data. Importantly, they add value to their overall look at reference in Auslan by pairing this with an exploratory analysis which is intended to give insight as to the kinds of strategies (lexical manual signs, English fingerspelling, invisible surrogates, etc.) used in particular circumstances. For that analysis they use principle components analysis and we recommend going to the original article for more details about how that was done and what the specific results show. Overall, their takeaway was that conventionalised signs such as lexical manual signs and English fingerspelling were used most often to introduce new referents. Next, English mouthing (easily combinable with manual signs) was used across many different circumstances. Also, humans and animals were often maintained or reintroduced with visible and invisible surrogates, meaning that after referents are introduced in a more conventional manner, less conventional kinds of strategies can be used to keep these referents in the discourse.

Finally, the researchers also discuss the importance of non-conventionalised strategies in reference, particularly invisible surrogates. Remember, these are references implicitly built up from other strategies that let the interlocutor know that something exists in the signing space.

In sum, Hodge et al. (

CONCLUSION

The aim of this chapter was to provide you with an overview of the many kinds of linguistics one can do with corpora. Corpora provide an empirical basis to explore and test hypotheses, but the kinds of questions one can ask are limitless.

FURTHER READING

Newman et al. (

NOTES

1. tho and they' d might have struck you here as strange for academic writing.

2. Although Zuraw's corpus is unpublished, there are now large, web-corpora of Tagalog available including a TenTen family corpus (

GETTING STARTED

The basic steps for corpus linguistics:

(1) put together corpus data

(2) get corpus data loaded into a program for analysis

(3) label/annotate your corpus data (4) search your data

(5) analyse frequencies and distributions of your data While it would be ideal if we could give readers one straightforward, foolproof way of doing this, we cannot. Corpus data comes in many different formats, and there are easily hundreds of methods, tools, and strategies that are possible. What we might recommend today (a certain corpus tool, a certain programming package, or module) might not be available three years from now, or might have advanced so much that our information is out-of-date. This is both the exciting and frustrating part of corpus linguistics. There is always something new being developed that can improve our work, but it means specific previous strategies eventually become obsolete. This chapter will guide you through common strategies for searching and analysing data.

CORPUS QUERIES

Frequency lies at the heart of corpus linguistics. The main reason to use a corpus is to find real examples of language use in context and count how frequent they are.

The primary aim of corpus linguistics is to understand linguistic patterns and explore how and why they occur. The first step is to obtain frequencies, and the more complicated subsequent steps of understanding what the frequency means is the job of the researcher.

To obtain frequencies or examples, one must engage with the corpus or 'query' it. Most corpus linguists will not read through an entire corpus. Usually, it is too large for us to engage with systematically. Instead, we will pull out relevant information and put it into a format that makes it possible to study it further. Some kind of software on a computer or interface on the web will be used to do this. Most text editors and word processing programs have a search function that will allow a user to find segments, words, or phrases. Specialised corpus software and web interfaces also have means of finding examples and often have ways to save those examples to a new document or spreadsheet for further analysis. Many of these programs will provide basic descriptive statistics of the data, such as number of occurrences, bigram frequency (cf. 5.6), mutual information (MI) (cf. 5.7), etc. It is worthwhile to explore various tools whether you are new to corpus linguistics or are a seasoned veteran.

Example software and what they can do

AntConc and additional Ant-programs -These are text processing programs specialised for corpus linguistics, providing concordances (examples in context) and frequency counts, as well as providing basic functions like breaking up texts into smaller parts and allowing for the export of search results to various formats.

ELAN -This software is intended for audio-visual data and allows a user to transcribe and annotate their data in a number of ways. Additionally, it has corpus search functions that can be used over one or more files to find example strings (words, phrases, parts of words, etc.) in context. It also provides n-gram statistics and can export data into a number of formats for further processing and analysis. Examples in Chapters 6, 7, 10.

Example web interfaces and what they can do

These sites have multiple corpora plus an interface to access the corpora for searches, word frequency counts, bigram counts, and more. Each of them has some corpora that have been annotated with additional information like parts of speech (cf. 7.2.3) and lexeme (cf. 2.2.2). In some cases, you can also download the corpus data for your own processing with additional software.

English Corpora -a site with several very large corpora of English, including a 14-billion-word web corpus, as well as additional corpora of specific English genres and other languages. The corpora can be purchased for a fee, or used online through a simple user interface.

Programming languages such as R (R Core -packages in the 'tidyverse' are also used for efficient data processing, particularly with specialised syntax using pipelines.

ggplot2

There are many additional specialised add-on packages.

Quanteda

stringi

Sketch Engine -a site with several large corpora from various languages. Sketch Engine (and NoSketch Engine) also allow users to upload their own corpora for 'word sketches' (detailed collocate information).

Corpus queries

Corpus queries 71

FREQUENCY LISTS

Frequency is the most basic corpus measurement. In many early corpus linguistics works, you will find frequency tables as a primary account of data. Frequency tables display the amounts of something occurring (a word, a morpheme, a word category, a construction, etc.) usually in one condition or another. Frequency tables are helpful, as they are fairly intuitive to understand. In one condition there is more of something, in another condition, there is less and then we can build hypotheses and extrapolate theories from that. In corpus linguistics, we worry about both type frequency and token frequency (cf. 2.2.3) that can tell us different things about our corpora. As a review:

Token frequency -a count of all instances of something in a corpus.

Type frequency -a count of all the unique types there are of something in a corpus So, in a corpus of a million words, there will be a million word tokens. Of these, there may only be 200,000 word types.

Further, we can make a distinction between wordforms and lexemes, which, depending on the research question can be an important distinction (cf. 2.2.2). Take a look at this sentence: He went away and then he was gone altogether. This sentence has nine separate word tokens and eight word types, as he is repeated. Capitalised He and lowercase he should be considered to be the same word type. This sentence has seven lexemes because went and gone are both inflexions or wordforms of the lexeme GO.

Example Python modules and what they can do

The Natural Language Toolkit (NLTK)

Below is a table of the top ten most frequent lexemes from the one billion word COCA

Notice that for both COCA and iWeb the most frequent words are function words. This is the case for any corpus. Because of this, sometimes stop lists are compiled of function words and any word in the stop list is not reported in frequency counts. The idea is that the less meaningful words are excluded, giving someone more insight into the corpus. Exercise 5.1 Stop words

(1) Compile an example stop list for English and one other language by listing out all the stop words you can think of.

(2) Look on the internet if you can find stop lists for your additional language. You will certainly find stop lists for English.

(3) How did your stop lists compare to the ones you found online?

(4) How different are the stop lists for English and the other language? What are the differences and why do you think that is the case?

(5) Think of a research question where it would be important to not exclude stop words.

Corpus queries

Keywords

Keywords are used in some kinds of corpus linguistics to show differences between corpora or between sub-parts of a corpus. Keywords are calculated by assessing all the word frequencies in each of the two (sub)corpora and doing either chi-squared tests or log-likelihood measurements to assess what words are statistically more frequent in one (sub)corpus than in another. These are then the keywords. Usually, these kinds of keywords are lexical items (nouns, adjectives, verbs) that give us an idea of the topics in the corpus. However, the keywords could be, say, pronouns if one corpus is conversational and another is from monologic or written sources.

GRAPHICAL FREQUENCY DESCRIPTIONS

Frequency plots

Corpus word frequencies are often plotted on graphs, and as seen below in Figure

Dispersion plots

Corpora are not random bags of words. They consist more or less coherent texts, and these texts consist of ordered strings of words (cf. 2.2.1). There are multiple ways to measure the dispersion of a word or collocate. One option is a dispersion plot. This is a visualisation of where a word or collocate occurs in a corpus. Two words with the same frequency might occur often only in a handful of texts, or more consistently across the entire corpus. These words would have equal frequency, but different dispersions. Within a text, some words may be restricted to particular sections, which is also useful to know. For instance, we might see the end most often at the end of a corpus of children's stories and rarely at the beginning or in the middle of the texts in that corpus. Take a look at Gries (2010b) for more on measures of dispersion.

ZIPFIAN DISTRIBUTION

An early trailblazer of corpus linguistics is George Kingsley Zipf. One of the first things many corpus linguists plot is the frequency of the words in a given corpus to see which words are most and least frequent and to examine the frequency distribution. Often that distribution follows Zipf 's law.

Many distributions of corpus data follow the Zipfian distribution, where there is a 1-to-1 logarithmic (log) relationship between the rank and frequency of events (i.e. words). In Figure

Logarithmic transformation is converting a measure to a log scale. A log scale puts a wide range of values on a more compact scale, where the next value on the scale is a multiple of the previous value.

Log 10 scale increases: 0.1, 1, 10, 100, 1000, 10,000, 100,000 and so on Log 2 scale (binary logarithm) increases:

Note that the tables below display information about bigrams, not trigrams. That is, the tables report information about two-word co-occurrences, not three-word cooccurrences. While there will certainly be instances of a woman who in the SOAP, the table is reporting separate frequencies of a woman and woman who. You can check for yourself the frequency of a woman who in the SOAP and SCOTUS.

Exercise 5.2 Corpus laws

Look up information about George Kingsley Zipf. Who was he and what were his most important contributions?

What is the Zipf-Mandelbrot distribution?

What is Heap's law? It should be obvious to you that the w-1 frequencies are higher than the w+1 frequencies for the most frequent preceding words in both corpora. This shows a very strong likelihood for woman to be preceded by a, meaning the collocation of these words is highly predictable. This kind of predictability can be captured with measures such as conditional probability, entropy, and MI scores (cf. 5.7).

Just like with word frequencies, bigram frequencies may sometimes be calculated using stop lists so that common function words are excluded. The idea being that the word a in the sentence You want to be a woman of mystery is not actually giving much information about woman because although a co-occurs with woman very often, a also co-occurs with so many other different words. Whether stop lists should be used in calculating bigram frequencies really depends on the research question. For instance, researchers interested in semantic connections between words will probably want to exclude many function words through a stop list, but researchers interested in processing sequential information may not.

Bigrams can also be limited to certain kinds of combinations. One noticeable difference in the words preceding woman in the SOAP versus the SCOTUS is the adjectives used. This is a clue that adjectives may be a place where we see more register effects than with function words. So, we can look at the most frequent bigrams of adjective + woman in our two corpora to see if these are very different. In the english-corpora.org query language, this would be done with a search for _j* woman, as _j* is a shorthand to mean all adjective types. There are numerous measures of association that can be used to capture these kinds of differences, including conditional probability, MI, and informativity among

Below are some common measures that can be used to measure the association between words. There are many more and some strains of corpus linguistic research favour different measurements, either due to historical development of the sub-field or due to specific research goals.

Joint probability P(xy) -Joint probability is simply bigram probability: how often do words co-occur with each other? It is usually normalised by the corpus frequency (note that in scientific notation, e is used for very large or very small numbers. e-5 means move the decimal rightwards 5 digits). Because we are sometimes working with very small numbers, probabilities are often logarithmically transformed (cf. 5.5). This puts the numbers on a different scale, making probabilities and frequencies more comparable. Comparing these probabilities, we see that the backward transitional probability is larger than the forward transitional probability. This matches the intuition we should have from above, where beautiful woman is more predictable given woman than given beautiful.

Here are some good resources on kinds of association measures and examples of their use:

•

•

Analogous to conditional probability, conditional entropy is a measure of uncertainty of x when y is known. To compare how different corpora are, one can use relative entropy, also called Kullback-Leibler Divergence

•

• Gries (2010b) on useful statistics for corpus linguistics

CONCORDANCES AND KEYWORDS IN CONTEXT

Concordances give contextual information about a word, showing the context in which it occurs. Often a concordance display gives information about the word by putting that word in the middle of a line with a certain amount of words preceding and following it. This kind of display is called keyword in context or KWIC. This is a good method for a heuristic or first-pass look at data. Concordances are usually more understandable for a human than a list of bigram frequencies. Some corpus programs will highlight collocates of a word of interest with different colours depending on their parts of speech. This can help a researcher start to identify patterns.

A KWIC can usually be sorted alphabetically or by frequency of co-occurrence of w-1, w-2, w-3, w+2, w+3 etc.

Below is an example concordance from COCA of adjective + woman, showing the first five lines. The data is sorted alphabetically by w-1.

Exercise 5.4 A KWIC look

Using COHA, find the KWIC view and return results of woman. Sort by w-1 and then by w-2. What kinds of contexts are occurring in each of these instances?

Now try again using _j* woman and see that you can return contexts on various bigrams (cf. Figure

TRIGRAMS AND N-GRAMS

A bigram refers to two words that occur next to each other and a trigram refers to three words that occur together sequentially. The term n-gram can be used for any number, that is, n of words that occur in a sequence. Usually we do not use the term n-gram to refer to single words (which would be 1-grams) or bigrams (2-grams). However, for sequences of three and especially four or more words, researchers often use this shorthand: 4-grams, 5-grams, and so on.

In a large corpus, you will see many bigrams that occur more than once. However, you will see far fewer 3-grams (a lot of) or 4-grams (a lot of people) repeated, as in

COLLIGATIONS AND GRAMMATICAL CATEGORIES

Finding strings of characters or words is relatively easy in a corpus. However, many times researchers are interested in sets of words or 'kinds' of words that may Exercise 5.5 n-grams -quotidian or unique?

Write three 5-gram sequences in English that you think may have a chance of being repeated more than once in a corpus. Use the 14 billion word iWeb to test your theory and see how often each occurs (if at all). You may need to change the sort/limit options to return results with a minimum frequency of 1. Also try putting 5-gram sequences into an internet search engine surrounded by quotes.

If you cannot find any repeated 5-grams, try 4-grams. Also, look for 6-grams or higher n-grams using a web search.

What kinds of n-grams were you able to find that were repeated? encompass many different strings. For instance, if you wanted to examine noun modification by adjectives in English, or even qualitative adjectival modification of human nouns, there are many different nouns and adjectives to search for (cf. 2.2.4 on colligations and collostructions). So how do we find these? Or, for instance, if you were interested in investigating the dative alternation

A corpus that has at least some level of annotation will be very helpful in finding categories of words. One of the most common types of annotation is PoS tagging (cf. 7.2.3), which indicates the part of speech of each word in the corpus (also called grammatical tagging). Depending on how the tagging was done, there may just be simple categories such as verb, noun, adjective, or the categories may be more refined such as past tense verb, present tense verb, etc. One thing to watch out for with PoS tagging is that it is often automated. Hand-tagging an entire corpus, even a small one, is a monumental effort. Therefore, rules are often used to describe to a computer when to label a word with a particular category or another and then a tagger is run over the corpus. Usually a tagger is trained on a smaller hand-annotated sample and then applied (tested) on a larger corpus. This means that you get a lot of coverage quickly, but that there may be incorrect labels given to words. Some words may also get more than one possible label. This happens when a word is ambiguous and the tagger's rules cannot unambiguously determine the word category. There are also semantic taggers that give meaning information, among others. Tags are often somewhere 'in the background' , so if you are using an interface to query your corpus, you may not see the tags, although they will constrain your results. If you are using any tagged corpus, it is good to look through a portion of the actual tagged data before you start your searches so that you can adapt your search to what is really available in the corpus, not just what you expect or hope to be available. Searching for some constructions or grammar phenomena may require more than PoS tagged corpora.

For many languages there are no tagged corpora available, so to find grammatical phenomena, corpus linguists may have to rely on string searchers. Many constructions use particular words or strings or a limited set of these. When this is a small number, regular expressions can be helpful (cf. 5.11). Another way one can find grammatical constructions or search for grammatical patterns is to first identify the possible structures in a smaller tagged corpus and then use string information in a larger corpus (à la

Some kinds of grammatical phenomena are difficult to find with string searches. In that case, additional annotation for specific categories is probably needed. That is why many corpora are hand-annotated for a limited set of phenomena, although this can be time-consuming (cf.

Corpus queries

We have put a chart below of common regular expressions. Online you can find websites that help you practice your regular expressions on sample data.

In some, but not all, regex implementations, there are special regex character classes called POSIX character classes. These are written in lower case between brackets and colons and can be very helpful shorthand, but also always have an alternative regular expression.

Corpus queries

Corpus queries 87

Corpus queries

(1) every capitalised word in the text You need to be able to return/find:

(2) the last word of every question in the text

(3) all the text strings between punctuation

To do this (A) determine what it is that you need to return with a regex, then (B) in the regular expression part of the webpage type in appropriate regular expressions to return 1-3. Adjust/debug your expressions as needed. Answers at the end of this chapter. This does not mean that the original question has to be abandoned entirely, but you may have to confine yourself to certain aspects, or ask the question somewhat differently. In carefully documenting your queries, you also make your research accountable and replicable by others.

WORKFLOW

ANSWERS FOR EXERCISE 5.6 ON REGEXES

(1) Match every capitalised word in the corpus:

(A) All strings starting with an uppercase letter, followed by zero or more lowercase letters

Where is Bob? He's in the office. Could you call him? Sure, no problem.

(2) Match the last word of every question in the corpus:

(A) any number of letters, upper or lower case, followed by a question mark (B) [a-zA-Z]*\? or \w*\? 2  = Where is Bob? He's in the office. Could you call him? Sure, no problem.

(3) Find all text chunks between punctuation: FURTHER READING

NOTES

1. In Information Theory, MI can be used to measure the mutual dependence between any two random variables, not just words. In corpus linguistics, it is also often used to measure the strength of association between phonemes or between a word and a construction it can occur in. 2. For our example text, these regexes return the same information. \w also includes _ so if the example had an underscore, then they would no longer return the same information.

STEPS FOR AN IDEALISED CORPUS

Corpus types and goals

We have so far considered mostly pre-existing corpora, whether those from large, wellresearched languages or those stemming from smaller-scale documentation. In this chapter we turn our attention to the process of corpus building (or corpus compilation) itself. Often, especially at more junior stages of your career, you will not be in a position to build an entirely new, large corpus of a language, especially not if that language has a long research tradition and hence a large academic community. However, when it comes especially to the documentation and description of smaller languages that have not previously been investigated in much depth, you may be involved in corpus compilation to a considerable degree, including having to make relevant decisions on corpus design and structure. Finally, whether you work on well-researched or lesser-studied languages, considerations of corpus design will also apply to compilations of small, focused research corpora which may or may not draw on larger, pre-existing corpora.

The following sections are arranged along these three basic types of scenarios. We will present the issues of this chapter from a practical perspective, assuming that we are the compilers of a corpus. Many of the considerations involved here are also relevant for the reverse perspective of a corpus user or analyst who needs to understand considerations of corpus compilation (both theoretical and practical ones) in order to properly evaluate their corpus findings. While this typology is quite selective, leaving aside many of the corpus types we mentioned in 3.3, the three scenarios presented here (summarised in Table

Corpus building 92

A Type 1 corpus in Table

A Type 3 corpus is created with a fairly narrow research focus in mind, possibly in an attempt to answer a single research question, or a small set of interrelated questions. Such corpora may draw on pre-existing texts, including those contained in a larger, general corpus, or include specifically collected texts, for example, texts elicited during controlled experiments. While Type 1 and Type 2 are normally dynamic corpora (and in principle often monitor corpora [cf.

For any of these types, corpus building involves the selection and/or collection of texts or text excerpts and their inclusion in some form of data infrastructure. The latter term is to be understood here in a broad and non-technical sense, meaning simply that in order for a range of texts to form a corpus they need to be compiled in some form and accessible in some way. In other words, the texts need to form some framed whole, a collection rather than a series of texts.

A very first step for a corpus builder is to identify texts that are relevant for the envisaged corpus and should be considered for selection or collection. This initial step crucially depends on the corpus and its goals, for example, which language or variety and which situational features thereof should be represented (cf.

Corpus building

Corpus building 94

Any corpus building project is influenced by two competing sets of factors: the first are represented by certain design principles that take into consideration ideals of representativeness and balance given the intended purposes of the corpus (cf. 3.1). The second set of factors are essentially those concerning practicalities of data availability and necessary efforts of data processing and representation vis-à-vis confined resources and other considerations. These two sets of factors actually create a tension between the ideal representative corpus and a deviation thereof. In practice, the process of data collection and/or selection will ultimately be guided by the latter considerations. We will first outline general corpus design principles and then turn to the more practical issues of data collection and/or selection in Section 6.2.

Identification, selection, and evaluation

A principled problem for corpus compilation lies in the fact that we can never be entirely sure about the entire range of text varieties found in the community of users of any given variety. For better-studied languages, we will often have at least some common-knowledge idea of attested text varieties, but corpus compilers will also need to draw on relevant findings from studies of text varieties (e.g.

For hitherto under-studied languages, this stage may involve much more research and identification of text varieties as part of linguistic and/or ethnographic fieldwork of a language community. As discussed in 3.1.6, a good first step in this process is to elicit names for text varieties or speech events from native speakers. Think of ways to collect maximally spontaneous, spoken texts from speakers of any language or variety. How would you go about doing this? What can you do to minimise planning opportunities? Write down different steps in your collection.

Once we have identified all text varieties that should be included, we need to decide how to collect relevant specimens and which ones of these to select for inclusion. Collection is chiefly relevant from a practical point of view, and we take it up again in the following section. Text selection is a non-trivial aspect of corpus building for general corpora: since we aim at a high degree of general representativeness, we need to consider carefully the composition of our corpus. One way to go is to approximate the composition of the population as estimated in some way (cf. Brown corpus) whereas the alternative is to aim for a balance of text varieties (cf.

What scientific considerations do suggest for a general corpus is that we should include a large range of text varieties with different situational characteristics, including large proportions of spoken and/or signed text varieties in the interest of greater representativeness. Moreover, although authenticity is not a demarcation criterion for corpora, spoken texts in particular should come from common text varieties as well and not be restricted to scripted or semi-scripted TV, radio, or otherwise broadcasted texts or texts elicited in experimental setups.

In order to be able to determine the size of our text collection and relevant proportion in subsections, we need to evaluate it accordingly. As we have discussed in 3.1.1, determination of corpus size is a non-trivial issue: we need to set the unit of measurement we find most appropriate -for example, orthographic or grammatical words -and then make sure that we can actually count tokens thereof -which may require tokenisation processes before counting can take place (cf.

Corpus building

Corpus building 96 compilation and processing/annotation in fact go hand in hand for most general modern corpora. After evaluation, you may determine that additional texts or text types are necessary to achieve better representativeness.

In a final step, we need to make the corpus accessible to the scientific community in order to fulfil the scientific imperative of accountability and to enable further scientific developments based thereupon. A further feature desirable from a scientific point of view may be modifiability or manipulability, that is, we may want to offer the opportunity to add further texts to the corpus as appropriate in given research contexts or to modify the corpus composition in other ways (e.g. by removing some texts, etc.). We will discuss different ways to achieve these goals in the following section.

Considerations of mode and script differences

General corpora are required to contain large proportions of spoken and/or signed texts, and likewise other more specialised corpora will contain texts of these modes. These modes differ from written texts in that the raw data is not readily amenable to inclusion in our corpus. For one thing, both require initial audio/audio-visual recording, which in turn brings about a myriad of situational considerations that we have to consider. These texts also need to be processed. In particular they need to be annotated, at least transcribed, and it is the transcription that will eventually resemble our corpus text. An essential requirement for corpus building is that all aspects of text production be available in some way to the users of the corpus. For instance, it may be relevant to know where pauses, disfluencies, or construction restarts occur in order to evaluate the use of a particular structure properly. This means that the transcription of spoken/signed texts needs to contain fairly meticulous special annotations for characteristics of text production. Furthermore, the corpus text should be linked to the recording media of the primary spoken/signed text in a time-aligned format. In 21st-century corpora this means that the corpus text and the linked media file be accessible from a single entry point, for example, a website or a corpus query software.

Corpus builders have to consider the use of different scripts. Since mainstream corpus linguistics focuses on English and other Western European languages, the Latinbased script used in these languages is most common, and relevant corpus software and query mechanisms are based on it. For languages with other scripts, for example, Cyrillic scripts in many languages of Eastern Europe and Central Asia or various scripts of East Asian languages (Mandarin, Japanese, etc.) corpus builders will either need to use encoding such as Unicode (cf. 5.11) or add a layer of transliteration to the corpus text. Solutions will depend on specific circumstances, and we merely point out the general requirements here.

CORPUS BUILDING IN CONTEXT

In practice, corpus building can never fully comply with the design principles just outlined. In this section, we will go through a number of scenarios and conditions that pose restrictions on corpus design as desirable from a purely scientific point of view. This does not mean, however, that considerations of corpus design become irrelevant on the ground, and we will point out what can be done better even under difficult circumstances. Finally, the shaping of any specific corpus-building project will ultimately depend on its purposes.

Whole-population corpora

While we will mostly be talking about restrictions on text data availability and usability for corpus building in this section, there is also the reverse deviation from the corpus design ideal, where a corpus can indeed be an exact copy of the population under consideration. Such population corpora are restricted to specific research contexts where a restricted set of texts is also the object of the research agenda. We mentioned this possibility in Chapter 2, namely, that someone interested in the specific properties of Obama' s published speeches could indeed consider all these speeches, and thus sampling is not necessary. A real example is

Availability of texts: copyright and privacy

According to our design criteria and our generalised corpus-building scheme, we select and collect texts carefully following considerations of representativeness. This ideal is confronted by restrictions on data availability. Data availability can be restricted in all sorts of ways. Consider again the major difference between published and private texts: where a text is published it is fixed to some medium in some basic format, whether digital or analogue. Although the texts are accessible, there are copyright restrictions in both cases, which limits the availability of published texts for corpus-building enterprises severely. Even some texts available through the internet may have some copyright protection or restrictions on usage. Private texts, on the other hand, are not subject to copyright restrictions, but here participants who have produced the texts may be much less willing to make these publicly accessible. Often private texts will, therefore, never make it into a corpus. Where they are included, permission needs to be obtained and documented, often under the umbrella of a university's ethics administration and in the form of informed consent. Sometimes,

Corpus building

Corpus building 98 when authors have waived their right to privacy (whether knowingly or not) this data can be used to compile corpora, such as the Multilingual Amazon Reviews Corpus

Technical, methodological, and other considerations

Restriction on resources, availability, and technical considerations -written texts that exist in digital format prior to any considerations of corpus building are fairly easily included in the corpus from a technical point of view. Web-crawlers can help compile massive amounts of written data for languages with a web presence. Texts that exist only in printed or even handwritten form on paper require work on digitisation so that the text is machine-readable. Options for dealing with this include optical character recognition (OCR) systems and manual typist work. For many languages, there are no written forms until language documentation projects start, so time-consuming transcription of spoken or signed texts are necessary.

Vera'a encyclopaedic texts about flora and fauna

Vera' a community members expressed concern over younger generations not having any knowledge of the names of floral and faunal species and the details of their specific features and uses. So, in addition to the more common texts from oral literature (myths, legends, fables) we also collected more than 100 plants and over 200 fish descriptions. We first recorded spoken texts, and one community linguist created edited versions based on the original recording together with its transcription. Similarly, two community linguists wrote up descriptions of bird species, and a smaller group of speakers created a mono-lingual encyclopaedic dictionary with definitions of parts of a house. Descriptive texts of this kind are not part of the traditional verbal behaviour of the community. Yet, their inclusion in the corpus does serve the community' s interest. Crucially, the descriptive texts formed part of the basis of our comparative study of object realisation in Vera' a

A corpus-building project can be explorative, and the encounter of new text varieties may be part of the evaluative step that may trigger further text collection. Specific scientific requirements -corpus linguists may need to manipulate, randomise, or control for different situations, often particularly relevant in experimental or comparative research designs. Options include elicitation, stimuli-based tasks, narrations of picture books or short movies (well-known examples include Mercer Myer's

Matukar Panau interactional texts

Matukar Panau research focuses on spontaneous speech and interaction between people of different ages, genders, and clans. This research programme requires the inclusion of larger amounts of conversational data, whose collection has been supported with stimuli-based tasks, including San Roque et al. ' s (2012) Family Problem set which combines individual picture description, a discussion between participants, and storytelling. Our project then has much more conversational texts, albeit about some specific topics, than mythical or procedural texts. (While this may seem like a fairly superficial exercise in a textbook, note that these kinds of formulation are in fact part of a grant proposal, for example, in the area of language documentation, where researchers have to demonstrate that they can plan a corpus project not only in terms of their research goals but also in consideration of other stakeholders.)

Corpus building

Corpus building 100

PRE-PROCESSING: TRANSCRIPTION, TRANSLATION, AND DIGITISATION OF TEXT

As pointed out above, spoken and signed texts need to be transcribed before they can be included in a corpus. And where the corpus text is in a language that is not known to the scientific community it needs to be translated to a more widely known language. We will explain both steps and related protocols in turn.

Spoken data transcription

Raw spoken-language texts cannot generally be analysed as such with standard corpus-linguistic tools and are therefore transcribed, that is, transposed into a written form, before they are included in a corpus. This written form is what we treat as the corpus text since this is what can be searched and further annotated. The major goal is to provide an accurate rendition of the spoken text in written form, including certain paralinguistic aspects. Transcription may be at a low level of granularity, transcribing only speech streams, or highly detailed including laughter, lengthening, breathiness, etc.

Let's look at an example from a prominently spoken corpus of English, the Santa Barbara Corpus of Spoken American English (SBC) (Du

(1) (6. The primary purpose of the transcription is to make the spoken text searchable. Without the transcription it would still be possible to find the same instances, but this would involve re-listening to the recordings and noting all relevant instances, a painstaking and unreliable procedure. The same applies to specific properties of speech production, for example, overlapping of speech or pausing: since these are captured as part of the transcription they are likewise searchable, for example, by using query expressions like the square brackets to extract instances of overlap or triple dots to find shorter pauses. Obviously in order to use these annotations, familiarity with the conventions is essential.

The task of transcription is by no means trivial and the decisions taken -where to note a pause or whether and where a word is truncated or not etc. -are not incontestable since human transcribers are always led -and potentially misled -by their understanding of the text and their expectations (cf.

Corpus building

Corpus building 102

These decisions, once fixated in the form of the written text that serves as the basis for corpus analyses, will carry over to all kinds of subsequent analyses. 2 Furthermore, a fundamental question is whether the transcribed word forms should be represented according to the conventional phonetic alphabet of the IPA or the more or less standard orthography of the language under consideration (cf.

We saw in (6.1) that in order to render the spoken text adequately, the transcriber needed to deviate from strict orthographic conventions. This better reflects the raw data. However, sometimes deviations may in fact be problematic.

Linking transcriptions to raw data

Since transcription is a non-trivial task that raises numerous analytical and even theoretical questions, it is vital for corpus users to have the possibility to consult the original raw data. There are different ways in which the raw data -the audio and/ or video recordings of the speech event -can be made accessible to corpus users: in the SBC, the solution is to store two types of file for each corpus text, one audio file containing the text recording in WAV format and a text file containing the transcription thereof. As we saw in (6.1), individual passages in the text refer to passages in the recording through time code information that allows users to navigate across the two files. A more sophisticated way of establishing a direct link between the recorded speech signal and its annotation is offered by specialised software that is designed to build up time-aligned annotation of media files. Time-alignment means that the annotation -of whatever kind -is directly linked to the rendition of the speech signal. The media file is also linked so that the signal can be played back and navigated from within the software. One example of such software is ELAN (ELAN Version 6.0 2020). ELAN has been developed by the Max-Planck Institute for Psycholinguistics in Nijmegen (Netherlands) and can be downloaded free from their website. 3 It has become one of the standard tools for working on lesser-studied languages and building corpora thereof. Like the SBC, a corpus created with ELAN will have two components, namely, a set of media files with the raw audio and/or audio-visual data and a corresponding set of ELAN document files (a special kind of XML file). But the key feature of ELAN is that users can create annotations while navigating and playing back the linked media file. Figure

Leaving aside all the detail in functionalities, the centre of the window consists of a sonographic rendition of the recorded speech signal. This information is taken from the accompanying WAV audio file which is linked to the ELAN file. Underneath this waveform are two annotation tiers. Both are cut up into chunks, and these segments are where annotation values are placed. Their borders correspond to begin and end time of a corresponding stretch in the sound files, and in this way, they create a direct link between a specific passage in the media and its transcription. This has several advantages. First, it relieves the burden on transcription to represent the raw data in maximal detail since the original recording can be accessed directly. This latter possibility has further advantages, for instance, research into the prosodic properties of spoken texts relies on phonetic measurements -for example, of pauses and speech rate; cf. Seifart (In Press) and references therein for an example -that will have to draw on such direct scrutiny rather than rough estimation according to acoustic impressions by human transcribers. Second, subsequent corpus users can much more easily re-evaluate the accuracy of annotations and potentially modify them or add further annotation detail, as required for any research agenda. Finally, it can be advantageous when working on hitherto lesser-studied languages if speakers of the language do the transcription, and do it in the form of a working orthography that deliberately abstracts away from all the phonetic detail. Phonetic detail can more easily and often better be determined objectively and through the use of specialised software. Hence, it can be strategic to separate these two transcription tasks and this can be done best if the media recording is directly linked with its annotation.

Corpus building

Corpus building 104

Rendering corpus text

We said in 6.3.1 that spoken texts are commonly transcribed in orthographic form. Typical for LD corpora is what could be called working orthographies which emerge as a result of the collaborative documentation project. Some kinds of orthography would be easier or more difficult for community members to learn and teach others. For corpus building efforts, in particular, one needs to consider character encoding. Unicode is easier to work with than characters with diacritics, for instance. One also needs to consider what a character represents. Having a series such as <ng> to represent a single sound may be easier for input than an engma character (ŋ), but will require consideration later on of what character counts mean for measures such as word length or consonant to vowel ratios, etc., or processes such as forced alignment (cf. Gippert 2006 for a discussion of textual encoding of language documentations and Seifart 2006 for a discussion of orthography development).

If the corpus-building effort is for a language that has been documented by different people or even by the same person over several years, there may be multiple orthographies that will require standardisation. This should be documented, describing what changes have been made. Variable spelling may be of potential research interest at some point, so copies of files should be made and stored separately.

Considerations of space preclude more detailed discussion here, but it should be kept in mind that transcription is not a theory-free undertaking and that the way it is done will have bearings on the further processing and analysis of the data (see Ochs 1979 for a classic paper,

Translation

A further layer of annotation vital for many corpora is a free translation into a major world language (Schultze-Berndt 2006). The purpose here is obvious: with our interest in modern languages encompassing thousands of different languages no corpus user can be expected to have command over every language under study. A free

Enriching time-aligned annotations

A sub-corpus of the Vera'a corpus was contributed to a larger collaborative cross-corpus project researching prosodic properties of speech in different languages (PI Frank Seifart, ZAS Berlin)

Choice of metalanguage

The term metalanguage refers to the language we use to describe language and linguistic structures, as well as the metadata thereof. This encompasses both natural languages, as well as various formalisms and various symbols, like the @ to indicate laughter in the Du

Free translations and some other types of annotation essentially involve a natural language as a metalanguage. The general rule of course is that this should be a language that all potential corpus users understand. Depending on the field of study and the area where the language(s) under study is spoken, this may be, for example, Spanish, French, Mandarin, or English since scientific communities working in these respective areas often share these respective languages. For our purposes, the metalanguage

Corpus building

Corpus building 106 is always English. This is not because we think that English is linguistically fittest for this purpose -this can surely not be said for any language in the world -but simply because it does appear to be the language that everybody interested at least in principle in any language(s) in the world know, at least at the time of writing. It is obviously also the language all readers of this book know, so we generally take English to be the metalanguage used in free translations and other annotations that involve natural language expressions. For many LD corpora, it may be advisable to create multiple versions in different languages so that the corpus be accessible to people of relevant regions.

DATA FORMATS

Once we have decided what to include in our corpus, we have to actually put together the different texts in such a way that we can analyse all data in essentially the same way concerning any particular research question. This requires that texts be similar in two regards: text data format and pre-processing and annotation. Moreover, compilers need to think of the use of their corpus through some kind of interface: this can include fairly simplistic general methods of text searches (as found in text editors), more specific simple search engines (like AntConc; Anthony 2020), or specific corpus software and interfaces like ANNIS

There are a number of options, and we do not provide any specific suggestions here. The main point that you need to be aware of is the fact that any digital text is encoded in some form. This means you have to make sure that when working with different software or on different machines the text and letters that look the same really are the same in terms of their encoding. If encoding does not match, you will potentially not find relevant text. Moreover, you should consider differences in format: if data remain in different formats that cannot be combined for a search, then multiple searches will have to be used for corpus queries. For some purposes, such as finding an illustrative example for a paper, this may be fine. However, if you want frequency tables, n-gram counts, or proportions (cf. Chapter 5), compiling the corpus data into a standard format is an important step. If you are dealing with multiple file formats, exporting or converting everything to plain text format is probably the simplest way to compile it all together. Further options include exporting/importing texts to a single software to give the data a unified format. Also possible is compiling the corpus in multiple formats. Multi-CAST is an example where both options are available: users can work with the data in ELAN (where multiple files can be queried) or load it into R as the package multicastR (Schiborr 2018).

INCLUDING METADATA

Alongside corpus text data we have to integrate metadata. Metadata is generated at various parts of the documentation process, as discussed above. Participant metadata, data about each individual, including names, ages, genders, languages they use, and perhaps, occupation, education level, and other characteristics. Each text that is part of the corpus will also have metadata such as the date and place it was recorded, who spoke or signed, who else was present, and what media files are associated with the text. Metadata will also provide information about other situational characteristics, for instance, what register and genre properties a text has, what its global topic is, etc. One of the simplest ways to record metadata is as a simple table, which can be stored in a comma or tab-delimited value format (CSV or TSV). These formats are easy to read for many computer programs and are non-proprietary, unlike, for example, Microsoft Excel, which means they will last longer and can be easily converted to other formats.

Metadata can be integrated into a corpus in various ways. If you are writing your own computer scripts to build your corpus, you can integrate (or merge in) your metadata, bringing in all the metadata for each participant and all the metadata for each file for every token in the corpus. Metadata can be used to limit searches to a particular subsection of the corpus, or can be used for examining variation due to some aspect of the individuals (i.e. gender, education, second-language influence) or the files (i.e. register, time period).

For language archives, one will either have to fill in metadata forms, for instance, the metadata catalogue of PARADISEC (catalog.paradisec.org.au/), or adhere to a standard metadata format, for instance in the DoBeS (tla.mpi.nl/project/dobes/) or the ELAR (soas. ac.uk/elar/) archive. The latter two use a standard called IMDI. IMDI files can be generated easily via an online interface called CMDI Maker (cmdi-maker.uni-koeln.de/) where one can fill in the information relating to participants, circumstances of individual sessions, overall project, etc. as discussed above. This gives standardised metadata results, among other things, in the data appearing on the archive' s website in a structured way.

Exercise 6.3 Metadata search

Go to the Matukar Panau collections in PARADISEC and ELAR and find the item "Kadagoi Lovinea Rapalau Ambrose and Ambrose Kainor describe Bilums" (DGB1-crafts02) in both collections. What metadata can be seen for (

Corpus building

Corpus building 108

Some software programs used for corpus research, such as some concordancers, may not be able to integrate metadata. In fact, if metadata is included at the top or bottom of the file, it may be read as text rather than meta-information about the text. Because of this, corpus compilers will often use special characters in a special way so that metadata is regarded as such. Here are some examples: @speaker: Megan, <speaker>Megan</speaker>, speaker_Megan. Some researchers will include basic metadata in file names (i.e. Megan_19991103_monologue_Canberra) and use the names, often in combination with a file folder structure, to organise their data.

PUBLICATION OF THE CORPUS

The final step in corpus compilation is publication. In the 21st century, this is understood as published online. There are always reasons why it may be difficult to get data published, and of course, this step also requires some resources if the corpus is to be presented in a well-structured and well-designed way. Webpublished corpora do not necessarily need to have query interfaces. The main point really is that interested users have access to the data and can understand how it can be used, which in turn requires metadata on the corpus as a whole. As mentioned above, Multi-CAST is available in various formats, and all data can be downloaded from the corpus website. 4 A corpus website should also have all relevant information on the corpus and its various texts or in this case subcorpora. Part of this information about the corpus is a citation so that it can be referenced, as we do in this book. All these aspects of corpus publication are intended to make the use of corpora by multiple parties, and that of any linguistic data more generally, the norm in the language sciences.

CONCLUSION

The main point of this chapter is to understand the tensions between the principles of corpus design, as discussed in Chapter 3, and specific considerations of actual corpus building. In particular, LD corpora may come with severe limitations on corpus design, given that related projects have limited funding, and also because such projects are often undertaken by individual academics (though in collaboration with community members). This is the typical constellation for corpus building today, and it is a major reason why only a few larger corpora are web-accessible to date. However, we can hope for an increase of recognition of the usefulness and importance of corpora from a range of diverse languages, even if they are still relatively small and will be so for a long time, at least in comparison with the large corpora of well-studied languages. This will hopefully also spur further efforts in this area.

FURTHER READING

Mosel (

NOTES

1. Note, however, that large-scale descriptive work is not precluded here, as is evident from the seminal work by

CORPUS ANNOTATIONS AND RESEARCH AGENDAS

In this chapter we will be dealing with corpus annotation. Corpus annotations are special types of additional text added to the corpus text that encodes interpretative linguistic information

[…] watching as the woman entered her living room, kicked off her shoes, and sat down in her chair.

COCA.FIC The Antioch Review 1996(winter), 54.1, p. 60: Kenneth J. Emberly: Whole days and nights.

In this excerpt from a written fictional text in COCA, any human reader will understand that both instances of her refer back to 'the woman' . But this is not directly encoded by any means here: the possessor of the living room or the shoes could be someone else. Our understanding of these co-reference relations is the result of our interpretation of the wider discourse context, which is in turn guided by world and cultural knowledge -it would be less plausible that someone kicks off their shoes in someone else's living room in many English-speaking and other cultures. But even more basic aspects of this example are not directly extractable from the corpus text; for example, the grouping of words into phrases: how do we know which words belong together, for example, her living room, and form what kind of relationships with other words and phrases? Various types of corpus annotation can make these aspects explicit and searchable.

Standards of corpus annotation

Corpus annotations should follow a set of general standards. First of all, they should be linked to the corpus text data and their metadata so that corpus users can evaluate how annotators arrived at their annotation decisions; at the same time the annotations are ideally extractable in order to be analysed outside the corpus infrastructure itself, for example, by programmes for statistical analysis. Second, the annotation practices need to be documented. This can be done in the form of an annotation manual that outlines the conventions that have been applied in creating a set of annotations and at the same time serves as guidelines for users who intend to implement the annotations themselves on their own corpus data. Moreover, this enables users of the corpus to plan the construction of their queries targeting these annotations. The documentation should also outline what research can be or has been undertaken with them, and refer to this research so that users understand their theoretical context.

In Section 7.2 we present a selection of conventions for annotation that target different linguistic levels. We will refer to additional literature in the further reading section for more exhaustive overviews. Our main purpose here is to explain the basic implementation of corpus annotations and how they add value to a corpus by enhancing its amenability to a wider range of research questions. We will repeatedly refer back to Chapters 4 and 5 where different types of annotation were relevant. Section 7.3 is devoted to comparative corpus annotations in the context of corpus-based typology. This section is tightly connected with Chapter 11 where we will explain various types of research that builds on these or similar types of annotation systems.

The transcription of spoken raw data and idiomatic translations of corpus texts are dealt with in Chapter 6 as part of corpus building.

Corpus annotation involves enormous amounts of work. This means that when we decide to use an annotation system or devise one ourselves, we need to make sure that the work is worthwhile, that is, the information we require cannot be extracted in some other way using data that is readily available, such as smarter ways of querying.

TYPES OF ANNOTATION AND ANNOTATION SCHEMES

Corpus annotations capture different types of information pertaining to different levels of linguistic representation. The specific design of annotation systems is codetermined by the specific research interests involved and therefore, we will need to refer to various research projects time and again.

Corpus annotation

Corpus annotation 112

Annotation for phonetics and prosody

Phonetic and prosodic annotation is fairly specialised: while it is obviously only relevant for spoken language corpora, it is not usually undertaken in many spoken language corpora. Speech corpora are a special type of spoken corpus (see 4.2.2) that are intended for corpus-based investigations into the phonetic and phonological structure of spoken language use. The first type of annotation targets individual sound segments and is essentially a type of transcription of spoken language raw data, phonetically a very close and exact one. In principle this can be done by using the symbols of the IPA to render the corpus text. In corpus linguistic practice, however, annotators have often resorted to alternative renditions of IPA conventions. Various phonetic alphabets consist of ASCII characters or Latin letters and combinations thereof. These are often easier to input, display, and search than IPA symbols (cf. 3.2.2).

Prosodic information is also rendered in more general spoken language corpus annotation conventions like Du

As for prosodic annotations, language documentation-based corpora often contain basic information about prosodic chunking in the form of time-aligned transcription according to IUs and by including pauses as well as disfluency phenomena

Morphological annotation

Morphological annotation captures the meanings of word forms and their component meaningful units, that is, morphs (rather than syllables for instance). It is applied to a corpus text which has already been annotated and translated, as required. As in the example from Vera'a in Figure

The most common set of conventions for morphological glossing are the so-called Leipzig Glossing Rules (LGR)

Corpus annotation

Corpus annotation 114

[2010:27-29] for a succinct explanation). The following examples from Japanese illustrates how the system works: As you can see from this example, morphological glossing requires the segmentation of word forms in the first instance so that individual morphs can be given their meaning or function glosses. By convention affixes are separated by hyphens, for example, the causative (CAUS) and the past tense (PST) suffix. Grammatical functions are captured by a set of more-or-less standardised abbreviations (i.e. NOM -nominative, ACC -accusative). The basic purpose of morphological glossing is making explicit the component parts of the meaning of the utterance provided in the free translation. In

Another challenge related to the morphology of word forms is to determine what lexeme they belong to, as was discussed in Chapter 2.2.2. The addition of information about word form-lexeme relations is called lemmatisation, and this essentially constitutes a layer of annotation of the form shown in

Parts-of-speech tagging

One of the most prevalent types of corpus annotation applied to corpora from betterstudied languages is parts-of-speech (PoS) tagging or simply tagging. As the name suggests the annotation picks up the word class membership of word forms, traditionally called parts of speech. Many of the classic and larger corpora of English contain PoS tagging, for example, COCA or the Brown family corpora. The motivation for PoS tagging in English follows from observations that parts-of-speech membership cannot be read off the form of word forms themselves, as is clear from textbook examples that illustrate the ambiguity resulting from this indeterminacy,

Corpus annotation

Corpus annotation 116

for example, Fruit flies like a banana. Surface word forms in English often have multiple parts-of-speech membership which means that just searching for these will potentially return false positives, that is, hits that are in fact not what we were searching for. For instance, searching for like in the interest of finding verbs that are semantically similar to love, enjoy, or appreciate can return the false positives of preposition tokens in similative constructions where it contrasts semantically with alternatives like equivalent to, akin to.

The annotations are called 'tags' because they are appended to corpus words, as shown in example (7.5) from the Brown corpus.

(7.5) on_IN other_AP matters_NNS, _, the_AT jury_NN recommended_VBD that_CS :_:

In this example we see that the word form matters is identified as belonging to the part-of-speech 'nouns' indicated by the initial N rather than a verb form (which would have a tag beginning with V). Thus, where a corpus user is interested only in instances of matters that are plural forms of nouns they can search for a string <mat-ters_NNS> rather than just <matters>. Let's consider relevant instances of our case example like in the Brown corpus, given in (7.6): Tagging of corpora is done with a clearly defined and confined inventory of tags (a controlled vocabulary) that is called a tagset. Tagsets are developed for specific languages, and often indeed for individual corpora, so that when you use a corpus, you need to consult the corpus metadata, manual, or other documentation.

Brown tagset

The Brown corpus comes with its own tagset comprising 87 different tags. As you will have realised when considering the examples above, tags pick up more information about word forms than just parts-of-speech membership. For instance, the tag <_NNS> classifies the word form as a common noun in its plural form. Further, tags are clearly marked as such by their preceding underscore. This makes it possible to search just for tags, for example, _NNS to find all instances of plural nouns. A crucial property of tags is that they have a hierarchical and decomposable structure, as can be seen from the subset of tags for nominal word forms in Table

Tagset designers typically construct tags to be mnemonic, that is, you should be able to recognise and memorise their meaning fairly easily based on the letters used. While the Brown tagset seems to achieve this goal to some extent, there are obvious limits given the possible variation in single-letter abbreviations for any number of distinctions, as is visible in the abbreviation R for 'adverbial' .

The Brown tagset also picks up specific forms that are particularly relevant for the analysis of grammatical (sentence) structure, thus breaching strict assumptions of word class membership; this is typical of tagsets for English. For instance, different forms of the verbs be, have, and do receive their own specific tags lacking an initial V for classification as verbs, as shown in Table

Corpus annotation

Corpus annotation 118

In modern corpora from English, tagging is often done automatically by a so-called tagger which is a software programme that automatically appends tags to word forms based on a computer-linguistic analysis of their morphological properties and their syntagmatic environment, that is, which other word forms precede and/or follow each word form. For the London-Oslo/Bergen Corpus (LOB) a tagger known as CLAWS was used which stands for 'Constituent Likelihood Automatic Word-tagging System' . CLAWS was further developed over seven versions, and the most current version of the FLOB and FROWN corpora have been tagged with CLAWS7

London-Lund tagset

Half of the London-Lund Corpus (LLC) (LLC; Svartvik 1990) consists of spoken texts, and for this a tagset was designed specifically to capture some features of words in spoken texts. 3 The LLC tagset is much larger than the Brown tagset, comprising 204 tags, reflecting finer-grained and additional grammatical distinctions, for example, the case forms of pronouns, and including cliticised forms whereby clitic and host are treated as one 'contracted' token word (cf 2.2.2). Tagging of contracted forms combines the two underlying word forms with a <*>, resulting in tags like <BHdem*VB+3>, where the latter part <VB+3> stands for '3rd person form of the verb be' thus differentiating that's from that. In addition to tags for these, the LLC tagset contains tags for expression -including multi-word expressions -that the authors consider characteristic of spoken language, as listed in Table

The actual distribution of such forms across texts of different modes is beyond the discussion in this chapter and is more a matter of systematic register studies, such as those mentioned in 3.1.6. What is relevant for our purposes here is that corpus and tagset developers entertain a considerable degree of freedom in designing tagsets to serve their specific needs, so that tags do not necessarily reflect the absolute exactness of linguistic analysis and classification. In other words, the maxim for the development of tagsets -and annotation systems more generally -is not that they are linguistically 100% accurate but that they are useful and overall consistent in their operationalisation.

PoS tagging has been applied to corpora other than from English; a famous tagset for German is the Stuttgart-Tübingen tagset (STTS)

Syntactic annotation

We have seen that PoS tagging often also includes information relevant for syntactic analysis, and PoS tagging is part of all syntactic annotation systems that we are aware of. But in addition to PoS tagging syntactic annotations also pick up more aspects of a syntactic structure. Reflecting the two basic conceptions of syntactic structure in modern linguistics, the systems can be classified as either annotating constituent

Corpus annotation

Corpus annotation 120 structure or dependencies. Corpora containing syntactic annotation for constituent or dependency structure are called treebanks since syntactic structure is commonly visualised in the form of trees in models of syntax. Treebanks typically have PoS tagging and also semantic annotation of argument structure

A famous example of a treebank with constituent structure annotation is the Penn Treebank developed at the University of Pennsylvania, first released in 1992 comprising texts with 2.8 million token words

(7.7) Penn Treebank I: skeletal bracketing annotation of phrase structure

Recalling your introductory course in syntax you will notice that this phrase structure annotation skips any terminal nodes with lexical categories and only contains phrasal projections. Remember, however, that lexical category information will be contained in the PoS tagging that we have discussed above in its original Brown version, so that, for example, the NP-embedded PP would be fully annotated as in (7.8) (note that the separator in Penn tagging is forward slash / rather than underscore _): These searches will thus give us a count of NPs with and without recursive structures, that is, where an NP occurs embedded in a higher-order NP either as an initial possessor NP or as the complement of a preposition of an NP-embedded PP. We could also further investigate which types of PPs occur as embedded ones, for example, what the proportion of possessor PPs with of is, searching for

The first query should return all instances of of PPs, and the second query all instances of PPs where the preposition is not of.

The bracketing annotation of its second release

(7.9) Penn Treebank II enriched bracketing PS annotation

Corpus annotation

Corpus annotation 122

In (7.9) the first NP represents the subject of the matrix clause, and this is noted by the functional tag -SBJ (cf.

The second type of treebank annotations encodes dependency relations. Probably the best-known example is the Prague Dependency Treebank (PDT) (cf.

Semantic annotation

Compared to other types of annotation discussed thus far semantic annotation is lesser developed and practised in corpus linguistics. One major concern in the corpus-based semantic analysis is word sense disambiguation which is not inferable from the surface structure of the corpus text itself and needs to be determined by a human interpreter. Recall Gries' (

Other potential requirements for semantic annotation of corpora pertain to the assignment of semantic categories and semantic fields, as well as annotations for specific semantic domains, for example, events classes, temporal or aspectual distinctions, or semantic roles. Various annotation systems have been developed for these different domains. Annotation of semantic categorisation is useful, for example, for various investigations of text content. Given that semantic annotation systems can be exceedingly complex by comparison to grammatical annotation, given the huge range of distinctions, we will not discuss these here in greater detail. We do provide further reading suggestions at the end of the chapter. Moreover, some semantic aspects are part of the typologically oriented annotation systems, and we will outline these in 7.3.

Discourse and reference annotation

Language use does not consist merely in the production of individual utterances; instead, utterances are interrelated with each other in what is called text or discourse (cf. 2.2.1). The various interrelations between utterances in a discourse lead to (text/discourse) coherence so that interlocutors (or writers/readers) share the meaning built up during production and reception. Most of these coherence relations are not marked overtly, and language users infer these based on their interpretation of preceding discourse in connection with world knowledge and their expectations concerning the continuation of the discourse. Two aspects of discourse coherence are particularly relevant for which corpus annotation systems have been developed. First, the relationships between utterances/propositions (e.g.

Corpus annotation

Discourse coherence is partially established through co-reference relations. A hugely prominent topic in this area is anaphora resolution (which relates mainly to perception) and referential choice. The latter relates to production and has been investigated with corpus-linguistic methods. Co-reference relations are probably the prime example of implicit information that can only be determined by human interpreters and that for this reason require corpus annotation to make them explicit. An annotation system developed to these ends is the University Centre for Computer Corpus Research on Language (UCREL) discourse annotation system, as documented in

English UCREL discourse annotation (a) (6 the married couple 6) said that <REF=6 they were happy with <REF=6 their lot

(b) There are nine categories in PGA Tour statistical service. Assists isn't (6 among them 6). If it were <ELLIP=6, Ben Crenshaw …

Exercise 7.2

Consider the UCREL discourse annotation system just outlined. How could you use these annotations to investigate questions of reference tracking? Do the following:

• Download a plain text from English (or another language you know well).

• Read through the text and add the reference tags exemplified in

• Note what analytical problems you encounter during the annotation process. What questions do you have to ask yourself?

• Complete a text of minimally 10,000 words. Now search for instances of • (a) Full NPs (b) pronouns (c) zero

• How can you construct queries with regular expressions that give you these instances?

• Now write up your results. How many instances of each category do you find? What are the ratios?

The annotation system has two basic facets: (1) it captures the identity of referents mentioned by different referring expressions, (2) it captures various aspects of the anaphoric relation. The symbol '<' signals that the co-referent is an antecedent and the relation is anaphoric (rather than cataphoric). 'REF' stands for an exact co-referent, so that the pronoun they refers to the same entity as the antecedent the married couple in

CORPUS ANNOTATION IN LINGUISTIC TYPOLOGY

Recent years have seen a surge in cross-linguistic typological research based on corpora. We devote an entire chapter to this emerging field in linguistics (see Chapter 11).

In this section we outline annotation procedures that have been developed with a comparative perspective in mind. For some of the annotations discussed thus far, corpus linguists have developed dedicated comparative perspectives. One of the most prominent of these is the EAGLES standard for PoS tagging in different languages which yields basically comparable annotations (cf. e.g.

Corpus annotation

Corpus annotation 126

UDs: Universal dependencies

Currently probably the largest and most influential effort in corpus-based typological research draws on universal dependency treebanks, better known as UDs. UDs are extensively annotated with PoS tags and syntactic dependencies (cf.

The first column has numerical identifiers of each corpus word form. The next one has the word form followed by lemma annotation. The fourth column hosts the universal PoS tag followed by a language-specific PoS tag, followed, in turn, by a list of language-specific grammatical features of the word form. The next column contains a number cross-referencing the word form ID in column 1 that identifies the head of which the word form in question is the dependent. For instance, in Table

The captivating aspect of UD annotations is that they are relatively easy to implement but enable a huge range of investigations. For instance, you can easily determine the expression of subjects and objects by pronouns and NPs (cf. our investigation of Vera'a). You can also determine the relative length and complexity of phrases (understood as dependency structures) by considering the number of head indices in the head column cross-referencing the head word of the phrase in question, or all phrases dependent on the verb node. These findings can then be related to the function of these phrases -or their dependency relation to the verb -comparing, for example, subject and object NPs. Similarly, phrase length can be related to their linear position within the clause by relating these to the ID numbers of their constituents; in this way NP complexity can be related to their ordering in the clause, a prominent research question in typology (see Chapter 11). Finally, constituent order in terms of grammatical relations can be determined in relation to the word form IDs. These questions link to a range of well-established research agendas on constituent order and languages processing, and we will outline some UD-based research in this area in Chapter 11 on typology.

This brings us to the crucial feature of UDs, namely their cross-linguistic comparability. This is achieved primarily by including very general PoS tags as well as general definitions of dependency relations; specific dependencies as cross-referenced to heads, are not language-specific anyway. In this way, the research questions just outlined can be undertaken across any number of UDs. One can then, for example, compare the extent to which the order of say subject and object varies across languages, or to what extent their ordering is interrelated with their relative length and complexity.

SCOPIC: Social cognition parallax interview corpus

The Social Cognition project led by Danielle Barth and Nick Evans is an example of a cross-linguistic corpus with annotations targeting specific research questions.

The project tackles the interconnections between cultural backgrounds of users of different languages and the way they construe and verbalise the entities and events they talk about in connected discourse

The social cognition project is based on a specific corpus-building project, the parallax corpus SCOPIC

Corpus annotation

Corpus annotation 128 set of pictures that conjointly form a little narrative, the Family Problem Picture Task

In order for the data to be analysable for the project, specific corpus annotations are required. We illustrate this with one of the first research questions that have been addressed in this project, namely, how users of diverse languages preferably make reference to various human beings during the task. The focus here is on lexical choices, that is, which lexical nouns are used as heads of respective NPs with human reference. Users of any language have many choices available to refer to a human being in a given context. For instance, one of the pictures in the Family Problem set figures a policeman who returns personal belongings to a recurring male character. People can refer to this man with nouns like 'man' , '(her) husband' , '(his) father' , 'prisoner' , or many other nouns. The project aims at identifying whether languages show specific preferences. For instance, some languages may have an abundance of kinship term expressions, including the use of 'uncle' for the policeman in some languages from the Pacific area, thus ascribing a specific respectful relationship to the social role.

The multiple annotation schemes of SCOPIC are organised along functional categories. Each language in the study is annotated for expressions that relate to many functional categories relevant to social cognition. Within each broad functional category, researchers code a "TAG" and a "TERM" for each instance of the phenomenon. A TAG comes from a closed and cross-linguistically fixed list of category choices and indicates the type of expression being used for the relevant instantiation of a particular functional category. A TERM is the citation form of a language-specific instance of that phenomenon. The same tag, for example, KN (kin term) can be used with many different language-specific terms, such as KN_mom, KN_mother, KN_dad, KN_spouse. The same TERM (i.e. linguistic form) may also sometimes appear in different tagged categories, as in: KN_boy, GN_boy, GKN_boy where the noun boy might be used as a kinship term, a generic term (GN) or a term that is ambiguous as to whether it is kin or generic (GKN), or in a possessed NP like our boy, the kinship status would be clear and we would use the label PKN_boy.1pl for a possessed kinship term. This reflects the widespread many-to-many mapping between function and form in language.

Multi-CAST: Multilingual corpus of annotated spoken texts

Our third example of an annotated cross-linguistic corpus is Multi-CAST which stands for multilingual corpus of annotated spoken texts

Corpus annotation

Corpus annotation 130 of each utterance. On the 'grammatical word' tier the text is divided into grammatical words which is essentially a step of tokenisation. In the Vera'a example you can see that it includes the separation of clitics from their host word forms. It is also on this level that clause boundaries are inserted that mark the beginning of clauses, represented by '##' or '#' , depending on the type of clause. The grammatical word tier forms the basis for all further annotation, that is, morphological glossing, GRAID and RefIND which are all successively symbolically associated. This is important because it enables systematic analysis of annotations across these layers. The free translation is presented at the bottom here, but structurally it depends on the utterance tier. Given this structure, the transcriptions entered in ELAN exhibit explicitly marked up structure in the XML output that can then be imported into other platforms and also is further analysed based on a more common vertical format, as will be shown shortly below.

Turning now to the content of the annotations, the main part of GRAID annotations targets the form, function, and semantic properties of syntactic constituents, in particular, verbal arguments. These are constituents that typically enter syntactic relations like subject, object, etc. GRAID expressions are three-barrelled, with the generalised form as given in (

The first position registers the form of a linguistic expression, for example, <pro> for 'pronoun' or <v> for 'verb' . The final position registers the function, for example, <pred> for 'predicate' or <s> and <a> for typological categories that denote the subject of intransitive and transitive clauses, respectively. The function position is separated by a colon <:>, hence yielding, for example, <v:pred> for a verb form with predicative function. The second, medial position is reserved for semantic properties of the expression, for example, <1> for first person or <h> for human reference. Since first and second person are taken to entail human reference (only humans can speak or think), the human/non-human distinction applies only to third person where 'human' is overtly annotated as <h>. Semantic feature annotation is separated from the form slot by a <.>. The absence of an annotation in the second slot is read as 'non-human' . For instance, <np:p> is a NP in P function with a non-human referent. Note that this convention is quite parallel to what we observed in tagsets where an element in a particular position of a tag contrasts with absence of any symbol. An important aspect of form annotations in GRAID is that it includes zero (covert) forms. GRAID differs from other syntactic annotation system like the Treebank II annotations discussed in 7.2.4 in that zeroes are confined to instances where they contrast with a possible overt form, so that, for example, notional subjects of infinitive clause constructions in English do not receive a regular zero annotation. Moreover, zeroes have to have a specific referent that is retrievable from the discourse context.

Appended to the layer of GRAID annotations are the two layers of RefIND annotations. The first one of these is a layer of referent indices quote similar to those in the UCREL scheme discussed above. Note that while the annotation as such seems simplistic, the operationalisation of underlying categories is particularly complex in this area. For instance, it can be quite difficult to decide whether a particular nominal construction does really have a referent so that a new index needs to be used.

The second layer of RefIND annotations is a rough classification of new referents, distinguishing, for example, <new> from <bridging> corresponding roughly to a distinction between 'brand-new' and 'evoked/implied' referents. These latter annotations are applied only to the first mentions of a referent where they are introduced into the discourse. Now let's have a look at a set of examples that illustrate the annotation practice and the rationale behind the system. Examples (7.13) -(

(

Corpus annotation

Corpus annotation 132

In all three examples you also find the referent indices that pick up each referent at their successive mentions.

The GRAID annotations alone enable a number of corpus queries on the annotations as presented here, and as they appear in the ELAN files. For instance, you can determine the number of all instances of annotations with A function annotations and then only those with pronoun and A function annotations to then calculate the proportion of pronouns within the A function. You could then compare this proportion to that in the P function. And you can basically do the same thing with any other property, for example, humanness, instead of the form of expression. These and similar considerations have been at the centre of attention in some research within linguistic typology, and we will explain more about all this in Chapter 11. The crucial feature of these annotations is that they are applied in the same way across languages, and they look exactly the same for what are the same categories in a typological perspective. That is to say that annotators of any given language corpus are asked to annotate structures in a typological comparative perspective. For instance, the form gloss <pro> is understood here to also include pronominally used demonstratives and not just all members of the pronoun form class in a given language. These annotations thus abstract away from the language-specific structures that morphological glossing and most PoS-tagging capture. As such, it is much more straightforward for users not familiar with each language to cross-linguistically analyse patterns like the ones just outlined. Combining the levels of GRAID and RefIND annotations will open up further possibilities: for instance, we can search for all instances where an annotation appears on the RefIND tier combined with a search for a syntactic function on the GRAID tier to get all functions in which a discourse referent is introduced. This question is relevant in some areas of typology, as will be explained in Chapter 11.

Exercise 7.3 Multi-CAST searches and quantification

Download at least two corpora from Multi-CAST (after you have installed the free software ELAN) and design a number of queries, using regex (cf. 5.11) to determine:

(1) What is the proportion of zero arguments within all "core" argument functions S, A, P?

(2) How do the different functions differ?

(3) What cross-linguistic differences do you observe? Taking a closer look at the clause structures by going back to the corpus data, try to develop some ideas as to what may potentially explain these differences, or at least be part of the story. structure. This means, for instance, that for each referent index we can calculate the distance to its antecedent mention by finding that instance and determining either the number of hashes (which represent clause boundaries) which equals number of clauses or the number of word forms, that is, all lines excluding those for hashes. In the same way, the form and function of the antecedent can be determined. All these are relevant for an analysis of referential choice (see 8.4). But given that all this can be readily done across languages, this system in fact enables large-scale cross-linguistic research in this area, as is done in

You may realise that this system of annotation is easier to handle for annotators than a system where various aspects of anaphoric relations and the like have to be entered during annotations; for example, for many instances of given referents addition of the relevant index is pretty easy and therefore, very quick. At the same time, the system is also maximally flexible and versatile, so that we can analyse, for example, anaphoric distance in terms of different units of measurement.

CONCLUSION

The purpose of annotations is to provide extra value to corpora. They allow for queries and calculations that would otherwise be difficult or impossible to obtain. As we saw above, a myriad of annotation schemes exist and they can range from very general applicability to appropriate for ultra-specific research questions.

Exercise 7.4 Annotation scheme effort

Design your own annotation scheme for a particular problem that is interesting to you, such as tracking contraction (or not) or positive or negative affect of adjectives. Brainstorm what needs to be included to capture the main aspects of this problem?

Find a short text and trial your system. To start, you can use a simple tabular format:

Tokens (words, morphs, sentences, etc.)

Aspect 1 Aspect 2 Aspect 3 Etc…

Token 1

Token 2

Token 3

What adjustments are needed?

Now trade annotation schemas with a classmate or trial it on a different language or text type. Do new kinds of decisions need to be made?

What aspects of this activity were the most difficult or time consuming?

Corpus annotation 135 FURTHER READING

NOTES 8.1 INTRODUCTION

Corpus linguistics, at its heart, involves counting things. This can be very simple, like counting all the words in a corpus or a specific word within a corpus, but can also become very complicated. For instance, counting the number of times something happens under various conditions and then seeing if that is different from what we would expect if the distribution was completely random. This chapter will focus on a few kinds of statistical analyses that are often used in corpus linguistics and will try to get you in a position to turn a research question into something specific and testable. This chapter will explain some of the most frequently used quantitative techniques in corpus linguistics today, such as logistic mixed-effects regression, as well as focus on some newer techniques becoming more popular, such as classification trees (and random forests) which are types of recursive partitioning. We also present some basic types of clustering.

We present some means of displaying frequency distributions in corpora before moving on to inferential statistics. There, we concentrate on our strength: variationist corpus linguistics. There are many means to evaluate corpus-level variation (such as Kullback-Leibler divergence) in order to compare corpora with each other. However, our work focuses more on issues of linguistic analysis: when would you use structure X versus structure Y? Based on what aspects of the context? Corpora are extremely useful for these kinds of questions, as are quantitative approaches. Below we focus on some of the main means to assess data to help answer these questions. In principle, we are interested in what humans do with language (cf. 2.3). Often, we are interested in a more specific population, such as American English-speaking 2-5-year-old children or speakers of Matukar Panau (cf.

However, we never have information from an entire population. What we have are samples. We have a sample, for instance, of 30 Matukar Panau speakers, speaking for a total of 40 hours in a specific setting. We can describe characteristics of that sample, such as the frequency with which people use serial verb constructions. We can also infer characteristics about the whole Matukar Panau speaking population in all instances of speech from that sample, that is, how frequently all speakers under all circumstances will use serial verb constructions. However, we are also aware, when we do this, that we cannot be certain about the population, only about the sample.

There are different kinds of sampling methods: random sampling, representative sampling, convenience sampling. Based on how we conduct research projects, sampling may be more or less representative (equal amounts of ages, genders, clan membership, etc.). In corpus linguistics, sampling is rarely truly random (a random person from a population, or a random text from all existing texts), and is more often a convenience sample (who we can record, what we can find on the web, which newspapers we have access to, etc.) (cf. Chapters 6 and 10).

We sample to estimate parameters. Parameters are values that characterise an entire population and statistics are estimates of those parameters within a specific sample.

Dependent and independent variables

In statistics, the term variable has a specialised meaning. It is the property of something that can be classified or measured.

Numerical aka Quantitative:

Interval -ordered variable with equal intervals, lacking a zero point or having an arbitrary zero point. 1 We can add and subtract interval variables but not multiply or divide them. Interval variables are uncommon in corpus linguistics.

Ratio -ordered variable that includes zero and what we normally use when we are working with numerical variables. Zero is meaningful for ratio variables (zero token counts of parsimonious in a corpus signal an absence of that word in the corpus).

For numerical variables we can also distinguish between continuous and discrete. Continuous variables have meaningful points between all numbers, like word duration in milliseconds. Usually, continuous variables will have decimal points. Discrete variables have measurements that cannot be divided, like token counts or word lengths in characters.

Distributions

Statistical tests have 'assumptions' about the data they are used to evaluate. Parametric tests need to meet assumptions like following a normal distribution and independence. Otherwise non-parametric tests need to be used. If you use a statistical test on data that is not meeting the assumptions about the data, it might work, but your evaluation of that data will be misleading. Before carrying out statistical tests, you need to check the assumptions of the test, check your data, and make sure they match.

Normal distribution

If you have heard of any distribution it will likely be the normal distribution. This is also called the bell curve in less technical terms and a Gaussian distribution in more technical terms. Data that follows the normal distribution has many data points in the middle and fewer data points at the lower and higher end of whatever scale is being used. It is often used when talking about scores on tests: Few students will fail, few students will excel, and most will do fine or they will be "average". This use of "average" to mean acceptable, but not great, comes from the fact that most people/scores/whatever will be in the middle of a distribution and so will reflect the mathematical mean. Corpus data are rarely normally distributed though.

Exercise 8.1 Variable types

Generate your own list of appropriate variables. Brainstorm nominal, continuous, discrete variables you might really use for a corpus study. List 5 IVs and 5 DVs for each variable type. Can you also think of some ordinal or interval IVs or DVs that might be used in corpus research?

The spread of a distribution is how far away the lowest and highest numbers on the scale are from the mean. Spreads can be narrow (maybe scores on an assignment range from 70 to 95) or wide (maybe scores on a final exam range from 0 to 100). We use standard deviations (SDs) to help us understand the average distance from the mean (i.e. how many SDs a data point is away from the mean). Larger SDs are associated with wider spreads of a distribution and vice-versa. Sometimes data points that fall too many deviations away from the mean (higher or lower) are thrown out of analyses as outliers because they are too different from the mean. This can mean that they disproportionately affect analyses, giving us something we cannot trust, or it may mean that they were spurious data points in the first place (Did a student in fact not sit for the test, or did he completely misunderstand the assignment? Did something go wrong with the corpus-building process and result in something funky that needs to be thrown out?). Checking your outliers can be a good sanity check to make sure everything is going as expected. Outliers can also indicate that there is something really interesting going on with your data, which is another good reason to check them.

The parameters of a normal distribution are its mean and standard deviation.

A handy way to understand distributions is by plotting them using histograms. Histograms are a special kind of bar plot that display counts of continuous variable measurements (the figures in Section 5.4.1 on frequency were bar plots but not histograms because they were counting categorical data). The data in histograms are usually binned, and each bar represents a bin of data that falls within that range. The bin width (range) can be changed depending on what makes sense for your data: do you need a bin width of 3 for data with a range of 1-12? Or a bin width of 10 for data with a range of 1-100? Figure

Poisson Distribution

Poisson distributions are a common distribution of data seen in corpus linguistics. This distribution has to do with the average rate of occurrence in a population. We expect higher rates (or counts of something like a type of word) when there is more opportunity to observe the event being counted (i.e., longer texts have more words).

The assumption of the Poisson process is that we know the average time between events, but not their exact timing (i.e., we have an expectation of how many verbs should occur in a text, but not whether they are evenly distributed or more at the end, or beginning, or occur close together in chunks, etc.). Further assumptions of the Poisson process are that events are independent of each other, do not occur at the same time, and that the variance is equal to the mean and the average rate is constant.

The parameter of the Poisson distribution is lambda (λ): the rate of change

You can think of lambda as the expected number of events in a specific interval. Do we expect 800 or 80 verbs in a 1,000-word text? Some recent corpus linguistic questions using a Poisson distribution are by

Other kinds of distributions you may encounter include Gamma, Beta, and Bernoulli.

Range and spread

An important measure of spread is range (defined in Chapter 2.2.3). For more detail, we can turn to quantiles, a common way of breaking up distributions into ordered chunks, often four chunks, each of which are called a quartile. The first quartile is made up of 0-25% of the scale, the second is made up of 25-50%, etc. The interquartile range (IQR) is the difference between the 75th and 25th percentiles of the data, giving us the range of the middle of the distribution. This is an additional way to understand the spread of the distribution, like with the SD of a distribution.

Boxplots are a common graphical way of depicting the spread of the data and use the IQR. The data that falls within the IQR will be in the box in the centre of the plot. The horizontal line across the box indicates the median. The so-called whiskers (made up of the dotted vertical lines and solid horizontal lines that go out from the box) mark the 1.5 IQR above the third quartile and below the first quartile. In a normal distribution, the whiskers will be of the same length; if they have different lengths then it is a sign that the distribution is skewed.

Skewness is the measure of asymmetry in a distribution. The skewness of a normal distribution is near zero. When plotted, a skewed distribution will appear to be squished on the left or right of zero.

In Figure

Null hypothesis

The null hypothesis is a starting point for many types of hypothesis testing (aka confirmatory) statistics: it states that there is no difference between either samples or characteristics of a population. We then test this hypothesis against our data and usually hope to disprove it. If we find that there is a significant difference in, say, pronoun usage in newspaper versus narrative texts, then we reject the null hypothesis (that pronoun usage is the same) and then can confirm an alternative hypothesis: that there are proportionally more pronouns in, say, narrative texts.

Chi-squared test

The chi-squared test is a test of independence between categorical variables. In doing a chi-squared test, you are evaluating whether the number of observed frequencies of something is significantly different, that is independent, from expected frequencies if the null hypothesis were true. You can use a chi-squared test if you have a random sample from the population of interest, these observations are independent, you have a minimum of five observations, and observations fall into clear categories (these are the assumptions of the chi-squared test that need to be met; the minimum requirement of 5+ observations is a rule of thumb for robustness sake). You would use a chi-squared test if you have two or more groups of observations that you can put into two (or more) categories. Usually, this is the kind of information you would put in a table of counts. As an example, let's look at counts of Matukar Panau morphemes. The main word classes in Matukar Panau are nouns and verbs. From a parsed and glossed sub-corpus 2 of 8,961 words, verbs and nouns make up 5,375 word tokens. Table

Statistical description and analysis

Statistical description and analysis 144

Take for example the same dataset, but also including the next two most frequently observed word classes, pronouns, and adverbs and classifying them as monomorphemic or multimorphemic. Our counts are in Table

Pearson's Chi-squared test results for Table

Finally, it is worth noting that you can certainly run chi-squared tests on groups with more than two categories, just like you can run this test on data with more than two groups.

Correlations

Correlations are measures of two quantitative measures, rather than categorical ones. They show the degree of linear association between the variables. 5 Positive correlations are when two variables increase with each other: as a child' s age increases, their height increases as well. Negative correlations are when one variable increases as another variable decreases: the hotter it is outside, the fewer the layers of clothing you may wear. If these measures increase or decrease together, the correlation will be strong, and closer to 1 or -1. If measures have no real relation to each other, the correlation will be closer to 0: the temperature outside has no relation to a child' s height.

There are different statistical tests to measure correlations that are more or less appropriate for your data. A very common measure of correlation is r, or Pearson's product-moment correlation coefficient. It is a parametric test, meaning it is appropriate for normally distributed data. Kendall's tau and Spearman's rho are appropriate for non-normally distributed data (so be sure to check your data distribution!). Plotting the data is a helpful way to see the relationship between the variables. In

Statistical description and analysis

Statistical description and analysis 146

It is helpful to know the relationship between variables in your corpus generally. When conducting certain kinds of multivariate predictive analyses (cf. 8.4), it is important to make sure that predictor variables are not too strongly correlated with one another. In these cases, they are labelled as collinear (they show multicollinearity) and it means that they predict each other (as well as/instead of) the dependent variable. This makes estimating the fit of generalised regression models mathematically difficult because it is difficult to correctly attribute the contribution of each predictor when they are similar. Likewise, theoretically, it does not make sense to do so. Why would you include both a child's age and their school year as predictors of their vocabulary size? One might predict vocabulary size better than the other, but including both is probably superfluous. Sometimes you do want to test collinear variables at the same time, and then you are better off with something like recursive partitioning than logistic regression models. We look at both of these kinds of multivariate approaches in Section 8.4.

MULTIVARIATE PREDICTIVE APPROACHES

One of the main advantages of using a corpus is that it gives you a lot of information about the data you are examining. You have rich information about the context and often will have information about the speaker as well. You are at a disadvantage if you do not take this information into account when doing statistical tests. Therefore, much of the statistics we see for corpus linguistic data are multivariate rather than univariate as in Section 8.3. Multivariate means that multiple IVs are being assessed for their contribution to a single DV (there are ways of assessing multiple DVs simultaneously, but we won't be covering them here). We will discuss here one very common kind of multivariate statistical model called mixed-effects multiple regression, which has various implementations depending on the kind of DV you are examining. We will completely skip non-mixed-effects regression (fixed-effects only) because it is almost never appropriate for corpus data. If you are doing multiple regression, you are capable of and should be doing mixed-effects multiple regression. We will also discuss a newer method that is becoming popular in corpus linguistics and linguistics more widely, called recursive partitioning, which includes analyses like classification trees and random forests. Finally, we will take a look at clustering.

To illustrate both regression and recursive partitioning methods, we take as an example one linguistic problem that we, the authors, have looked at together, namely, what predicts the form of subjects and objects in the Oceanic language Exercise 8.

Correlations

Looking at Figure

Are the correlations for each word class positive or negative?

Which word class has the strongest correlation? Vera'a of Vanuatu

Let's take a sample of the factors we looked at and apply them to our data to help understand the statistical methods. Take a look at the articles for more information on the theoretical applications, more factors, and more details! In our list of factors, it should be clear that there are several kinds: inherent properties of the referent (expressed as either pronoun or zero), properties that stem from how the referent is used in a particular clause, and wider contextual properties. A corpus is useful to observe the variation of these properties in constellation with each other and see how each of these independent variables tend to affect our dependent variable. Further, a multivariate analysis is needed to be able to assess the contribution of each independent variable. All of our variables are categorical rather than continuous.

Form of the referent: is the referent a (1) pronoun or (2) a zero. Later on, we'll look at a more complex DV where there is a three-way division between (1) pronoun, (2) zero, (3) lexical NP for a portion of our data. For now, the table below shows our token numbers for a 2-way DV. Function of the antecedent (same or different): Antecedents are any reference to the same referent before the reference being looked at. So, if our token is she, in the sentence the cat jumped on the couch and then she went to sleep, and has the same referent as the cat, the antecedent of she is the cat. In our example, both the cat and she both have S function. But antecedents can also have a different function, as in this example, where the cat has the function P: I fed the cat and then she went to sleep. In data coding, we kept track of the function of the antecedent, and then created a variable comparing the function of the referent and antecedent so we can assess if having (1) the same or (2) a different function has a bearing on the form of the referent. For this variable, S and A functions of antecedents were combined, in line with Vera' a grammar treating these conjointly as 'subjects' . In looking at the table below, it should be apparent that while there are many more cases of different functions, there are some clear patterns in that the same functions are more likely to result in zero expression of referents and different functions are more likely to result in pronominal referent expression. Distance of the antecedent (aka anaphoric distance): Antecedents may be one clause previous to the referent (as in both of the cat examples above), or further away (I fed the cat and then I put the food away. She went to sleep then). The first time a referent is mentioned, it has no antecedent, and these cases were excluded in the original studies. We collapse all possible distances into two levels: (1) antecedent 1 clause away (

Pronominal referent Zero expression

Pronominal referent Zero expression Total

Pronominal referent Zero expression

Text:

As with speakers, we may expect that differences in the use of the DV across different texts, so we code for the 16 different texts that our tokens come from. Now let's take a look at some specific multivariate methods.

Mixed-effects multiple regression

When you look at journal articles reporting on corpus linguistic studies where researchers are using multiple regression, you will see many ways to describe it. Some of these are different analyses, and some are just different names for the same kinds of analyses. Multiple regression can be 'linear' or 'generalized linear' . Linear regression is appropriate when the dependent variable you are trying to predict is continuous (like the duration of a word in milliseconds) and the data is conditionally normally distributed (normally distributed around its predicted values). In linear regression, we assume that there is a linear relationship between the DV and the IVs: the value of the DV is increased or decreased as there is more of (some level of) an IV. Linear regression can be used when the distribution of the DV is Gaussian (i.e., normal or following a bell curve when plotted).

Also common in corpus linguistics is generalized linear regression. This type of regression has subtypes such as logistic or multinomial and this distinction has to do with the distribution of the data and the type of DV, whether it is numeric or 2-way categorical or 2+-way categorical among other possibilities. We will discuss this a bit more further on. Logistic regression models are easier to fit and easier to interpret than multinomial regression, and are what you will see most commonly in multivariate quantitative corpus linguistics. Sometimes this requires adjusting research questions so that you can operationalise your question as X versus Y rather than A

Statistical description and analysis

versus B versus C versus D and so on. To take the Vera'a example above, our analyses

Another meaningful distinction to be on the lookout for is whether the analysis is labelled as 'mixed-effects' or not. Most analyses of corpus data should, rightly, be labelled and conducted as mixed-effects regression. Not only does multiple regression involve multiple IVs but these IVs are conceptualised as one of two different types: fixed-effect or random-effect. A regression analysis that uses both types is mixedeffects. The main difference between fixed and random effects is that we are interested in the contributions of the different levels of the fixed effects. We expect that the fixed effects would show similar patterns, even if we added new data or applied our model to a different dataset. Random effects reflect a random sample of the population we are interested in, for instance, a particular group of speakers from a population of all speakers, or a handful of texts from all texts that could be produced. A regression model fits a coefficient to each level of the fixed-effects predictor (how much change from the mean does each level reflect). The model fits an SD for the entire set of coefficients associated with different levels of the random-effects predictors.

The random effects predictors are treated as random deflections from the population mean. Mixed-effects models provide better estimates of coefficients associated with the predictors of interest compared to fixed-effects-only logistic regression and help avoid the spurious significance of fixed effects

Each regression model has an intercept. You can think of the intercept as a baseline estimate of the DV that will be affected by adding or changing IVs. For categorical IVs, one level of the IV will be part of the estimate of the intercept (depending on how variables are coded) and this is called the reference level. For continuous IVs, the intercept reflects the estimate if the IV was zero. Sometimes analysts centre their IVs (which tells us how far from the mean each IV datapoint is), and in that case, the intercept reflects the centre value of the IV.

Coefficients (sometimes called the estimate or beta) show how much change from the intercept (or baseline) happens in the prediction of the DV for an IV and are log odds ratios. Log odds ratios, as opposed to odds ratios, are centred around zero (cf.

Levshina 2015:261-263 for more on this). The coefficients will be either positive or negative. In a linear regression model, positive and negative values have clear mappings: IVs or levels of the IV that make the DV smaller (for instance make a word shorter) will be negative, IVs or levels of the IV that make the DV larger will be positive. For logistic regression, positive or negative will reflect either an increase or decrease (respectively) of the probability of the reference level of the outcome variable. With our Vera'a argument expression, positive coefficients show an increase in the probability of the IV being associated with pronoun use.

A standard display of a model output will show the effects of levels of the IV aside from the reference level. 6 This is because the reference level is already part of the intercept estimate.

Let's make this more concrete and apply a mixed-effects logistic regression model to our Vera'a data. Our DV is the form (pronoun vs. zero) of the argument (all S, A, P); our fixed-effects IVs are (1) Animacy, (2) Number, (3) Person, (4) Function/Grammatical Role, (5) Same or Different function of the antecedent, and (6) Distance of the antecedent; and our random-effects are (1) Speaker and (

Here we use the package {lme4}

There is a lot of information in these two tables, so let's take them bit by bit. First, we have Table

Table

Statistical description and analysis

Statistical description and analysis 152 standard errors are really high, then that is a sign that something has gone wrong, like not having enough data to estimate properly or there is no effect.

The p-value represents the chance that the null hypothesis would be true if we observed this sample of data. The 0.002 value of the A function means that there is a 0.02% chance that we would see an association this strong if the null hypothesis were true (meaning we can reject the null hypothesis). A high p-value like 0.823 for the P function, means that this result could easily be obtained by chance. There is a tradition of considering p-values below a certain threshold, like 0.05 or 0.01 to be significantly unlikely to occur by chance, and then these IVs or levels are then deemed "statistically significant". It is important to remember that even if a result is significantly unlikely to be obtained by chance, it is still possible to be randomly obtained, and that statistical significance in a model does not directly translate to real-world importance. The p-values we obtain in statistical modelling are never 0 or below 0. Therefore in the summary table or results, we can report these as being less than 0.01 or 0.001 or so on.

We can interpret our results as showing:

Arguments in Vera'a are significantly more likely to be pronouns when they are animate, in S function, first person and non-singular, with a different antecedent function with an antecedent two or more clauses away.

Alternately:

Arguments in Vera'a are significantly more likely to be zeros when they are inanimate, in A function, singular third person referents, when the referent is mentioned for the first time or the antecedent is only one clause away, and when that antecedent has the same function. Theoretically, this is an interesting result because, first, we see some effects of accessibility and attention as pronouns are more likely when the antecedent is a different function and a few clauses away. Additionally, the P function showed no significant difference from the S function, which is surprising as these are quite different functions. We know from other research that patients -the typical role of P arguments -are often inanimate while agents -the typical role of A arguments -are often animate, so this is something we can explore further with adjustments to our model. One kind of adjustment we could do is include an interaction between two (or more) of the IVs.

Interactions

Interactions are useful in modelling when we want to hypothesise that some of our IVs have a relationship with each other and therefore, a more complicated one with the DV. Say we have two continuous IVs such as transitional probability and word rate and are thinking about their effect on word durations. We know that words that are highly probable in a given context can get shorter, and we know that high speech rates (articulation speed) can also make words shorter. But perhaps when words are highly probably in context AND speech rate is generally high, they get especially short (not just in an additive way). That would mean there is a positive interaction, and we would see this reflected with a positive coefficient for an interaction term in a model with a high standardised score and low p-value. If the effect was additive, then we would see either a positive or negative coefficient but with a low standardised score and p-value that lets us know the result is likely to be by chance. If, however, the coefficient for the interaction term was negative with a high standardised score when the fixed effects were positive, that would let us know that the interaction results in an inverse relationship: words that are both quickly spoken AND highly probable are relatively longer.

For categorical variables, the outcome can be a bit more complicated. Say we have two IVs that both have a negative coefficient like Grammatical Function and Animacy in our example model. We can take a look at a table to see how these variables interact with our DV:

Statistical description and analysis

Statistical description and analysis 154 Table

We include the interaction between animacy and function in our model specification and our new model is summarised below. Like Table

Some other aspects of mixed-effects regression

• Random Slopes -Random intercepts are included in models when we expect (or want to test) that all levels of the random effect reflect different baselines in behaviour that will be similarly affected by the IVs. A random slope, on the other hand, would be included for a random effect where we expect that some variables will have differing effects on its levels. For instance, we might expect different speakers to be affected by the time of day differently: morning people  may be more talkative early in the day and night owls may be less talkative until later on. You would include random intercepts and random slopes for predictors where you expect both the baselines AND the directions or effect sizes to vary (and you have enough data to test this). Like all aspects of modelling, whether or not you include a random slope has to do with the nature of your research question and what you think is going on with your data. For more discussion of random slopes and how necessary (or not) they are, there are some wellknown articles such as

• Linear regression -appropriate in corpus linguistics when you are delving into a problem that should be modelled with a continuous IV: word duration, speech rate, vowel duration, F1 or F2 height for vowels. Note that a continuous DV is not the only thing necessary for linear regression: the data must be conditionally normally distributed (if not, a generalised linear regression such as Poisson regression may be more appropriate). DVs may need to be transformed before modelling including the exclusion of outliers, normalisation, or centring. Also, note that linear regression copes better with collinear predictors than logistic regression. However, you still need to check for multicollinearity between your IVs. Take a look at

• Multinomial regression -appropriate when you are interested in modelling a DV that is categorical with three or more levels. For instance, going back to our Vera'a problem, if we were interested in modelling the outcome of pronouns versus zeros versus lexical NP referent expression, we might choose to use multinomial regression. Multinomial regression can be complicated to interpret and often quite large datasets are required in order for models to converge (fit the data and give a result) (cf.

Statistical description and analysis

Statistical description and analysis 156

Recursive partitioning

Recursive partitioning is about dividing up data (partitioning it) over and over (recursively) to create sub-groups of data that are similar to each other. In seeing what divisions divide the data, we get insight into what kinds of meaningful distinctions can be used to classify our data into different sub-groups. Recursive partitioning as a methodology started in the medical sciences testing the impact of pieces of DNA sequences on illness, but has been adapted to many other research areas including corpus linguistics

Trees

Recursive partitioning can be applied to a continuous DV, and in these cases it is called a regression tree. The tree will bin the data into sections of the DV that are most similar to each other. A categorical DV will use a classification tree and be implemented for a 2-way or 3+-way DV in a reasonably straightforward way. Binary classification trees use binary splits to classify data (data goes into either this group or that) and multiclass (or multinomial) classification trees will use multiple-level splits to classify the data. Binary classification is much more common and what we will focus on here, just be aware that other options exist. The important thing to remember is that for classification trees, the term binary is used for the splits made at each decision stage, not for the structure of the DV.

A binary classification tree divides the data into two groups based on which data points are most different from each other, using the given variables. The analysis determines which IVs, at which level(s), makes the best differentiation of the data's dependent variable. For each node (or sub-group), the data is then split into two more sections based on which data points are most different from each other using the remaining independent variables and remaining levels of independent variables. Because each set of data under a node is looked at anew, the same variable can be used again in a lower level of the tree, with whatever levels are still relevant for that node. This feature of the classification makes it useful to explore non-monotonic (non-linear) relationships between the IV and DV, and predictors that impact only a portion of the data. This splitting (division of the data) continues until a stopping criterion is reached, at which point the data in each node should be relatively homogenous. Some implementations of classification trees, such as those produced by the {party}

The algorithm for creating the tree model will not necessarily use all the IVs available as part of the model specification. If there are IVs that would not make a significant split in the data, they go unused. This makes recursive partitioning a reasonable way to explore whether or not some predictors are influential or not for your data, although this needs to be done with some care.

Let' s apply a binary classification tree to our Vera' a data, excluding here Speaker and Text because their many levels may cause too many splits in the tree, making it difficult to read. We put in a criterion for our stopping threshold at 0.99 and we also specify that the maximum depth of the splitting should be 3, that is the tree should stop splitting (if there continue to be significant differences) after three splits. We do this to keep the tree from splitting very small amounts of data and to keep the visualisation reasonable to read. 8  Our tree is more conservative in what variables it includes: Animacy, Function, Number, Person. However, by adjusting the depth, we have to be careful about how we talk about variables that are not included since our specification removed some IVs that would have been in place if we had not made this adjustment (for us, Antecedent Distance and Antecedent Function). There are many different parameters one can adjust (in the {party} implementation or others), so it is always important to read the documentation and make reasonable choices that reflect your research question and data.

When describing decision trees in text, you can refer to the node numbers (found in our figure at the top of each section/bin of the data, followed by the number of tokens in that group) and the split numbers (the numbers at the top of the circles). The nodes show the proportion of zero or pronominal expression for the final partitions of the data. We would describe our tree as follows:

This classification tree has five splits and five nodes. Figure

The right side of the tree shows three splits for the animate referent data. Split 5 and split 9 are splits in the person data. Speech act participants (the first and second person) are significantly more likely to be expressed by a pronoun, and the first person even more so, as seen in node 10. We see that the third person referents are further

Statistical description and analysis

Statistical description and analysis 158 split into singular and non-singular referents under split 6, with non-singular referents significantly more likely to be expressed as pronouns.

Other predictors (Antecedent Distance and Antecedent Function) do not appear in the tree in Figure

We can also take a look at our data with a 3-way DV, bringing back lexical NPs. We will not go through the description of the tree here but include the plot to show that it is relatively easy to create a tree for a multiple-level categorical DV. In the Figure

Forests

In a random forest analysis, many classification trees are computed based on different subsections of the data and subsets of IVs, creating a "forest" of trees. We do not look at the whole forest tree-by-tree, rather we average across them to get a sense of our data. When many classification trees are averaged, factors can be ranked by their importance, determined by which factors most often make a significant split in the data, especially at higher levels in the tree. The analysis resamples across subsections of the data, and uses different samples of the IVs (here four out of eight possible ones) to increase the variation in the possible trees. Slightly different data subsets

Statistical description and analysis

Statistical description and analysis 160 and variable combinations will result in potentially different variables performing well (sometimes animacy of the referent, sometimes person, etc.); one will be the best factor most often for predicting the outcome of the DV. This one factor will be ranked higher than the others in a variable importance ranking. A conditional permutation of the variable importance ranking, although computationally costly, ensures that the evaluation of a variable's importance takes into consideration its behaviour in relation to other variables in its ranking

The forest in Figure

Clustering methods

Clustering is another quantitative method that is found in corpus linguistics reasonably often. Clustering is about grouping together data points that are more similar to each other, as distinct from less similar data points. Good clusters have low intracluster distance and higher intercluster distance. Different kinds of data and questions require different kinds

Statistical description and analysis

Statistical description and analysis 162 of clustering, but methods can calculate the appropriate number of clusters, how data is grouped, and then gives some kind of visualisation of the data. As an example, we show clustering (Figures 8.8 and 8.9) of a set of six languages by how their benefactive construction is used in multilingual SCOPIC (cf. Chapters 7 and 11).

We will not focus on clustering here, but we list below some of the main clustering types and some resources to find out more about clustering corpus data.

• Hierarchical clustering: builds a tree-like structure, putting together leaves (objects/languages/tokens/groups) that are most similar based on a similarity or dissimilarity matrix of numeric data

• k-means: determines the number of clusters appropriate for the data based on a dissimilarity matrix of numeric data, with a theoretical central point for each cluster, based on a dissimilarity matrix. Data points in the cluster have the least Euclidean distance from that centroid as opposed to other centroids

• k-medoids: similar to k-means, but with a medoid (a data point is selected as the central member of a category) rather than a (theoretical) centroid

• Correspondence Analysis and Multiple Correspondence Analysis: used for categorical, non-numeric dependent variables, plots dependent variables, as well as independent variables associations measured as chi-square distances.

Useful to group data that show similar profiles of use (cf. example in 4.2.1)

Refer to the further readings section for more resources on clustering.

MAKING STATISTICAL CLAIMS

Not everyone worries about the details of statistical analyses, and when some people read through statistical analysis, they often skip over a lot of the details and just look at the p-values. Despite this, reporting only a p-value is not sufficient if you want to accurately report your results. Minimally, you need to provide the test statistic (e.g. r or χ 2 ) and the p-value if there is one, degrees of freedom if possible, as well as the amounts of tokens you looked at (n); you can enhance this by reporting some confidence intervals or error rates, so people can evaluate the test statistics themselves. When reporting multivariate analyses, it often makes sense to present data in a table, as well as in a paragraph format to guide people in interpreting the results. When stating an outcome, include the statistic in addition to stating that something is significant. Statistical abbreviations and indicators should be italicised. Here are some examples:

High following transitional probability (β = -0.25; z = 5.63; p < 0.001) and high Preceding Joint Probability (β = -1.66; z = 39.65; p < 0.001) are associated with lower levels of contraction, meaning the less probable the particular context, the less likely contraction is to occur.

̅ ̅

When subjects are humans, they are more likely to be expressed with a pronoun than when they are non-human animates (β = 0.58, p < 0.05).

Statistical description and analysis 163

CONCLUSION

Corpus linguistics can use a variety of statistical methods, from very simple to very complex. Understanding more about why and how these methods are used will help you to better understand and conduct studies. See below for more resources to help you get started.

FURTHER READING

For information on a range of descriptive and predictive statistics, we recommend looking at

NOTES

1. Temperature in Celsius or Fahrenheit is an interval variable, as 0˚ does not mean no temperature so it is arbitrary in this sense. 2. Available at:

Informally you can imagine for Table

A BRIEF INTRODUCTION TO SOCIOLINGUISTICS

Overlap in studies of variation

Sociolinguistics and corpus linguists have many of the same goals. Both are fields that are interested in people's real usage of language and how it is affected by context. And both seek explanations for observed patterns and variations of usage, acknowledging that both internal and external context play a role. The main difference is that in sociolinguistics, there is a stronger focus on the social factors that condition linguistic behaviour. The most important ones are the characteristics of the language user and their community (age, gender, sexual orientation, class, region, etc.) in conjunction with the communicative situation. Sociolinguistics is a subfield of linguistics where corpus studies are a possible methodology, alongside surveys and experiments and more. The most common kind of data source for variationist sociolinguistics is the sociolinguistic interview, which often includes having participants read a list of words and passages of text as well as a conversational component

Here are some other differences in the foci between sociolinguistic corpus research and other kinds of corpus linguistics:

(1) Generational change: Sociolinguistics often has a focus on change over time. as opposed to much of corpus linguistics which instead focuses on synchronic language use or use within a particular period. Sociolinguistics often focuses on a shorter period of time than historical linguistics. One of the main reasons for this is that the varieties that many sociolinguists are interested in do not have long historical records. Additionally, much of sociolinguistics focuses on phonetic variation, and audio and audio-visual records of language varieties do not go far back into history. Finally, much of our older historical records of language use do not have the kind of detailed metadata required to carefully study communities. The ' Apparent Time Construct' (cf.

(2) Style shifting and non-standard language: Sociolinguists are interested in 'style shifting': the change in language behaviour when language users find themselves in different social situations. Of particular relevance are those situations that speakers find themselves in most typically in their daily lives where they are assumed to use 'vernacular' speech. Other kinds of corpus linguistics are interested in variation between many kinds of register and genre, and often in the differences between spoken and written modes. There is a motivating reason for the focus on the vernacular in sociolinguistics. The idea in sociolinguistics is that vernacular speech will show the most systematic variation. Sociolinguists will often compare the vowel sounds in various stages of a sociolinguistic interview, but taking that part when an interviewee is speaking about something emotional or exciting, as being the most 'real' kind of style. Speakers 'style shift' when they move from one style to another, and are more influenced by prescriptive rules and what they judge to be prestigious in some registers. To this end, many corpora used in sociolinguistics are made up entirely of the vernacular free speech portion of a sociolinguistic interview (similar considerations are relevant in documentary linguists, cf. Chapter 10). This is also because longer stretches of language use occur in this portion of interviews, meaning researchers have a better context to study their targets of interest.

Sociolinguistics also has a long history of working with speakers of non-prestige dialects (especiallyof English). This means that sociolinguists see, understand, and sometimes feel the consequences of racism, xenophobia, sexism, and difficulties stemming from socio-economic disparities. Some sociolinguists, then, make it a part of their research to make more people aware of these perceptions and their consequences and fight against stigmatisations (cf.

(3) Socio-phonetic variation: Sociolinguistics has a long tradition of looking at phonetic variation whereas much of other kinds of corpus linguistics is concerned with lexical, morphological, or syntactic topics. The latter of these are

Corpora in sociolinguistics

Corpora in sociolinguistics 166 more readily accessible in a broad range of available corpus data, including written text corpora. Unless a spoken text corpus has been phonetically annotated (cf. 4.2.2 and 7.2.1), corpus linguists are often relegated to researching topics that can be represented by strings of characters, which are easily read by a computer. However, much of sociolinguistics has focused on phonetic variables, especially vowels, and on non-standard varieties of languages, neither are well represented in mainstream corpora which often include mainly written text in the standard variety of a language. And even where transcribed spoken texts have been included in a corpus it can be difficult to assess the degree of standardisation (cf.

The focus on vowels in sociolinguistics is motivated by their specific characteristics: vowels are continuous and lack hard breaks between one another. Speakers may thus produce a vowel a bit differently each time they pronounce it, and listeners will mostly still understand it as the intended sound. The variation in production from each person can build up to variability within a population (or community). Vowels, then, can change quite quickly over time. If you listen to speech from an elder (or young person) of your community, you might notice some words are pronounced differently than your own. Sounds, especially vowels, can also vary quite drastically across different regions and countries, even within a single language. This makes sounds a hotbed of variability that can be measured (by formant frequency, among other measurements) which makes for interesting studies.

(4) Sociolinguistic fieldwork: Like documentary linguists (Chapter 10), many sociolinguists collect their corpus data themselves during fieldwork in speech communities, with whom they tend to become very familiar. This means different kinds of questions are asked for the data and that many explanations for patterns of language use come from understanding the communities, people, and personalities the linguist is working with. Sociolinguists often want to situate language in its social and interpersonal contexts, and so much more information is needed about specific people and their relationships to and within their communities (cf.

Variables in sociolinguistics

In sociolinguistics, a variety of language can be an accent, style, dialect, language, or a specific kind of language used for study. From decades of sociolinguistic research, we know that groups of people use a constellation of linguistic features (what sociolinguists call the variants of linguistic variables). Some people will use one variant of a linguistic variable more often, others will use another. Some of these differences are at the level of consciousness and others are below. Some variants are also strongly associated with some groups as identifying features. In sociolinguistics, variants are discussed as 'stereotypes' , 'markers' , or 'indicators' following a tradition set out by William

Stereotypes are linguistic features that many people will consciously associate with a group of language users, although they may not be accurate. For instance, people may associate African American Language (sometimes called African American English, African American Vernacular English or Black English) with copula deletion: He my brother. Or people may associate young women's speech with a creaky voice. Or people may associate Australian English with a lot of lexical shortenings like arvo (afternoon), barbie (barbeque), poli (politician), rego (registration), and tradie (tradesman). Markers and indicators are features that language users of a variety (and non-users of that variety) are not aware of but help distinguish a particular variety.

Markers are variables that language users have a subconscious awareness of, which is demonstrated through the increased or decreased usage of that variable in more informal versus formal varieties of speech. For instance, a young professor may use a variant less while lecturing and more while she is at a bar with friends without actively choosing to do so. An indicator is consistently favoured by language users of a variety no matter the social situation they are in. These notions form a continuum, and markers can readily become stereotypes as people become aware of them.

We know from previous research that for many variables, most people do not use one variant exclusively. People use mostly one variant (as with an indicator) or mostly one variant in a particular situation (as with a marker). The distribution is probabilistic. Some people have a higher probability of using one variant over another in a particular context, and that context may have to do with their interlocutor, the situation, the topic of conversation, or more. A corpus approach can be used then, in conjunction with a sociolinguistic question, to describe the distribution of the variants across such external contextual features. To be able to assess these factors properly, good metadata is needed about the language users as well as good metadata or annotations about each specific context of use.

DIALECT AND REGIONAL VARIATION

For over 100 years, researchers have been interested in how people from different regions speak, where the linguistic boundaries of these regions are, and what variants -so-called isoglosses -characterise linguistic regions. A variety associated with a particular region in this way is called a (regional) dialect. This kind of work was done with questionnaire-based surveys and short interviews in different locations, often to create atlases of dialects.

Many corpora used in sociolinguistics are comprised of sociolinguistic interviews with speakers of a particular variety. For instance, the West Virginia Corpus of English in Appalachia includes word lists, reading passages, and casual conversations from 67 speakers, who are of different ages, sexes, regions, family backgrounds, occupations, and orientations to social institutions (cf.

Corpora in sociolinguistics

Corpora in sociolinguistics 168 section, we will go through three approaches to the question of regional dialect variation. The first is an approach coming from corpus linguistics towards sociolinguistics, comparing American and British English uses of run, using established corpora

Cross-dialectal variation in large corpora

In an attempt to fathom the differences between two global varieties of English, Glynn (2014) reproduces

The most frequent meaning of run Glynn finds is 'fast pedestrian motion' just as with

Dialectal variation in corpora of sociolinguistic interviews

Regional sociolinguistic corpus research often focuses on the features that are associated with a specific dialect. Like any linguistic community, a particular regional community is not a monolith and we expect variation according to age, gender, education levels, attitudes, etc. Further, a feature that may be strongly associated with a particular community can still have predictable variation within that community.

-no notable difference between genders or age groups -no notable difference between speakers of different social classes for this variable, although in the same corpus there are social class differences in the production of -ing versus -in' in words like walking

Corpora in sociolinguistics

Corpora in sociolinguistics 170 -no notable difference between northern and southern regions of West Virginian Appalachian English

Corpus-based dialectometry

In dialectometry researchers analyse the relationships between linguistic and regional variables, such as longitude and latitude (cf.

Linguists have often described dialect regions using the results of surveys as part of dialect atlases. However, the use of corpora is also possible for dialectometry.

Corpus-based dialectometry requires dialect corpora that are representative of multiple geographic areas associated with a particular language variety. The corpora have to have enough data per locale to compare multiple features across those locales.

The steps of dialectometry are broadly as follows (cf.

(1) Establish the set of features to be investigated and compared, for instance (cf. Szmrecsanyi 2013):

-negative suffix -nae versus particle not (I cannae/cannot do it); -of-genitive versus 's-genitive (my father's home vs. the home of my father);

Exercise 9.1 Reflection

What is the difference in the approach to speaker communities between Glynn (2014) and

-is versus are or was versus were with inversed plural subjects in existential constructions (there is/are/was/were several involved)?

(2) Determine the realisations of the features for each location of interest (3) Compare the features across locations. Create some set of measurements, usually in a matrix form that allows for a calculation of the (dis)similarity between each of the locations sampled. Corpus-based dialectometry will focus on the frequencies of these features (4) Map the results and conduct follow-up analyses to confirm the goodness of the results

Corpus-based dialectometric studies have been conducted for American English

SOCIAL FACTORS OF VARIATION AND CHANGE

Some sociolinguistic variables are conditioned not by area, but by some broader social divisions. Some variants are more prestigious than others and people have different ways to show how they orient themselves to standards and norms by their use (or not) of prestige variants. But prestige is a complex notion. Sociolinguists often write about overt and covert prestige. Variants that are part of a standard or have normative valuations like being 'nicer' , 'better' , and 'correct' have overt prestige, that is, people are aware of the prestige of the variant. Some variants have covert prestige or some quality that people orient to privately or subconsciously

Ethnicity

One of the key areas of research in sociolinguistics is ethnicity. Whether we want it to or not, ethnicity carves up our social space in some real ways. In sociolinguistics of American English, a great deal of research has shown that there are clear differences

Exercise 9.2 Prestige type

In a corpus of 15th c. letters,

Corpora in sociolinguistics

Corpora in sociolinguistics 172 in speech characteristics between white Americans and African Americans. It is important to remember when studying ethnicity as a factor affecting linguistic choice, that there is nothing deterministic about ethnicity (or any other social factors) (not all African Americans speak African American English (AAE)). And there are important notions of overt, covert, and local prestige to take into account. Speakers of many non-standard varieties also speak standard varieties and choose to use features of (non-)standard varieties to signal their orientation to different communities and ideals associated with these.

Let' s take a closer look at ethnicity in sociolinguistics in more detail by reviewing a wellstudied syntactic alternation: the dative alternation (cf. 4.2.5), diving into a corpus of African American English from

Like much sociolinguistic corpus-based research, establishing a large enough corpus for the variable of interest is an issue for these authors. They combine spoken interview data from the Sociolinguistic Archive and Analysis Project (SLAAP)

The authors annotated their data by hand for factors including those listed below. Factors that came out as significant in their analysis are in bold.

-whether the recipient was realised by a noun or pronoun (prepositional dative constructions are more likely when the recipient was not realised by a pronoun: He gave the book to the man) -whether the theme was realised by a noun or pronoun (prepositional dative constructions are more likely when the theme was realised by a pronoun: He gave it to the man) -the animacy of both participants -the difference in length between the two expressions (prepositional dative constructions are more likely when the expression for the recipient is longer than that for the theme: He gave the book to the very important man) -age of the speaker (collapsed into the categories of antebellum, older contemporary, younger contemporary) -sex of the speaker -corpus variety (contemporary interviews or historical letters)

The authors then compare their AAE results with the give portion of the Standard American English (SAE) data from

From this the authors conclude that the dative alternation is probabilistically equivalent across the varieties: the same factors affect the data in a similar way. This indicates that the dative alternation is not a sociolinguistic indicator; it is not being used to distinguish the varieties, even subconsciously.

Sex and gender

Like with age and ethnicity, gender is a complex notion and the kinds of linguistic forms associated with different genders may be different from community to community. Studies of major societies, particularly Western and English-speaking communities, have shown that when there is stable variation in a community, women are more likely to use prestige forms more often than men (cf.

Corpora in sociolinguistics

Corpora in sociolinguistics 174 correlations".

Ms is a title for women that does not specify their marital status, like Miss (unmarried) and Mrs (married) do. In this way, it is equivalent to Mr. The introduction of Ms was supposed to eliminate linguistic discrimination by having equivalent female and male terms. However, when societal discrimination exists, linguistic discrimination will continue to exist, although it may change in how it is deployed. Holmes' research shows that there are various attitudes towards the use of the title Ms in New Zealand. Some regard the term as signalling a feminist identity (which may in itself have positive or negative connotations). Others still imbue the term with having something to do with marital status, feeling that it is used by and/or for women who are separated, divorced, widowed, or partnered (in de-facto/common law relationships), and hence fall out of the two traditionally 'normal' and valued categories of being either married or not yet married.

In a study of Australian student usage of Ms,

Holmes sees parallels in the attitudes of university-aged New Zealand women, finding that some women construct a conservative female identity where marital status is an important aspect of this femininity. Other women may see Ms, or any title, as old-fashioned and prefer to use first names without titles.

Interaction of constraints

Like internal factors (cf. 84.1), external factors such as gender and class can interact. Through much sociolinguistic research, it is also clear that many processes of language change come about through women leading change in their communities.

Exercise 9.3 Women in corpora

Just like in the Wellington Corpus of Spoken New Zealand English, in many available English corpora, men are mentioned and discussed much more than women. Why could this be?

As an activity, compare multiple English corpora, such as the COCA or COHA, or look at multiple genres within one corpus. Search for basic terms like 'man' , 'woman' , 'male' , 'female' , pronouns like 'he/him' and 'she/her' and titles like 'Mr' , 'Ms' , 'Miss' , and 'Mrs' .

What is the ratio of the use of male words versus female words? What genres show the strongest differences and why might that be? Until 1994 South African society was governed by a regime of apartheid, a system of legally sanctioned and practised racial discrimination and segregation . People were labelled as part of different 'races' and were segregated from birth, through childhood, education, work, were not allowed to marry and were eventually buried in separate cemeteries. 'Races' included 'Black' people, mostly speakers of Bantu languages, 'Coloured' people of multiple ancestries including people of Khoesan cultures, 'Indian' people who had ancestry from the country of India, and 'White' people who would either be first language English speakers (L1) or second language English speakers (L2) who spoke L1 Afrikaans, another Germanic language closely related to modern Dutch. This complex, discriminatory situation led to five distinct types of English in South Africa. After apartheid collapsed in the 90s, there was a regime shift in South Africa and now the political and much of the economic power is held by Black South Africans (although not all are economically advantaged). An elite group has formed, in part through access to expensive private school education. Private school education means Black students now have exposure to a variety of English that was previously associated with English-speaking Whites.

White South African English (WSAE) is a prestige dialect but is no longer associated with only white South Africans. It is an L1 English variety that many young, middleclass Black South Africans are now acquiring, especially young middle-class Black women. This is now replacing the L2 English variety that was previously spoken by many Black South Africans, Black South African English (BSAE). This L2 variety has a strong influence of Bantu L1 that's spoken in multilingual South Africa.

Among other features, BSAE lacks a schwa [ə] (a central vowel) which WSAE does have. The lack of schwa is a stereotype (in the sociolinguistic sense). It is used purposefully in some advertising campaigns orienting to BSAE, with slogans like "it' s obvyas" with obvyas showing the choice of a full vowel [a] instead of

The primary comparisons were between the female versus male and private-schooled versus non-private-schooled young Black speakers. Conditional inference tree analyses on F1 and F2 of the schwa vowel for different environments (word initial, final and medial) showed that female private-schooled young people use more centralised variants and are most different from their male non-private-schooled counterparts who use more peripheral variants. The relationship between the female non-privateschooled speakers and the male private-schooled speakers is more complex, with some tests showing significant differentiation amongst the groups due to class and others due to gender. Overall, this is a sign that the women are leading this change.

Depictions and attitudes from the media also strengthen the idea that this dialect shift is tied up with both gender and class. When this change is discussed or portrayed in the media, typically women are used as examples of the vowel centralisation. The prestige of WSAE is connected to upward mobility, aspiration, and a sign that speakers of it, particularly female speakers, have education and advantages that were not available to them during the apartheid period. However, there is also some resistance to this change. There is a local prestige of BSAE, with males' dialect variation showing a strong connection to BSAE that has local prestige and stronger connections to African languages that are used by many of the multilingual speakers in the community.

VARIATION AND LANGUAGE CHANGE

Variation and change are explicitly connected in sociolinguistics. Before a change can take place in a community, variation has to exist. Someone in the community will be the first to produce a new variant, thus adding a further variant to a variable, some other people may also start using that variant, and the variation can spread and eventually lead to change in the variety of languages (or not). There are some cases where we can study variables and their changing variants over time and see whether their probability of use has changed (or not). There are some amazing corpora available that have data from different time periods, for example, COHA for American English, which covers texts from between the 1810s to the 2010s (spoken ones as published on TV or in movies from the 1930s). In other circumstances, researchers can use different corpora of the same variety to examine what variants have been used over different periods. These approaches are classified as real time studies. For sociolinguistic studies to be carried out, the corpora also need a fair bit of information about the language users (which is problematic in COHA). These kinds of corpora do exist, but they are rare. An alternative approach, and one that is more commonly used due to the ease of resourcing data, is an apparent time study.

An apparent time study uses data from one time period within a variety of languages, but that includes data from language users of different ages. If older language users are doing something different than younger language users, it is taken to indicate that there has been a change in the linguistic community. Such differences could indicate that older people took up a change that younger people did not. But

Corpora for real time studies

New Zealand English, including interviews of settlers in the 1930s and 1940s describing where they and their parents come from -Origins of New Zealand English (ONZE) -

How do you know how many ages you should look at to determine what kinds of changes (or not) are taking place in a community? Looking at more than two generations shows better that there is an increased frequency of a variant. Looking at just two generations may mean that you are positing a generational change where one does not actually exist. Some variation is stable across time, and may not be a change in progress. Age can be assessed as a continuous variable, or binned, meaning people born within, say, ten years are put together as a group. A continuous variable may be too detailed for some analyses, where we do not expect differences between people born in 1978 and 1982. Estimations of an effect over a continuous variable of age may also be affected if there are only a few people over very different ages (for example, ten 20-year olds, two 25-year olds, and thirty 70-year olds,). If a researcher wants to convincingly show a generational change, they need to show multiple age groups to make sure the change in frequency of a variant carries over from generation to generation.

In some communities, age may also be more sensibly looked at as a cohort rather than as a number

Let's take a look at a study that compares two kinds of age-related changes.

Fruehwald examines a set of variables in this dataset (vowel changes and whether or not speakers use um or uh for filled pauses). He uses a series of GAMs (cf.

Other outcome variables support both generational change and lifespan instability. For instance, the vowel /ey/ as in make and same raises and fronts (assessed by normalised F2-normalised F1), especially for women born after 1950. Earlier generations show no change during the lifespan, but for women in the 1960 and 1980 birth Exercise 9.4 Emic grouping If you were to conduct a sociolinguistic study in your community, how would you emically group language users according to age? What might be some considerations for emic groups of gender, class, and other external factors?

Corpora in sociolinguistics

Corpora in sociolinguistics 180 cohorts, there are lifespan changes moving in the same direction as the community (that is: an increase in raising and fronting as the year of the interview becomes later). Men show generational changes, but not lifespan ones. Fruehwald explains: "a woman born in 1940 would have the same /ey/ whether they were interviewed in 1990 or 2000, but a woman born in 1960 would have a higher and fronter /ey/ in 2000 than in 1990".

Fruehwald takes these careful results not only as support for the Apparent Time model of language change but also an indication that age is a complex variable that is best studied by taking into account the language users' year of birth and the year of interview to assess changes.

What is apparent across these studies is that variables like age, gender, class, and ethnicity are complicated and often interact. Although much of our knowledge of sociolinguistics comes from English-speaking communities, particularly those in the United Kingdom and the United States of America, there is a lot of good sociolinguistic research being done in other communities. In all kinds of communities, sociolinguistic work is best done with careful ethnographic research and with emic considerations of groups of people. Sensible age cohorts are different across cultures, women have different roles and access to (kinds of) power across communities, and non-traditional groupings (such as clan) may play a role in sociolinguistic variation.

CONCLUSION

Corpus linguistics is a central approach in sociolinguistics that is interested in the use of language as dependent on a range of social factors. These are generally captured in corpus linguistics as part of the external contextual features. We have seen how essentially corpus-linguistic investigations can be levelled to address sociolinguistic questions. A major remaining challenge is the lack of sufficiently rich corpora from diverse languages that contain the relevant primary data as well as metadata.

FURTHER READING

Meyerhoff (2015a) is a general introduction to the field of sociolinguistics, and it includes extensive discussion of basic quantitative research, including corpus-based work.

Specific studies on non-English sociolinguistics

Corpora in sociolinguistics 181

General references on sociolinguistics-documentation

NOTES 10.1 LANGUAGE DOCUMENTATION: CAPTURING THE DIVERSITY OF HUMAN LANGUAGES

A major concern in modern linguistics is the study of the many diverse human languages in the world, and linguists know very little about a substantial number of these. Diversity in human languages is huge: linguists estimate that there are approximately 6,500 languages used today. Each language can have distinct varieties associated with particular areas or regions (regional dialects), or a social group (social dialects), or some other extra-linguistic parameter, in the same way that English has its varieties. The task for linguists to grasp the diversity of human languages is immense.

Many languages have not been studied before at all, and often we have no descriptive or analytic work available, like grammars or dictionaries, and also, there are no raw or primary data, that is, no documentation of how the language is used. Language documentation is the area of linguistics that aims at recording the observable use of language in a given society as much as possible, and doing so in as many societies and associated languages around the world as possible. The goal of language documentation is to create a lasting record of languages' use, which crucially includes a corpus of transcribed and translated records gathered during fieldwork (cf.

Modern language documentation began in the late 1980s -beginning 1990s, at least partly as a reaction to the dramatic decrease in the diversity of human languages.

10

Corpus linguistics and language documentation KEY WORDS

Corpus versus Documentation

Documentations as Convenience Samples

DOI: 10.4324/9780429269035-10

Corpus linguistics 183

Quite suddenly linguists became aware of drastic rates of prospective language loss around the world, sparked by a seminal presentation by Eastern German linguist Johannes Bechert in 1987

Researchers expect that close to 6,000 of the 6,500 world's languages will disappear in the next 80 years. The urgency to support the ongoing use of these languages and to ensure there are good records before they are lost forever is tremendous. Documentary linguists want to capture examples of many kinds of language use in context so that if a language dies out, there is a record of how people used it. This explains in part why documentarians' work differs from that of classical descriptive and typological linguists in its primary focus on data collection rather than analysis and comparison, and creating a corpus is part of a documentation project. There is, however, more to the enterprise of language documentation as we will see.

Defining language documentation

Language documentation to a large degree deals with methods of linguistic fieldwork: documentary linguists go out and spend extensive periods of time in communities that use one or more languages that are the target of the specific documentation project. But this alone does not make them any different from a descriptive linguist who would typically also undertake fieldwork in order to capture information (vocabulary, phonological, and grammatical rules) that would end up in a published dictionary or grammar. The difference lies in the overall ends of fieldwork: while a descriptive linguist aims at getting all the data they need for their specific academic work, "[t]he aim of [a] language documentation […] is to provide a comprehensive record of the linguistic practices characteristic of a given speech community"

Language documentation in practice

Given the goals outlined above, language documentation involves the collection of audio or video recordings of numerous communicative events as they occur in their typical environment in an attempt to capture them in all their detail. Given that video recordings offer many advantages and are relatively easily done these days, video has become the gold standard of recording format: video recordings of entire communicative events give us a good sense of how a language is used over longer stretches of communication and how its use interacts with the physical environment, other participants, and bystanders (think of the use of demonstrative adverbs like here, there, and yonder). They also allow us to capture gestures, for instance, how someone points to a place indicated by a particular demonstrative adverb or how someone engages with an interlocutor with facial and manual gestures. So, a typical record in language documentation could be a video-recorded conversation, for example, about building a canoe, where we see people around a half-hollow tree trunk point at different spots and comment on what needs to be done, and possibly explaining to the researcher what is going on. Video recordings also allow us to document sign languages; see Chapter 4.3 for some details on their corpus-based investigation.

Language documentation shares with corpus linguistics the basic goal of representativeness. Yet, language documentarians often work under circumstances that are not purely research-oriented, and communities are -and should be -involved in steering the direction of a project and what it should cover. In essence then, these records are largely

Exercise 10.1 Reflect on the advantages of video as the main recording format

(2) Name some purposes of video recordings of different event types other than linguistic analysis. What user groups could benefit? What other kinds of material could arise from these?

(3) What circumstances can you think of that might suggest communicative events should not be recorded on video?

Corpus linguistics 185 convenience samples (cf. 2.3) of language use: documentary linguists record what they come across or what is being suggested to them during a field session. They are never in full control for ethical reasons, and also, they may not know (as community-outsiders) or be fully aware (even as community members) of all the communicative events relevant. This bears some consequences for corpus building in language documentation because it means that our corpora are mostly haphazard and opportunistic.

10.1.3 What are "linguistic practices"?

Himmelmann' s (1998:166) use of the term linguistic practices refers not only to verbal behaviour, that is the use of language as immediately observable and, thus, directly recordable. It also includes a second component of metalinguistic knowledge which comprises everything people know about the language and are conscious of. For instance, speakers of many varieties of English are often aware of subject-verb agreement, that is they possess conscious knowledge that a verb in the present tense with the third person singular subject ends in -s. They may also be aware of varieties of English where no ending occurs in third person singular verbs and may hold negative views about this. All this belongs in the area of metalinguistic knowledge. It is different from what we have discussed under the label of linguistic knowledge (cf.

How can metalinguistic knowledge be recorded during language documentation and how are they relevant to corpus building? First of all, it may come up in naturally occurring, recorded conversations: in many communities, people talk about the use of certain forms, in particular if they feel that one or the other variant is of lower prestige, like the lack of subject agreement in some English varieties. Second, capturing metalinguistic knowledge can also take the form of recording discussions of linguistic structures between the researcher and one or more users of a given language. Third, writing in a native language may provide valuable clues as to metalinguistic knowledge, for instance, with regard to how a stream of speech is broken down into segments like words (see

Exercise 10. The kinds of texts and interactions that are recorded and transcribed as part of a language documentation project may not be the kinds of texts one would record if planning a corpus. We discussed in Chapter 3.1 how the size and composition of a corpus reflect in various degrees its representativeness and saturation. However, the practical limitations on corpus building outlined in Chapter 6 are particularly relevant in documentation projects. Here, a corpus, especially a small one, can be specific to one genre or even one person. Researchers must simply be upfront about what makes up the corpus and be aware that not all corpora are appropriate for grand generalisations about a language. Practical considerations are also relevant when it comes to the processing and annotation of data collected in a documentation project. At the same time, however, given that documentations target languages that are not known to a wider scientific community, a greater minimum of annotation is key for documentation corpora, as will be discussed further in 10.3 (cf. Chapter 7 on annotations for requirements on annotations and their added value).

In Good for a genre-specific corpus, and a starting point for some research questions.

Elicited sentences? Not ideal as the sole basis of a text corpus, but could be used for phonetic measurements. Paradigms and word lists? These are the result of linguistic analysis and as such not appropriate for a corpus, but are good to be included in the apparatus once texts are collected. After assessing what is present in the collection, then you can plan for what you would like to add and focus on that in your corpus building enterprise. As a rule of thumb, something is better than nothing. This truism is always worth repeating given that much of linguistic work on diverse languages is based on data that is not accessible to others. Also note that limited variation bears advantages in allowing for investigations of small sets of variable characteristics, for example, specific variation across registers as outlined for Vera'a in 3.1.6 (see also Mosel 2014 on this point).

(3) What metadata and annotation files do you find? Can you access the data? Try to access an annotated text and find the respective word for 'woman' . How do you go about doing this? (Note you have to register for free and sign a code of conduct before you can access the archived material.)

Multiple purposes of language documentation

While language documentation lends itself to many different purposes, the two primary ones are for a community to have a record of how their language is used at a particular point in time, and for researchers to increase the knowledge of how human languages work. Other user groups are other academic disciplines, like ethnographers, musicologists, social scientists, and so forth, as well as journalists or other kinds of writers, or government agencies who conduct language planning and devise education policies. Crucially, the open-endedness of language documentations also makes them amenable for future purposes that no one is thinking of yet. As mentioned above, video recordings are particularly well-designed to serve a variety of purposes since they give a broad impression of communicative events, including paralinguistic behaviour like gestures and facial expression, the physical surroundings, as well as other aspects of social behaviour in a given culture, for instance, how different people dress, what levels of distance they keep between each other, etc. They reflect much better than word lists, dictionaries, or written texts how the language works in long stretches. For language communities, language documentation gives a broad sense of how their language is used. As such it can prove a valuable resource for current or future language maintenance and educational purposes (see Mosel 2012 for an example).

Corpus linguists are interested in how linguistic data is conditioned by context. To achieve this, we need annotations and metadata to turn collections of data into corpora that will form the basis for corpus-linguistic investigations of language use, and what this tells us about the structure of a specific language's grammar, lexicon, etc., as well as the conventions of the language use and that of the community members' thereof.

FROM COLLECTION TO CORPUS: CORPUS BUILDING FROM LANGUAGE DOCUMENTATION

The building of a corpus within a language documentation project is not generally different from any other corpus building project, as discussed in Chapter 6. Yet, the building of an LD-based corpus faces particular challenges through the typically severer limitations of resources and the fact that potential academic users of the corpus have typically no prior knowledge. The first aspect has already been addressed above.

Making an LD corpus accessible to a broader scientific community is a key consideration. Table

Corpus linguistics 189

In Chapters 6 and 7 we have discussed various layers of annotation and how they add value to a corpus. The necessary processing steps of raw data for LD corpora consists not only in time-aligned transcriptions -given that LD corpora target primarily spoken language use -but also in the translation of the transcribed text into a language widely known by anticipated user groups. In this way the corpus text is searchable and users can grasp the meaning of specific passages. What is crucial for languagedocumentation-based corpora is that they remain closely linked to the raw recording data. Additional important layers of annotation are translation, morphological glosses where software such as ELAN can be useful (cf. Chapters 6 and 7). Moreover, for LD corpora it is much more important that they are interlinked with a lexical database and a sketch grammar of the language (cf.

Corpora versus language documentations

An important difference between a corpus and language documentation is that the latter is relatively stable, that is to say the primary data and related metadata are not further manipulated. All that may change in the course of documentation and analysis, and hence, a growing understanding of the data at hand are the annotations that make up the core of the corpus. For a corpus then, it is perfectly normal to have multiple versions reflecting different stages of understanding, as well as for different purposes. This also means that one will often need to do transformations, exclusions, and further annotations to get the information needed for a particular question or research goal. Possibly a somewhat extreme but quite illustrative example is the creation of corpora within the previously mentioned MultiCAST collection

RESEARCH QUESTIONS APPROPRIATE FOR SMALL CORPORA

As discussed in 3.1.1, corpus size is important because we know that in a large sample, we have a better chance of finding what we are looking for, and a better chance of seeing typical patterns of usage. A small sample is more likely to be affected by chance and we may see spurious results. Small samples, unless carefully balanced, will be much more affected by the genre of the source content. A corpus of Pear stories

Function words

Very few words occur with enough frequency in a small corpus to make a robust study of them. However, function words (grammatical words) do occur often and regularly and are thus often put on a stop list (cf. 5.3). In a small corpus, particularly of an under-researched language, function words have an advantage because they are so frequent, and it may be interesting to determine the full range of their contexts. As we have described in previous chapters, one of the main focuses of corpus linguistic research is variation. If there is an element of variation in the function words, this can become a fruitful research path.

Below is a list of the nine most frequent words from the 2020 Matukar Panau corpus (150,740 words). In this corpus, there are 14,021 different words (types). In a list of the ten most frequent words of a large English corpus, all of the words will be function words. In the Matukar Panau list, we see function words and a content word (tamat, 'man'). The language features of Matukar Panau also affects the most frequent words. It does not have determiners or obligatory person pronouns, and a lot of the grammar of the language is done with morphology on the verb or nouns rather than independent words as in English. This means that we see many different inflected verbs in the language, none of which end up being very frequent. What we do see in this list of grammatical words is that the most frequent and fourth-most frequent words are related. Main is primarily a proximal demonstrative, but can also be a relativiser or clause boundary marker. Mainangan is the focused variant of the proximal demonstrative. Together, there are over 13,800 tokens of main and mainangan which would be the basis of a respectable study!

Exercise 10.3 Collection versus corpus

Look at collections available at the Pacific and Regional Archive for Digital Sources at

(1) Choose five collections and name them, include a citation for each collection.

(2) For each describe the material present that would make up a corpus. What is the format of the data? How much is available (in time or words or another appropriate measure)?

(3) What else is present in the collection and who is/are the target user(s) of that material?

(4) How consistent is the material across the five collections?

(5) From one collection, select annotated data whose annotations can serve as a small corpus. Is it possible to determine word frequencies? Genre-specific word frequencies? Keep track of your selection and search procedures.

(6) What layers of annotations can you find in annotation files in different collections? What linguistic questions could you address based on these annotations?

Grammatical marking

As with function words, grammatical marking can be a fruitful area of study even in a small corpus because it tends to be obligatory and frequent, although this obviously depends on the language of study. If there is a presence or absence of grammatical marking due to certain context features, this can be appropriate for a corpus study, particularly as part of the language description processes. For instance, is object marking optional? Does it occur with some persons or numbers more often than others? A related question is the variable realisation of objects as either pronoun or zero, and in 8.4 we have presented such a corpus-based study for Vera' a in some detail. Grammatical marking can also be studied in terms of co-occurrence. What kinds of verbs co-occur most often with imperfective versus perfective aspects and how do the aspects change the meanings of verbs?

Exercise 10.4 Cross-linguistic frequent words

(1) Search the web for "word frequency lists". There are many freely available in a variety of languages and genres, many more so than corpora for the same languages. Many lists are used for computer linguistic applications. Identify three real use cases of word frequency lists from your web research.

(2) Small Corpora Skew: look at the ten most frequent words of some small language or genre-specific corpora. Are the words function words or content words? What are the content words? Does the genre of the small corpus affect the content words that occur? Name several advantages and disadvantages of using a small, genre-specific corpus, and list possible research questions that could be answered with a small genre-specific corpus.

(3) Language effects of corpora: look at the ten most frequent words in some isolating versus agglutinating languages. How does the language type affect which words are included in the most frequent lists? While grammatical and functional information can be found often even in a small corpus, it is not the only thing linguists want to study. Particular adjectives, nouns, verbs, or adpositions may not show up often enough, but classes of these words may still be large.

In the language documentation process, word analysis like parsing and glossing helps the researcher understand the structure of the language. If that information is recorded in a machine searchable way, like through FLEX or other programs, then that material can become an annotated corpus. Something to be aware of, however, is that during fieldwork on an under-described language, one may change analyses. If the annotations are not changed throughout the corpus, that can cause issues later on.

A researcher should then balance the efforts and rewards of making changes throughout an entire corpus; keeping some glosses underspecified, as described above, is a relevant technique here.

Constructions (syntactic variants)

Constructions bigger than the word level include kinds of clause combining, word order, order of constituents, constructions with particular arguments, and constructions with particular functions or meanings like possessives, comitatives, existentials, and so on. Like classes of words, particular constructions are often more frequent than particular words, giving linguistics enough data for research. Constructions, however, can be difficult to search for unless they have a particular feature or string that can be searched for, or unless additional coding or annotation has already gone into the corpus, for example, in the form of GRAID annotations mentioned in 10.3.1 that enable searches for different types of clause constructions including the distinction between intransitive, (mono-) transitive, and ditransitive clause constructions. In particular questions concerning the latter constructions can often not be addressed based on verb searches alone because in many languages relevant verbs are not restricted to ditransitive constructions (recall also the

Zeros

Refer back to Table

The study of subject and object realisation by some overt form versus zero is a well-studied research area that includes major and minority languages. In languages like English and German, both subject and object tend to be overt: if they are not expressed as a full NP, a pronoun will often be used to fill the relevant syntactic position. In languages like Japanese and Mandarin, but also Spanish or Polish, where an NP is used, subject and object positions will more typically be left unfilled, that is zero (cf. Chapter 11). Contrasting entire languages in this way is, however, simplifying it quite a bit. For one thing, even in these relatively wellstudied languages matters are more intricate: for instance, subjects in English are much more frequently zero than objects

Other studies that could include zeros are those of copula absence, like

Phonetic information

Even in a small corpus, there are many phonemes. There are various forced aligners freely available that can automatically segment an audio file into phonemes if there is an available matching transcription. Forced aligners use a dictionary (word list) of the language, an acoustic model of the language and their own algorithms to match and segment the audio file to a transcript. Some forced aligners can build an acoustic model of the language during the forced alignment procedure, so an existing model is not necessary, which is a major benefit to under-documented and minority languages. Once segmentation has been checked, various phonetic scripts,

Corpus linguistics 195 also freely available, can be used or adapted to collect measurements. Measuring vowels is particularly useful, as we know from sociolinguistic research that a great deal of social variation is expressed through vowels. A recent project force aligning seven hours of Matukar Panau data resulting in close to 50,000 monophthongs and hundreds of thousands of vowel measurements

What we can't study

Small corpora can be the basis for the study of many things, but some topics are very difficult to investigate without a large corpus. A small corpus, unless designed to contain particular data, will be unhelpful in investigating the behaviour of particular infrequent words or collocations. Any rara (rare phenomena) are unlikely to be found in a small corpus, or if found, will be infrequent. A handful of examples of a particular token is not enough to give a confident sense of the full range of its behaviour, even if they can give a general sense of meaning.

Additionally, many measures of frequency and predictability that require bigram/ n-gram information will be skewed heavily by the data available in a small corpus.

Many studies have shown that frequency and predictability affect language processing, production, and representation

Advantages of small corpora

Although we cannot do everything with a small corpus, there are advantages to a small, single researcher or small team-built corpus. If you build a corpus out of language documentation data, you probably have a very good idea of what is in the corpus and the context for that information, much more so than with a multimillion-word corpus. Large corpora will often contain a fair number of errors, missing metadata, and incorrectly coded information. Because the corpus is large, this does not necessarily have a large impact on overall patterns. However, with a small corpus, there is probably a lot less of this "junk" to throw out. Small corpora from language documentation probably contain a great deal of spontaneous language data. This data is often considered the best kind of language data available to understand how people really use language because it is less considered and belaboured than written text. A small spoken or signed corpus, therefore, can still be a good representation of how people use their language.

CONCLUSION

The take away message from this chapter is that the corpus linguist who works within language documentation has fairly little control over the kind and quantity of corpus data they can include in a corpus. When working with corpora based on language documentations, corpus linguists need to work with what they have, and this may often require flexibility. It also means that the research we can pursue based on these corpora may be limited. At the same time, it is worth noting that corpus-based work on lesser-documented languages is of particular value given how little we know about language use in many languages.

FURTHER READING

On the distinction between descriptive linguistics and language documentation, the seminal text is Himmelmann (1998).

Exercise 10.5 Areas of study

Traditional typology has been based on abstract features of languages understood as abstract systems, for example, what case marking languages possess or which word order. In some work each language as a whole is treated as representative of one feature value, for example, a specific number of cases. A finer-grained approach is multivariate typology where features are differentiated according to construction types

• Universal Dependencies (UD) (cf. 7.3.1) provide a system of consistent, crosslinguistic annotation of grammar (parts of speech, morphological features, and syntactic dependencies). Over 90 languages have some UD annotation and there are various repositories where specific language data can be viewed or downloaded

• Some books and documents have been very widely translated and have been used as parallel translational corpora in typological research comparing the use of different linguistic expressions for the same content: The Bible, Le Petit Prince, Harry Potter, and Declaration of Human Rights, among others.

• Content-controlled stimuli-based corpora like collections of Pear Film renarrations

We are involved in recent corpus-based typological projects aimed more at spontaneous spoken and signed language for investigating issues of language user choice during discourse production. These corpora were introduced in Chapter 7 in relation to their specific annotations, and we will take them up again in this chapter.

• Multi-CAST -spoken, monologue narrative corpora annotated according to cross-linguistically standardised conventions for investigating reference and discourse

It is worthwhile to place the following disclaimer right at the beginning of this chapter: while we consider corpus-based approaches in typology highly relevant, this area is only emerging as a subdiscipline of typological linguistics. This means that we cannot introduce you here to a set of fully fledged corpus studies of typological distributions in the way that we did, for example, in Chapter 9. Rather our purpose here is to set out the ways in which corpus studies have been and will be relevant to

UNIVERSALS AND DIVERSITY IN LANGUAGE USE

In corpus linguistics, corpora serve as the empirical basis for the systematic study of language use. The main concern of CBT is to systematically study language use across diverse languages. A key concern here is what is known as usage-based approaches which seek to explain attested grammatical structures across languages in terms of the communicative function of language in use and relevant constraints on language processing. These two facets of language are what functionalist traditions have considered to be the universal basis shared by diverse languages. This suggests that many aspects of language use should be universal, but there is also the possibility that language use, related communicative functions, and underlying patterns of processing are different across linguistic communities. CBT is expected to tap into these questions.

What is universal in language use?

Corpus linguistic studies have been brought to bear on universals of use for a long time. A classic finding is

Another set of universals is connected to the production of spoken language: speech is generally divided into units between pauses (pause units), and it is produced with variable speed (speech rate) across a connected set of pause units. Recent work

Corpus-based typology

by Frank Seifart and associates finds that the allocation of pausing and speech rate alternations follow the same regularities across languages. Reporting findings from nine spoken language corpora,

A final example of apparently universal patterns of spoken-language processing comes from information management and is also related to the processing of nominal expressions and their referents. But in this case, matters are more controversial, with recent research questioning a long-standing perceived view grounded in seminal work of information management by John Du

The preference only becomes visible once larger stretches of connected discourse are investigated, as found in corpora. And indeed, Du

Building on his initial investigation of a small corpus of spoken Pear stories from Sakapultek, Du

Corpus-based typology

Over recent years, this view has been challenged in work by various authors, including the second author of this book in close collaboration with Geoffrey Haig (and Nils Schiborr in recent years). This research was a major driver to develop the multilingual corpus Multi-CAST (cf.

Collating aggregated corpus findings from a total of 19 corpora from 16 diverse languages, including five from Multi-CAST, they find that discourse does not follow the 'ergative' pattern postulated by Du Bois; instead, S-arguments pattern in between A-and P-arguments when it comes to the rates of lexical forms

(1) Human referents are of greater concern to human language users and tend to be topical, being verbalised frequently across sentences (2) Topical referents in this sense tend to receive reduced forms of expression (pronoun, zero) rather than full lexical NPs because referents are given (3) Humans are the huge majority among A-argument referents because they tend to fulfil more agent-like semantic roles, contrasting sharply with P in this regard, whereas S is overall least specified for semantic roles.

This then explains the low rate of new and lexical A-arguments, which is thus not due to an underlying constraint in information management but follows from these more general discourse properties. The S-role occupies an intermediate position because it is more open to human and non-human referents and more open to a variety of semantic roles. That newness does not seem to pose a massive challenge to reference processing is suggested by a later study by

Summing up this section, corpus linguistic studies can reveal regularities of language use that are likely to be universal. All these universals on language use are statistical universals of language use 1 : They reflect tendencies in language use, and hence, require approaches like corpus linguistics to be discoverable. Although some of these regularities had been suggested a long time ago, corpus linguistic approaches are capable of discovering regularities that have not been dealt with in classic typological research.

Universal constraints on diverse patterns of language use

A somewhat different line of corpus-based research addresses questions that have been addressed over several decades in linguistic typology. A prominent example is word order typology, which we shall focus on here. Since the 1960s, word order has been a major concern, and languages have been classified according to the order of verb (V), subject (S) and object (O) on clause level, the order of head noun (N) and adjectival modifiers (A) in the NP, or whether languages have prepositions or postpositions (i.e. the order in an adpositional phrase). As with any grammar-based typology, word order typology in the classic sense has been undertaken by classifying each

Corpus-based typology language as belonging to exactly one type, for example, English as an SVO language, including the possibility that there is no dominant word order (i.e. no type can be determined). In addition to these mono-dimensional classifications, typologists have found a range of correlations between word order types across structural domains, for example, that OV languages are more likely to have Adjective-Noun order in NPs and have postpositions in adpositional phrases. This invokes the general principle that languages tend to show head-dependent alignment across these structural levels (so-called 'harmony' in word order) so that on each level, the head either precedes or follows the dependent. This type of harmony is widely regarded as preferable for language processing since it allows language users to recognise the syntactic relations between phrases easily and quickly. Furthermore, typologists have formulated a set of implicational universals that essentially capture unidirectional correlations between types, for example, Universal #25: If the pronominal object follows the verb, so does the nominal (i.e. full NP; DB & SS) object.

(cf.

Universal #25': For every language, if the percentage of pronominal objects on the right of the verb is greater than 75%, so is the percentage of nominal objects on the right of the verb.

This reformulated universal captures the fact that in all UD corpora investigated by the authors there is considerable variation, yet the relative tendencies are such that

Corpus-based typology post-verbal position is either open to both forms or restricted to one whereas preverbal position is open to both. What is further significant about this reformulation of the universal is that it yields a generalisation about language use that also generalises across languages. These findings can then be related back to grammar-based typologies, in order to estimate how accurate is the latter despite the mentioned problems. For example,

Another line of corpus-based research on word order is not so much concerned with the proportions of specific structures and their subtypes, but rather takes on the questions of how variable word order is in each given language, and how languages compare in this regard? We mentioned in passing above that English shows overall quite a strict word order, and so do languages like Japanese or many Oceanic languages, including Vera'a. Other languages, like Russian or many Australian languages, are well known to show great variability in word order. Variability as such can be measured in corpora as entropy (cf. 5.7).

The answer is, largely yes, as

As indicated above, high entropy in word order variation appears to correlate with whether a language possesses rich morphology or not. This invokes the idea of a trade-off: if the word order is highly variable, it is less informative about the content of the sentence. Morphology can thus be seen as 'stepping in' where word order fails to provide the clue (and vice versa).

Corpus-based typology can be represented variably in corpora, and this means that case marking needs to be investigated as a variable of language use as well. It is therefore necessary to investigate to what extent the two syntactic functions are formally distinguishable in actual usage. As with word order,

In summary, the findings presented constitute an observation of stark diversity across languages. But at the same time, they hint at underlying principles that also constrain variation, and they do so in a grounded fashion since they demonstrate actual language users' productions in usage contexts. As such, they point much more directly to considerations of processing constraints, similar to findings from psycholinguistic experiments on comprehension. Needless to say, corpus linguistic studies of this sort cannot explain why languages opt for one or the other type of subject and object encoding, and the explanation for this lies in deep and intricate histories of language evolution with many stages of development, each a story worth telling in itself. In the following section, however, we will take up a number of differences across languages that are observable in language users' behaviour and can be related to either correlating linguistic structures, as well as underlying cultural or other factors.

How does language use differ across languages and cultures?

An additional strain of corpus-based research in linguistic typology focuses on diversity in language use. The usage patterns observed here are not immediately related to universal constraints on processing and human communication, but instead, point to culturally entrenched differences in the habits of language usage.

Referential density

The example we mention here is the use of zero forms of reference across languages: it is well known that languages differ greatly as to the possibility to not use an overt form of reference in discourse. Given that the choice of lexical forms is often more clearly related to certain discourse factors that hold across languages (cf. 11.2.1 above), the question practically boils down to whether languages are generally more likely to use zero (called "pro-drop" languages in some frameworks) or pronouns. Languages like Italian and Mandarin are basically of the former and languages like

Corpus-based typology

English or German of the latter type. But similar to word order typology discussed in 11.2.2 above, the problem with such typologies is that it is not straightforward to classify language systems according to this parameter given the pervasive variation in language use. 2 Bickel (2003) takes a radically different, corpus-based approach to tackle this question: he examines corpora of 30 Pear stories (between 29 and 134 clauses in length (56.8 on average)) from three languages spoken in the same area in the Nepalese Himalayas: Belhare (Kiranti, Sino-Tibetan), Maithili (Indo-Aryan), and Nepali (Indo-Aryan). In doing so, he determines per text (and thus per speaker) what he calls its referential density (RD). The RD of a text is determined by counting all possible argument positions (S, A, P, obliques; cf. 11.2.1 and 11.2.2 above) and among these all positions are filled by an overt form (pronoun, lexical NP) rather than being left zero (where a referent is clearly identifiable from preceding context although no overt form appears); then the number of overt argument positions is divided by the number of all argument positions to yield the RD. The mean RD values per individual language corpus are: Belhare -0.41, Nepali -0.47, Maithili -0.62

(1) What is the referential density in each text of the first corpus you have downloaded? Confine yourself to argument functions, excluding adjuncts. What are the relevant query expressions you use? Document these.

(2) Determine the RD of the entire corpus and differences between the texts therein. Describe these in terms of the basic statistical Exercise concepts we have introduced in Chapter 8.

(3) Do the same for the second (and all other) corpora you have chosen and compared the corpora.

Corpus-based typology language community (being all from the same geographic area) and can therefore not account for observed cross-linguistic differences. Instead, Bickel identifies significant correlations between RD values and the syntax of clause combining in the three languages: put simply, some dependent clauses in these languages require case marking on its syntactically most prominent argument, and these are regularly realised overtly for this reason. According to

What makes these findings different from what was discussed in 11.2.2, is that here observed diversity is not directly limited by universal considerations of language processing, efficiency, or the like. The main predictor of RD levels is a language-specific correlate. As such, this work is directed towards explaining language diversity by reference to other diverse structures, rather than accounting for the constraints on diversity by way of universal regularities. Although Bickel's study of these three communities is designed specifically to control for some factors, other correlates are possible, whether these are situated in linguistic structures or social and cultural parameters of communities. In a further study of RD,

Social cognition and SCOPIC

The theoretical principle behind social cognition is that people know the social facts (e.g. kinship, status, ownership) that place them and others within an interconnected social context and of psychological facts about their own feelings, attentions, desires, and their estimations of these for others

Taking human reference as an example in a sub-section of SCOPIC, the data show that there are three main semantic categories of lexical reference that are used widely by most languages in our sample

(11.   -2018-10-31-8:20-8:23 6   (11.6) Literal Translation: That's-right (it's) that-one-him-there, because (the) shirt colour (is the same for) all-of-them-there.

Auslan: MDP, SocCog-asf01-DP_B_c12b -05:39-05:46 7

In our study of human reference

In sum, SCOPIC allows for comparison between languages and between aspects of internal and external context. Allowance for an additional layer of comparisonbetween different layers of annotation -allows for an integrated and informative perspective on social cognition aspects of language.

ISSUES IN CORPUS DEVELOPMENT AND CROSS-CORPUS TYPOLOGICAL RESEARCH DESIGN

In this section, we briefly describe a number of issues relating to the specific requirements on corpus design for various research agendas in CBT. We also point to desiderata and envisaged developments at the end of the section.

Different types of multilingual corpora

CBT generally draws on multilingual corpora or at least corpora that are in some way relatable to one another. There are essentially three approaches in corpus development that have been followed to date, distinguished by the parameter of cross-corpus comparability:

(1) Parallel text corpora: texts that are translational equivalents, for example, translations of The Bible text, like the Gospel of Mark

For other research questions, control of content is less of an issue: for instance, the relatively low lexicality of A-arguments should be observable in any text (but see below). Likewise, the allocation of pauses or final lengthening is not dependent on content since the regularity applies to any nouns compared to verbs, and any word form in final versus non-final position, regardless of text content. The same is true of the variable placement of subject and object phrases or variability in case marking, as investigated in UDs. Original text corpora are thus an option where the phenomenon of interest can be considered ubiquitous enough to be represented sufficiently in the corpus (cf. Seifart In Press). This is obviously the case for the phenomena mentioned here, that is, pausing, new reference, etc., compared to human reference which may not be salient in certain texts.

Corpus property biases in CBT research

As is generally true for any empirical research, including corpus linguistics, research results can turn out to be artefacts of the data used. This is essentially what

Corpus-based typology angle and behaviour of characters in relevant contexts. These considerations are likely to explain why the Sakapultek corpus is among the very few corpora that show the postulated discourse-ergative pattern. Hence, corpus-based typologists need to carefully evaluate the specific properties of the corpora they use vis-à-vis the phenomena they are researching. . This is essentially true for all research based on UDs which contain primarily written published texts. This is significant, in particular, where relevant research is intended to reveal constraints on language processing: one would have to keep in mind that the findings may apply strictly to standardised written text production (which is often influenced strongly by traditions of formalised literacy education). Also, it needs to be kept in mind that published texts (like the one you are reading right now) usually go through multiple stages of editing, and often multiple writers are involved. This is not to say that UD-based research is invalidated by these considerations. But we should generally aim for using spontaneous data in research on processing since the more generally cross-linguistically relevant regularities will primarily come up in spoken discourse. That the differences can be severe is generally clear from cross-register studies of nominal expressions of reference, where written texts have many more such structures, and these also show much higher complexity than those in spoken language texts. Hence, considerations of dependency lengths, a notorious question in UD-based work, may potentially be much less relevant in spoken language corpora simply because rearrangement of generally shorter and simpler phrases will have little effect.

Corpus design versus bootstrapping for specific research questions

Finally, requirements on corpus design notwithstanding, we should point out that CBT researchers are generally encouraged to make the most of the corpus data they have. This is particularly true for corpora drawing to a large extent on documentary corpora where limitations on data collection are highly relevant (cf. Chapter 10). For instance, stimulus-derived texts can be of much less value for a community than recordings of texts from the relevant traditional 'orature' , and documentary linguists for ethical and research-practical reasons are encouraged to comply with a community's desiderata. There may often be possibilities to bootstrap one's data in such a way that it be useful for a particular research question. Regarding, for instance, referential density research, one cannot compare uncontrolled texts across the board, as explained above. Yet, in the absence of controlled texts, one can still aim at finding overall comparable contexts to

Corpus-based typology

Corpus-based typology 215 compare levels of RD. An example is the chains of same-subject anaphors (to the extent that identifying a category 'subject' is possible in the languages under investigation), that is, where a single referent is the subject of a series of clauses so that each anaphor has an antecedent in the preceding clause in the same function. This is a context that can be assumed to trigger either the use of some form of pronoun or zero across languages, regardless of the overall content of the texts involved, and hence it will make sense to compare rates of pronoun versus zero within this 'envelop of variation' (Torres Cacoullos & Travis 2019 on comparisons along this type of context) (cf. Schnell & Schiborr to appear for a case study based on Multi-CAST).

CONCLUSION

Corpus-based typology makes explicit the main theoretical goals of corpus linguistics in across-linguistic a cross-linguistic perspective where we are generally interested in the different properties of all human languages and their global distribution. We examine not only intra-language and context-dependent variation but also how that variation scales up to the inter-language level. Corpus linguistics takes real samples that are used to generalise about language, and corpus-based typology helps us to confirm (or confront the idea) that these assumptions are applicable for a more diverse sample. In doing this research, we see the need for corpora of more diverse languages, especially under-studied ones that require documentation, in order to test hypotheses about language structure and use. Also, we need to develop many more corpora of spoken-language texts which are particularly relevant to CBT research tapping into questions of language processing.

The findings from corpus-based typology also feed back into ideas about corpus building, composition, and annotation: more diverse languages require different considerations of register, representativeness, and potentially, require adaptations of annotation and querying strategies. It is still an emerging field and will help shape the more general perspective on corpus linguistics as a field in the 21st century.

FURTHER READING

Schnell and Schiborr (to appear) provide a succinct overview of the current work on corpusbased typology.

NOTES

1. These quantify structural tokens in usage, where statistical universal in traditional typology quantify structural types as part of individual language systems. 2. Classification of a language as "pro-drop" in relevant frameworks involves more intricate diagnostics than how frequent zeroes are in discourse, but these diagnostics have their problems as well.

Note: Bold page numbers refer to tables; italic page numbers refer to figures and page numbers followed by "n" denote endnotes.

age-graded variation 164-165, 169, 173, 177-180 AIRBUS-ATC corpus 34 ANNIS 106 annotation scheme 111-125; discourse and reference annotation
Statistics in Corpus Linguistics

Do you use language corpora in your research or study, but find that you struggle with statistics? This practical introduction will equip you to understand the key principles of statistical thinking and apply these concepts to your own research, without the need for prior statistical knowledge. The book gives step-by-step guidance through the process of statistical analysis and provides multiple examples of how statistical techniques can be used to analyse and visualize linguistic data. It also includes a useful selection of discussion questions and exercises which you can use to check your understanding. The book comes with a companion website, which provides additional materials (including answers to exercises, datasets, advanced materials, teaching slides etc.) and Lancaster Stats Tools online (

vaclav brezina is a senior lecturer at the Department of Linguistics and English Language, Lancaster University. He specializes in corpus linguistics, statistics and applied linguistics, and has designed a number of different tools for corpus analysis.

Statistics in Corpus Linguistics

x

Commonwealth & Protectorate and Restoration 7.2 Comparison of two periods in the EEBO corpus: results of the bootstrap test 7.3 Final evaluation of the results: its, must and pestilence 7.

About This Book

What Is This Book About?

This book is a practical introduction to statistical procedures in corpus linguistics, a discipline that uses computers to analyse language, organized according to linguistic topics. These range from vocabulary and grammar to sociolinguistics, discourse analysis and historical investigations of language.

The book offers an overview of the state-of-the-art methodologies of language analysis using corpora and introduces new techniques that have not previously been used in the field. No prior knowledge of statistics is assumed; instead, all necessary concepts and methods are explained in non-technical language.

In addition, all procedures described in the book can be easily carried out using Lancaster Stats Tools online (see 'How Should You Use This Book?' below). Throughout the book, many examples (case studies) of the application of corpus statistics are provided and standard reporting of statistics is shown.

The emphasis of the book on the practical aspects of statistical analysis of language is also reflected in its focus on research design and the implications of different 'shapes' of data for statistical analysisfor this reason, the companion website offers complete datasets used in this book for easy replication of the analyses. Corpus linguistics is an extremely versatile methodology of language analysis applicable in a wide range of contexts, in linguistics, social science, digital humanities and elsewherethe book thus aims to facilitate meaningful use of corpora for as wide a range of users as possible.

Who Is This Book For?

The book is intended for anyone interested in corpus linguistics and quantitative analysis of language. This includes students and researchers in the field of linguistics, sociology, history, psychology, education etc. The main goal of the book is to help readers understand key principles of statistical thinking in order to be able to make informed decisions about the applications of particular statistical techniques. To facilitate this, in addition to the expository parts, the book also includes discussion questions ('Think about…') and exercises which the readers can use to better engage with the material and to check their comprehension of the subject matter; answers to the exercises are provided at the companion website (

xvii How Should You Use This Book?

The book reflects the needs of students and researchers who are looking for a practical guidebook on corpus statistics, which is grounded in the current literature in the field and reflects the best practice. The book can be used as a course book or for independent study. After reviewing general statistical principles in Chapter 1, readers can follow their own path through the book according to the linguistic topics of their interest. Statistical techniques introduced in the book are cross-referenced and included in the Index at the end of the book.

The book comes with a companion website -Lancaster Stats Tools online -(

In particular, we'll be exploring answers to five questions:

• What is the role of statistics in science and corpus research? (Section 1.2) • What are the key terms in corpus statistics? (Section

• How can we explore and visualize data? (Section 1.5)

• How can statistics be used in corpus research? (Section 1.6)   1.

Before reading this section, think about the following questions:

1. What is science? What are the basic features of scientific enquiry?

2. Which of these statements about language are scientific statements? (a) Women's speech seems in general to contain more instances of 'well', 'y'know', 'kinda', and so forth . . . (b) Words are easy, like the wind. (c) Passives are most common by far in academic prose [compared to other registers], occurring about 18,500 times per million words. (d) The faculty of language can reasonably be regarded as a 'language organ'.

(See next page for one more example) (e) Our results show that there were significant changes in at least one formant

Unlike other sources of information such as mythology, philosophy or art, science relies on the systematic collection of empirical data and testing of theories and hypotheses. One of the most influential theoreticians of science, Karl Popper, defined a scientific statement or theory as something that can in principle be falsified

Corpus linguistics is a scientific method of language analysis. It requires the analyst to provide empirical evidence in the form of data drawn from language corpora in support of any statement made about language. Another scientific requirement corpus linguists follow in principle is replicability of results. This means that researchers need to be able to confirm the findings of one study in follow-up studies (see Section 8.3). In order for the results to be replicable, corpus linguists need to make their choice of corpora and analytical techniques transparent. It is also good practice in corpus linguistics to make corpora available to other researchers who can explore the same dataset further and thus advance knowledge in the field.

Let us have a look at two examples that illustrate this point. First, imagine that we are interested in the number of adjectives different British fiction writers use in their texts. We might hypothesize that using more adjectives leads to more colourful descriptions in novels. We have randomly selected 11 fiction texts by different authors from the British National Corpus (BNC) and counted the number of adjectives in each text; this is their absolute frequency (see

The mean is calculated in the following way:

mean ¼ sum of all values number of cases exploration and must lead to doubt being cast on claims made using such corpora. If corpus linguistics wants to retain its scientific status, it should not be content with statements such as 'this feature was found in a large corpus that is, however, not available'. 5 Because the texts are of different length, we have taken the relative frequencies per 10,000 words to show how many adjectives on average each author uses in 10,000 words (see Section 2.3 for the explanation of relative frequency). The relative frequencies have been rounded to the nearest integer.

Applied to the dataset above:

¼ 591:45 ð1:1Þ

Because the mean describes our sample, it is part of what we call descriptive statistics. Another example of a mathematical representation of complex linguistic reality is a line, in statistics called a regression line or line of the best fit (see Chapter 4 for an explanation of regression models). Imagine that we are interested in whether the authors that use more adjectives also use more verbs. We can list the frequencies of verbs 6 just below the frequencies of adjectives to see whether there is any relationship between these two linguistic features:

The graph in Figure

Basic Statistical Terminology

Think about . . .

Before reading this section, think about the meaning of the following terms. Have you heard them before? If so, in what context? Would you be able to define them?

• assumption

The following is an overview of basic statistical terminology used in this book. It includes key terms with examples from corpus research and is ordered from basic concepts to more complex ones which rely on the understanding of the previous terminology. Mastering these terms will make reading of the rest of the book, and many papers in corpus linguistics, much easier.

Corpus (pl. corpora) is a specific form of linguistic data. It is a collection of written texts or transcripts of spoken language that can be searched by a computer using specialized software. A corpus usually represents a sample of language, i.e. a (small) subset of the language production of interest; in some limited cases of very specialized corpora, a corpus can include the whole population, i.e. all language of interest to the researcher (see Section 1.4). The software that is used to search a corpus usually implements basic types of statistical analysis such as the statistical identification of collocations and keywords (see

Note that preparing the spreadsheet in the appropriate format is as important as the statistical analysis that follows. The book offers many examples of datasets based on different corpora, which are suitable for different types of analysis. It is always useful to compare your data to the model examples provided (full datasets are available from the companion website) to see if your data is in the appropriate format.

Dataset is a series of corpus-based findings that can be statistically analysed. It is a systematic collection of individual results that can be stored in the form of a table in a spreadsheet program (e.g. Excel, Calc etc.), each line representing an individual data point or case and each column representing a separate variable. Figure 1.3 provides an example of a dataset with five variables and multiple cases, each case representing one speaker. Note that example datasets used in this book are available at the companion website. It is important to study them for the particular 'shape' of data that lends itself to particular types of statistical analyses.

Variable, as the name suggests, is something that can vary and take on different values. For example, speaker's age is a variable that can take on different values from about one year (when children typically learn their first words) to over 100. Much corpus research can be characterized as searching for variables in corpora and analysing the relationship between them. An important distinction needs to be made between linguistic variables and explanatory variables. Linguistic variables capture frequencies of linguistic features of interest in the corpus. Explanatory variables (sometimes called 'independent variables') capture contexts in which the linguistic features appear. For instance, an explanatory variable can be the genre/register or date of publication of a text as well as speaker's age, gender and language proficiency, to name only a few. The dataset in Figure

Variables (both linguistic and explanatory) can be either nominal, ordinal or scale variables. A nominal variable has values that represent different categories into which the cases in a dataset can be grouped; there is no order or hierarchy between the categories. Speaker's gender is an example of a nominal variable because we can assign speakers in the dataset to one of two groups:

(1) male speakers and (2) female speakers. There is no hierarchy in this classification. For convenience, we often use numbers to indicate the group membership. In the dataset in Figure

• Is there a relationship between speaker's gender (a nominal explanatory variable) and the use of personal pronouns (a scale linguistic variable)? • Does a speaker's English proficiency (an ordinal explanatory variable) have an effect on the use of the first-person pronoun (a scale linguistic variable)? • Is there a relationship between the use of the first-person and the second- person pronouns (both of which are scale linguistic variables)?

The frequency distribution of a variable provides information about the values a variable takes and their frequencies. Distributions of scale variables can be shown in a histogram (see Section 1.5). Figure

As a benchmark in statistics, one of the common distributionsthe normal distribution 8is often used. The shape of the normal distribution is a symmetrical bell as shown in Figure

Although a lot of data in the natural and social world follows the normal distribution, most linguistic data is positively skewed ( ), i.e. there is more data to the left of the distribution than the right, as we saw, for example, in Figure

introduction

Outlier or rogue value? When we look at distributions we often check for outliers. Outliers are extreme values, i.e. values that are very far from the other values. Section 1.5 will introduce boxplots, a useful means of identifying outliers. When we find an outlier we need to check if the outlier is a genuine value or a measurement errora so-called rogue value. A rogue value can be caused, for instance, by mistyping data in a spreadsheet or by a tagging error in the corpus. An outlier, instead, is a valid data point, which for some reason stands out from others. While outliers are not in themselves 'errors', they present problems for statistical models because they may obscure the general tendency (see 'measure of central tendency' below) in the data and the researcher must decide how to go about the analysis of data which includes outliers. If there is a good reason, outliers can be excluded (bracketed out) from part of the analysis that focuses on the central tendency in the data.

The measure of central tendency or 'average' provides one summary value for a series of values of a scale variable. It is a simple statistical model that is usefully paired with dispersion (see below) to complete the summary description of the data. Different types of average can be used. In corpus linguistics the most useful ones are: mean, median and 20% trimmed mean. Mean (M or x̄), as we have already seen, is the sum of all values divided by the number of cases (see Section 1.2). The mean is a useful measure in distributions which do not have extreme values (outliers) that sway the mean towards them; in distributions with outliers, the mean might represent the outlier more than the rest of the values, which leads to the mean failing to be a useful model. Take for instance the frequency of adjectives in 11 fiction texts taken from the British National Corpus (BNC) used as an example to calculate the mean in Section 1.

If we had ten instead of eleven values, which is an even number, the median would lie half way between the two central values 565 and 567, as demonstrated below.

Dispersion is the spread of values of a variable in a dataset. Take again the adjective counts analysed in Section 1.2 sorted from the lowest (508) to the highest (699) value:

An alternative dispersion measure to the range 1 and the interquartile range is standard deviation. Standard deviation (SD) is the square root of the sums of squared distances of the individual values from the mean. This gives us an indication of the overall distance of individual values from the mean (see Chapter 2 for more detail).

Statistical measure is a general term for any statistic we calculate. It can be as simple as the mean or it can involve complex statistical modelling such as mixedeffects models

1. We start with the hypothesis we want to test called the alternative hypothesis or H 1 . For example, a sociolinguistic H 1 can claim that men and women differ in the use of swearwords. 2. We formulate the null hypothesis (H 0 ) that is the reverse of H 1 . To put it very simply, the null hypothesis states that there is nothing special going on in the corpus or corpora we analyse, e.g. there is no difference between two (sub)corpora. In our example, the H 0 would therefore claim that there is no difference between men and women when it comes to the use of swearwords. 3. We test the null hypothesis using a statistical test such as the independent samples t-test. Before doing this, however, we need to check that our data satisfies the assumptions of the selected test (see 'assumptions' below). 4. We usually get two important values from a statistical test: (a) the test statistic and (b) the p-value. Based on the p-value (i.e. the probability value of the observation in the corpus by chance alone) we decide whether to reject the null hypothesis. If the p-value is small enough, usually smaller than 0.05, i.e. 5%, we reject the null hypothesis and conclude that the observed difference is unlikely to be due to chance and therefore the result is statistically significant. This means that the difference observed in the corpus (sample) is likely to be a true difference in the population (all language use). If the p-value is equal to or is larger than 0.05 (or 5%) we conclude that there is not enough evidence in the corpus to reject the null hypothesis. We need to be careful when interpreting a result like this, which should not be taken to mean that the alternative hypothesis (H 1 ) is false or that the null hypothesis (H 0 ) is true: there is simply not enough evidence to reject H 0 ; if we collect more data the statistical test might turn out significant. Note that 0.05 or 5% is the conventional cut-off point which can be imagined as the risk we are willing to take when inferring from the sample to the population (see p-value below). If we are willing to take only a smaller risk than 5%, we can decide on the p-value cut-off point 0.01 (1%) or even 0.001 (0.01%).

A p-value is often the most visible sign of a statistical test (see above). However, it would be misleading to reduce all statistics to p-values. A p-value is a probability value (p stands for probability) and is one of the outcomes of a statistical test. P-value can be defined as the probability that the data would be at least as extreme as that observed if the null hypothesis were true. In the example of a sociolinguistic research looking at the use of swearwords by men and women we would get a p-value that would give us the probability of seeing the observed or even more extreme difference between the two groups if the null hypothesis were true, that is, if there really was no difference in the population and the difference observed in the corpus (sample) was merely due to chance as the result of a sampling error.

Assumptions of a statistical test, as traditionally understood, are conditions that should be met for the statistical test to produce valid results. One of the typical assumptions of a number of statistical tests called parametric tests (e.g. the t-test or ANOVA) is the normality assumption. This assumption presupposes that the frequency distribution of the linguistic variable does not deviate considerably from the normal distribution. This is because parametric tests such as the t-test typically compare the means which may not be good models of the values of linguistic variables if the distributions are too skewed (see 'measures of central tendency'). In such cases, non-parametric tests such as the Mann-Whitney U test (non-parametric version of the t-test) can be used. These non-parametric tests typically compare sums of ranks of values rather than the means of the actual values (see Section 6.3). However, statistical research shows (e.g.

When discussing individual statistical tests in this book, the assumptions of these tests will be listed and discussed in more detail.

The confidence interval (CI) in inferential statistics is an attempt to move away from the dichotomous thinking that is often connected with NHST, statistical tests and p-values. Rather than a yes/no decision about statistical significance, the confidence interval provides an estimation of the true value of a statistical measure (such as the mean) or of a difference between two statistical 1.3 Basic Statistical Terminology measures (such as the difference between two means) in the population. A confidence interval, as the name suggests, is not a single value but a range of values (that can be visualized as error barssee Figure

Effect size in descriptive statistics is a standardized measure, that is a measure comparable across different studies (see Section 8.3 for the discussion of metaanalysis), that expresses the practical importance of the effect observed in the corpus or corpora. For example, if we establish by a statistical test (see above) that two groups of speakers (e.g. men and women) differ from each other in the use of a particular linguistic variable, i.e. there is a statistically significant difference between these two groups, we still need to see how large this difference is and whether it is practically important. To help us with this judgement, effect size measures such as r, odds ratio or Cohen's d can be used. Table

2. What does it mean to say that a corpus is representative?

3. Are large corpora always better than small corpora?

A corpus is a collection of texts

With about 400 million people who speak English as their first language and an additional hundreds of millions who speak English as their second language

In corpus linguistics the term 'representative' is often used to describe a corpus. Representative is a descriptor of a sample when it has similar characteristics to the population it is drawn from. This allows us to draw conclusions about the population from the sample. Ideally, this would be achieved by truly random sampling 10 where each text ever produced and each spoken interaction that has ever taken place would have the same chance of appearing in the sample. Random sampling is, however, impracticable because there is no catalogue of all language production that we could refer to when doing the sampling; besides, not all language produced is recorded in some form. To make the sampling manageable, corpus designers often start with a set of categories within which they aim to collect an unbiased sample. These categories are called the sampling frame. Table

The Brown family consists of 15 genre-based categories according to which the total of 500 2,000-word text samples are selected. Each Brown family corpus thus consists of approximately one million words of written English (500 × 2,000). In this traditional corpus design, the aim of the corpus creators is to achieve an unbiased sample of texts in the categories from the sampling frame. Again, we would ideally use the random sampling procedure within the categories. Yet in practice, the selection is guided by text selection principles (see below) to avoid bias in the selection process. By bias we mean a systematic but often hidden deviation of the sample from the population.

The following is a list of the most common types of bias and related text selection principles to avoid this bias:

• Text sample bias: different sections of texts (e.g. beginning, middle and end) have different linguistic properties. For example, ends of texts tend to include language that summarizes the main point of the text (in sum, to conclude, they lived happily ever after). If a corpus samples only certain sections, e.g. by taking the first or the last 2,000 words of each text, these sections will be overrepresented. In corpora that do not include whole texts but only text samples (like the Brown family) it is therefore important to achieve a balance in terms of the different sections of texts represented.

• Topic bias: topic bias is created when many texts on the same topic get included in the corpus (unless the corpus is deliberately constructed as a specialized corpus on the topic). These texts usually contain a number of specific topicrelated vocabulary items that are repeated multiple times in each text. This is connected to a so-called 'whelk problem' (see Section 2.4) when some infrequent lexical items become dramatically overrepresented in the corpus. Topic bias is a problem especially in small corpora where each individual text forms a relatively large proportion of the corpus. In most cases, corpus designers should therefore consciously select texts on a range of topics.

• Non-coverage bias: some texts are more 'visible' than others because corpus designers see them as prototypical for different reasons. For example, published texts might be given a preference over private letters or emails. Corpus designers need to actively seek to cover as wide a range of texts as possible.

• Traditional text type bias: this type of bias is a specific case of non-coverage bias. When selecting the sampling frame we are often predisposed to see as salient the text types that have traditionally been included in language corpora such as those in the Brown family sampling frame (see • Legal considerations bias: corpus designers often face the problem of copy- right; this is especially the case when corpus creators want to share their corpora with other researchers. Legal considerations may thus lead to a selection of texts to which copyright does not apply (older out-of-copyright texts, texts under creative commons licences etc.), which, however, creates a problem with biased sampling. Currently, there is no clear solution to this issue as approaches may differ according to legal requirements in individual countries. Corpus designers, however, should be mindful of this problem.

• Practicality bias: some texts such as webpages are easier to obtain than others.

This practicality consideration may lead to creating a corpus that overrepresents easily obtainable texts. Corpus creators need to resist this temptation and strive for a range of texts regardless of whether they can be obtained easily or not.

• Self-selection bias: this bias is created when contributors (i.e. authors of texts) are asked to provide texts on a voluntary basis. For instance, if we want to create a corpus of classroom writing and ask students to volunteer and contribute their texts, we may end up with texts from highly motivated students that will not reflect the written production of the class as such. Corpus designers therefore need to reach out to different groups of contributors and use a range of incentives to obtain a representative sample. So far, the traditional approach to corpus design has been considered. However, a different approach to corpus representativeness and sampling emerged from the web as corpus initiative

As discussed, a corpus is usually a sample of language. However, in some specific cases, a corpus can include the whole population. For example, in studies of literature, corpora comprising all works by a particular author include the whole population. If we wish to compare the speech of Prince Hamlet with the speech of his friend Horatio in Shakespeare's famous tragedy, we will be working with all the evidence there is about the linguistic behaviour of these two characters. Similarly, if we collect all newspaper articles about a particular topic in a given period we will be looking at the whole population of articles that were written on the particular topic.

Having discussed different aspects of corpus building, one basic question still remains to be answered: how large should a corpus be? There is no universal answer to this query because corpus size depends on the research question and the kind of linguistic features we want to investigate. For the investigation of grammatical structures such as passives, which are generally fairly common, even a small corpus (e.g. one million words or less) can be sufficient. On the other hand, many lexical items and their combinations are fairly infrequent even in very large corpora and we may easily encounter the data sparsity problem. To illustrate this issue, Table

From this example, we can derive a general rule: unless the corpus represents the whole population, the absence of evidence is not the evidence of absence. In other words, if an expression does not appear in a corpus, this doesn't mean that this expression is non-existent. As corpus users we therefore need to think critically about the nature of the evidence that corpora provide in terms of their quality (representativeness and balance) as well as their quantity (corpus size).

Finally, a few words need to be said about the role of statistics in the analysis of corpora. Let us start by considering some general principles followed by the discussion of specific research designs. In the process of corpus analysis, there are four separate but interconnected dimensions (see Table

We usually start our analysis with data exploration during which we look at frequencies and distributions of linguistic variables (see Section 1.3 for statistical terminology) and often produce graphs which capture the main patterns in the data (see Section 1.5 for data exploration and visualization). If our corpus represents a sample rather than the population (which is typically the case), we should consider the amount of evidence we have in the sample. In other words, we can use inferential statistics to enquire whether the observed effects and differences between (sub)corpora can be generalized to the population, i.e. all language that the corpus or corpora represent. Inferential statistics produces p-values or confidence intervals and we use words such as 'statistically significant' or 'nonoverlapping 95% confidence intervals' to describe the inferences. Currently, there is a debate in a number of disciplines such as psychology, sociology and applied linguistics about the place that inferential statistics, especially p-values, should have in the research process.

EFFECT SIZE

How large is the effect in the sample? (standardized measure)

effect size e.g. Cohen's d, r

LINGUISTIC INTERPRETATION

Is the effect linguistically/socially meaningful?

significant. Practical importance uses standardized statistical measures to express the size of the effect; here we are trying to evaluate the magnitude of the effect (e.g. how large the difference really is between two groups). Finally, we need to relate the observed effect back to what we know about language and society and interpret the results in the context of linguistic and social theory. This crucial step seeks to discover linguistic and social meaningfulness of what we observed in corpora.

When analysing corpora, we also need to think about the type and format of the data that needs to be obtained from the corpus in order to answer the research question. This is a so-called research design; research design is important because it has considerable implications for the specific statistical procedures that we can use with the data. In general, three main types of research design can be distinguished: (1) whole corpus design, (2) individual text/speaker design and (3) linguistic feature design. Figure

In the whole corpus design, the unit of analysis is usually the whole corpus, sometimes also large subcorpora. In Figure

In sum, selecting the right corpus, analytical procedure and corpus design is the first step in successful corpus analysis. As researchers, we need to think carefully about what the corpora we are working with represent and how they can reveal interesting findings about language and society.

1.5

Exploring Data and Data Visualization Think about . . .

Before reading this section, think about the following questions:

1. Why is looking critically at data before analysis important?

2. What types of errors can we encounter in a dataset?

3. What types of graphs do you know?

Bad data leads to bad results. No matter how sophisticated our statistical analysis is, if we come up with wrong numbers or make a mistake when copy-pasting the data from a spreadsheet into the statistical software, we will end up with results that are simply wrong. One way of preventing these accidental errors is to keep a research journal which records every step of the analysis so that the procedure can be easily checked or repeated. Another, and probably even more important, aspect of good data analysis is constant questioning of the 'sanity' of the data: Is this the expected size of the corpus or have I counted also part-of-speech tags by mistake? Does this effect make sense given what I know about the distributions of words in texts? What is the reason for this unusual data point? etc. Constantly questioning the data can help avoid making trivial errors and misinterpreting the results.

In addition, in order to understand the main trends in the data, data visualization is crucial. Effective data visualization summarizes patterns in data without hiding important features. The following example illustrates how effective visualization works. Figure

However, Figure

In sum, a boxplot provides much more information about the data than a simple barchart. While the barchart in Figure

If we want to go beyond the sample and generalize about the population we can calculate 95% confidence intervals for the mean value of variable x in the three corpora. The 95% confidence intervals are displayed as error bars in Figure

The type of visualization we use should provide a means for us to better understand the general patterns in the corpus data. This largely depends on the research question and the type of study (research design) we are dealing with. Figures 1.17 provides an overview of different types of more specific graphs used in this book with references to the chapters where these are discussed in detail.

All graphs presented in this section can be easily produced using the Graph tool from Lancaster Stats Tools online. A friend of mine who is an excellent novel writer once told me that she thinks academic writing is dry because academics use very few adjectives. I thought about this for a moment and then, instead of producing a witty reply (which I couldn't think of anyway), I said: 'that's an empirical question'. Next, I offered to check her hypothesis in the BNC, a corpus that samples both academic writing and fiction. The following is a short report I prepared for my friend. Because my friend has very little knowledge about corpora and statistics I used the margins of the report for explanatory notes. Both the report and the notes are reproduced below.

This study is based on the BNC. In particular, two subcorpora of the BNC were extracted to investigate the difference in the use of adjectives in fiction and academic writing. Table

BNC represents the use of British English in the early 1990s. So the corpus is already fairly old. However, the use of adjectives is a stable enough linguistic variable, so there is no need to worry about the representativeness too much. If you are worried about this, though, this study can be replicated with some more recent data.

In this study, each file representing a text written by a single author was considered as a separate observation. The following hypothesis was tested:

This just says that I looked at individual differences between writers, not only at group averages.

• Hypothesis (H 1 ): Academics use fewer adjectives than fiction writers.

This is your hypothesis, remember?

The data was first explored using a boxplot. Then, 95% confidence intervals were calculated for the two subcorpora and r was used as a standard effect size measure. Finally, the independent samples t-test was used to test the null hypothesis that there is no difference between the two groups of writers:

These are different statistical techniques. You don't need to understand the details.

Null hypothesis (H 0 ): There is no difference between academics' and fiction writers' use of adjectives.

Null hypothesis is part of the formal statistical procedure. It is a negation of your hypothesis. The results suggest that there is indeed a difference between academics and fiction writers in terms of their use of adjectives. However, as the boxplots in Figure

Here we go;) Larger homogeneity is signalled by a smaller box and shorter 'whiskers'.

Moving beyond the sample, 95% confidence intervals can be calculated. Figure

The difference is not only statistically significant, it also appears to be linguistically meaningful.

With a big smile on my face I presented the report to my friend. 'Hm, interesting . . . ' she said. 'But I still think that the adjectives fiction writers use are somehow richer.' 'That is a different research question!' I exclaimed in exasperation. Only much later did I add 'Maybe you are right, though . . . Why don't we find out?'

Exercises

1. As a warm-up exercise (with a twist), divide the following shape that represents three quarters of a square into four identical shapes. Feel free to skip this exercise if you want to focus on statistical techniques immediately. After you have done this, take a whole square, but this time divide it into five identical shapes.

2. Calculate the mean for the following numbers:

3. What is a model in scientific thinking? What is a 95% confidence interval? (a) An interval that shows that we can be 95% confident in the correctness of the result within this interval. (b) The measure of objectivity of our findings. (c) An interval constructed around a particular measure in a sample in such a way that the true value of the measure in the population will fall within this interval for 95% of samples.

viii. What is a p-value? (a) The probability that the null hypothesis is true. (b) The probability of seeing values at least as extreme as observed if the null hypothesis were true. (c) The probability of seeing a unicorn in Lancaster.

7. Imagine you have 500 texts (the population of interest), 250 written by a male and 250 written by a female author. However, you don't know which one is which. For the purposes of your study you want to select an unbiased sample of 40 texts that would represent the population. Use the Random number generator from the Lancaster Stats Tools online to create a list of 40 random numbers between 1 and 500 and note these down.

8. In the Answer section at the companion website, you can find which texts were written by a male speaker and which by a female speaker. Check the answers in Exercise 7 and calculate the number of male and female speakers that were selected.

Male speakers in the sample: Female speakers in the sample:

Did you get an approximately equal representation of male and female speakers in the sample?

9. Later, you also find out that half of the 500 texts were written by a young speaker and another half by an older speaker (see the Answer section at the companion website to find out which texts these are

15. Use the Graph tool from the Lancaster Stats Tools online and the data provided there to create graphs visualizing the main patterns in those datasets.

TH I NGS TO R EM EM B ER

• Corpus linguistics is a scientific method.

• Successful application of statistical techniques in corpus linguistics depends on the use of a well-constructed unbiased corpus.

• Statistics uses mathematical expressions to help us make sense of quantitative data.

• Effective visualization summarizes patterns in data without hiding important features.

• Although most visible, p-values form only a (small) part of statistics.

• 'Statistical significance', 'practical importance' and 'linguistic meaningfulness' are three separate dimensions which shouldn't be confused.

Advanced Reading

Biber

Lüdeling & M. Kytö (eds.), Corpus linguistics: an international handbook, vol. 2, pp.1287-1304. Berlin: Walter de Gruyter.

A type is a unique word form in the corpus. When we ask about word types we are asking about how many different word forms there are in the text/corpus. In the two sentences from the 'Think about' task, three formsthe (7, 20), that (10, 25) and time (11, 26)occur twice each (note that times is not counted 1 Some tools (e.g. CQPweb, Sketch Engine) include punctuation in token counts. Others (e.g.

#LancsBox) stay closer to the simple (surface) definition of the 'token' as presented in this book and do not count punctuation. In addition to punctuation, other sources of variation in token counting include: treatment of clitics (e.g. 'll in he'll) and hyphenated words (well-known), tokenization decisions based on morphological analysis (part-of-speech tagging) as well as tokenization decisions for non-segmented languages (e.g. Chinese).

together with time under this definition); each of these is counted as only one type. We are therefore left with 23 word types.

Both word tokens and word types are identified based on the form of a word (external appearance, if you like). To identify lemmas and lexemes, we first need to perform linguistic analysis of the text; lemmas are based on grammatical (morphological) analysis, while lexemes are based on both grammatical and semantic analysis. A lemma is a group of all inflectional forms related to one stem that belong to the same word class

Let us now think about how the different notions of a 'word' (type, lemma and lexeme) influence the kind of analysis we can carry out in corpus linguistics. Using types is the most straightforward approach, based on distinguishing different word forms regardless of their grammatical function or meaning. However, while type is a very useful category, it may obscure some meaningful differences, e.g. uses of the form clean as an adjective (a clean shirt) versus as a verb (to clean something). On the other hand, if we want to use lemma as the unit of analysis, we need to automatically process the corpus to assign each form its part-of-speech and lump together all inflectional forms related to the same basethis involves a certain percentage of errors. Similarly, identification of lexemes, although desirable, involves semantic tagging and semantic disambiguation, which again introduces a certain percentage of errors (even higher than part-of-speech, or POS, tagging) and, moreover, cannot be done fully automatically. Table

Finally, since word counts (mostly token and type counts) are a part of almost every statistical equation that is discussed in this book, it is important that you have a good grasp of these definitions. The exercises at the end of this chapter (see Section 2.8) together with answers, which are provided at the companion website, will help you test your understanding of these crucial concepts.

Reporting Statistics: Tokens, Types, Lemmas and Lexemes

What to Report

When talking about words in corpus linguistics, we need to specify if we mean tokens (running words), types, lemmas or lexemes. When describing corpora, we should always include the information about the exact token count. Because token counts differ from tool to tool we should also provide details about the tool and/or how the token count was arrived at.

How to Report: Examples

• The text consisted of 120 running words and included 69 different types.

• In our study, we used the 100-million-word British National Corpus (exact token count: 98,313,429; BNCweb; punctuation excluded from token count) . What are the most frequent words in English? Try to come up with a list of the top ten most frequent words:

1) , 2) , 3) , 4) , 5) ,

6)

, 7) , 8) , 9) , 10) .

Look at the wordlist in Table

relative frequency ¼ absolute frequency number of tokens in corpus Â basis for normalization ð2:1Þ

For example, the relative frequency of the definite article the, as shown in In this case, we have chosen one million as the basis for normalization, which is a common baseline in corpus linguistics. This means that, on average, there are over 61,000 instances of the definite article for every million tokens in the corpus. In fact, in all corpora of written English you can expect the definite article to be at the top of the wordlist, with an absolute frequency roughly equivalent to 6% of the overall number of tokens in the corpus. The relative frequency can therefore be considered as the mean frequency ('mean' is a statistical term for a type of average; see Section 1.3)that is, the mean of the frequencies of the word in hypothetical samples of x tokens from the corpus, where x is the basis for normalization (in this case one million). This idea about the mean frequency will be useful later, when we come to discuss other statistical measures.

In smaller corpora, smaller bases for normalization than one million are more appropriate, e.g. normalization per 10,000 or even 1,000 words. The reason for this is that relative frequency is used not only to compare frequencies of a particular type in two (or more) corpora, but it is also used to present evidence about the frequency of words and phrases in a form that is easier for a reader to grasp than the absolute frequency would be. If we choose a basis for normalization that is too large relative to the actual corpus size, this can 'blow up' our numbers artificially and thus effectively misrepresent the (limited) evidence we have. For example, imagine a very small corpus that contains 11,000 tokens, including the type homeostasis which appears only once in the corpus. If we choose one million as the basis for normalization, we will get over 90 per million words as the relative frequency of homeostasis. Although the proportion is mathematically correct, this is extremely misleading for the reader, given the behaviour of rare words. When a rare word such as homeostasis occurs just once in a small corpus, it is highly unlikely that it would occur at the mathematically equivalent 90 times in a million words. It might just as easily occur once in the hypothetical million-word corpus, or five times, or maybe not at all: the actual smaller corpus simply does not give us enough evidence to extrapolate. In this example, a more appropriate basis for normalization would therefore be 10,000 which gives us the relative frequency of approximately 0.9. It is important to stress that relative frequencies should never be used to hide the absolute frequencies but should be reported together with absolute frequencies.

It is crucial to always remember that a corpus is a sample of language (see Section 1.4). It provides evidence about the use of words and phrases; however, this evidence can be limited even in fairly large corpora. For example, in the 100-million-word BNC, over half of the types occur only once (we call these words hapax legomena

Put informally, Zipf's law tells us that when we start with the most frequent item in the wordlist (regardless of the size of the corpus), the second most frequent item will have only half of the frequency of the first item. The third most common word will have one-third of the frequency of the first item; and so on. In other words, the amount of evidence that we can get from corpora about words diminishes rapidly. This can be seen in Figure

Formally, Zipf's law can be expressed as:

absolute frequency of a word Â its rank in a wordlist ffi constant ð2:3Þ or absolute frequency ffi constant rank in a wordlist ð2:4Þ

where the constant is the frequency of the first item in the wordlist. Note that Zipf's law represents an approximation. The actual frequencies of words in a real corpus will differ to a certain extent from those predicted by this model. The practical implications of Zipf's law for corpus linguistics are as follows: we have to learn to critically evaluate the amount of evidence we have for our claims. For hapaxes and low-frequency words, this evidence is naturally limited. To answer some research questions, we therefore need very large corpora (billions rather than millions of words), in which even fairly infrequent words occur multiple times. We also need to look for further (comparable) evidence of word behaviour from multiple sources (see Section 8.3 on meta-analysis).

Reporting Statistics: Absolute and Relative Frequencies

What to Report

When reporting frequency of words, we should report both the absolute (raw) and the relative frequency. The relative frequency needs to be normalized to the appropriate basis that is similar in size to the corpus or its parts (subcorpora or texts) that we are interested in.

How to Report: Examples

• The preposition of is the second most frequent item in the BNC (AF = 3,042,376, RF = 30,945.68 per million). • The word corpus occurred 20 times in the text (13.3 per 1,000 words).

2.4

The Whelk Problem: Dispersion Think about . . .

Before reading this section, think about the following questions:

1. Do you know what a 'whelk' is?

2. How often do you think we use this word?

So far, we have been looking at frequencies of words. However, to fully describe a word or phrase in a corpus, we need to introduce another concept, namely dispersion. This concept is best illustrated with the so-called 'whelk problem'. This term was introduced by

As an example, imagine a one-million-word corpus, which is divided into six parts of unequal size (each representing a different genre/register) as described in Table

Range 2 (R) is a very basic and fairly crude measure of dispersion.

As we can see immediately (look at the row 'Includes w?'), the range for the word w in Table

The range 2 is sometimes also calculated as a percentage out of the total number of corpus parts: We can say that the range 2 of word w in the one-million-word example corpus from Table

However, range 2 is not a very good measure for quantifying the amount of dispersion across individual corpus parts because it is based on a simplistic YES/NO decision about the presence or absence of a word or phrase in each part, disregarding the actual frequencies in the different parts. Range 2 also does not take into account the size of the parts. To illustrate this, imagine a different word (w 1 ) which has the same absolute frequency in the example corpus as word w (i.e. 50), but a very different distribution, namely 46, 1, 1, 0, 1, and 1 in the six corpus parts. When we calculate the range 2 of w 1 , we'll get the same number as for word w (i.e. 5 out of 6 or 83.3%), although the large majority of all occurrences of w 1 are in only one part (Part 1) whereas word w is, in comparison, more evenly spread out across the corpus. This lack of sensitivity of range 2 as a dispersion measure is its major limitation. Because of this, range 2 can be used for a first (simple) exploration of the corpus data; but for further analyses more sensitive dispersion measures are preferable.

Standard deviation is a classic measure of dispersion, which is used very often also outside corpus linguistics. It expresses how much the individual values in a dataset (here, the relative frequencies of w in the individual parts of the example corpus from Table

In Figure

The superscript reminds us that we are looking at a simple form of standard deviation calculated assuming that we are dealing with the whole population. This form of standard deviation (SD p or σ [sigma]) differs slightly from the sample standard deviation (see below). For the data in Table

A brief explanation of the procedure: squaring the distances (

There is another (slightly modified) way of calculating the standard deviation which we will be using when we consider inferential statistics, rather than the descriptive statistics we are discussing here. This is so-called sample standard deviation (SD). It is calculated in almost the same way as the standard deviation described in equation (2.9) above. The difference is that the sum of squared distances is divided not by the total number of corpus parts but by the total number of corpus parts minus 1 (the reason for this is connected with the notion of 'degrees of freedom' explained in Section 6.3). standard deviation sample ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi sum of squared distances from the mean total no: of corpus parts À 1 s ð2:11Þ

For the purposes of describing dispersion in a corpus the basic version of standard deviation from equation (2.9) should be used. Standard deviation is a useful measure when we want to see how homogeneous or heterogeneous the distribution of a word is. Standard deviation always needs to be considered in relation to the mean (the relative frequency of the word in the corpus overall). In our example from Figure

Because the SD needs to be considered in relation to the mean, we cannot use this measure to compare the dispersions of different words (or phrases) that occur with different frequencies. In these cases, other measures of dispersion such as the coefficient of variation, Juilland's D or DP (see below) are more appropriate.

The coefficient of variation (CV) describes the amount of variation relative to the mean relative frequency of a word or phrase in the corpus. The more variation in the frequencies of a word/phrase in the individual parts there is, the more uneven the dispersion. The equation for CV is very simple:

Coefficient of variation ¼ standard deviation mean ð2:12Þ

For word w in the example corpus in Table

CV ðwÞ ¼ 4:55 5 ¼ 0:91 ð2:13Þ

The coefficient of variation is a standardized measure; this means that it can be compared across different words and phrases in one corpus. The closer the coefficient is to zero, the more even the distribution of the word or phrase is. The maximum value of the coefficient of variation depends on the number of parts in the corpus, and is equal to the square root of the number of parts minus 1 ð ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi no: of corpus parts À 1 p Þ. Sometimes, the coefficient of variation is multiplied by 100 and presented as a percentage of variation. This is, however, problematic, because the coefficient of variation can be greater than 1 (when the SD is greater than the mean) which would give a percentage higher than 100. However, a true conversion to percentage can be achieved by considering the maximum possible variation in a given corpus. We know that the maximum value of CV depends on the number of corpus parts and can be calculated as ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi no: of corpus parts À 1 p . The following equation thus converts a CV to a percentage out of the maximum possible observed variation. When we apply this to word w in the example corpus from Table

This means that the dispersion of w is less than 50% of the maximum possible variation. The maximum level of variation would be reached if the word occurred only in one part of the corpus.

Juilland's D is a measure of dispersion that builds on the coefficient of variation. It is a number between 0 and 1, with 0 signifying extremely uneven distribution and 1 perfectly even distribution. Juilland's D was originally developed for use in frequency dictionaries

In essence, Juilland's D is an inverse number to CV (and CV % ). While CV tells us about the amount of variation in the corpus (larger CV means more variation in the frequencies), Juilland's D tells us about homogeneity of the distribution (larger Juilland's D means a more even distribution and less variation). The following formula is used to calculate Juilland's D:

Coefficient of variation ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi no: of corpus parts À 1 p ð2:16Þ

For the example from Table

This value (0.59) shows an uneven distribution, which, however, is closer to 1 (perfectly even distribution) than to 0. Juilland's D has been criticized in the literature

The expected proportions are calculated by taking one-by-one the sizes of the corpus parts (number of tokens) and dividing them by the total number of tokens in the corpus; this is to establish their proportional contribution to the overall size of the corpus. The assumption is that if a word or phrase is evenly distributed in the corpus it should follow the proportional distribution calculated in this step, hence the expected (or baseline) distribution. The observed proportions are calculated by taking, again one-by-one, the absolute frequencies of the word or phrase of interest in the corpus parts and dividing these by the absolute frequency of the word or phrase in the whole corpus. This is done to establish how much proportionally each part of the corpus contributes to the overall frequency of the word or phrase. By comparing the observed and the expected proportions (taking absolute values of the difference) and putting the differences together we get the DP measure. For example, DP for the values in Table

This value (0.38) indicates an uneven distribution, which, however, is closer to 0 (perfectly even distribution) than to 1. In this case, DP provides a similar picture to Juilland's D.

Reporting Statistics: Dispersion Measures

What to Report

When reporting frequencies in corpora we should also include the information about dispersion. The decision about which dispersion measure to report should be motivated by the aims of the researchwhat aspect of dispersion is necessary for us to be able to interpret the frequency correctly. The main options (there are others) include range (R), percentage range (R%), population standard deviation (SD p ), sample standard deviation (SD), coefficient of variation (CV), percentage coefficient of variation (CV%), Juilland's D and DP.

How to Report: Examples

• The word corpus occurs 773 times in the BNC (6.9 per million) but only in 201 texts (R% = 5%). • The definite article the is the most frequent word (type) in the BE06 corpus occurring in the texts with the mean relative frequency of 51.64 per 1,000 (SD = 14.17). • Swear words are unequally distributed in the BNC64 corpus. For instance, fuck occurs 123 times in only 14 out of 64 speaker samples with DP = 0.85.

2.5

Which Words Are Important? Average Reduced Frequency Think about . . .

Before reading this section, think about the following question:

How would you find out which words in English are important to know? Arguably, words that occur frequently across a large number of contexts are important because they are very likely to be encountered in a variety of communicative situations. For example, for learners of a language it is crucial to know which lexical items they should learn first. These are often not concrete words (as we might wrongly assume) but general abstract terms.

For instance, the most widely used noun in English is time, which can be found frequently across different contexts in both speech and writing: we speak (and write) about time all the time. Recently,

Average reduced frequency (ARF) is a measure that combines frequency and dispersion

• Absolute frequency of the word • Corpus size (total number of tokens) • Positions of the word in the corpus The absolute frequency and the total number of tokens were discussed in previous sections (see

where v ¼ total corpus tokens absolute frequency of word ð2:20Þ and minðdistance n ; v Þ signifies the smaller of two values: (1) value of the distance between two occurrences of the word or (2) value of v. This is done for all of the distances between two occurrences of the word in the corpus.

Although the equation looks complicated, the idea behind ARF is fairly simple: we notionally divide (this is not done physically but as part of the calculation) the corpus into x parts of the same size (v). The number x is the absolute frequency of the word we are interested in. This means that the number of the notional parts, and their length (v in equation (2.20)), depend on the frequency of the word in the corpus. Then we simply count the number of notional parts that include the word that we are interested in. We call this count the 'reduced frequency'. The purpose of this exercise is to disregard occurrences of a word which are close to each other (i.e. fall within the same part) and count them only once. Then, in addition, to make this procedure robust, we repeat the process for every possible beginning-point in the corpus (think about the corpus as a circle rather than a line) and calculate the mean of all the reduced frequency values that we get via the procedure described above. This mean value is the value of the average reduced frequency.

Let's demonstrate this with the following example: imagine a one-million-word corpus in which we search for two words w 1 and w 2 . Both words occur five times in the corpus, i.e. with absolute frequency of 5. The first word (w 1 ) occurs only in one text, at corpus positions 1 -5 -10 -15 and 20, while the second word (w 2 ) is evenly distributed at positions 1 -200,000 -400,000 -600,000 and 800,000. Note that 'p' in Figure

The ARF for w 1 is calculated as follows: First we establish the length of the notional parts, v v ¼ 1; 000; 000 5 ¼ 200; 000

Then we calculate the distances between the individual occurrences of the word (w 1 ) in the corpusto do this we need to use the corpus positions. Note especially how distance 1 is calculated: it is the distance between the last and the first occurrence of w 1 in the corpus.

• distance 1 = 1st occurrence + (total corpus tokens -last occurrence) = 1 +

(1,000,000 -20) = 999,981 • distance 2 = 2nd occurrence -1st occurrence = 5 -1 = 4

• distance 3 = 3rd occurrence -2nd occurrence = 10 -5 = 5 • distance 4 = 4th occurrence -3rd occurrence = 15 -10 = 5

• distance 5 = last occurrence -4th occurrence = 20 -15 = 5

Finally, all the terms are inserted into the ARF equation:

As we can see from the calculations above, the ARF for word w 1 is approximately 1. This should be interpreted as follows: because the five occurrences of w 1 are all very close to each other, they should be counted as if they were only one occurrence. On the other hand, if we calculate the ARF for w 2 , we'll get a number close to 5. This means that because w 2 is evenly distributed throughout the corpus, its frequency should not be reducedwe should continue to consider each instance as a separate occurrence. Here's the mathematics: If you find it difficult to get your head around the details of this process, don't worry: the ARF is designed to be calculated automatically by a computer (see the ARF calculator in Lancaster Stats Tools online). The purpose of explaining the ARF in this section is to help you understand the general principles of this powerful measure.

Reporting Statistics: ARF

What to Report

ARF can be reported in addition to absolute and relative frequencies of a word. It can be used to rank-order words in a frequency list to highlight the most frequent and evenly dispersed items.

How to Report: An Example

• The following table shows the top four lemmas in the BNC ranked according to ARF; the absolute frequency figure is also provided. When looking at texts and corpora we can think about how different words (types) are used to communicate meanings. Some words (especially grammatical words) are often repeated, others are used only a few times. To measure whether overall a text or corpus uses a wide range of vocabulary or only a limited range of lexical items which get recycled, we can calculate a lexical diversity statistic

The simplest lexical diversity statistic is the type/token ratio (see Section 2.2 for the definition of types and tokens). Type/token ratio (TTR) expresses the proportion of types (different word forms) relative to the proportion of tokens (running words). The idea is that a larger number of different word forms (types) relative to the number of all words in text (tokens) points to a lexically more varied text. Type/token ratio is calculated as follows: type=token ratio ¼ no: of types in text or corpus no: of tokens in text or corpus ð2:23Þ

For the two texts from the 'Think about' task the type/token ratio is 0.8 (28/35) and 0.93 (28/30) respectively. This shows that text B (academic text) is more lexically diverse than text A (informal speech). This is a valid comparison because the texts are of comparable size (have a similar number of tokens). However, we have to remember that the type/token ratio is very sensitive to the length of the text; it decreases as the text becomes longer and more words get used again (recycled). The simple type/token ratio from equation

Standardized type/token ratio (STTR) is a label used by

Reporting Statistics: TTR, STTR and MATTR

What to Report

When reporting different versions of the type/token ratio it is important to also report the parameters influencing the outcome. For TTR the text length needs to be reported; for STTR and MATTR the standard segment size and the window size respectively need to be reported.

How to Report: Examples

• Simple type/token ratio (TTR) was used to compare the texts because they were of the same length (2,000 tokens). The TTR values are as follows: 0.36 (text 1), 0.33 (text 2) and 0.39 (text 3). • MATTR (window size: 100) of Dickens's Christmas Carol is 0.67.

2.7

Application and Further Examples: Do the British Talk about Weather All the Time?

In this section, we'll go through an example of a research project applying the statistical procedures that were introduced in this chapter. Imagine that our task is to investigate typical topics (content words) that are frequently discussed in British English. This can give us an insight into the general public discourse practices of British society. In particular, we want to find out if weather is one of the prominent topics typical of British culture (as the stereotype would suggest). The corpus we want to use is BE06, a corpus of current written British English,

When we transform the Zipf's law equation

The next step is to decide what we actually want to analyse. We have seen that there are different definitions of a 'word' (see Section 2.2) and therefore we need to make a decision about the definition that we will be using. Our options are type, lemma or lexeme. After considering the pros and cons, let's assume we have decided to work with lemmas in our research. Lemmas are usually a good choice if we are interested in culture or discourse, since the distinctions that get submerged are the inflectional differences which are generally not relevant for this kind of research. At this stage, we can start analysing the data. The frequency list based on lemmas confirms our initial assumption (based on Zipf's law) about the number of words with the frequency of 30 and over: there are 3,196 such lemmas in the corpus. In fact, the actual number of lemmas is higher than expected, which shows that Zipf's law provides only a rough estimate of word frequency distributions. Looking now more closely at the list of lemmas, we note that among the 3,196 lemmas meeting our inclusion criterion there are 13 lemmas related to weather (this analysis had to be done manually by going through the lemma list): cloud, cold, flood, heat, hot, ice, rain, storm, sun, temperature, warm, weather and wind. As can be seen from Table

As we can see, most of the items are fairly evenly distributed in the corpus with Range values 8 to 15 and Juilland's D values 0.7 or 0.8. Nevertheless, there is one exceptionthe lemma flood. Although this lemma occurs in a majority of corpus parts (Range 10) it has a very low Juilland's D (0.2). This signifies a very uneven distribution in the 15 corpus genres. Indeed, a closer inspection of this word confirms that flood occurs mainly in one genre-based part of the corpus representing the official documents. In fact, the majority of occurrences (38 out of 54) come from a single parliamentary report on floods.

After this initial exploration, to answer the research question about the prominent topics in British public discourse we need to look at the weather terms in the context of other words (lemmas) in the corpus. For this, we need to sort the lemmas according to their prominence in written British English and consider the position of the weather terms in this ordered list. We can calculate the average reduced frequency (ARF), which combines frequency and dispersion, and order lemmas according to their ARF values. Table

Exercises

TH I NGS TO R EM EM B ER

• There are different concepts of a 'word' -token, type, lemma and lexeme.

• Zipf's law describes the distribution of words in corpora and their rapidly diminish- ing frequency.

• To fully describe a word in a corpus we need to provide both the word's frequency and its dispersion.

• Different dispersion measures (range, SD, CV, CV%, Juilland's D, DP) are appro- priate in different situations.

• The average reduced frequency (ARF) is a measure that combines both frequency and dispersion; it can be used with corpora that are not divided into different parts (subcorpora).

• TTR is a measure of lexical diversity; it is sensitive to text length.

• STTR and MATTR are alternative measures of lexical diversity that can be used with texts of varying lengths. So far, we have looked at words in isolation. In this chapter, we will explore meanings of words in context, which is an area important to both linguistic and social analyses. Topics discussed herecollocations, keywords and manual coding of concordance linesplay a key role both in the study of semantics ('dictionary' meanings of words) and in discourse analysis. The chapter starts with a simple premise: word meanings can best be investigated through the analysis of repeated linguistic patterns in corpora. Techniques such as keywords help us to draw attention to words characteristic of particular texts or corpora that can be further investigated using methods such as collocation, i.e. investigating repeated co-occurrence of words, and concordancing, i.e. analysing examples of word use in context. Five questions in particular are addressed in this chapter:

• How do we identify collocations? (Section 3.2)

• What are collocation networks? (Section 3.3)

• How do we identify keywords and lockwords? (Section 3.4)

• How can the manual coding of concordance lines be made more reliable? (Section 3.5) • How can the techniques discussed in this chapter be used in linguistic and social research? (Section 3.6)

Collocations and Association Measures

Think about . . .

Before reading this section, think about the following questions:

1. What associations come into your mind when you see the word love?

2. Why do you think the word has these associations for you?

It is a well-known fact in corpus linguistics that words occur in combinations that we call collocations. More than fifty years ago,

The following example demonstrates how the identification of collocations works in practice:

My love is like a red, red rose that's newly sprung in June: My love is like the melody that's sweetly played in tune. As fair art thou, my bonnie lass, so deep in love am I: And I will love thee still, my dear, till a' the seas gang dry. Till a' the seas gang dry, my dear, and the rocks melt wi' the sun : And I will love thee still, my dear, while the sands o' life shall run. And fare thee weel, my only love, and fare thee weel a while! And I will come again, my love, thou'it were ten thousand mile.

(Robert Burns, 'A Red, Red Rose')

The example above is taken from an English version of Burns's famous poem (originally written in Scots) 'A Red, Red Rose'. For the purposes of illustration, all occurrences of the word love with their immediate contextone word to the left and one word to the rightare highlighted and the poem is displayed without line breaks to create one paragraph of run-on text.

Let's assume that we are interested in the use of the word love in the poem. We will call the word love, our word of interest, a 'node'. A node is a word that we want to search for and analyse. The words around the node are candidate words for collocates. Collocates are words that co-occur with the node in a specifically defined span around the node, which we call the collocation window. Metaphorically, we can imagine this as a 'magnetic field' around the node that attracts particular collocates like a magnet attracts metal objects. In this case, the span (collocation window) is one word to the left and one word to the right, sometimes abbreviated to 1L, 1R. In the example given above, within a 1L, 1R collocation window around each occurrence of the node, we can observe the words that co-occur with love, namely, in the order of appearance, my (three times), is (twice), in (once), am (once), will (twice), thee (twice), only (once), and (once) and thou (once). Note that in each case, the frequency of co-occurrence was provided in the brackets; we call this value the observed frequency of collocation. Let us consider the word which occurs most frequently with love in our example, my. At this stage, we need to ask: is my really a genuine collocate of love in the poem? In other words, is my really strongly associated with love? To find out, we need to find a way to evaluate the observed frequency. We have three basic options:

1. No baseline: we compare the observed frequencies of all individual words co-occurring with the node and produce a rank-ordered list. 2. Random co-occurrence baseline ('shake the box' model): we compare the observed frequencies with frequencies expected by chance alone and evaluate the strength of collocation using a mathematical equation which puts emphasis on a particular aspect of the collocational relationship. 3. Word competition baseline: we use a different type of baseline from random co-occurrence; this baseline is incorporated in the equation, which again highlights a particular aspect of the collocational relationship.

The first (simplest) option does not involve any statistical calculation. We merely produce a rank-ordered list of words co-occurring with the node based on their frequency, such as, in our example, my (3), is (2), thee (2), will (2) . . . The words towards the top of the list are the strongest collocates by the frequency count. The disadvantage of this approach is that the top collocates will typically be function words co-occurring with the node merely by the virtue of their frequency anywhere in the corpus. Frequency-based collocates are therefore fairly generic (we can expect a similar set of collocates for almost any node) and have only a limited usefulness.

The second option involves a comparison with a random co-occurrence baseline. We ask whether it is possible that the combination of words in question (e.g. my and love) occurs repeatedly only due to chance. Consider this:

1. The poem has 107 tokens (see Section 2.2 for a definition of 'token'). 2. Love occurs 7 times in the whole poem. 3. My occurs 8 times in the whole poem. 4. My occurs 3 times in combination with love and 4 times in combination with other words.

Imagine also that there were no associations between words in the poem and words appeared randomly in the text. This situation is illustrated in the example below, which shows the words of Burns's poem in random order: fare art And like red, sweetly in love love, And gang wi' played like dear, life shall rocks sprung the Till deep my my And still, weel, again, ten the the while! is till And As I: a' only come were sands sun: dry, and gang it a' the still, My thee will in my bonnie My red is a run. my love thee thou, melt the seas and thou' I the I lass, I melody thee a my am rose love dear, that's love newly love fare love, will o' so dry. fair thee will that's in while June: my seas tune. mile. thousand weel dear, How many times would you expect the words my and love to co-occur by chance alone? In the random example above, my love occurs once; in fact, if we run the random simulation multiple times we will get an average (mean) number close to one. We call this process establishing the random co-occurrence baseline and the resulting value is called the expected frequency of collocation. The expected frequency of collocation does not have to be established empirically but can be calculated as follows:

expected frequency of collocation ¼ node frequency Â collocate frequency no: of tokens in text or corpus ð3:1Þ

When considering collocation window sizes larger than one, a correction could be applied to account for the fact that there is a greater chance of words randomly co-occurring with the node. The corrected expected frequency of collocation is calculated as follows:

expected frequency of collocation ðcorrectedÞ ¼ node frequency Â collocate frequency Â window size no: of tokens in text or corpus ð3:2Þ

In our example, the expected frequency of collocation of love and my in the 1L-1R span would be calculated as follows:

expected frequency of collocation ðlove; my; correctedÞ

We can see that the observed frequency of collocation of love and my (3) is larger than the expected frequency (1.05). To compare the difference between the two values, different association measures can be applied (see

Finally, to avoid the potentially problematic 'shake the box' model of language, some association measures operate with a different type of baseline; these measures do not include E 11 in the equation. The baseline needs to be understood on the caseby-case basis derived from the specific formula of the association measure.

Generally, to be able to understand the equations, we need to consider the terms that enter these equations. These are best displayed in the form of contingency tables (showing all possible combinationscontingenciesof word co-occurrence). Table

The expected frequencies table is derived entirely from the observed frequencies table, using the equations shown in Table

Finally, one question remains to be answered: which is the 'best' association measure? Frustratingly for some, the answer is: it depends on which aspects of the collocational relationship we want to highlight. Some collocation measures such as MI highlight rare exclusivity of the collocational relationship, favouring collocates which occur almost exclusively in the company of the node, even though this may be only once or twice in the entire corpus. Other metrics, such as Dice and log Dice, and MI2 favour collocates which occur exclusively in each other's company but do not have to be rare. Others can take into account directionality (Delta P) or dispersion

To choose a specific association measure we first need to define the type of collocations we are interested in (based on our research question). We can think of most association measures as highlighting collocations along two main dimensions: frequency and exclusivity. Frequency refers to the number of instances in which a node and collocate occur together in a corpus. Exclusivity refers to a specific aspect of the collocation relationship where words occur only or predominantly in each other's company. According to how the association

Node absent measures rank the individual collocates, association measures can be placed on a two-dimensional scale indicating the extent to which these association measures highlight the frequency and/or exclusivity of the collocational relationship (see Figure

Reporting Statistics

What to Report

For the sake of replicability of results, all major parameters that can affect collocate identification should be reported. For this purpose,

How to Report: An Example

• The following items were identified as top five collocates of the adjec- tive new in the BE06 corpus using the MI statistic (3b-MI(

Collocation Graphs and Networks: Exploring Cross-associations

Think about . . .

Before reading this section, think about the following questions:

1. What words come to mind when you see the word university? Write down at least five associations.

2. Review the list and underline those words which you think are more closely associated with university than the rest of the words in the list.

Collocation graphs and networks build on the idea of collocation introduced in Section 3.2. A collocation graph is a visual representation of the collocational relationship between a node and its collocates. Instead of a list of collocates displayed in a table (the usual format), the graph shows the relationship between the node and the collocates by displaying collocates closer to or further apart from the node. 2) is expressed as the length of the link between the node and the collocate: the closer the collocate is to the node, the stronger the relationship (think of a magnet). The frequency is displayed as the shade of the colour of the collocate: the more intense the colour the more frequent the collocate is. Finally, the position of the collocate in text (whether it occurs predominantly before or after the node) is shown by the position of the collocate in the graph (left, middle or right). For instance, fall, falling and fell occur always before the node love, while you occurs both before and after love with approximately the same frequency.

Collocation networks are extended collocation graphs, which show larger association patterns than those seen from the immediate collocates discussed so far

It is best to illustrate this idea with an example of a relatively simple collocation network based on the one-million-word LOB corpus (see Figure

Another example that demonstrates the concept of collocation networks is the word university and its cross-associations that can be seen from Figure

Collocation graphs and networks are useful summaries of complex meanings of words in texts and corpora. These networks can provide useful information about key topics in texts and discourses as well as their connection. For effective building of collocation networks specialized software is necessary that is able to run multiple comparisons of word co-occurrence and display the data in a visual form. Such a tool, #LancsBox

Keywords and Lockwords

Think about . . .

Before reading this section, think about the following question: Which of the lists in Table

Identifying keywords is one of the crucial techniques in corpus linguistics

So, let us unpack the keyword procedure: a corpus of interest (C), sometimes referred to as a 'focus corpus'

1.

How to Choose a Reference Corpus?

Typically, a reference corpus is larger than or similar in size to the corpus of interest so as to provide a large enough amount of evidence about word frequencies (see question 2 below). Generally speaking, the larger and the more similar the reference corpus is to the corpus of interest the more reliable and focused the comparison is. This is because every two corpora are different in a number of aspects which get highlighted to different degrees in the keyword procedure. To illustrate the point of the relative heterogeneity of any two corpora, let's compare two excerpts taken from a corpus of American (AmE06) and a corpus of British (BE06) English respectively, each excerpt consisting of exactly 100 words.

Text A Text B

Democrats call those shifts too little, too late. "Changing direction in Iraq starts with changing the people we send to Washington," Shays' challenger, Diane Farrell, said Saturday in the Democratic response to Bush's radio address.

Something behind him went 'gloink'. It was a small, subtle and yet curiously intrusive sound, and it accompanied the appearance, on a shelf above Rincewind's desk, of a beer bottle where no beer bottle had hitherto been.

He took it down and stared at it. It had recently contained a pint of Winkle's Old Peculiar. There was absolutely nothing ethereal about it, except that it was blue. The label was the wrong colour and full of spelling mistakes but it was mostly there, right down to the warning in tiny, tiny print: May Contain Nuts. Now it contained a note.

Democrats, who since the Vietnam War have battled voter perceptions that they are soft on defense, are finding a more receptive audience for the argument that they could do a better job of protecting America and conducting its foreign policy. In the poll, 52% say the Iraq war has made the USA less safe from terrorism. Nonetheless, Republicans continue to view the issue of terrorism as . . . We can start enumerating the differences between the texts by pointing out that text A is about American politics, while text B is a part of a story about a person called Rincewind. Text A comes from a newspaper, text B from a novel. Text A uses American spelling (defense), whereas text B sticks with the British spelling conventions (colour). We may also begin to observe individual lexical differences reflecting the topics of the two texts. Some readers might also notice the unusual onomatopoeic word gloink in text B and words related to the war in Iraq (Bush, Iraq, terrorism, war etc.) in text A. These are only a few differences that illustrate the aspects in which two texts can be different from each otherthe same is true, on a large scale, about two corpora.

The crucial question to ask in this context therefore is: what kind of language do the corpus of interest (C) and the reference corpus (R) represent and how is the composition of each corpus reflected in the comparisonthe keyword procedure? (See Section 1.4 for the discussion of corpus representativeness.) In practice, every keyword (or lockword) identified should be related back to what we know about the composition of the two corpora (C and R) and the multiple sources of difference between them. We should also consider which words would get highlighted as keywords had we chosen a different reference corpus. In fact, we can select multiple reference corpora and carry out the keyword procedure with each of them and compare the results.

2.

How to Deal with Absent Words?

When comparing two corpora, it often happens that a word that occurs frequently in one corpus is absent from the other corpus. The question is how to deal with these words in the keyword procedure. We know that unless a corpus represents the population (all language use), absence of evidence is not evidence of absence (see Section 1.

• Is X a positive keyword or is the reference corpus not large enough?

• Is Y a negative keyword or is the corpus of interest not large enough?

In practice, some corpus tools let us set the minimum cut-off limits for the frequencies of words in C and R before considering them in the keyword procedure. This, however, needs to be done very thoughtfully.

3.

What Statistical Measure to Use for Comparing Corpora?

To answer this question, let us take as an example the two corpora from which excerpts A and B (see above) were taken, AmE06 and BE06, each consisting of approximately one million words. Let us consider five lexical items that stand out as different in texts A and Bwar, defense, pint, Rincewind and gloinkas well as two more general (grammatical) words, the and he. This time, however, the comparison will be made between the whole corpora that include texts A and B respectively. Let's also assume that AmE06 is our corpus of interest (C) and BE06 is the reference corpus (R). We can see in Table

To help make these decisions, traditionally the log-likelihood (LL) statistic has been used to establish whether the differences between C and R are likely to be due For example, to calculate the log-likelihood statistic for the word war from Table

Instead of using the values of the LL statistic, which are relatively difficult to interpret, for the identification and sorting of keywords,

The constant k simultaneously serves as a filter that allows focusing on words above certain relative frequencies in the corpus. For example, if we use 1 as the constant, we highlight low-frequency unique words, while 100 would filter out words that occur with the relative frequency smaller than 100 per million words if the relative frequency per million words is used. The simple maths parameter for the word war (with k = 100) is calculated as follows.

simple maths parameter ðwarÞ ¼ 620 þ 100 267 þ 100 ¼ 1:96 ð3:7Þ

The interpretation of the value of the simple maths parameter is more straightforward than that of the LL statistic: given that the relative frequencies of the word are larger than 100 (which we specified when choosing k), we can say that war occurs approximately twice as much in C as in R.

Currently, the question of which statistic best suits the identification of keywords is an open one. Other suggestions for sorting principles are %DIFF

In sum, the term 'keywords' might be slightly misleading because it suggests that there is a single set of words that characterize a particular corpus. However, as we have seen, the keyword list is a result of multiple decisions in the process starting with the selection of the reference corpus and finishing with the choice of the particular statistic. To demonstrate this point, let's return to the question from the 'Think about' task at the beginning of the chapter. If you have chosen your favourite keyword list for American English, you might be interested in knowing which procedure was used to identify these keywords. Table

Reporting Statistics

What to Report

The outcomes of the keyword procedure are influenced by three crucial parameters: (i) the selection of the reference corpus, (ii) implementation of minimum frequency cut-off points and (iii) the choice of the statistical measure. It is also customary to report whether all identified keywords or only the top 10, 50, 100 etc. were used. The parameters listed above are usually reported in the Procedure subsection of the Method section. This, however, needs to be complemented with a careful description of the corpus of interest C in the Data subsection of the Method section. • AmE06, which represents written American English sampled in 2006, was used. AmE06 consists of 15 genre-based subdivisions which can be grouped into four broader genre divisions: newspapers, general prose, academic prose and fiction . . .

Procedure

• Words typical of American English were identified by comparing AmE06 and BE06 using the keyword procedure. BE06 was used as a reference corpus because it was created using the same sampling frame (Brown family sampling frame) and therefore represents the same genres of written English sampled around the same point in time (

Inter-rater Agreement Measures

Think about . . .

Before reading this section, think about the following question:

Which of the concordance lines in Table

Finally, let us focus on an area that has not yet received sufficient attention in corpus-based discourse studies, the inter-rater agreement. Inter-rater agreement, which is an estimate of how reliable and consistent a coding is, should be reported in studies working with a judgement variable. This is a variable that involves categorization or evaluation of cases (e.g. concordance lines) by the analyst that might bring an element of subjectivity into the study. The larger the element of subjectivity, the larger the need for double coding and reporting of inter-rater agreement. For example, if we want to group the occurrences of the word time into ten different semantic categories (think of dictionary definitions), this would involve a certain amount of subjectivity because semantics is notoriously fuzzy. Should time in a particular context be categorized as X or should it be subsumed under Y? Another example is the categorization from the 'Think about' task where you were asked to decide which concordance lines show the use of the word religion in a positive context and which in a negative context. Clearly, your understanding of what religion is as well as your evaluation of religion may have an impact on judging the examples as positive or negative (see the discussion below). On the other hand, situations such as categorizing sentences according to the grammatical categories of the verbs into active and passive constructions involves relatively little subjective judgement and doesn't require double coding. Such situations also usually lend themselves to automatic methods; in a part-of-speech-tagged corpus, active and passive constructions can be searched for and counted automatically.

So how is inter-rater agreement approached? Before calculating inter-rater agreement, we need to find a second rater, another researcher on the project or a colleague who is willing to help. The first step is to carefully explain the coding system to themusually a written coding scheme with concrete examples which both raters can refer to is a good idea. We then ask the second rater to independently code the same dataset or (especially if the dataset is large) a random sample taken from the dataset. Finally, using an appropriate statistical procedure, we can estimate how reliable our coding is or, in other words, how much subjectivity is involved in the coding. Before discussing the details of the statistical procedure, let's have a look at some data. Let's assume that we have asked two raters, a religious person and an atheist, to code the concordance lines from the 'Think about' task. The results can be seen in Table

We can see that the religious person and the atheist agree in six cases and disagree in four cases. These four cases of disagreement represent situations in which the context does not provide enough evidence about the evaluation of the example and the raters therefore rely on their own understanding of the situations. The first statistic that we can calculate easily is the raw agreement. Raw agreement is a metric, often expressed as a percentage, which provides the proportion of agreement cases in all cases. Raw agreement is calculated as follows: raw agreement ¼ cases of agreement total no: of cases ð3:8Þ

For the example of the two raters, the raw agreement is:

We can thus say that in the example above, the raw agreement is 0.6 or 60%, which is a relatively low number. Ideally, we would be aiming at agreement of 80% and above.

Raw agreement is a useful first approximation of the reliability of the rating. However, we need to consider the fact that some agreement between the raters would be achieved even if both raters coded the cases randomly. More sophisticated inter-rater agreement measures such as Cohen's Kappa (κ) or Gwet's AC 1 therefore take this agreement by chance into account and subtract it from the raw agreement. Cohen's κ has been traditionally used for nominal variables; recent studies (e.g.

In the example above, Cohen's κ is calculated as:

agreement by chance ¼ 0:7 Â 0:3 þ 0:3 Â 0:7 ¼ 0:42 ð3:11Þ

Note: 0:7 Â 0:3 is the mathematical expression of the chance of positive rating by the first rater (7 out of 10) and by the second rater (3 out of 10). 0:3 Â 0:7 is the mathematical expression of the chance of negative rating by both raters.

κ ¼ 0:6 À 0:42 1 À 0:42 ¼ 0:31 ð3:12Þ

In the example above, AC 1 is calculated as:

Note: In 10 2Â10 , 10 is the number of positive ratings by both raters (7 + 3) and 2 Â 10 is the number of ratings by both raters. 1 À 10 2Â10 À Á is the complementary probability, i.e. the probability of not being categorized as positive by chance.

Both measures, κ and AC 1 , produced low numbers: 0.31 and 0.2 respectively. These are closer to 0 (which is the baseline for random agreement) than to 1 (which shows absolute/perfect agreement).

The scale below can help interpret the results of κ

So far, we've looked at a simple situation where we have two raters who have been coding a dataset using two categories (positive or negative). However, if we have more raters or a different number of categories or the rating is done using ranks or numeric values that can be placed on a scale, we need to select different measures of rater agreement. Table

What to Report

The following information should be provided for the reader to evaluate the reliability of coding of a judgement variable: (i) number of raters, (ii) amount of data coded (the whole dataset or random sample), (iii) inter-rater agreement measure, (iv) p-value and (v) interpretation of the result. The information described above should be reported in the Method section of the research report.

How to Report: An Example

• Following the coding scheme described above, two independent raters coded a random sample of 100 concordance lines from a total of 1,053 containing the word religion in the corpus. Gwet's AC 1 measure showed agreement between the raters (AC 1 = 0.7, p < 0.001). A review of the differences between raters found no systematic pattern of disagreement.

Given the nature of the judgement variable, the amount of agreement was deemed sufficient.

Application and Further Examples: What Do Readers of British Newspapers Think about Immigration?

This section illustrates the statistical procedures introduced in this chapter in the context of a short discourse analysis study. The study focuses on the perception of 'East European immigrants' by readers of two British newspapers, the Guardian (a 'heavyweight' newspaper, politically leaning to the left) and the Daily Mail (a right-wing mass-market newspaper), using two corpora based on readers' comments below articles on immigration on the newspaper websites. It is assumed that because the two respective newspapers attract a different readership, the analysis of the comments will reveal different perspectives on immigration.

At this stage, it might be useful to provide some historical context for the study. In January 2014, Britain opened its job market to citizens from Romania and Bulgaria. In the run-up to this event, the British press frequently debated the possible impact of this decision on the British economy and quality of life in Britain. In the media, comparisons were also made with a previous event ten years

The data covers the period from 2010 to 2013. All articles in the Guardian and the Daily Mail containing the search term 'east europeans' or 'eastern europeans' were identified and the reader comments on these articles were extracted. 'East (ern) Europeans' is a collective term frequently used by the British press to refer to people from new European Union countries (e.g. Romania, Bulgaria, the Czech Republic or Poland). Overall, 942,232 tokens were extracted from the Guardian (GU corpus) and 2,149,493 from the Daily Mail (DM corpus).

First, to show an overall difference between the two corpora, top ten positive keywords were identified for both the GU and DM corpora. When extracting keywords for one of the newspapers, the comments of the readers from the other newspaper acted as a reference corpus in order to highlight words specific to the Guardian or Daily Mail readership. For the identification of keywords,

Apart from the two 'obvious' keywords Guardian and DM referencing the two newspapers, we can see an interesting pattern of differences between the two corpora. While the keywords in the GU corpus appear more neutral and related to the theoretical aspects of the immigration debate (economic, argument, debate), the keywords found in the DM corpus predominately point to negative aspects of immigration (homeless, benefits, police, squatters). The emotional intensity in the discourse of Daily Mail readers can also be seen from the frequent use of capitalization

(3) Rarely is the distinction made between asylum seekers, immigrants and illegal immigrants. Personally, I have no time for people who easily take a swipe at hard working low-paid legal migrants who often take jobs that unemployed UK citizens sometimes find unpalatable.

(GU, 29/04/2010) (4) There you go again. Is "immigrants and asylum seekers" some kind of single entity to you? (GU, 29/03/2010)

In addition, the collocates blame and blaming also stand out in the Guardian debate. Although blame and blaming are words with negative semantic prosody, here they critically reflect the tendency to use immigrants as scapegoats for different social issues which many Guardian readers perceive as unjustifiedsee the example below.

(5) Sure there are issues, but blaming immigrants for everything isn't going to address the real issues is it? (GU, 06/06/2010) When we compare Figures 3.6 and 3.7, the most distinct difference can be seen in the labels used to denote the quantity of the immigrants coming to Britain. While the Guardian readers largely use the term influx, the Daily Mail readers also use flood, wave and swamped.

To investigate if the word immigrant(s) was used predominantly in a positive or negative context in the two reader discourses, a random sample of 100 concordance lines was extracted from each corpus and manually coded by two raters on a 5-point Likert scale from very positive (1) to very negative

We can see that Guardian readers use the term immigrant(s) in more positive than negative contexts, while the Daily Mail readers use very few positive evaluations of immigrant(s); the DM corpus is thus dominated by negative and very negative evaluations (together over 50% of comments). (a) You need to identify technical terms connected with the word process in a corpus of research articles on organic chemistry, e.g. petrochemical process. Note that technical terms are exclusive and relatively rare combinations of words with a specific meaning. (b) You want to study the associations that the word enemy (node) has in the newspaper discourse. You are interested to see content words around the node rather than frequent grammatical words. (c) You want to write a dictionary of collocations for learners of English that would include a broad range of fixed expressions such as find out, take responsibility, dire consequences etc. The collocations you include need to be recognizable as specific meaningful units and they need to occur as frequent combinations.

2. Look at the information in Table

Use the online Collocation Calculator to calculate four association measures: MI, LL, Delta P and log Dice.

• Number of tokens in the whole corpus (N): 1,001,514

• Frequency of the node in the whole corpus (R 1 ): 164

• Collocation window size: 6 (3L, 3 R)

3. Discuss how the association measures from Exercise 2 rank the six collocates. Which association measure would you choose?

Collocation Networks

4. Compare the pairs of collocation networks in Figure

7. Calculate the SMP statistic for the words in Table

Inter-rater Agreement 8. The following ratings were obtained in three situations involving a judgement variable. Calculate the inter-rater agreement in each situation.

Rater A

Transcriber A: 1, 4,

9. Look at the examples in Table

They show how speakers of English as a foreign language express disagreement. Decide how polite (or impolite) these speakers are when they express disagreement. Use the following rating on a 5-point Likert scale:  After the rating, answer the following questions:

• How confident are you about the ratings you have provided?

• Would you consider politeness a robust judgement variable?

• How important do you think it is to have another rater for this judgement variable?

10. Compare your coding in Exercise 9 with the coding of the same dataset by a different rater (e.g. ask a friend to help you with this exercise). Using the Agreement calculator, calculate the appropriate agreement measure.

Measure calculated: ,Value:

• If available, keep adding more raters and calculating the inter-rater agreement.

11. Imagine you need to produce a research report based on the dataset discussed in Exercises 9 and 10. Report the results of the inter-rater agreement measure from Exercise 10. Refer back to the 'Reporting statistics' box.

TH I NGS TO R EM EM B ER

• There are many association measures each highlighting different aspects of the collocational relationship (e.g. frequency or exclusivity). There is no one best association measure.

• Collocations can be presented in a tabular (table) or visual form (graph).

• Collocation networks show complex cross-associations in texts and discourses.

• The keyword procedure is in essence a comparison which depends on a number of parameters. There is no such thing as one set of keywords.

• For judgement variables an inter-rater agreement statistic should be reported.

Gwet's AC 1 and AC 2 , Cohen's and Fleiss's as well as interclass correlation can be used depending on the situation. This chapter focuses on the statistical analysis of lexico-grammatical features in language (such as articles, passive constructions or modal expressions). We start with a discussion of two types of approaches to lexico-grammar

• How can lexico-grammatical variation best be described? What types of research design can be used? (Section 4.2) • How can lexico-grammatical variation be summarized and what simple statis- tical measures can be computed? (Section 4.3) • How can we build complex models that account for multiple variables that predict lexico-grammatical variation? (Section 4.4) • How can the statistical techniques discussed in this chapter be used in the analysis of lexico-grammar? (Section 4.5)

4.2

Analysing a Lexico-grammatical Feature Think about . . .

Before reading this section, think about the following situation:

A friend who is learning English shows you a sentence in a newspaper article entitled 'Google unveils new logo at turning point in company's history'.

When looking at lexico-grammar from a broad perspective, we can see that there is a large amount of variation related to the situations in which language is used. For example, even purely grammatical words, such as articles in English, which we might expect to be stable in language, show considerable variation in their distribution in different registers of spoken and written English (speech, fiction, newspapers, general writing and academic writing).

The stacked bar chart in Figure

From Figure

We can now return to the 'Think about' task and reflect on the question using the evidence presented in Figure

The first step is to use a different research design (see

The dataset in Figure

(1) Lands were granted to a group of men known as feoffees, who became the legal owners of the land, while the grantor enjoyed the use of the landsin other words, all the rights and profits arising from them. But because the feoffees were the legal owners, the lands could not be taken into wardship if the grantor died leaving an heir under age . . . (BNC, file: E9V) (2) In September, a month after the RSPCA conference, she was in England for the publication of her new book. (BNC, file: A7D) (3) The kit includes a fine brass pendulum and chain along with a detailed book to point you in the right direction. (BNC, file: CBC) (4) This effect, which is strongest over the frontal lobes, was first observed in 1964 by Grey Walter, who called it the Contingent Negative Variation.

(BNC, file: A0T)

Before looking at the types of analyses we can carry out using a Linguistic feature design (see

Table

A word of caution: we need to realize that not all linguistic variables lend themselves to the type of research design described above. Ambient linguistic variables such as discourse markers, hesitations or swearwords that can appear in any possible context and do not have a clearly defined lexicogrammatical frame cannot be treated in the same way as the variables from Table

(5) It's about time that was done. (BNC, file: KBB) (6) Well, you know, it you see, time were, I don't know I suppose, I don't know but I never seemed to be afraid . . . (BNC, file: HDK)

In sum, we have seen two approaches to the analysis of lexico-grammatical variation. One approach looked at general distributions of lexico-grammatical features in broadly defined subcorpora, while the other focused on individual linguistic contexts, in which lexico-grammatical features are used. The approach that would arguably be the most fruitful for answering the question in the 'Think about' task is the exploration of the linguistic contexts in which articles in English are used (following the Linguistic feature research design). The statistical techniques employed in such an exploration are the topic of the remainder of this chapter.

4.3

Cross-tabulation, Percentages and Chi-squared Test Think about . . .

Before reading this section, think about the following questions.

1. Which of the following expressions do you say most often?

• I must go.

• I have to go.

• I need to go.

Can you think of contexts in which you would use each of them?

A good starting point for any data exploration is a simple summary table. In Chapter 3 (see Sections 3.2 and 3.4), we have seen contingency tables that were constructed for collocation and keyword analyses to show all possibilities (contingencies) of word (co)occurrence. Cross-tabulation (cross-tab), which is explored in this section, is a similar technique

A small aside: the information included in a simple cross-tab table can be visualized in a mosaic plot (Figure

Note that the presentation of the data in a mosaic plot is the reverse of the cross-tab table: in the mosaic plot, the categories of the explanatory variable are listed horizontally, while the categories of the linguistic variable are displayed vertically. For more information see

Returning to the cross-tab table: often, in addition to frequency counts, crosstabulation tables include percentages for easier comparison across categories. The percentages can be computed from row totals, column totals or the grand total according to the following equation: percentages in a cross -tabulation table = cell value relevant total Â 100 ð4:1Þ

It is important to note that each of the three percentage options (percentages out of row total, column total or grand total) has a completely different interpretation and is useful for a different type of comparison. For example, the percentage out of row total of the first cell in We can say that out of all non-determined contexts a large majority (96.2%) prefer the indefinite to the definite article. In other words, in this context, there is a 96.2% (or 0.962) probability of occurrence of the indefinite article. More generally, we can interpret the percentage based on the row total as the probability (preference where over 50% or dispreference where less than 50%) of a given variant of the linguistic variable in a given context.

Alternatively, we can calculate the percentage based on column totals. Using the same example from We can say that out of all indefinite articles, 92.6% occur in non-determined contexts. Although this may sound similar to the previous option, the logic of this statement is different. Here, we don't compare the two variants of the linguistic variable but two contexts (determined and non-determined).

Finally, we can calculate the percentages from the grand total as follows:

% contextually non-determined indefinite articles in the corpus = 25 100 Â 100 ¼ 25%

ð4:4Þ

Note that because the grand total in Table

So far, we have seen only a very simple form of cross-tabulation (2 × 2 table). With a larger number of explanatory variables (and their categories) we can create more complex cross-tabulation tables. In the 'Think about' task, you were asked to think of the contexts for the use of must, have to and need to. All of these are modal expressions with a similar meaning indicating strong obligation, in other words, indicating that something is necessary or should be done. Table

The last point to discuss in this section is the appropriate test for statistical significance that can be used with cross-tabulation; a statistical significance test evaluates the amount of evidence against the null hypothesis (see Section 1.3). 1. Independence of observations. We assume that every observation such as the use of an article in English (see Table

Chi-squared is calculated according to the following equation:

Chi-squared ¼ Sum for all cells of ðobserved frequency À expected frequencyÞ 2 expected frequency ð4:5Þ

We know from Chapter 3 (Section 3.2) that expected frequencies are frequencies that we would expect to see if there was no relationship between the variables in the data, i.e. if the null hypothesis were true. Expected frequencies function as a baseline that we use to establish if there is a real relationship between variables, in this case an effect of the explanatory variable on the linguistic variable. Expected frequencies are calculated as follows:

Expected frequency ¼ row total Â column total grand total ð4:6Þ

Let's take as an example the use of articles in English discussed above. The data, so-called observed frequencies, are provided in Table

The chi-squared test value (statistic) is then calculated as follows:

Chi-squared ¼ ð25 À 7:02Þ 63 is significant at the 0.05 level and the 0.01 level respectively. In our case, the p-value associated with the test value 85.25 is very small p < 0.0000000000000001, which is usually reported as < 0.0001. Remember, statistical testing is not a competition for the smallest pvalue (see Section 1.4). In the statistical testing procedure, we evaluate the amount of evidence which we have in the data for the rejection of the null hypothesis that states that there is no relationship between the variables in the data (the use of articles and the type of context in Table

Cramer's V ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi chi-squared total observations Â ðno: of rows or columns; whichever is smaller À 1Þ s ð4:8Þ Applied to our example we get:

The resulting value of 0.923 can be interpreted as a very large effect size. Cramer's V ranges from 0 to 1 and Table

Another option, which is in many cases preferable to the general effect size such as Cramer's V, is the probability ratio (PR, also known as relative risk or risk ratio). As the term suggests, it is a ratio of two probabilities from the crosstab table comparing the probability of a particular linguistic outcome (e.g. the definite article) occurring in one context type relative to the same outcome occurring in the other context type. Because this effect size is a ratio of two values, it is suitable only for simple 2 × 2 cross-tab tables where there are only two categories (levels) of each variable. It is calculated as follows: probability ratio ¼ probability of outcome of interest in context 1 probability of outcome of interest in context 2 ð4:10Þ

The probabilities of the definite and indefinite article in two context types are calculated in Table 4.7. 11 As can be seen, these are the values in the cells divided by the row totals. Unlike in controlled experimental research such as in medical studies,

• PR of indefinite article in non-determined vs determined context ¼ 0:962 0:027 ¼ 36

• PR of indefinite article in determined vs non-determined context ¼ 0:027 0:962 ¼ 0:03

.29

• PR of definite article in non-determined vs determined context ¼ 0:038 0:973 ¼ 0:04

• PR of definite article in determined vs non-determined context ¼ 0:973 0:038

For instance, we can see that the indefinite article is 36 times more likely to occur in a non-determined context than in a determined context. Conversely, the indefinite article's probability to occur in a determined context is 0.03 times its probability to occur in a non-determined context; that is a very small probability in comparison. The scale on which the probability ratio operates is 0 to infinity and the interpretation is as follows:

• A probability ratio of 1 means there is no difference between the two contexts.

• A probability ratio smaller than 1 means the linguistic outcome of interest is less likely to occur in context 1 than in context 2. • A probability ratio larger than 1 means the linguistic outcome of interest is more likely to occur in context 1 than in context 2.

Sometimes, an alternative measure called odds ratio is reported instead of probability ratio. Odds ratio uses odds instead of probabilities. However, its interpretation appeals less to common sense than probability ratio because we tend to understand probabilities better than odds

odds ratio ¼ probability ratio Â probability of outcome of interest in context 2 probability of outcome of interest in context 1 ð4:11Þ

For more discussion of odds ratio see Section 4.4. Finally, it should be noted that in addition to the effect size measure we should also compute the confidence intervals (95% CIs) for effect size to be able to estimate the range within which the effect is likely to occur in the population (language use in general). Reporting Statistics: Cross-tabulation and Chi-squared

What to Report

In the case of simple situations with one linguistic and one explanatory variable, we report a cross-tab table with percentages as well as the chisquared test results. For the chi-squared test the following should be reported: (i) degrees of freedom (see note 9), (ii) test value, (iii) p-value, (iv) effect size (probability ratio or Cramer's V or both) and (v) 95% confidence interval for the effect size.

In more complex situations (with more explanatory variables), the cross-tab tables alone are sufficient with detailed description of the important/interesting contrasts. If we want to report inferential statistics with complex tables, logistic regression needs to be performed (see Section 4.4).

How to Report: An Example

• There was a significant association between the context type and article type (χ2 (1) = 85.25, p < .001). The overall effect is large: Cramer's V = . In this section, we will discuss a powerful statistical technique called logistic regression. So far, we have explored the nature of lexico-grammatical variables (Section 4.2) as well as some relatively simple techniques for dealing with lexicogrammatical variation (Section 4.3). In the following discussion, we'll look at the analysis of lexico-grammatical variation with logistic regression, a technique that uses explanatory (sometimes also called predictor) variables, which can be both categorical and scale, to estimate their effect on the linguistic (outcome) variable, which has to be a categorical variable.

For those interested in the details, the following is one possible form of the logistic regression equation:

Explaining this equation introduces some new terminology: e is a mathematical constant that is approximately equal to 2.71828,

Before proceeding to the explanation of the details of the logistic regression technique, let us review the basic terminology. In the 'Think about' task, you reviewed terms that are used synonymously in the context of logistic regression applied to the study of lexico-grammar; these terms can be divided into four groups:

• Features of lexico-grammar that are the focus of the research: 'linguistic variable' = 'outcome variable'. • Terms related specifically to the structure of the outcome variables: 'variant (of a linguistic variable)' = 'outcome'. • Contextual variables that help us explain the use of lexico-grammatical fea- tures: 'explanatory variable' = 'predictor variable' = 'predictor'. • Terms related to the structure of categorical variables that are used both as outcome variables and predictor variables: 15 'category of a variable' = 'level of a variable' = 'value of a categorical variable'.

There are several stages of the logistic regression procedure: (i) Data checking (prerequisites and assumptions), (ii) Building a model and (iii) Interpreting the model. To illustrate the logistic regression technique, we'll use the example of the definite article discussed previously (see e.g. Figure

STAGE 1: Data checking (prerequisites and assumptions) Before analysing the data (which we call building a model), we need to check that the dataset is suitable for this type of analysis and that the assumptions of the statistical test are met. First, we need to make sure that the dataset is organised according to the principles of the Linguistic feature research design (see Section 1.4). This means that each occurrence of the linguistic feature of interest such as the definite and the indefinite article is on a separate line and is properly annotated for explanatory variables as shown in Figure

Statistical model:

Combination of predictors with different weights.  Second, as in any type of quantitative analysis, the variables should be measured and coded accurately and consistently. Multivariate analyses (such as logistic regression) are especially sensitive to measurement errors, which can have a multiplicative effect due to multiple variables used in model building. Because many, especially functional, aspects of lexico-grammatical variables (e.g. syntactic/semantic function of a linguistic feature) are coded manually, the consistency and accuracy of coding should not be underestimated. If a judgement variable is involved, double coding of a certain portion of the data (e.g. randomly selected 20%) is recommended (see

Third, we need to have enough data. As a general principle, the more explanatory variables we use, the more data (cases or lines in the dataset) we need to have.

Fourth, we need to check whether it makes sense to build a model. If one predictor perfectly explains (classifies) all cases, then building a model that estimates the effect of different predictors is redundant. There are also mathematical reasons for why such a model cannot be constructed

Fifth, we need to check the assumptions of the test itself (see Osborne 2015: 85-130). There are three main assumptions: (i) independence of observations, (ii) no (multi)collinearity and (iii) linearity. (i) Independence of observations. As with the chi-squared test discussed in Section 4.3, we assume that every observation such as the use of an article is independent of another observation. In corpora, this assumption is usually violated to some extent due to the nature of language, where linguistic features are interconnected, and also due to corpus sampling that is done at the level of texts, not individual linguistic features. Because when studying lexico-grammatical variation, we usually presuppose a certain uniformity among texts of the same type, we don't have to worry about this problem too much apart from making sure that in our sample, linguistic features come from a range of texts. This can be done by taking a random subsample of linguistic features from the corpus. For studies which need to control for individual texts or speakers a method called mixed-effect modelling is used (see Section 6.5 for a detailed discussion of this). (ii) No (multi)collinearity between predictors. Collinearity is characterised by high correlation (r = 0.8 and above) between predictor variables (see Section 5.2 for an explanation of correlation). This needs to be checked for both scale and categorical variables.

With the dataset in Figure

By default, statistical packages provide a comparison of any model we build with the baseline model. If our model doesn't perform better than the baseline model, it is pretty useless because the selected predictors have little effect and we can therefore discard the model. In Table

With md2 out of the way, let's focus on the comparison of md1 and md3. At first it might seem that a model with more predictors is a better one. This, however, is not true. In the same way as we compared our models with the baseline model, we can compare any two models (md1 and md3 in our case) to see if one is significantly better than the other. In addition to statistical significance, which is measured by the log-likelihood test, we also use AIC (Akaike information criterion) to establish which model is the most efficient by reaching significance with as few variables as possible. AIC is calculated as follows: Looking at the model summary, we can see that overall the model is statistically significant with a likelihood ratio test value of 89.79

Let us now focus on the effect of individual predictors by looking at the second part of the output called 'Coefficients'. The coefficients (estimates) are displayed in a table. The first row always displays a so-called intercept (or constant), a baseline value in the model estimating the situation where all predictors are at their baseline values. Remember that for categorical predictors such as the Context_type these are the values that we set as baseline; for scale predictors this is always 0 (see 'STAGE 2: Building a model' above). In md1, the intercept estimates the odds of the definite article occurring in the baseline (that is nondetermined) context. The odds are very small: 0.04 (with 95% CI 0.002, 0.189).

Let us pause for a moment and explain the units which are used to measure the effect of the predictors. The units are odds and log odds (logits), the latter being computed by taking the natural logarithm of odds. Internally, the logistic regression, as the name suggests, operates with log odds. Because these are relatively difficult to interpret, we usually convert log odds to simple odds. Odds are defined as follows: In practice, odds are often used in sports betting. There we ask questions such as what are the odds of our team winning the game? If the odds are 2 to 1 the probability of our team winning is twice as large (i.e. 66.7%) as the probability of our team losing (33.3%). In our example, the odds of the baseline value (intercept), i.e. a definite article occurring in the non-determined context, are calculated as follows (relevant probability values are available in Table

In our example, the odds ratio that shows the effect of the determined context type on the use of the definite article is calculated as follows (relevant probability values are available in Table

For each row, the output table also lists standard errors, the significance statistic Wald's z and the corresponding p-value, which tells us if the specific estimate is statistically significant. Standard errors express how accurately the estimates reflect the value in the population; the smaller standard errors are the better. The Wald statistic (z) is computed by dividing the estimate with its standard error. The effect size for each parameter is the odds ratio discussed above, which is supplemented with 95% confidence intervals, showing us where the odds ratio is likely to lie in the population.

In more complex models (those with multiple predictors), we evaluate the estimates one by one. The following table shows the addition to the coefficients output when we include the scale NP_length predictor in the model measuring the length of the noun phrase. We can see that NP_length is not a significant predictor (p > 0.05) and the odds ratio is very small. 1.037 is close to 1, which means no effect. This observation is confirmed by the confidence interval which actually includes 1; this is a sign of a statistically non-significant result, because in the population the effect can well be null (odds ratio 1). The reason for showing this output is to discuss the meaning of the odds ratio in the case of a scale predictor. The odds ratio of a scale predictor indicates how many times larger (if > 1) or smaller (if <1) the odds of the outcome of interest are compared to the odds of the baseline outcome with one unit change of the scale predictor. In our example, the unit of noun phrase length was one character. The estimate (1.037) therefore indicates that the odds of the definite article the increase 1.037 times with every character added to the noun phrase. This means the longer the noun phrase is the more likely we are to see the definite article. However, as noted, this effect is not statistically significant and we would normally exclude it from the model because there is not enough evidence for it in the data.

In sum, logistic regression is a powerful method that gives us a detailed insight into the effects of different contexts on the linguistic output. It needs to be noted that in this section we have explored a type of logistic regression called binomial logistic regression, that is a logistic regression with an outcome variable with two categories like the and a/an. This represents a typical case in lexico-grammatical research that investigates a competition between two linguistic features within a given lexico-grammatical frame. If, however, the outcome variable has more than two options (categories), a similar technique called multinomial logistic regression is used. Multinomial regression follows the same principles as binomial regression, but the comparisons are somewhat more complex (see

Reporting Statistics: Logistic Regression

What to Report

Because logistic regression is a complex procedure, its successful use depends on a number of steps, as discussed in this chapter. These steps should be briefly outlined in our research report to allow replicability. First, we should indicate how the lexico-grammatical frame was defined and which variables were used and why. Second, we should let the readers know how the data was obtained (e.g. a random subsample of all occurrences of the linguistic features from a corpus) and coded as well as whether any part of the data was double coded (if so, inter-rater agreement statistic needs to be reported). Third, we need to provide details about the overall statistics of the model (LL, p-value, C-index) as well as a table of individual coefficients, including statistical significance information, the odds ratios and 95% confidence interval for the odds ratios.

How to Report: An Example

• Because the focus of the research was the variation between the definite and the indefinite article, all occurrences of these linguistic features were found in the corpus. One hundred cases were then randomly selected and coded for the presence or absence of the definite article (outcome variable). In addition, two contextual variables (predictors) reported in the literature as having an effect on the use of articles in English were also measured. These were the context type and length of the noun phrase. • The context type was a significant predictor of the type of article used.

4.5

Application: That or Which?

While writing this chapter, I encountered the following situation: my word processor underlined with a wiggly line a phrase that included a relative pronoun which, signalling a potential grammatical error or inaccuracy (see

The correction offered was to either add a comma in front of which or use the relative pronoun that instead, with the reasoning being as follows: 'If these words are not essential to the meaning of your sentence, use "which" and separate the words with a comma' (Microsoft 2010).

Having overcome the initial resentment at the fact that a computer was correcting my grammar, I started thinking about how to test whether the rule the grammar checker was applying actually reflects how language is used. I devised a study using BE06 and AmE06, two one-million-word corpora of current written English. The study is reported below.

This study is based on BE06, a balanced corpus of contemporary British English, and AmE06, a balanced corpus of contemporary American English. Each corpus consists of four major written genres (general prose, fiction, newspapers and academic writing).

In BE06 and AmE06, there are 4,736 instances of which and 22,749 instances of that. However, while which is used predominantly as a relativizer in contexts such as the one in Figure

The suggestion from the word processor ('If these words are not essential to the meaning of your sentence, use "which" and separate the words with a comma') has two aspects: a formal and a functional aspect. The formal aspect requires which to be separated by a comma, while the functional aspect requires which to be used in situations where the clause introduced by which is not 21 Part-of-speech tags are grammatical labels automatically attached to words using a part-ofspeech tagger; for more information see

essential to the overall meaning of the sentence (can be left out), something that is in grammatical terminology called a 'non-restrictive' (or 'non-essential') clause. By implication, that, the other relativizer, is to be used in complementary distribution to which, that is in 'restrictive' ('essential') modifying clauses, not separated by a comma. To test both the formal and the functional aspect of the suggestion, two research questions were formulated:

• RQ1 (formal): Is which preceded by a comma or a dash (-) while that appears without a comma or a dash (-)? • RQ2 (functional): What are the factors that affect the use of which and that?

The results of the search of BE06 and AmE06 that answer the first question are displayed in Table

Overall, that (7,472) is used much more often than which

The chi-squared test confirmed that there is a significant association between the relativizer (which or that) and the presence of a comma or dash (χ 2 (1) = 4,595.47, p <.001). The overall effect is large: Cramer's V = 0.689, 95% CI [.669, .709]. A comma or dash is 24.8 (95% CI [21.5, 28.7]) times more likely to appear in front of which than in front of that. Conversely, no separator is 2.7 (95% CI

In sum, we can see a clear preference for no separator occurring before that; a separator may or may not appear before which, although there is a preference for a comma or dash to appear. However, it cannot be stated as a categorical rule that which should be always separated with a comma because as we can see from Table

Apple will just veto and refuse to distribute any application which does not meet its terms.

(BE06_E33) Table

From Table

Overall, the model that includes all the predictor variables ('Variety', 'Separator', 'Clause type', 'Syntax' and 'Length') is significant (LL: 222.31;

So much for the study. The burning question, however, remains: was the computer right after all? If the suggestion by the computer were to be taken as a categorical rule, the answer is certainly 'no'. The study demonstrated that there is a combination of multiple factors that favour or disfavour the use of which (and that) and these factors have to be interpreted as probabilities (or odds, to be precise), not certainty.  The following variables were coded in the dataset:

• Outcome variable: must vs have to and need to combined (baseline).

• Predictor 1 (Variety): British vs American (baseline).

• Predictor 2 (Genre): fiction vs general prose vs press vs academic writing (baseline).

• Predictor 3 (Subject): I vs you vs other subject (baseline). By virtue of being in a text together, many linguistic variables are related in some way. For example, in English the relative frequency of adjectives in texts is related to the relative frequency of nouns because adjectives modify nouns and therefore typically occur together with them; at the same time, this doesn't mean that nouns do not occur without adjectives. As can be seen from Figure

It expresses the amount of covariance (variation that the variables have in common) in the data in terms of the standard deviations (SD 1 and SD 2 ) of the two variables in question; the combination of standard deviations here is used as the standardized measurement unit of 'statistical distance'. Covariance is calculated by computing the means for variables 1 and 2 (mean 1 and mean 2 ), taking each single value of variable 1, calculating the distance from mean 1 and multiplying this by the distance of variable 2 from mean 2. This is expressed by the equation below.

covariance ¼ sum of multiplied distances from mean 1 and mean 2 total no: of cases À 1 ð5:2Þ

The process might look complicated, but the idea is very simple. For illustration, let's extract five texts from those used to produce Figure

The covariance of nouns and adjectives in the five texts from Figure

The covariance is then entered into the equation for Pearson's correlation and standardized using standard deviations. The standard deviations are calculated according to equation (2.11) from Chapter 2 (standard deviation sample). In this case, the correlation between nouns and adjectives in the five texts is positive (as is clear from

• 0 no effect • ± 0.1 small effect • ± 0.3 medium effect • ± 0.5 large effect In addition, the correlation coefficient should be complemented with a pvalue or a confidence interval (CI) to indicate whether there is enough evidence in the corpus to generalize the correlation to the population. The p-value is a result of a test that evaluates the null hypothesis which states that the correlation in the population is 0 (i.e. there is no correlation). The statistical significance of a correlation is directly related to the number of observations (cases). With large numbers, which are typical in corpus research, small correlations are statistically significant, as can be seen from the graph in Figure

Figure

Let's return to our example of the five texts from BE06 displayed in Figure

Let's move on to the Spearman's correlation. Spearman's correlation (r s ), sometimes also denoted by the Greek letter rho (ρ), is used with ordinal data (ranks) or with scale data when the parametric assumptions are violated; in the latter case, the scale data is converted into ranks. Because we are dealing with ranks, we have no means, SDs or distances from the mean at our disposal. Instead, covariance is measured by looking at the differences between the ranks. Spearman's correlation is therefore calculated as follows:

6 Â sum of squared rank differences number of cases Â ðnumber of cases squared À 1Þ ð5:5Þ

Let's take again the five texts from Figure

where z 0 ¼ 0:5 ln 1þr 1Àr À Á ; once computed, Lower/upper limit z need to be converted back to r. 3 Remember that the number 95 indicates the percentage of samples taken from the same population for which the confidence interval contains the true value (i.e. value in the population) of the measure (see Section 1.3).

When the squared rank differences from Table

In this case, the non-parametric correlation (r s ) is once again very largethe same cut-off values as for Pearson's correlation, i.e. 0.1, 0.3 and 0.5, are used for conventional interpretation of the strength of the effect. However, the p-value is larger than 0.05 (p =.083), i.e. by convention we conclude that we don't have enough evidence to reject the null hypothesis which says that the correlation in the population is 0. The five texts therefore do not provide enough evidence that Spearman's correlation is not nullwhat we are seeing could be the result of chance. The difference between the p-value associated with Pearson's and the p-value associated with Spearman's correlation can be explained by the fact that by converting the actual values to ranks we lost some information and hence also the power to reject the null hypothesis. For this reason, Pearson's correlation is preferable with scale variables, while Spearman's correlation is used with ordinal variables (ranks).

Finally, two remarks need to be made. First, Pearson's correlation coefficient r can be used to account for the amount of variability in one variable shared by the other variable. For this, the coefficient needs to be squared; the product, r 2 , is called the coefficient of determination and is calculated using the simple equation below:

Let's take, for example, the nouns and adjectives in BE06 (Figure

Second, when dealing with multiple linguistic variables we can calculate pairwise correlations (either Pearson's or Spearman's). These are usually displayed as tables (correlation matrixes), series of scatterplots or visualization matrixes. As an example, take nouns, adjectives, verbs, pronouns and coordinators in BE06 as our linguistic variables of interest. What is the relationship between these five linguistic variables? The following are three different modes of presenting the results of the same analysis.

Figure

The initial observation can be confirmed by looking at Pearson's correlations, shown in Table

For those for who prefer further visualization of the results reported in Table

To sum up, correlation is a powerful technique for exploring the relationship between variables in corpora. With large corpora, which consist of hundreds or thousands of files, even small correlations (smaller than 0.1) will be statistically significant. We therefore need to consider very carefully the meaning of the correlations for the linguistic relationships between different features of language. Traditionally, the correlation coefficients (r, r s ) are reported together with the related p-values. Often, however, reporting confidence intervals (CIs) instead of p-values is preferable because CIs provide a more precise estimate about the actual value of the correlation in the population. It is also important to interpret the size of the correlation (effect size)the observed correlation is best compared with similar correlations in the data or those reported in the literature.

The most economical way of reporting correlation (used especially when reporting multiple correlations in a table) is to add a single (*) or double (**) asterisk next to the correlation coefficient. This conventionally means p<.05 and p<.01 respectively. We can also type out the p-values (although with large corpora with many files these are always very low) or specify the CIs.

How to Report: Examples

• There is a strong positive correlation (r = .52, 95% CI [.46, .58]) between the number of nouns and the number of adjectives used in English texts. This value, however, is not as large as the correlation between verbs and pronouns (r = .81, 95% CI [.775, .836]), which explain each other's occurrence in two-thirds of the cases (r 2 = 66%). • There is a very strong negative correlation (r s = -.83, p < .01) between the number of nouns and the number of pronouns used in English texts. These show a complementary distribution. • English verbs and adjectives in written texts are in an inverse proportional relationship (r = -.63**). So are verbs and nouns (r = -.65**). The negative correlation between verbs and coordinators is only small (r = -.14**) with little observable impact on the style of writing.

Classification: Hierarchical Agglomerative Cluster Analysis

Think about . . .

Before reading this section, think about the following question: which colour terms in Figure

In the previous section, we looked at the relationship between linguistic variables. We saw that many of them are related to some degree. In this section, we'll shift the focus to 'objects' (words, sentences, texts or speakers etc.) that can be characterized using multiple linguistic variables. Instead of looking at the relationship between linguistic features, we'll be looking at the relationship between objects as characterized by these features. The question we will be asking is relatively simple and is demonstrated in the 'Think about' task: how do we classify objects based on linguistic variables? Here, we used a simple example of colour terms characterized by the frequency of use and word length. We saw that black and white are the most frequently used colour words, while more specific terms such as aquamarine, turquoise and burgundy are much less frequent. The latter terms are also the longest and for some perhaps more difficult to pronounce. So how would we go about classifying these colour terms based purely on what we know about their frequency and number of letters? An obvious way of going about this would be to use the plot (Figure

Returning to the question of how to calculate the distance between green and blue in the graph in Figure

where x A is the first coordinate of point A, x B is the first coordinate of point B, y B is the second coordinate of point B etc. We can keep adding variables determining the position of the objects and move from a two-dimensional space to a multidimensional space.

In a two-dimensional space created by the two linguistic variables as in our example, we will use only two pairs of coordinates: x A , x B and y A , y B .

Euclidean distance ðgreen; blueÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ð0:11 À 0:58Þ 2 þ ½À0:32 À ðÀ0:76Þ 2 q ¼ 0:64 ð5:11Þ

However, there are alternative approaches to calculating the distance between A and B. Imagine you are in a big city such as New York and you want to get from A to B. Unless you have a helicopter, you won't be able to go directly. Instead, you'll have to go down one street, then make a 90 degree turn and go down another street. The distance between A and B measured like this, i.e. following the grids at right angles, is called the Manhattan distance. Manhattan distance is calculated according to the following formula:

where jx B À x A j etc. is the absolute value, i. e. a positive number of the difference between the coordinates of A and B.

For green and blue in our example, we'll get:

Manhattan distance ðgreen; blueÞ ¼ j0:11 À 0:58j þjÀ 0:32 À ðÀ0:76Þj ¼ 0:91 ð5:13Þ

As expected, the Manhattan distance is larger than the direct Euclidean distance, although when used for cluster analysis both distance measures yield fairly similar results; however, Manhattan distance is more robust when dealing with outliers (see Section 1.3 for a definition of an outlier). Other types of distance measures include Canberra distance (which is a standardized form of Manhattan distance), Squared Euclidean distance (which places more emphasis on objects further apart) and Percent disagreement (used when working with categorical variables)

At this stage, we have all we need to start the cluster analysis. There are many different types of cluster analysis

As can be seen from Figure

However, with multiple clusters the cluster diagram quickly becomes too complex and potentially hard to read. For this reason, the results of the cluster procedure are often displayed as a hierarchical tree plot (or dendrogram). The tree plot seen in Figure

So far, we have assumed that the method of joining small clusters into larger ones is straightforward. However, in the cluster procedure we need to specify exactly how this is to be done. The question we need to ask is: which of the data points inside a small cluster should be taken as representing the position of the whole cluster? Typically, four answers can be provided:

(1) the closest point to the neighbouring cluster with which we want to merge our original cluster (so-called SLINK method), (

Step 1

Step 2

Step 3

Step 4

As always, there are benefits but also disadvantages to each method. The SLINK method, for instance, is very simple. It leads, however, to the 'chaining' effect visible in the right branch of the dendrogram in Figure

One final point about clusters needs to be made. So far in the example we have considered two linguistic variables in a two-dimensional space. The cluster technique can also be used with multiple linguistic variables. The procedure is similar to the simple example described above with the only difference being the fact that with multiple variables we are looking at the distances between data points in a multidimensional space. An example of the use of the cluster technique with 44 linguistic variables is offered in Section 5.5.

What to Report

Cluster analysis is largely an exploratory visual method to show patterning in the data. Both the parameters for cluster identification (data transformation, distance measure, cluster combination method) as well as the results of the analysis (tree plot) need to be reported. The Method section of the research report describes the analytical procedure and the parameters used. In the Results and Discussion, each tree plot (dendrogram) needs to be carefully discussed. The main question to be addressed is: how many meaningful factors can be observed in the plot?

How to Report: An Example

• The data was analysed using the hierarchical agglomerative cluster technique (z-score data transformation, Manhattan distance, Ward's method).

For an example of reporting and discussing results of the cluster analysis see Section 5.5. How do the two excerpts below taken from the BNC differ in terms of the language they use?

As users of a language, we are multi-stylistic. This means that we can employ a variety of styles of speech and writing depending on the situation. For example, we use a different type of language when talking informally to friends than when we are asked to write a research report. This ability to change the style of speaking/writing is directly reflected in the genres or registers of language we produce, as can be seen in the two examples from the 'Think about' task. For instance, you might have noticed that the spoken dialogue (Excerpt A) uses shorter syntactic structures (so called utterances) than the academic text (Excerpt B), which employs relatively long and complex sentences. At the same time, Excerpt B is much more 'polished'it lacks hesitations (erm), repetitions (I'm, I'm feeling) and false starts (Yes, that is also a thing to . . . erm, I'm) that are typical of informal speech. In addition, there are numerous other linguistic features that characterize informal speech such as frequent use of personal pronouns, contractions and discourse markers, while academic writing typically uses a large number of nouns, prepositions and passives. The issue we are thus presented with in the linguistic analysis of registers is how to make sense of the large amount of functional variation in the data and how to characterize individual registers by looking at the underlying principles of systematic variation

The full multidimensional analysis has four main stages: (1) Identification of relevant variables, (

Step 1: Identification of relevant variables. Before performing the statistical analysis (step 2), we need to identify a large number of linguistic variables in the texts of our corpus. The corpus needs to include different registers (e.g. informal speech, news reporting, academic writing, popular fiction etc.) and we need to identify relevant variables, that is, those that can distinguish between the registers in our corpus. For example, all the variables mentioned in Section 5.2 play an important role in register identification. Usually, several dozen (40over 140) variables are identified;

A more extensive list (141 items) combining lexico-grammatical and semantic features can be found in

Three important details need to be mentioned before we move to step 3: first, prior to carrying out factor analysis, we need to check that the individual linguistic variables correlate reasonably well (above 0.3) with some other variables in the dataset, otherwise we wouldn't be able to combine variables into factors. The checking can be done by looking at the Pearson's correlations between pairs of variables; the correlations are usually displayed in a form of a correlation matrix where each variable is correlated with the rest of the variables in the dataset.

Second, to optimize the results of the factor analysis, the factors (represented by the two axes in Figure

Third, we have to decide how many factors we want to extract. The idea is to extract as few factors that explain as much variation in the data as possible, because the whole point of factor analysis is to reduce a large number of linguistic variables into a few underlying factors. To help us decide, a visualization technique producing a scree plot is used. A scree plot is a graph exemplified in Figure

There are different answers to this question: often factors with eigenvalues above one

Factor 1 can be interpreted as follows: first we look at the variables with high positive loadings (Group A). These variables co-occur in texts and we can assume that they have a common communicative function. When we look at the range of variables in this group we can see that these variables mark informal, highly interactive language with frequent use of firstand second-person pronouns, contractions, adverbs and that deletion. Next, we look at the variables with negative factor loadings (Group B) which have a complementary distribution to the variables with positive loadings. This means that they occur infrequently in texts in which the variables from Group A occur with high frequency and vice versa. Again, variables from Group B share a communicative function, which can be labelled as 'academic-type description' with passives and many modified nouns and nominal forms ending in -tion, -ment, -ness and -ity.

Step 4: Placement of registers on the dimensions. The final step involves placement of the registers on the dimensions. This step can also help interpret the communicative functions of the dimensions from step 3 by looking at the types of texts in which the variables that load high on each factor occur. First, we need to standardize the dataset (see Figure

Dimension score text ¼ variable 1 Group A þ variable 2 Group A …

À variable 1 Group B À variable 2 Group B ð5:14Þ

A dimension score is the sums of z-scores 2 for the variables with high positive loadings (Group A) minus the z-scores 2 for the variables with high negative loadings (Group B). Finally, the mean dimension score for each register is calculated by taking all text dimension scores belonging to the same register and calculating the average value. Each register is then placed on a one-dimensional scale according to the resulting mean value of the dimension score. This can be demonstrated with Figure

We can see that registers with high Dimension 1 scores are different types of fiction in which a lot of interaction takes place. They are therefore placed towards the Involved end of the dimension. On the other hand, government documents and academic writing have a low dimension score clustering towards the Informational end of the dimension. The other registers are somewhere in the middle, some closer to the Involved end and others closer to the Informational end. In addition, to test whether there is a statistically significant difference between the registers placed on Dimension 1, one-way ANOVA is computed (see Section 6.3 for more information); R 2 is also reported showing the amount of variation in the registers explained by this factor. The process of register placement is repeated with each of the extracted factors.

Reporting Statistics: Multidimensional Analysis

What to Report

Multidimensional analysis involves a number of complex steps and statistical procedures which need to be reported for full replicability of the results. The success of MD also directly depends on the reliability of the automatic identification of linguistic variables (tagging) in corpora. The following information should be reported:

(1) Linguistic variables used: all linguistic variables should be listed together with the information on how much they overlap with

(2) The tagging procedure (i.e. which tagger and, where appropriate, which version of the tagset was used) and whether its reliability has been checked.

(3) Type of multidimensional analysis performed: Full MD or Comparison with

(4) Factor analysis rotation (e.g. promax or varimax) and number of factors extracted.

(5) Results: (a) factor loadings, (b) dimension plots, (c) ANOVA and r 2

The Method section of the research report describes the analytical procedure and the parameters used. In the Results and Discussion, each dimension plot needs to be carefully discussed. The main question to be addressed is: what is the underlying functional variation in registers that the method revealed?

2. How to Report: An Example For a contextualized example see Section 5.5.

Application: Registers in New Zealand English

This micro-study is directly inspired by Richard Xiao's Multidimensional exploration of world Englishes

past tense (1), perfect aspect (2), present tense (3), place adverbials (4), time adverbials

MAT analyser

When dealing with multiple linguistic variables, the first step is to look at the correlation between these variables. For this purpose, a 44 × 44 correlation matrix was produced (Figure

This negative correlation indicates that texts in the corpus with on average longer words (as measured by the number of characters) have very few, if any, contracted forms and vice versa. This correlation makes sense linguisticallylonger vocabulary items occur in formal written texts, which rarely use contracted forms. On the other hand, informal spoken registers have many contractions and shorter words. The relationship between contractions and mean word length can be visualized using a scatterplot

Here, we can see that these two variables alone can help distinguish between different registers. In the top left corner of the two-dimensional space, cluster informal conversations. When we move down the diagonal, we can see overlapping clusters of public dialogues, unscripted monologues, letters and pieces of creative writing (left) as well as scripted monologue (right). These are marked by a gradually decreasing number of contractions and increasing mean word length.

When we move beyond the two-dimensional space from Figure

In Figure

To explore more dimensions in the data, multidimensional analysis was performed. Because of space constraints, only a subset of the results of this analysis (first two factors/dimensions) will be reported. Rather than a comparison with

Looking at Factor 2, the largest positive loadings include different types of modals, conditional adverbials if and unless as well as verbs such as command, propose and recommend. Second-person pronouns were disregarded because they have a higher loading on Factor 1. The only negative loading is attributive adjectives. The underlining function of the positive features is modalized production typical of written instructional texts, while the other end of the dimension is descriptive production typical of academic writing and popular informational texts. We can therefore use the label 'Modalized vs descriptive' production. Note that this -0.875 a The numbers in brackets refer to the order of presentation of

dimension is completely different from Biber's (1988) Dimension 2: Narrative vs non-narrative discourse. Biber's narrative/non-narrative features are, however, salient also in New Zealand English; they appear in Factor 3 (not discussed here). Finally, it is worth mentioning that both dimensions show statistically significant differences among the registers as established by one-way ANOVA. However, in the case of the Dimension 1 almost 83% of the variation in the dimension scores of the individual texts is explained by their register membership, whereas in the case of Dimension 2 this number is less than 40%. Dimension 1 is thus a more powerful predictor of register variation than Dimension 2.

Exercises

1. Manually calculate the Pearson's and Spearman's correlations between verbs and adjectives in ten randomly selected texts from BE06. The data is provided below:  A (Press: reportage), B (Press: editorial), C (Press: reviews), D (Religion), E (Skills, trades and hobbies), F (Popular lore), G (Belles lettres, biography, essays), H (Miscellaneous government documents, foundation reports, industry reports, college catalogue, industry house organ), J (Learned and scientific writings), K (General fiction), L (Mystery and detective fiction), M (Science fiction), N (Adventure and western fiction), P (Romance and love story), R (Humour).

This classification is very useful; however, for some purposes it might be too detailed.

Group the individual text types into larger categories based on their functional similarity. Then design a study in which you could verify your grouping.

4. Table

5.

Use the data provided on the companion website and the MD tool to compare registers in current British and American English.

TH I NGS TO R EM EM B ER

• Correlations are used for the investigation of the relationship between two variables at a time.

• Pearson's correlation is suitable for scale variables, while Spearman's correla- tion assumes ordinal variables (ranks). Spearman's correlation can also be used with scale variables if the means as the measures of central tendency do not represent the data well (extremely skewed distributions).

• Hierarchical agglomerative cluster analysis is used for classification of words, texts, registers etc. The result of this analysis is a tree plot (dendrogram).

• The most complex type of analysis out of the three discussed in this chapter is multidimensional analysis (MD). MD analyses a large number of linguistic variables and reduces them to a small number of factors which are interpreted as dimensions of variation. Along these dimensions, different registers can be placed.  Is but a little way above our heads, My only love sprung from my only hate.

Staying for thine to keep him company. Either thou, or I, or both, must go with him. This shall determine that. O, I am fortune's fool.

The notion of style is central to the analyses described in this chapter. Following

But how can we identify such variables?

In variationist sociolinguistics as pioneered by

Although this formal definition of a sociolinguistic variable gives us complete control over the linguistic and social processes behind variation, it reaches its limits fairly soon when we look at other types of systematic variation beyond phonology and simple grammar

In contrast to Labov's formal approach, Biber (e.g. Biber & Conrad 2009) examines functional variation, where the analytical focus is on speakers'/writers' choices 'from the entire inventory of lexico-grammatical characteristics of a language, selecting the linguistic features that are best suited functionally to their situations and communicative purposes'

Finally, to provide a full answer to the question in the 'Think about' task (many individual aspects of the language have already been discussed): Speakers 1 and 2 are real people taken from the British National Corpus (BNC). Speaker 1 is a 17-yearold male student from the Midlands from a working-class background. Speaker 2 is a 14-year-old female student from London also from one of the lower social classes. Speaker 3 (Juliet) and Speaker 4 (Romeo) are fictional characters from Shakespeare's famous play. They are of comparable age and gender with Speakers 1 and 2, but differ in the historical variety of English they speak, social class (Romeo and Juliet come from wealthy families in Shakespeare's imaginary Verona) and, most importantly, the fact that Romeo and Juliet are products of Shakespeare's imagination, not real persons. Interestingly, the speakers deal with somewhat parallel topics: anger and aggression (Speakers 1 and 4) and dating and courtship (Speakers 2 and 3).

6.3

Group The conversation above comes from BNC64

So which statistical test should we use in this situation? The simplest option we have is the t-test, and in this case we'll use a version of the t-test that is called Welch's independent samples t-test. This test compares two groups of speakers (e.g. male and female speakers). The t-test compares the mean values of the linguistic variable (in our example, the relative frequency of personal pronouns in individual speaker samples) and takes into consideration the internal variation in each group expressed as variance. Variance (S 2 or SD 2 ) is the sum of squared distances of individual values from the group mean divided by the degrees of freedom. In fact, it is the squared version of sample standard deviation (see Section 2.4), hence SD 2 .

Variance is calculated according to the following formula:

Variance ¼ sum of squared distances from the mean degrees of freedom ð6:1Þ Figure

The logic of computing variance is simple: to see how much overall variation there is, we add the squared distances from the mean (we use squared rather than simple distances because distances below the mean are negative and would cancel out the positive distances) and divide this by the degrees of freedom. The degrees of freedom is a complex concept, which is used when dealing with calculations based on a sample (corpus) rather than a population; in corpus linguistics, this is the case most of the time. The degrees of freedom (df) signifies a number of independent ('free') components in our calculation, i.e. components that are not predictable from the previous components. In practice, it is the number of cases (texts/speakers) or groups (when looking at group variance) minus one. We subtract one from the number of cases (groups) because the last case is always predictable from the previous cases.

Let's return to the t-test, which uses variance as one of its components. Like every statistical test, the t-test has assumptions, which need to be reviewed before running the test. The most important assumption is the independence of observations. In the sociolinguistic context, the independence of observations means that each observation (text or speech sample) comes from a different (randomly sampled) speaker 4  and that the use of language by one speaker in the sample is not affected by the use of language by another speaker. We therefore need to make sure that we include each speaker as an observation (case) only once even if multiple texts/transcripts are available, otherwise we are counting the same person twice/multiple times and violating the first part of the assumption. The second part of the assumption is easy to maintain in written texts or monologues, but in a conversation, this assumption will be violated to some extent if speakers interact with each other (something known as a 'priming effect' in psycholinguistics and more generally as 'accommodation' in sociolinguistics). We have to be mindful of this effect and evaluate its impact in each individual case. Two other assumptions listed in textbooks (and often misunderstood) are the normal distribution and homoscedasticity. Normal distribution of the linguistic variable in the population is an assumed symmetrical distribution visualised as a bell-shaped curve (see Section 1.3). Homoscedasticity 5 is a technical term for the equality of variances, i.e. amount of variation, in two groups that we want to compare. As shown in the literature

Welch's independent sample t-test is calculated according to the following formula:

Variance of group2 Number of cases in group2 s ð6:3Þ 4 If we have two or more samples from each speaker and are interested in the difference in their language between sample 1 and sample 2 etc. (e.g. linguistic change/development), we are dealing with a so-called repeated measures design, which requires a different version of the statistical test (see below for more information). 5 Homoscedasticity is a bit of a tongue twister; you can use the more descriptive term 'equality of variances' instead.

As can be seen from the equation, there are three factors that have an effect on whether the test will be significant: (i) size of the mean difference, (ii) variance in each of the two groups and (iii) sample size (number of cases, i.e. speakers or texts, in both groups). The t-test value is large (and the test is significant) if there is a large difference between the means, small variance in the groups and a large number of cases; these factors combined show that there is enough evidence in the data that the two groups are different with respect to the use of the linguistic variable in question.

In our example, the means of relative frequencies (per 10 k) of personal pronouns for the male and the female subcorpora are 1,451.8 and 1,556.9 respectively, the variances are 22,256.5 and 23,820.5 and we have 32 speakers in each group (see the description of BNC64 above). When we enter this in the equation, we get: In this case, the t-test statistic (2.8), with the appropriate degrees of freedom (61.93) calculated using an equation that will not be introduced here,

In addition to the statistical test, we also need to calculate an effect size measure to evaluate in standardized terms (i.e. units comparable across linguistic variables and corpora) the size of the difference between the two groups. With the t-test, we have several options of effect size measures that include Cohen's d and r as two typically used effect size measures. Cohen's d is calculated as the difference between the two means expressed in standard deviation units.

Cohen's d ¼ Mean of group1 À Mean of group2 pooled SD ð6:5Þ

where

all cases À 2 s

In our example, the calculation of Cohen's d is as follows: This can be interpreted as a medium effect.

As with any other effect size measure, we also need to look at the 95% confidence interval for Cohen's d, which, in our example, is 0.18 to 1.21, as calculated automatically by statistical packages such as Lancaster Stats Tools online. This shows a likely range of the effect in the population (all male and female speakers of British English). Because this 95% CI is extremely wide ranging from a minimum to a large effect, we cannot be sure about the actual size of the effect in the population; this is due to a relatively small sample size (64 speakers). A small aside: for extremely skewed data (i.e. data severely violating the normality assumption) a robust version of Cohen's d has been proposed

In our case, r is 0.33, 95% CI [.08, .53], which, again, can be interpreted as a medium effect with the caveat that the true population effect may range from minimum (0.08) to large (0.53).

So far, we have compared two groups of speakers. However, what if we need to compare more? In such a case a test called one-way ANOVA

(1) cos there ain't [= isn't] no sign of them, is there ? (BNC64, M2) (2) I've won twice ain't [= haven't] I ? (BNC64, F5) BNC64 classifies speakers into four categories reflecting the socio-economic status of the speakers: AB -Managerial, administrative, professional; C1 -Junior management, supervisory; professional; C2 -Skilled manual; DE -Semi-or unskilled.

ANOVA has the following assumptions, which are similar to the assumptions of the t-test discussed above: (i) independence of observations, (ii) normality and (iii) homoscedasticity. The most important one is the independence of observations because as was shown in the literature the ANOVA test is robust against the violation of the normality assumption

One-way ANOVA ðFÞ ¼ Between-group variance Within-group variance ð6:7Þ

To illustrate the logic behind ANOVA, Figure

To calculate between-group variance, that is the variation in the data explained by social class as the explanatory variable, we need to take the group means and calculate their squared distances from the grand mean and multiply these numbers by the number of individual cases in each group. This is then divided by the appropriate degrees of freedom: number of groups minus one. Between-group variance ¼ cases group1 Â ðmean1 À grand meanÞ 2 þ cases group2 Â ðmean2 À grand meanÞ 2 þ … number of groups À 1 ð6:8Þ

Within-group variance is the sum of individual variances for each of the groups, each calculated according to equation (6.1). The degrees of freedom is the number of cases minus number of groups.

Within-group variance ¼ sum of sqared distances for group1 þ sum of sqared distances for group2 þ … number of cases-number of groups ð6:9Þ

In our example, between-group variance, within-group variance and ANOVA (F) are calculated as follows:

Between-group variance ¼ 14 Â ð2:5 À 6:6Þ 2 þ 16 Â ð3:3 À 6:6Þ 2 þ 17 Â ð10:4 À 6:6Þ 2 þ 13 Â ð10:0 À 6:6Þ The related p-value is p = .002 and hence we can conclude that the result is statistically significant.

Because ANOVA is an omnibus test, it detects statistically significant difference anywhere in the data (between any of the groups), but it does not tell us where exactly the difference lies. For this, we need to carry out so-called posthoc tests. Post-hoc tests are pair-wise comparisons of individual group means (similar to the t-test discussed above) with a correction for multiple testing; with multiple testing the probability of a falsely positive result (a so-called type I error) increases. This is because with each test that uses a p-value we are willing to accept that in a small number of cases (5%) the result will be statistically significant, even if the null hypothesis is true (there is no effect of the explanatory variable). With multiple testing, this probability dramatically increases. For instance, with four groups (as in our example), we can run six pair-wise comparisons, which will increase the type I error to 26.5%.

Like post-hoc tests, t-tests with Bonferroni correction are often reported; Bonferroni correction is fairly strict and hence the test is fairly conservative, i.e. it may not have the statistical power to detect small differences. Other options include Tuckey's HSD or different tests based on bootstrapping, a method of multiple resampling (see Section 7.3).

Finally, when reporting ANOVA, in addition to statistical significance, the effect size needs to be reported. The overall (omnibus) effect size that is sometimes reported is eta squared (η 2 ). The standard interpretation of this effect size is as follows

Interpretation of partial η 2 : η 2 > 0.01 small, η 2 > 0.06 medium and η 2 > 0.14 large effect.

A better (more specific) option, however, is to report effect sizes for differences between individual groups alongside the post-hoc tests. Cohen's d or r are the most commonly used effect sizes in this situation.

The t-test and ANOVA also have their non-parametric counterparts called the Mann-Whitney U test (also known as Wilcoxson rank-sum test) and the Kruskal-Wallis test. Non-parametric means that we don't need to know (assume) any knowledge about the parameters (such as the mean or standard deviation) of the variable of interest in the population

1. If the linguistic variable is a scale variable, all data is ranked regardless of the group membership with the highest value receiving rank 1 and tight scores both/all receiving an average rank (e.g. if all three top values are 10.5, the ranking will be 2, 2, 2 calculated as (1+2+3)/3). 2. The data is then divided into groups according to the explanatory variable, e.g. gender or socio-economic status, and the sum of ranks is calculated for each group.

For the Mann-Whitney U (Wilcoxson rank-sum) test, two U values are calculated,

ð6:13Þ

The idea behind the test is this: we take into account the actual sum of ranks for each group from which we subtract the minimum possible sum of ranks that the group can achieve [cases in group 1 × (cases in group 1 +1) /2]. Suppose that we have two groups of five speakers, one of which (group 2 ) has all the smallest ranks (1-5). The sum of ranks for this group is 1 + 2 + 3 + 4 + 5 = 15 and the minimum possible sum of ranks for this group is also 15 [(5 × 6)/2]. The resulting U, the smaller of the two options from equation (6.13), is 0 (15 -15). From the logic of the test follows that small U values show strong differences between groups as exemplified above.

The Kruskal-Wallis test (producing H values) works on a similar principle but takes into account ranks in multiple (3+) groups

Different effect size measures have been proposed for the non-parametric tests (Kerby 2014). As discussed above, an effect size that specifically quantifies the difference between two groups (rather than an omnibus effect size measure) is probably most useful to report. The Mann-Whitney U test can thus be supplemented by the rank biserial correlation

Rank biserial correlation ðr rb Þ ¼ mean rank group1 À mean rank group2 number of all cases

Like the Pearson's or Spearman's correlation coefficients (see Section 5.2), r rb can take on values from -1 to 1. The larger the value is in absolute terms, the stronger the correlation; a negative value signifies that group 2 has a larger mean rank value. Another option for effect size measure is to use probability of superiority (PS) discussed in Section 8.4.

A final note: if we have multiple samples for each individual speaker and are interested, for example, in how their language develops over time or how it was affected by another explanatory variable between two or more sampling points, a so-called repeated measures version of the tests discussed in this chapter needs to be used. Repeated measures tests match individual speakers across conditions and do not assume random distribution of speakers in different groups (see

Reporting Statistics: T-Test, ANOVA, Mann-Whitney U Test and Kruskal-Wallis Test

What to Report

When comparing groups of speakers using the t-test, ANOVA, Mann-Whitney U test and Kruskal-Wallis test, we report (i) the test statistic (t, F, U and H respectively), (ii) the degrees of freedom (for t-test and ANOVA), (iii) p-value and (iv) effect size (including 95% CI).

Exact p-values are reported unless they are smaller than 0.001; after this point p<.001 is reported. For ANOVA and the Kruskal-Wallis test, where multiple groups are involved, post-hoc tests and their relevant effect sizes should also be reported, where relevant. It is also possible to accompany the tests by data visualization such as boxplots and/or error bars showing 95% CIs.

How to Report: An Example t-test

• There was a statistically significant effect of gender on the use of personal pronouns, t(61.93) = 2.77, p = .007, with women (M = 1,556.9, SD = 154.3) using personal pronouns more than men (M = 1451.8, SD = 149.2). The size of the effect is medium, d = .69, 95% CI

• As we can see from Figure

Mann-

Whitney U

• The use of pronouns by female speakers (Mdn

Kruskal-Wallis

• There was a significant effect of social class on the use of the form ain't, H(3)= 15.57, p = .001.

Individual Style: Correspondence Analysis

Think about . . .

Before reading this section, look at the samples of transcribed speech. These come from two different speakers; three samples belong to one speaker, the remaining sample to another speaker. Can you tell which samples belong together? Are there any linguistic clues that can help you tell the speakers apart? Individual style has been investigated with a range of linguistic features both lexical and grammatical. In this section, we take a look at a technique which is somewhat similar to factor analysis discussed in Chapter 5 (Section 5.4), but that has been primarily developed for the analysis of cross-tabulation tables with categorical data, the type of data which was discussed in Chapter 4 (Section 4.3).

Correspondence analysis is a summary technique which outputs a correspondence plot. A correspondence plot is a visual depiction of a cross-tabulation table which is projected in a (typically) two-dimensional space using the chisquared distance as a measure of closeness/remoteness of the categories listed in the table. A 2D correspondence plot (e.g. Figure

1).

Let us briefly go back to the 'Think about' task. Samples 1, 3 and 4 represent speaker F1 from Figure

Conceptually, correspondence analysis is related to the chi-squared test discussed in Section 4.3, which can be performed on a two-way cross-tabulation table (e.g. Table

So how is the information in the table 'translated' into the visual representation in the correspondence plot? Let's start by taking two rows and two columns from Table

The first step in the correspondence analysis is to turn the information in the table into what is called profiles. Profiles are proportions (percentages) based on row and column totals (row and column profiles); the profiles are expressed as decimal numbers, e.g. 0.87, rather than percentages (see

The scatterplot in Figure

where x A is the first coordinate of point A, x B is the first coordinate of point B, y B is the second coordinate of point B etc. We can keep adding categories (different In effect, the chi-squared distance between two points is always larger than the simple Euclidean distance as can be seen in Figure

When we look at a correspondence plot such as the one in Figure

Reporting Statistics: Correspondence Analysis

What to Report

Correspondence analysis is best summarized by the correspondence plot, which is based on a cross-tabulation table; both the plot and the table should be reported. We should note the overall percentage of variation explained by the two factors as well as the linguistic interpretation of these factors. We should then attempt to identify meaningful clusters of speakers in the plot and comment on their relationship. Note that the chi-squared distance can be interpreted only within row/column categories and the distances between row and column categories cannot be directly interpreted. For instance, in Figure

How to Report: An Example

• The data in Table

6.5

Linguistic Context: Mixed-Effects Models Think about . . .

Before reading this section, look at the examples below. Is there a meaning difference between the two utterances? Can you imagine contexts in which you would say these utterances?

(i) This is really good.

(ii) This is very good.

One of the crucial tenets of the traditional variationist sociolinguistics is what Labov calls the principle of accountability

As an example, we will take the variation between utterances (i) and (ii) in the 'Think about' task; this example is inspired by

The first analytical step is to extract the data from the corpus and classify all the examples in the dataset (see Figure

Figure

The dataset in Figure

The statistical technique introduced in this section, whose application will be demonstrated with the dataset in Figure

Because of the similarity with logistic regression described in detail in Section 4.4, readers are referred to Chapter 4 for a detailed explanation of the basic terms and principles. In what follows, the main differences between logistic regression and mixed-effects models (mixed-effects logistic regression) are highlighted and an example of the use of this technique is provided. When we run the mixed-effects model technique we get an output such as that displayed in Figure

The interpretation of mixed-effects models has two main parts: (i) evaluation of the model characteristics and (ii) interpretation of the significance of the fixed effects. In our example, the model is statistically significant and shows the effect of age and syntactic position on the use of very as opposed to really (our baseline variant marked with the prefix A_). Older speakers prefer very and syntactic contexts with a predicative adjective (e.g. you're very lucky) also favour this intensifier. The other predictors are non-significant. This result is in line with

A Small Dictionary of Sociolinguistic Terms

In corpus linguistics and variationist sociolinguistics, different terms are used to mean the same thing or the same terms are used to mean different things

Variationist term Meaning

Corpus linguistics equivalent factor A type of speaker (e.g. male or female, young or old) or a type of context (e.g. syntactic position) which favours the use of a particular variant of the sociolinguistic variable. With mixed-effects models, it is important to report the type of mixed-effects model used as well as the details of the model. The focus of the reporting will be on the effect of the individual predictors.

How to Report: An Example

• Mixed-effects logistic regression was used with the individual speakers as random effects and gender, class, age and syntactic position as fixed effects.

The model, which was overall significant (p < .001), showed a significant effect of age and syntactic position: older speakers and predicative contexts favour the intensifier very.

6.6

Application: Who Is This Person from the White House?

I have a friend (they wish to remain anonymous), who every year, instead of a birthday card sends me a linguistic puzzle. This year, the puzzle had the form of a 'sociolinguistic riddle' with an attachment, which contained a file with transcribed speech. This is what appeared in my mailbox:

What follows is a brief description of my work over the following few days after receiving the email, which I spent solving the riddle. In forensic linguistics (my quest closely resembled the detective work of a forensic linguist), there are two basic approaches, which depend on the amount of evidence available: if the amount of evidence is small (a few sentences or paragraphs), close reading for signs of idiosyncratic language use (or shibboleths) is appropriate. If, on the other hand, as was my case, there is a lot of data available (the sample was approximately 200,000 words), the statistical approach is called for. The second part of the riddle was clear and matched the type of language in the sample. 'I work in Washington DC and speak for the President' indicated that the speech sample comes from a White House press secretary. Luckily, I could use a comparative corpus of White House press conferences (WH)

The first step was to decide whether transcript X came from a male or a female speaker. There are two women in the corpus (Dee Dee Myers and Dana Perino) and six men. The women contribute one sample each, while there are 33 speech samples from the male group. It is not possible to use traditional gender-distinguishing words such as lovely (preferred by women in informal British speech), because in the whole corpus (over 6 million words) there are only 20 occurrences of lovely. This is no surprise: we are dealing with a very specific formal spoken register of American English. We To answer this question, correspondence analysis was used with all the speech samples in the corpus plus transcript X (34 + 1). The analysis looked into the proportions of different word classes in these samples as the linguistic variables which are both frequent and independent of the topic discussed (see

The correspondence analysis clearly grouped individual speech samples from the WH press secretaries together. For instance, all speech samples from Ari Fleischer (A1-A4) cluster at the top left, while speech samples from Scott McClellan (S1-S3) cluster at the bottom right closer to the centre than speech samples from Tony Snow (T1) and Dana Perino (D1). What about the mysterious transcript X? In the graph, it clusters in the proximity of the sample from Dee Dee Myers and far apart from the other samples. With a high probability it can thus be assumed that transcript X comes from this WH press secretary. The speech of Dee Dee Myers, the first ever woman to hold the post, is characterized by the frequent use of pronouns, a relatively infrequent use of nouns, adjectives and conjunctions. These stylistic differences can be identified only by taking large speech samples and analysing them quantitatively.

Following the investigation described above, I replied to my friend's email: For those of you who are still not convinced, you can follow my example and try to search for the first sentence from transcript X on the internet: 'The good news is that the filibuster has been broken on national service.'

Exercises

1. Which linguistic disciplines use the notion of 'style' as one of their core concepts? How can it be operationalized?

2. Which of these cases of variation satisfy Labov's definition of a sociolinguistic variable and can thus be investigated using Labov's methods? Justify your answer.

(a) h-dropping in different social contexts: i.e. the variation between the pronunciation of e.g. the word hair as /heə/ or /eə/. (b) The variation between naming strategies for soft drinks in the US, e.g. soda, pop or coke. (

South (n1 = 10):

2.

3.

4.

TH I NGS TO R EM EM B ER

• Sociolinguistic variation can be operationalized in different ways: by employing the Labovian meaning-preserving sociolinguistic variable (formal approach), or by following the functional approach and looking at the distribution of linguistic features in groups of speakers.

• The t-test and ANOVA (as well as their non-parametric counterparts: Mann-Whitney U and Kruskal-Wallis) are used to investigate the effect of explanatory social variables (gender, social class) on the use of different linguistic features.

• Correspondence analysis is an exploratory analytical technique, which compares the use of multiple variables in different speakers reducing them to two factors and producing a powerful visualization -a correspondence plot.

• Mixed-effects models is a group of sophisticated statistical techniques which can account for multiple variables at the same time and include the effect of individual variation between speakers.

The notion of change over time is central to the analyses described in this chapter. From a statistical perspective, time is a continuous (scale) variable; this means that we can measure time on a continuum of centuries, decades, years, months, weeks, days, hours, minutes, seconds, milliseconds etc. A study that involves time as a variable is called a diachronic or longitudinal study. We sometimes also measure time relative to the stages of the human life, that is age, which is an important variable in sociolinguistic (see

To carry out diachronic studies, we need appropriate corpora, which we call historical or diachronic corpora. Diachronic corpora sample different stages of language or discourse development across time. Examples of historical corpora for the English language include the Brown family,

In addition, when considering diachronic representativeness, we need to deal with certain limitations inherent in historical data. We have to realize that only a small fraction of the language from the past has been recorded and preserved. A historical corpus is usually not a balanced sample of the language from a given historical period; it is, rather, a narrow lens that provides an insight into the language that has been preserved

Second, we need to consider alternative interpretations of linguistic development based on the evidence available. If there is not enough evidence, or the evidence is biased towards, for example, a certain genre or certain types of speakers/writers (see 'diachronic representativeness' above), the patterns we observe are ambiguous. Generally, the further back in history we go the less evidence has been preserved. For example, all preserved texts from the Old English period (450-1100) consist of no more than three million running words,

The issue can be mitigated by adding more data points which allow us to track the trajectory of language change more precisely. However, where acquiring more data points like this is not possible, we need to be open to alternative interpretations. To capture some language changes a straight line (the simplest model) is adequate. Yet for other diachronic processes, more complex models (complex curves) are more appropriate. For example, phonological and grammatical changes often follow an S-shape curve

Third, the fluctuation of the meaning of linguistic forms (diachronic polysemy) is a phenomenon that needs to be carefully considered when looking at the development of language; the same linguistic form often changes meaning (or its set of meanings) over time, so in diachronic analyses we also need to provide an account of the semantic development, not only an overview of changing frequencies of linguistic forms. For example,

The sensitivity to the three areas discussed above, that is the diachronic representativeness of corpora, alternative interpretations of linguistic development and fluctuation of the meaning of linguistic forms, distinguishes corpus linguistics from linguistically naïve quantitative methods of the 'big data' approaches to language, such as culturomics

We can see how culturomics and similar approaches trade the representativeness and sensitivity to meaning fluctuation for a large amount of evidence (hence the 'big data' approaches); this, however, is never a good deal

Finally, let us look at some visualization options with diachronic data. We have already seen three figures (7.1, 7.2 and 7.3) that visualize language change in the form of a line graph. A line graph is a simple display, which plots the time variable on the x-axis and the frequencies of linguistic variables on the y-axis. Line graphs help us interpret the patterns of change in corpora. Note that the actual line in the graph is produced by connecting the data points and already represents an interpretation of the data. For this reason, in Figures 7.1 and 7.2 (but not in Figure

Other options for visualizing diachronic data include boxplots and error bars, sparklines and the candlestick plot. Boxplots and error bars were discussed in detail in Section 1.5. They offer an opportunity to look inside each diachronic sampling point and analyse variation between individual texts in the historical period. The error bars can be used to display 95% confidence intervals around the mean values for each historical period. Figure

When dealing with a large number of variables that develop over time, we can use sparklines as efficient summaries of multiple individual trends. A sparkline is a small graph the size of a single word that can be seamlessly incorporated into text

The use of must in the seventeenth century is marked by a large amount of fluctuation .

In addition to the overall trend, the sparkline can provide information about the minimum and the maximum value (solid points above and below the line). It is thus an informationally rich form of display that takes incomparably smaller space than if we were to describe the data in words. The sparkline in the example above displays 100 different sampling points.

An alternative to multiple sparklines is a candlestick plot. A candlestick plot is a type of data visualization, where the development of a linguistic In addition, the colour of the box (filled vs unfilled) shows the direction of the change: a filled box shows decrease, unfilled increase. In Figure

In sum, the analysis of historical language data presents a challenge, which, in addition to the usual methodological considerations in corpus research, has special requirements connected with the diachronic dimension. Effective visualization using line charts, boxplots, error bars, sparklines, candlestick plots etc. is a useful starting point for any diachronic investigation. Broadly speaking, the most important question that we have to deal with when comparing two corpora is whether the frequencies of the relevant variables are significantly larger or smaller in corpus 1 than in corpus 2. In addition, when dealing with diachronic comparisons, we need to assess whether the observed differences in frequencies are related to change in the discourse/language over time or whether these are related to other sources of variation.

To be able to see the difference between the two sampling points a simple percentage increase/decrease was calculated. Percentage increase/decrease is a statistic that indicates by how many percentage points the value of a particular linguistic variable increased or decreased between two time periods. It is calculated using the equation below. where corpus 1 is a corpus from an earlier period and corpus 2 is a corpus from a later period. In Table

In this section, we will focus on the bootsrapping test proposed in

In practice, when performing the bootstrap test, we need to trace the distribution of the linguistic variable in individual texts and normalize the frequenciesthat is, we need to get relative frequencies (see Section 2.3). An example of such a dataset is provided in Figure

Table

(i) Standard deviation, coefficient of variation, Pearson's correlation, corrected means ratio

The dendrogram based on Table

Reporting Statistics: Variability-Based Neighbour Clustering (VNC)

What to Report

VNC is largely an exploratory visual method to show diachronic segmentation of the data. Two parameters are used for cluster identification: (i) the distance measure and (ii) the amalgamation rule as well as the results of the analysis, the dendrogram (tree plot), need to be reported. The main question to be addressed is: how many meaningful clusters representing continuous historical periods can be observed in the plot? A scree plot can be used to help determine the answer to this question.

How to Report: An Example

• The data was analysed using the variability-based neighbour clustering technique

1. Obligatory: Obtaining (relative) frequencies from the corpus of the linguistic variable of interest for each of the periods (e.g. years, decades etc.) covered by the analysis. 2. Optional: Computing differences between two consecutive values by taking value 2 and subtracting value 1; this is done to highlight high values preceded by low values (or vice versa) which indicate a more dramatic change. 3. Optional: Transformation of the values using binary logarithm (log2) to reduce extremes. This step is possible only if all transformed values are positive numbers because logarithm is not defined for negative numbers. Since step 2 typically produces also negative values, logarithmic transformation is possible with data from step 1. 4. Obligatory: Fitting a non-linear regression model (displayed as a curve in the graph), computing 95% and 99% confidence intervals (displayed as shaded areas around the curve) and identification of significant outliersdata points outside the confidence interval area.

As an example, let us look at the data from the 'Think about' task, which trace the occurrence of the term war used in The Times newspaper. Both optional and obligatory steps of the peaks and troughs procedure are shown in what follows.

1. Obligatory: For illustration, Table

1. Identification of collocates of a word of interest (node) across the time-series data. 2. Recursive estimation of the difference between collocates at any two consecutive points in time. 3. Use of the peaks and troughs technique (see above) to trace the points where major changes take place; these are identified as significant troughs because these are the largest points of meaning dissimilarity.

As an example, let's use the data from the 'Think about' task again. For UFA, instead of looking at the frequencies of war, we'll analyse the collocates of war in The Times for each period.

1. Table

. 2009 Collocates

periods are with respect to the collocates used. For example, over the period 1940-2009, 171 collocates of the node war were identified. In 1940, 19 of these collocates co-occurred with war (and 152 didn't), while in 1941, 23 of these collocates co-occurred with war, 15 of which were the same as in 1940 and 8 of which were newly introduced. This means that the collocate profiles in 1940 and 1941 overlapped in 92.98% of cases (out of 171) with AC 1 = 0.91. 3. Finally, the peaks and troughs technique is applied. Instead of frequencies, we use the inter-rater agreement index AC 1 and look for points with the lowest AC 1 scores as the points of meaning divergence. Figure

In Figure

Reporting Statistics: Peaks and Troughs and UFA

What to Report

For peaks and troughs, which has two obligatory and two optional steps, we need to report all the steps which were followed. For UFA, we need to report the details of the procedure for the identification of collocates (using collocate parameter notation described in Section 3.2) as well as the choice of the interrater agreement statistic.

How to Report: An Example

• The peaks and troughs technique was used (Gabrielatos & Marchi 2012);  the difference between the relative frequencies of the word war between consecutive points in time during the period of 1940-2009 was measured. The non-linear regression model (GAM) helped to identify four peaks

7.6

Application: Colours in the Seventeenth Century I'm sitting at my desk, the weather outside is miserablegrey and rainy. Through the window, I can see the mediaeval Lancaster castle on a hill, the Priory and the valley of the river Lune, all in a foggy mist; buildings, trees and people have the same indistinct colour. Only a flag with the typical Lancaster shade of red is flying from the castle like a drop of paint which an artist has left unintentionally in the picture. My thoughts take me back in history and I keep wondering about how colour was perceived in the past. Was it connected with the same associations, sensitivities and cultural frames? On a rainy Lancaster afternoon, I start searching the EEBO corpus. One billion words of early writing provide a unique insight into the use of colour words in the seventeenth century. I scribble notes, produce graphs and pvalues. Here's a special kind of a diary of my journey into the past with the 'corpus time machine':

My explorations, 12 November 2016

Question 1: Which colours were the most popular in the seventeenth century?

The line graph in Figure

Question 2: What is the story behind colour terms in the seventeenth century?

A summary picture can be obtained from a candlestick plot (Figure

Question 4: Does red, the most popular colour, have the same associations throughout the century?

The stable associations (consistent collocates) include nouns such as coral, dragon, flowers, iron, rose, roses, sea and wine as well as adjectives (mostly other colour terms): black, green, hot, scarlet, white and yellow. UFA (see Figure

• Visualization options include line graphs, boxplots and error bars, sparklines and candlestick plots.

• The bootstrapping test is used to compare two corpora (representing different points in time); it makes use of a technique of multiple resampling of corpus data.

• Peaks and troughs is a technique which fits a non-linear regression to historical data, producing a graph which highlights significant outliers in the process of historical development of language and discourse.

• UFA (Usage Fluctuation Analysis) is a complex procedure combining automatic collo- cation comparison in a given historical period and the peaks and troughs technique. This is the final chapter of the book; it is about bringing things together on different levels. First, it brings together the statistical knowledge discussed in this book and highlights ten key principles of statistical thinking applied to corpora. Next, the chapter introduces a statistical technique called metaanalysis. Meta-analysis is a way of bringing together results of multiple studies and combining them systematically. In this way, meta-analysis contributes to a better understanding of research results in our field. Unlike a standard narrative-form literature review, which typically considers individual studies in isolation, meta-analysis can combine results from multiple studies into a single mathematical synthesis. Although formal meta-analysis is now fairly common in a number of disciplines such as psychology, second language acquisition, medical science etc., its application in corpus linguistics has been problematic due to the general lack of reporting of effect size measures. This chapter argues in favour of standardized reporting of effect sizes in corpus research and shows how meta-analysis can be carried out. Finally, the chapter reviews common effect size measures and provides a guide for their interpretation.

In this chapter, we'll be exploring the answers to three questions:

• What are the key principles of statistical thinking in corpus linguistics? (Section 8.2) • How can we synthesize findings from multiple studies? (Section 8.3) • How should effect sizes be interpreted? (Section 8.

The devil, as the saying goes, is always in the detail. While the focus in statistical textbooks and in the field in general is on statistical techniques, interpretations of p-values etc., low-level operations such as getting data from a corpus tool into a spreadsheet and then into a statistical package often remain in the background. Nevertheless, these operations are equally important for obtaining reliable results: statistics, as we already know, is a discipline concerned not only with data analysis but also with systematic and reliable collection of data (see Section 1.2). If carefully compared, it is apparent that Tables 8.1 and 8.2 from the 'Think

One of the most important tasks for an analyst is to achieve a very good general understanding of the data. This often involves careful reading through the corpus manual to familiarize oneself with the corpus composition, inspecting the concordance lines to see the actual examples of language use behind the numbers we have obtained and producing overviews and simple graphs that reveal the main tendencies in the dataset. All this is subsumed under descriptive statistics. Without a reliable description of the data, generalizing from the sample (corpus) to the population (language as such) using sophisticated statistical tests and p-values lacks grounding. This makes meaningful interpretation of the results difficult, if not impossible. For example, the dataset from Table

4.

DATA: pay special attention to the quality of the corpus data and search procedures. --------------------------------------------------------------------------------------------------------------------------

The success of any research depends on the quality of the data and the effectiveness of the analytical procedure; yet, especially in corpus research, the quality of corpus data is rarely scrutinized. 'I found this in a corpus and therefore it must be true' is a sentiment which ascribes corpora a certain kind of magical power that they don't have. Even well-established corpora such as the BNC include errors, inconsistencies and possible bias. All corpora thus need to be approached critically. For example,

In computation and data processing, a well-known acronym, GIGO ('Garbage in, garbage out'), is used to remind us that the data needs to be approached critically, otherwise the reliability of the analyses cannot be guaranteed.

5.

EFFECT SIZE: calculate, report and interpret the size of the effect observed in the data. -------------------------------------------------------------------------------------------------------------

It is always important to think about the practical effect of the observations about language we make using corpora. To help us express this aspect of the findings, effect size measures should be used. In broad terms, effect size can be defined as the 'amount of anything that might be of interest'

Let's quickly review a brief example. When comparing two groups such as two subcorpora, the effect size measure Cohen's d is often used (see Section 6.3). Cohen's d for the difference between the use of the past tense in academic writing and mystery fiction (see Table

In addition to being aware of more general statistical principles (see Chapter 1), it is important to know how to apply these principles in the analysis of language corpora. For this, following the best practice in the field is essential. This books 7.

GRAPHICS: visualize data to identify patterns. ---------------------------------------------------

A picture, as is often said, is worth a thousand words. Effective visualization can help us discover important patterns and relationships in data. The basic principle of good visualization is simple: focus on displaying the data rather than simply adding 'visual frills'. Make the display informationally rich to allow relationships between variables to be observed

HIGHLIGHTING BOTH SIMILARITIES AND DIFFERENCES:

provide a balanced account of language use. ------------------------------------------------------

It is no surprise that when we look for differences between corpora or between various uses of a word or phrase, we usually find some. Most of the methodological thinking (not only in corpus linguistics but other disciplines as well) has been biased towards looking for differences and ignoring similarities. Similarities and null effects (effects that are small and/or not statistically significant) are thus often underreported in the literature (see the discussion of publication bias in Section 8.3). In corpus linguistics, the focus on differences, for example, leads us to reporting keywords and disregarding lockwords, words which are stable across corpora

9.

INTERPLAY BETWEEN STATISTICS AND LINGUISTICS: provide robust statistical analysis that is grounded in linguistic and social theory. ----------------------------------------------------------------------------------------------------------------------------------------------------------

Let's consider a methodological question. Which of these is worse in corpus research: linguistics without statistics or statistics without linguistics? Arguably, each of these hypothetical options fails in one fundamental respect. Linguistics without statistics lacks effective tools for analysing large quantities of language data, while statistics without linguistics can easily turn into a mindless exercise in number crunching without a connection to linguistic and social reality. We have to remember that not all linguistic research is quantitativequalitative research can bring important insights into the use of language; however, the choice of texts and examples for in-depth analysis raises the question of how typical these are and why we selected them (see

10. JARGON: use statistical terminology and notation where it helps express things clearly, but try to avoid unnecessary jargon. ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------For many people, statistics can be opaque. In statistics, we often try to capture complex relationships in data using mathematical expressions, which themselves might put many people off. Throughout this book, where possible, mathematical symbolism has been replaced by glosses in plain English; for instance, X 10 i¼1 i would be expressed as 'the sum of all integers from 1 to 10' (when you work out the maths both 'renditions' will give you the number 55). However, when reporting on the results of statistical analyses it is important to provide these in a standardardized form; again, this book shows how this can be done in the 'Reporting statistics' boxes.

8.3

Meta-analysis: Statistical Synthesis of Research Results Think about . . . Before reading this section, think about the following situation.

You are in a large city such as London and are looking for a theatre, say the replica of Shakespeare's Globe. You don't have a map so you have to rely on the information from passers-by. Table

The example situation in the 'Think about' task can be understood as a metaphor for the scientific pursuit. To find out the answer to our research question we go out to collect and evaluate data. In science, this is done repeatedly to ensure the reliability of the answer. The process of repeating a study with the same research question but a different dataset is called replication. In our example, replication is demonstrated with asking different passers-by the same question. And as in our example, in science we often get (slightly) different answers from different studies. What is, however, important for the development of our field as a whole is to be able to see the larger picture; we therefore need to put individual findings together and make sense of them globally. The statistical technique that does this is called meta-analysis. Meta-analysis is a quantitative procedure of statistical  a person walking a dog synthesis of research results, which is based on combining the effects reported in multiple individual studies and calculating the summary effect

Step 1.

Identification of relevant studies --------------------------------------------------------------------------------------------This step defines the area of research which we wish to investigate in the metastudy. The area is given by a research question that is answered repeatedly in multiple studies using different corpora. Of course, the research question can be broader or more specific thus defining the granularity of a meta-study. For example, a very broad question would be: Is there an effect of gender on the use of language? Guided by this research question, we would be able to identify hundreds of relevant studies but the answer would probably be very broad and non-specific: some linguistic features show gender-based patterns, while others don't; this can be quantified as a very broad summary effect. If, on the other hand, we define the area of research using a specific research question, e.g. Is there an effect of gender on the use of pronouns?, we can tap into something more specific and arguably more interesting; however, the number of relevant studies will be considerably smaller. In practical terms, it is important to explicitly specify inclusion criteria for the studies such as what linguistic and what explanatory variables we are looking for, requirements for research design and relevant time frame for the studies, e.g. studies published since 1990

Once we define the area of research, we need to search all relevant journals and available databases (e.g. Google scholar, Linguistics and Language Behavior Abstracts, ProQuest Dissertations & Theses, EThOS etc.) for studies, both published and unpublished, that address the research question of interest and meet the inclusion criteria. The aim is to collect all available evidence related to the research question. The reason to also consider quality unpublished studies (e.g. PhD theses) is the fact that we want to reduce the effect of a so-called publication bias. Publication bias is a well-documented phenomenon (e.g.

Step 2.

Extraction of relevant pieces of information from the studies (coding) --------------------------------------------------------------------------------------------------------------------------------------------------------------------------

The second step involves obtaining relevant data from the studies we identified in step 1. This involves carefully reading through the method and results sections of the research reports, articles and books and noting down information about (i) the (sub)corpora used, (ii) the method and (iii) the effect sizes observed in the study

Because in many cases the information about different aspects of the study needs to be inferred or extrapolated from the report and this might involve an element of subjectivity, it is good practice to double code a portion of the data (e.g. 20%) and calculate an inter-coder agreement statistic (see

Let us now have a look at an example of five studies reporting the effect of gender on the use of pronouns (I, me, my, you, your etc.). Table

Step 3.

Statistical synthesis -------------------------------------------------------------------------------------------------------------------------------------

The final step of meta-analysis puts the information from the individual studies together and produces a report of the resulting (combined) effect size and a confidence interval. In its simplest form, the statistical synthesis takes as input the effect size measure (here Cohen's d) and the number of cases (texts/speakers) in each subcorpus (see Table

For example, the inverse variance for the first study

The forest plot provides a summary of a meta-analysis by displaying the effect sizes of individual studies (filled squares) and their confidence intervals (whiskers) as well as the overall effect size (diamond); the size of the square represents the weight of an individual study in the analysis. studies were weighted by inverse variance and the random-effect model was used. The procedure identified a summary effect d = .47 with the 95% CI

Effect

This example demonstrates the basic thinking about effects and effect sizes that we can also apply to linguistic data. ES measures, as we know, quantify the observed linguistic variables and the differences and changes in their frequencies. It is important to gain a good understanding of what exactly the measures mean and how to interpret them. In our half marathon example, we can show easily what we mean by one metre or one yard by drawing a line that represents the unit of measurement. However, we need to realize that the decision about the practical importance of the effect has little to do with statistics: in our example, it is our personal decision about whether a 38-second improvement in a half marathon is worth the training time and energy. To make an informed decision, the 38 seconds need to be contextualized and compared with the times of other runners etc.

This section focuses on the use of effect size (ES) measures in corpus linguistics. The concept of effect size was introduced in Chapter 1 and different effect size measures have been discussed throughout this book. Table

To address the second question (how to interpret ES measures?), we need to gain a good understanding of how a particular ES measure works in practice. In textbooks, ES measures are usually ascribed standard interpretations, typically based on

To see the practical impact of corpus linguistic effects expressed as d, r, η 2 (r 2 ), Table

Exercises

1. What is the most important thing you have learnt from this book? Write this down in the space below.

2. Provide transformations of the effect size measures in Table

Final Remarks

Statistics is a powerful analytical tool. This book has demonstrated a number of different ways in which statistical techniques can be used to explore corpora. The robust evidence found in these electronic collections of language offers countless possibilities for both linguistic and social research providing a unique insight into patterns of language use. Statistics, if applied appropriately, can facilitate the process of analysis by serving as a zoom lens through which we can observe the linguistic reality: the details of individual examples of language use as well as the larger picture of grammar, vocabulary and discourse. We need to remember, however, that the lens should always be a transparent one: what we want to observe is not the tools themselvesa showcase full of sophisticated statistical techniquesbut the linguistic data. Our analysis thus should always be primarily focused on the data and should take data seriously; if our beliefs and theories are contradicted by the data we shouldn't simply dismiss the data as 'inconvenient' evidence (or hide it behind complex statistical jargon) but, on the contrary, we should engage with it, seeking to genuinely understand and explain the findings. Only in this way can our investigation be meaningful and truly scientific. Mastery of statistics is empowering. However, as statistical tools and analyses become more complex and sophisticated, they can also become rather daunting for the users. This is because statistical analysis involves many choices. Among other things, we need to select a suitable corpus, an effective analytical technique and an appropriate interpretation of the results. These decisions can often feel challenging especially for novice researchers. The growing demand in corpus linguistics for statistical sophistication and the lack of appropriate resources for beginner and intermediate users of statistics can thus easily lead to frustration. This book (together with Lancaster Stats Tools online) hopes to be a resource addressing this issue by offering readers a guide for making informed choices about statistics in language analysis. The main message of the volume is twofold. First, statistics is not about number crunching or remembering equations (computers are much better at these tasks than humans) but about understanding core, underlying principles of quantitative analysis. Second, I would like to encourage the readers not to let themselves be overwhelmed by the complex statistical techniques or the newest fads on the statistical marketplace. Every summer, a large number of students from different parts of the world come to Lancaster to learn about corpora and statistics during a week of Lancaster summer schools in corpus linguistics. These students often ask me what the best statistical test is to use with corpora, what the best collocation measure is etc. I usually respond: in many cases, the most powerful statistical technique is common sense.

284

Final Remarks
Tables

1.1 Common right collocates of "equal" and "identical" in the Corpus of Contemporary American English (COCA) 2.1 Key situational differences between an email to a friend and an email to a superior (boss) 2.2 Situational differences between news writing and news talk 2.3 Key situational differences between student presentations at a symposium and teacher presentations in the classroom 2.4 Texts for news writing and a news talk (COCA) 2.5 Texts from a classroom presentation and a symposium presentation 2.6 Texts from an email to a friend and an email to a boss 3.1 Distribution of part of speech categories following the word "say" 3.2 Distribution of part of speech categories following the word "say" in spoken discourse and in academic prose 3.3 Raw and normed frequency counts for "said" and "stated" in three registers 3.4 Distribution of "said" and "stated" in two registers 3.5 Distribution of the sentence position of "on the other hand" in spoken and written discourse 4.1 Information on corpora used for projects in this chapter 4.2 Passive voice 5.1 Examples of corpus projects 5.2 Coding scheme 6.1 Frequency of complement clauses 6.2 Complement clauses 6.3 Descriptive statistics for "hedges" in three disciplines through SPSS 7.1 First-person pronoun data in SPSS data view format 7.2 Calculating the mean score for each group 7.3 Calculating sum of squares within groups Figures 2.1 Situational variables 3.1 Distributional patterns of the word "say" in COCA 3.2 Concordance lines for the word "say" in COCA 3.3 Results from the keyword analysis for British and American press reporting 3.4 Icons on the top to choose from for the next project 3.5 Text analysis based on vocabulary frequency in Word and Phrase 3.6 More information about the word "thief" 3.7 Concordance lines for the word "thief" 3.8 Part of speech search in COCA 3.9 Distributional patterns of the 4-gram "on the other hand" in COCA 3.10 Concordance lines for the 4-gram "on the other hand" in COCA 3.11 Frequency search in Word and Phrase 5.1 Example of a text file with header information 5.2 Embedding header tags in AntConc 5.3 AntConc using three-word lists for vocabulary frequency comparison 5.4 Searching your own corpus: Concordance lines in AntConc for the word "and" 5.5 Sorting in AntConc 5.6 File view in AntConc 5.7 Search term distribution in full texts 5.8 The word "and" and its collocates 5.9 Collocates in concordance lines 5.10 Running your own n-grams in AntConc 5.11 Running a word list in your corpus in AntConc 6.1 Boxplot of the use of nouns by teachers and students 6.2 Variable view in SPSS

Preface

In our experiences teaching introductory corpus linguistics classes, we have found that undergraduate and graduate students gain both confidence and ability doing corpus analysis when they are given the opportunity to work with corpora and are exposed to hands-on experience with corpus tools and corpus analysis. While an understanding of the principles, approaches, and advantages of using corpora provides the necessary foundational knowledge of this approach to language analysis, there is a technical side to corpus linguistics that is best acquired through practice and experience with corpora. We have found that students are sympathetic to the benefits and advantages of using language corpora, but the real challenge is teaching them how to work with corpora. When "doing" corpus linguistics, students need to gain experience searching a corpus and interpreting the results of their corpus searches so that they can use this information to explain why their analysis and findings are important or relevant. In this book, we offer multiple opportunities to work on corpus projects by first including an entire chapter dedicated to smaller corpus projects (Chapter 4) and then providing students with information on how to build and analyze their own corpora (Part III). We have found that offering students the opportunity to build and analyze their own corpora gives them valuable experience in corpus building and sometimes even encourages them to build other corpora for projects outside of the class.

In order to allow the students to gain experience in "doing" corpus linguistics, we have intentionally limited the corpora and software used in the corpus projects. There are many different corpora available with different levels of public availability and numerous software programs that can be used for analysis (some free and some not). An extensive list of English corpora around the world is at the end of the book.

Each corpus and software program has its own idiosyncrasies and we have found that these different corpora and software programs are sometimes confusing to students who do not have access to the corpora and/ or struggle to learn one program or search interface in a corpus and then have to learn another. To address this issue, all of the projects in this book use the suite of corpora created by Mark Davies at Brigham Young University (

Because a good deal of corpus work involves quantitative data analysis, we also included some elementary statistical information (Chapter 6) and tests (Chapter 7). Keeping with one of the guiding principles of this book, we see this introductory information as a way to have students learn the basics of analysis with the hope that they may apply this knowledge in other projects or learn more about more advanced statistical techniques that they can use in the future.

There are many different descriptive and theoretical frameworks that are used in corpus linguistics. We have selected one particular framework to guide the students in their interpretation of their corpus findings. Register analysis has strongly influenced our work and we believe that this approach to understanding language use is broad enough to encompass the various types of projects that students choose to do. In Chapter 2, we outline the basics of a register approach to language analysis and then ask students to refer to this same framework when building their corpus and doing their corpus study. We recognize the relevance of other descriptive and theoretical agendas but feel that focusing on a single approach provides students with more extended experience interpreting their corpus results and motivating the significance of their findings. Without knowledge and practice using a specific framework, we have found that students can be quite enamored with the "button-pushing" aspects of corpus linguistics at the expense of interpreting the results of their searches.

We also recognize the importance of reporting on research in a cohesive way. To this end, we have included material dedicated to the specifics of writing a research paper and presenting research (Chapter 8). Our goal in this chapter is to provide both students and teachers with some guidelines for how to demonstrate and present their specific research projects.

In the final chapter (Chapter 9), we ask students to consider more advanced types of corpus research with the hope that this book will serve as an introduction to the field and encourage students to pursue these ideas at a more advanced level and perhaps even impact the field in significant ways.

Introduction to Doing

Corpus Linguistics and Register Analysis DOI: 10.4324/9781003363309-2

Language and Rules/Systems

While all humans use language to communicate, the ability to describe language is not nearly as advanced as our ability to actually use language. One defining component of the scientific study of language (i.e., linguistics) includes a description of how language works. Speakers of English are able to produce plural nouns that end in different sounds -we say batS and bagZ, not batZ and bagS. These same speakers can also produce plurals of nonsense words that we have never heard before -we would say bligZ and not bligS. Speakers of English also know that We worked out the problem and We worked the problem out are both acceptable sentences but We worked it out and *We worked out it may not be equally acceptable (the latter is likely to sound strange to many native speakers of English). The fact that we can agree on these aspects of English related to the pronunciation of plurals and word order points to the fact that language, in many respects, is predictable (i.e., systematic). Such aspects are not only related to sounds and the order of words, but they are also related to how we might use language in different contexts and for different purposes. For example, we would not be likely to ask a professor for an extension on an assignment by saying: "Hey, man. Gimme an extension". Instead, we are more likely to make such a request by saying: "Would you please allow me to hand in that assignment tomorrow? I have experienced some personal issues and have not been able to fully complete it yet".

While it may be difficult to explain these particular aspects of the English language, native speakers apply these "rules" of language flawlessly.

Chapter 1

Linguistics, Corpus Linguistics, and Language Variation

1.1 Language and Rules/Systems 1.2 What Is Corpus Linguistics? 1.3 Register, Genre, and Style -Is There a Difference? 1.4 Outline of the Book

In other words, one important component of linguistic description is to make implicit "rules" or patterns of language (knowledge we use) explicit (knowledge we can describe). It is safe to say that language users follow rules (and sometimes choose not to follow rules) for specific reasons even though they may not be able to explain the rules themselves (or even if we cannot agree on what a "rule" actually is). An important part of linguistic study focuses on analyzing language, describing, and in some cases explaining, what may seem on the surface to be a confusing circumstance of facts that may not make much sense.

When many people think of language rules, they may think of the grammar and spelling rules that they learned in school. Rules such as "don't end a sentence with a preposition" or "don't start a sentence with the word and" are rules that many people remember learning in school. Very often people have strong opinions about these types of rules. For example, consider the excerpt below taken from a grammar website on whether to follow the grammar rule of "don't split an infinitive".

Even if you buy the sales pitch for language being descriptive rather than prescriptive, splitting infinitives is at the very best inelegant and most certainly hound-dog lazy. It is so incredibly simple to avoid doing it with a second or two of thought that one wonders why it is so common. There are two simple solutions.

(1) "The President decided to not attend the caucus" can be fixed as easily as moving the infinitive: "The President decided not to attend the caucus". I'd argue that works fine, and not using that simple fix is about as hound-dog lazy as a writer can get, but split infinitives can be avoided entirely with just a bit more thought. How about: (2) "The President has decided he will not attend the caucus". What the heck is wrong with that?

It's hound-dog lazy, I say. Where has the sense of pride gone in writers?

(

Examples such as these are not uncommon. One would only have to look at responses to social media comments or blog posts to find many more instances of people who have very strong opinions about the importance of following particular grammar rules.

So far, we have looked at "rules" as doing two different things: 1) describing implicit, naturally occurring language patterns and 2) prescribing specific, socially accepted forms of language. Although both descriptive and prescriptive perspectives refer to language rules, prescriptive rules attempt to dictate language use while descriptive rules provide judgment-free statements about language patterns. Both prescriptive and descriptive aspects of language are useful. When writing an academic paper or formal letter, certain language conventions are expected. A prescriptive rule can provide useful guidelines for effective communication. However, descriptive approaches can be useful in uncovering patterns of language that are implicit (as in the examples described above). Descriptive approaches can also be used to see how prescriptive rules are followed by language users.

The concept of language rules raises another interesting question: Why are these rules sometimes followed and sometimes not followed? Consider the prescriptive infinitive rule described above. Is it accurate to say that those who write to not attend are not following a rule? In some respects, this may be the case, but there is another -perhaps somewhat misunderstoodissue related to language that deserves some attention and serves as a basis for this book: the role of language variation. It is an incontrovertible fact that language varies and changes. The type of English used in Britain is quite different from the type of English used in the United States. The type of English used in Britain or the United States also varies from region to region or among people from different socio-economic classes. The type of English used 150 years ago in the United States is quite different from the type of English used in the United States today. Language even changes and varies in a single person. The study of language variation seeks to understand how language changes and varies for different reasons and in different contexts. There are different perspectives on how to investigate and understand language variation. The perspective that we will take is, as you can tell from the title of the book, related to an area of language study called corpus linguistics.

What Is Corpus Linguistics?

One way to understand linguistic analysis and language is through corpus linguistics, which looks at how language is used in certain contexts and how it can vary from context to context. While understanding variation and contextual differences is a goal shared by researchers in other areas of linguistic research, corpus linguistics describes language variation and use by looking at large amounts of texts that have been produced in similar circumstances. The concept of a "circumstance" or "context" or "situation" depends on how each researcher defines it. Corpus linguistic studies have frequently noted the general distinction between two different modes of language production -written language and spoken language. From a written perspective, one may be interested in contexts such as news writing, text messaging or academic writing. From an oral perspective, one may be interested in language such as news reporting, face-to-face conversation or academic lectures. Although text messaging and academic writing are both written, the purpose of text messaging is quite different from the purpose of academic writing and we would likely expect some degree of language variation in these different written contexts. The same may be said with face-to-face conversation and academic lectures: both are spoken but they have different purposes and consequently have different linguistic characteristics. More generally, we might also expect that spoken language (in all its various purposes and contexts) would likely differ from written forms of language. Spoken language does not generally have the same type of planning and opportunities for revision that we find in many types of written language. We will consider how different circumstances (or situational variables) can affect language use in the following chapter. But, before we do, we would like to briefly describe what we mean by a corpus.

A corpus is a representative collection of language that can be used to make statements about language use. Corpus linguistics is concerned with understanding how people use language in various contexts. A corpus is a collection of a fairly large number of examples (or, in corpus terms, texts) that share similar contextual or situational characteristics. These texts are then analyzed collectively in order to understand how language is used in these different contexts. The result of this analysis is a collection of language patterns that are recurrent in the corpus and either provide an explanation of language use or serve as the basis for further language analysis. One common method used in corpus research is to look at the environment of a particular word or phrase to see what other words are found (i.e., "collocate") with the reference word. While there are many different corpora available (see a selection at the end of the book), as an example, we will use the Corpus of Contemporary American English (COCA), which is a publicly available collection of over 1 billion words of text of American English (freely available at www.english-corpora.org/coca/), to investigate the use of two words: equal and identical.

In many respects, equal and identical can mean the same thing (two things that are similar to each other), and they are often taken as synonyms of one another. For example, we can use both of these words in a sentence such as: These two students are equal/identical in their performance on the exam with the same general meaning. If we were asked to define the word equal, we may use the word identical in our definition

Additionally, the corpus can inform us about frequency differences between equal and identical as shown in Table

In other words, we can see that the word equal is more frequent than the word identical because the frequency of collocates shows a large difference between the two words. In fact, the word equal occurs 22,480 times in the corpus, and the word identical occurs 8,080 times.

In addition to information on collocation and frequency, a corpus will also allow us to examine the extent to which certain types of prescriptive rules are followed. Let us look at what a corpus might tell us about splitting infinitives. Earlier in this chapter, we saw that this rule can raise the ire of some people -to the point of associating some serious character flaws in those writers who do not follow it. The Corpus of Contemporary American English shows that we have examples such as to better understand (874 times in the corpus) compared with to understand better (94 times) and to really get (349 times) compared with to get really (151 times). Additionally, the sequence of words to better understand is most commonly found in academic writing while the sequence to get really is most commonly found in spoken language contexts. This evidence suggests that a strong prescriptive statement such as "don't ever split an infinitive" runs into serious problems when looking at actual language use. In fact, some examples of split infinitives are more frequent in formal academic writing; others are more frequent in spoken language. In other words, even though there are strong views on certain usage rules of English grammar, many of these rules may run counter to authentic examples of how language is used by reference to corpora. That is to say, the "rule" against splitting an infinitive is not always followed (i.e., there is variation in the application of the rule).

We have already seen a few examples of what corpus information can tell us. Now we will consider the defining characteristics of corpus linguistics as they will be used in this book. In a general sense, a corpus can refer to any collection of texts that serve as the basis for analysis. A person might, for example, collect examples of news editorials that are on a particular topic and refer to this collection as a "corpus". However, would we say that this person is "doing" corpus linguistics? In their 1998 book, Corpus Linguistics: Investigating Language, Structure and Use, Biber, Conrad, and Reppen define corpus research as having the following characteristics:

• it is empirical, analyzing the actual patterns of use in natural language texts • it utilizes a large and principled collection of natural texts, known as a "corpus", as the basis for analysis • it makes extensive use of computers for analysis, using both automatic and interactive techniques • it depends on both quantitative and qualitative analytical techniques

The third and fourth characteristics of corpus linguistics make reference to the importance of computers in the analysis of language as well as different analytical approaches. It would be difficult to imagine how one might use a 450-million-word corpus such as COCA without using a computer to help identify certain language features. Despite the large number of texts and the relative ease of obtaining numerous examples, a corpus analysis does not only involve counting things (quantitative analysis); it also depends on finding reasons or explanations for the quantitative findings. In Chapters 3 and 4, we will cover some of the specific directions we can explore in the corpus through software programs that allow for different types of analysis. It is important to remember that corpus methods do not just involve using computers to find relevant examples; these methods also focus on analyzing and characterizing the examples for a qualitative interpretation.

In addition to these four characteristics of a corpus, Elena Tognini-Bonelli, in her book Corpus Linguistics at

A final point to consider when looking at corpus research relates to various views that researchers may have about corpus linguistics. Elena

Researchers then analyze them grammatically

Register, Genre, and Style -Is There a Difference?

Researchers in the field of applied linguistics, and more specifically, in discourse analysis, have defined and distinguished notions of register, genre, and style when it comes to text analysis.

In contrast, genre studies differ from both register studies and studies of style in all of the four dimensions mentioned above. As Biber and Conrad in their Table

In this book, we take a register perspective to describe linguistic variation whether it is lexical, grammatical, or textual.

Outline of the Book

This book is divided into three main parts. In Part I, we introduce the concept of a corpus and locate corpus linguistics as an approach to language study that is concerned with the analysis of authentic language, and a focus on language variation, using large amounts of texts (Chapter 1). We then provide a register analytical framework for interpreting corpus findings (Chapter 2). In Part II of this book we focus on how to use existing corpora. We introduce a set of search tools and a set of language units that could serve as the basis for the analysis of already existing corpora (Chapter 3). We then provide 12 different projects that use existing online corpora in order to introduce the basics of how to work with corpora, interpret data, and present findings (Chapter 4). Once these basics of corpus analysis and an analytical framework have been addressed, readers will be ready to build their own corpora and conduct their own research study. Part III of the book takes you through the steps of building a corpus (Chapter 5) and then covers some statistical procedures that can be used when interpreting data (Chapters 6 and 7). Chapter 8 then provides a stepby-step process for writing up and presenting your research. Because this introductory book contains some of the basics of how to conduct a corpus research project, we do not cover many of the relevant issues that corpus linguistics is presently addressing in its research. In Chapter 9, we discuss some of these issues with the hope that this book has taught you enough about corpus research to pursue a more advanced study of the field.

As we have discussed in Chapter 1, language variation is a prevalent characteristic of human language. There are, however, many different ways to investigate variation. We could choose to look at how language varies in different areas around the world (for example, the differences between British and American English). We could also investigate how language varies by identity or social class. Another way of looking at variation would be to consider the differences among individual writers or speakers. We could, for example, study the speeches of Martin Luther King Jr. in order to understand how his "style" might differ from speeches given by other people such as Winston Churchill. We could also take a different perspective and examine variation in language use by reference to the different contexts in which language is used. This approach can be done on both large and small scale depending on a specific research goal; however, at the foundation of this approach to language analysis is the assumption that language variation is functionally motivated by reference to clear descriptions of context. This perspective on language variation is referred to as register analysis which uses a series of steps to describe and interpret linguistic differences across relatively general contexts such as written versus spoken language or face-to-face conversation versus academic lectures. These same steps can also be used to describe variation in more specific contexts. Large-scale investigations require a representative (and usually quite large) sample of language and a method to determine linguistic features that are frequent in a given context. The research goal of this broad approach to register analysis seeks to identify the linguistic characteristics of language used in general contexts such as face-to-face conversation or academic writing. More recently, similar methods used in the analysis of registers have been used to identify and interpret language variations that are not concerned with identifying and describing registers but are instead concerned with describing and interpreting language variation. This latter approach has been called a register-functional approach

Why Register?

As noted above, linguists have taken different approaches to investigate language variation. When traveling to different parts of the world, one may notice that there are different words for the fossil fuel that people put into their cars (gas, petrol, motor spirit). Not only do people have different words for things, but also the way that people say certain words can sound very different from region to region. In some parts of the United States, the words pin and pen both sound like pen. In some parts of the United States, people say caught and cot with different vowel sounds; in other parts of the United States, the words sound the same. The role that geographic location plays in lexical and phonological variation in these examples is generally covered in a field of linguistics known as sociolinguistics. Researchers in this field seek to understand how language variation is related to factors such as geographic region, identity, ethnicity, age, and socio-economic status. The traditional sociolinguistic approach frequently considers variation to be present when the same concept is expressed in different ways. From this standpoint, what counts as a variable must be a concept that is similar in meaning but different in the words used to describe it or in the phonological form of the word, or, in some cases, a different grammatical structure that describes the same concept (for example, the comparative structure more better).

Traditional sociolinguistic researchers will likely acknowledge that language doesn't just vary in the lexical, phonological, or syntactic form of similar concepts. Linguistic variation is not always the result of region or age differences but instead can be attributed to differences in mode (spoken versus written) or communicative purpose (informing versus persuading). Even within a specific mode or context, we find variation in specific written and spoken forms of language. Academic writing, for example, occurs in a different context than newspaper writing or letter writing. Viewing language variation in this way essentially "predicts" that contextual differences will result in the variation of linguistic features. In basic terms, a register is a variety of language that is characterized by both a specific context and the language used in the context. Variables in register analysis are not restricted to linguistic characteristics that are not meaning-changing; register analysis considers the context as a variable and looks at the different linguistic features that are found in specific situations.

In some respects, a register perspective is similar to traditional sociolinguistic approaches. Both sociolinguistic variation and register variation studies are interested in how social or situational characteristics relate to language use; however, register analysis considers a wider range of factors that are not only due to what are traditionally viewed as "social" factors (e.g., age, identity, socio-economic status). For example, when looking at potential differences between speaking and writing, the communicative purpose and topic are likely not as socially conditioned as are other components accounted for in register variation such as the relationship between participants. Seen from this perspective, register analysis takes into consideration a wider range of factors that may include social factors but may also include other factors, for example, topic, purpose of communication, and mode of communication. Another difference between sociolinguistics and register analysis relates to the linguistic features under investigation. Sociolinguistic studies are generally focused on a small number of language features that vary for purely social reasons. This approach allows us to understand why some people use the word gas and others use the word petrol. Register analysis takes a different view of language variation by using corpora to identify and interpret linguistic features. A register approach also uses a different type of analysis to investigate the extent to which linguistic features co-occur in given situations of use. From this perspective, the focus can either be on specific linguistic features or on the co-occurrence of multiple features found in particular situations of language use. Because register variation considers how linguistic features co-occur in a given context, a corpus linguistic approach is well-suited to register analysis because corpora provide large amounts of authentic texts for analysis. In fact, it would be hard to see how a register analysis could be achieved without the use of corpora. Looking at a smaller number of texts would likely not provide a representative sample of language use to allow for a characterization of a given register. However, as discussed above, the tools used in register analysis are also well-suited to identifying and interpreting variation in texts. For example, it is possible to look at two different types of writing tasks that vary in communicative purpose or in the amount of time provided to complete the task. The written text produced in two different conditions can be analyzed for variation in linguistic features and interpreted functionally. This is not to say that texts that vary in communicative purpose or time allotted to write a text are of different registers, but these contextual differences are likely to result in different linguistic features and that can be functionally interpreted. The number of texts needed for the latter approach does not necessarily have to be representative because the goal is not to identify a register but to provide a functional account of variation. The relevance of both applications of register analysis relates closely to the definition of corpus linguistics discussed in Chapter 1. Recall that corpus linguistics includes both quantitative and qualitative analysis. While the quantitative information can sometimes be fairly easy to obtain (after all, many times all one has to do is push a few buttons to obtain results!), proposing reasons for the quantitative information can be more challenging.

What Is a Register (Functional) Analysis?

If we see register as a variety of language, then we can describe register analysis as a framework to understand language variation. Register analysis is most readily associated with the work of Douglas Biber and his colleagues and students. According to

As we touched on in Chapter 1 and will explore further in Chapter 9, scholars are typically interested in taking one of two approaches to study variation using corpus methods. On the one hand, they focus on variation in the use of one individual lexical unit (e.g., words, collocations, ngrams, or lexical bundles) or in the use of an individual grammatical unit (e.g., subordinate clauses, modals, pronouns). They use a corpus to find out how these individual features vary across contexts/registers. On the other hand, scholars such as Biber and his colleagues are interested in describing language variation from another point of view. Instead of focusing on individual linguistic features, they are interested in characterizing texts from a comprehensive linguistic perspective. To do this, they search multiple linguistic variables at the same time in a corpus. These studies report on how these multiple linguistic features work together (i.e., how they cooccur) in texts, and then examine how their co-occurrence patterns vary in the different registers/contexts. This approach provides a comprehensive linguistic picture of the text under consideration and allows for a functional interpretation of the relevant linguistic features.

Following the three components of register analysis described above, we focus on describing situational variables in this chapter. In Chapter 3, we will show you how to search an existing corpus for the linguistic variables you may be interested in (whether lexical or grammatical), and in Chapter 4, we will take you through several projects that will help you learn how to do studies of this kind.

Describing Situational Characteristics and Identifying Variables

Prior to any linguistic analyses, register studies examine multiple aspects of the communicative context (often referred to as the "speech situation") that the sample texts are taken from. During the past few decades, a number of scholars (e.g.,

In addition to participant information, a situational analysis also requires a description of the environment and conditions in each context. Relevant in this aspect are the channel and the production circumstances. Channel refers to both the mode and medium of the language. Mode refers to the way the language is transmitted: speaking and writing are generally the two main modes of using language, but there are also gestural systems such as signing that can convey meaning. Medium refers to the relative permanence of the language. We may compare many forms of written language as being more permanent than many forms of spoken language. Written forms of language can be preserved for multiple readers or multiple opportunities for reference while spoken language is generally more short-lived. This is not to say that all written forms of language are permanent, or all spoken forms of language are transient. We need to differentiate between mode and medium to distinguish the differences between a written grocery list and a recorded speech. The grocery list may be written out, but it is not as permanent as a recorded speech. In addition to channel, a situational analysis also characterizes the conditions in which the language has been constructed. We may also want to use mode and medium when referring to different types of written language found on the internet or cellular phones (such as social media posts, tweets, and text messages), which vary in grades of permanence depending on the topic and potential effect they have. We likely can think of examples when a writer has either deleted or regretted a particular tweet, social media post, or text message. The production circumstances of language may also relate to the process of planning, drafting, and revising. Some types of written or spoken language require extensive planning, drafting, or revising while other forms do not. In the written mode, an academic research article has gone through an extensive planning, drafting, and revising process; in a social media post or text message, generally this is not the case. Even in the spoken mode, we can acknowledge the differences in planning between an academic lecture or business presentation and face-to-face conversation. We can see the effect of production circumstances in spoken language where it has been shown that planned discourse contains fewer filled pauses (uh, um) and hesitations than unplanned discourse

Next, we will take a closer look at the variables of setting and communicative purpose. Setting describes the time and place of the communicative events. A face-to-face conversation involves a shared physical space but may take place in a private or public setting. A telephone conversation may also be in a private or public setting but generally does not take place in a shared physical space. Another relevant variable related to setting includes whether the language has been produced in the present or in the past. For example, newspaper articles written in the 21st century are very different from those written in the 19th century. In addition to setting, variation also can be the result of the communicative purpose. In some contexts, the purpose is to inform, persuade, or tell a story while in other contexts the purpose may be to just interact and share thoughts, ideas, or feelings. There are also cases where we would expect that the language would be more or less factual. We would hope that a newspaper article or academic textbook would contain factual information. We generally do not expect facts in a fairy tale or work of fiction. Communicative purpose also includes the extent to which the speaker or writer uses language that expresses their attitude about the topic (something we might not expect in news reporting but might expect in a news editorial).

A final aspect of the situational analysis relates to topic. This is a very broad situational variable that has not been investigated in much detail. A conversation about where to find a suitable place to eat will likely have very different linguistic characteristics than a conversation about how to fix a broken refrigerator. In a similar way, an introductory psychology textbook will likely have very different linguistic characteristics than an introductory music theory textbook. However, the situational variable of communicative purpose sometimes is also relevant in relation to topic. One might argue that the communicative purpose of a conversation on finding a place to eat and fixing a refrigerator are quite different, but the two different textbook types share the same communicative purpose. Thus, topic and communicative purpose sometimes "overlap" or share relevance, but other times they do not.

Although we have covered seven different situational variables, there are cases in which all seven variables are not involved in a situational analysis and there are cases where additional aspects of the situation will need to be added. In a register analysis of potential differences between different types of news writing, many situational variables associated with participants, mode, channel, and production circumstances may not differ although the communicative purpose may. Editorial news writing often seeks to persuade readers to adopt or at least consider a specific viewpoint; news reporting does not share this same communicative purpose but instead seeks to inform readers. To examine the language of university classrooms as a context for a speech situation (or domain), we might need to consider discipline, level of instruction, or other aspects such as interactivity, as part of the situational analysis.

In Chapter 3, you will have the opportunity to do situational analyses in some of the projects, and in Chapter 5 you will do a situational analysis of your own corpus. To understand how to apply the situational variables to different text types, we provide three examples of situational analyses below.

Email to a Friend and Email to a Boss

There are at least three situational differences between a letter to a friend and a letter to one's boss: certain aspects of the relationship between participants and certain aspects of the production circumstances (see Table

News Writing and News Talk

In this section, we will look at two related contexts that also differ in some situational respects: news writing and news talk show language. While both of these texts share the situational characteristic of conveying (potentially new) information to an audience (under communicative purpose), there are many differences in other areas of situational analysis. Even the communicative purpose could be argued to be different in these situations in that the news talk show involves not only providing information but also presenting an opinion.

Classroom versus Symposium Presentations

Second, the most pertinent differences are apparent as we examine the relationship among participants. For example, while classroom settings most likely allow questions at any point in time during the teacher's presentation, presentation settings have set routines and questions can only be asked after the presentation has been delivered. In terms of the presenter's familiarity with the participants, it is clear that the teacher knows (or is at least familiar with) most of the participants in the classroom because the same set of people would meet weekly for 12 or more weeks at a time in the same physical space. In contrast, the presenter at the student symposium may or may not know the audience. Yet another difference in this area is the presenter's social/academic status in the group they are presenting. The teacher (addressor) in the classroom setting is superior to the audience (addressee) and has a high social/academic status in the community. In contrast, the presenter at the student symposium plays a subordinate role in the academic community (especially if teachers are sitting in the audience) and has a relatively low social status in the community. If the student is an undergraduate, s/he has an even lower status than that of graduate students. If the student is a master's student, s/he will have a lower status than a doctoral student, and so on. Finally, the teacher holds the power in the classroom while the audience holds the power at this particular symposium because the presenters are being judged for their performance.

Third, the production circumstances are also different in the two settings. Primarily, this difference is attributed to the ability of instantaneous revisions of the text. In the classroom, there is plenty of room to negotiate and revise the text on the spot since questions can be asked at any time and clarifications can be requested. At the symposium presentation, there is a lack of spontaneous interaction, or immediate requests for clarification; there is no room for immediate negotiation of the text. Clarification questions can be made after the presentation has been delivered. Table

In sum, there are enough situational differences in some of the aspects of this context to predict that language would be used differently.

Providing a Functional Interpretation

The third step in a register analysis requires the researcher to provide a functional reason for the linguistic features in a given text. Because register analysis seeks to describe the relationship between situational variables and linguistic variables, the occurrence of linguistic features requires a description of the context. In fact, from this perspective, language features occur because they are fitting to a specific context. In other words, the situational variables in a sense "lead" language users to adopt particular linguistic features. As we discussed in the previous section, these linguistic features can be single features, but they can also be a set of co-occurring features.

News Writing and News Talk

In this example, we will show a single feature analysis focusing on first person pronouns in the subject position.

In Table

Classroom Versus Symposium Presentation

In Table

In these segments, we would like you to see differences in terms of the use of nouns and grammatical features specific to conversation such as non-clausal units, tags, and so on

The question is: "Why are these two texts so different in their noun and conversational features and in their lexical density?" In other words, what may account for the differences? If we look at the situational differences between the two texts, there are many. However, perhaps most pertinent to these two segments is the production circumstances and the communicative purpose. In the classroom, there is no pre-set script to follow; that is, there is always room for asking questions, and the potential for interaction is always present. In contrast, at a symposium presentation, the rules are strict, and questions may be asked only at the end of the talk. Therefore, the presenter is expected to talk continuously for a period of time, after which the questions from the audience may be asked. And we found that an immature cynofields resides in the kidney that's where we found the most cells with those characteristics and I interpreted that we found also oh . . . oh . . . a relationship for those cynofields but those were more mature. We can say that because . . . The electro-microscopy results with that we can see the morphology and chronology and this is a human cynofield with a transmission electronic microscopy of the human cynofield and we did with a zebrafish we found very similar morphology that granules are round as same as the human ones and the nucleus is big at this stage so we found the cell that looks like cynofields so now we want to study its function we study the function by migration of recommendation to the infection and then we see they change their morphology. So we know that cycles-sum in human cynofields includes information response and we inject the fish with the cycles-sum we let them live for 6 hours in order to provide an order response and then to (syll) we sacrifice the single cell suspension and within the facts analysis of photometry and those are our results. We found we use a control also and we can see in the control the populations of cynofields are in not increase as dramatically with the one that we injected we cycle-sum and it was 20% more of population of those cell that we found in this gate.

In terms of communicative purpose, there are two major areas that may account for the differences. On the one hand, in the classroom, the purpose is to explain concepts and methods; at the symposium, the purpose is to report on the processes and results of a research project. In addition, in the classroom, the addressor (the teacher) periodically checks for comprehension to see whether the students are understanding the material. In contrast, at the symposium presentation, the addressor (the presenter) assumes comprehension and expects questions to be asked afterwards. For these reasons, there seems to be quite a difference in the frequency of the conversational features between the two texts. However, the same is not true for the use of nouns. Because the communicative purpose in both contexts is to convey information that is potentially new to the audience, the actual information seems to be packaged in similar ways; that is, the information is delivered through nouns either embedded in noun-noun sequences or in a series of prepositional phrases. In terms of how the information is conveyed, we see differences in the type-token ratio. The teacher uses more repetitions (hence the lower ratio number) while the presenter is conveying the information without that many repetitions.

Units of Analysis and Register Studies

As we mentioned at the beginning of the chapter, many corpus researchers choose to investigate a single linguistic feature to see its variation in multiple registers. Among the lexico-grammatical studies is

Another example of single feature analysis in corpus studies is found in studies focusing on two-or multi-word sequences (collocations, and ngrams or lexical bundles, respectively). Lexical bundles are the most frequently occurring word combinations in a register; that is, in situational language use. The most often investigated bundles are four-word combinations.

These approaches to studying language use in registers provide detailed analyses of these individual features and their individual patterns. Therefore, we can learn a good deal about the use of that one feature. While these studies are interesting and very informative for these features separately, as Csomay indicates, "comprehensive descriptions of variation in language use cannot be based on investigating a single linguistic form or a single linguistic feature in isolation" (2015: 5). When describing the linguistic characteristics of texts, relying on one feature at a time is difficult mostly because a) an a priori selection of that one feature is hard to predict, since we would not really know which feature will mark the difference in the situations we are comparing, and because, as mentioned above, b) language features are typically in relationship with each other and do not occur in a vacuum. In order to characterize a register, we would need to provide a comprehensive analysis. For a comprehensive analysis, we need to look at all linguistic features in texts. In addition, we need to examine their distributional patterns to gain a "full" picture as to what linguistic features registers may be made up of. While this is possible, we would still lack the understanding of how various language features relate to one another.

However, as mentioned above, in this book, we will mainly focus on individual linguistic features and their variation in registers for two main reasons. On the one hand, these kinds of studies are relatively easy to carry out without the necessary background knowledge for a more sophisticated type of study. On the other hand, and tied to the previous point, our book is for a relatively novice audience. It is written for someone doing corpus linguistics perhaps even for the first time and for someone who has limited or no basic statistical knowledge. In contrast, to carry out comprehensive linguistic analyses from the start, the researcher must have a solid background in computer programming and in multivariate statistical methods. While such methodology is not the focus of this book, based on the results of previous comprehensive studies, we will point to ways a cluster of already identified features could be analyzed and discuss how they could be useful for our own analyses.

End of Chapter Exercises

1. In this chapter, we have discussed some benefits of investigating language from a register-functional perspective.

Notes

1 Since these text segments are uneven in length (one is 157 words long and the other is 241 words long), we had to scale the raw frequency counts as if both texts were 100 words long. To do this, we need to norm the feature count with a simple statistic: (raw feature count / actual text length) * desired text length. We will discuss this technique more in subsequent chapters. 2 To calculate type-token ratios, we take the number of words in a text and divide it by the number of word types. If a word is repeated, it counts as a new token but not as a new type. For example, in the following two sentences, there are ten tokens (i.e., number of words) and eight types (because "the" and "cat" are repeated): He saw the cat. The cat was in the garage.

Part II

Searches in Available Corpora

DOI: 10.4324/9781003363309-5

When researchers use corpora for their analyses, they are interested in exploring the use of lexical items, or certain grammatical constructions. They may also investigate lexical or grammatical patterns to see how variation in those patterns may relate to different contexts. In register studies, as we saw in the previous chapters, contextual differences refer to differences in particular aspects of the situational characteristics in the construction and production of a given text.

In this chapter, we will use the Corpus of Contemporary American English (COCA) to illustrate the most commonly identified units of language that researchers use for their analyses: words, collocations, n-grams/lexical bundles for lexical patterns, and part of speech (POS) tags for grammatical patterns. We will illustrate how to identify these units of language by providing different tasks that will give you practice in searching and analyzing these units of language. In addition, we will suggest that you access other corpora to carry out further projects in this area, for example, the Michigan Corpus of Academic Spoken English (MICASE). We will also recommend a software tool, AntConc, to carry out a keyword analysis (further details on the software is in Chapter 5). This chapter is divided into four main sections: 1) Words with two subsections: KWIC (keyword in context) and keyword analysis (based on word frequency); 2) Collocations; 3) Ngrams; 4) POS tags.

Before we can start, you will need to do two things: 1) register with COCA as a student, so you can have extended searches in that database and 2) download the latest version of AntConc (currently 4.2.0), so you Chapter 3

Searching a Corpus

3.1 Words (KWIC and Keyword analysis) 3.2 Collocates 3.3 N-Grams 3.4 POS Tags can run that program for your keyword analysis. Both COCA and Ant-Conc are free of charge.

To register with COCA, go to COCA (www.english-corpora.org/coca/) and, on the top right, click on the icon with a person's head in a box (marked yellow) and then click on "Register". This site will periodically ask you to donate money; however, as we mentioned in the previous chapter, it is not a requirement for your registration. Once you sign in, the icon with the figurehead turns green instead of yellow.

To access MICASE you can go to the address below. It is free with no sign-up, but they do ask that you make appropriate references when you carry out research with the texts available through that corpus (

To download AntConc, go to Laurence Anthony's software page, and download the latest version (currently 4.2.0) to your system (Mac, Windows, or Linux) knowing that we are using the Windows version in our discussions (

Words

Keyword in Context (KWIC)

Let's say that we are interested in searching COCA to see how often the word "say" is used across three registers: spoken discourse, newspaper, and academic prose. As we described in the previous chapter, after logging into the latest version of COCA (currently 2023), you have different choices, namely, choose "list", which will provide you with concordance lines, "chart" which will provide a chart of the frequency of your selected keyword in different registers, or "word" which will give you a plethora of information about the word you are searching (e.g., collocations, synonyms, clusters, related words, and much more). Try your hand with the different options by looking up any keyword you wish so you can see what kinds of information is available to you!

The keyword that we are searching for here and now is "say". If you click on "chart", you get a summary of the frequency distribution for this word across registers. As we see on the chart in Figure

If we were interested in how our keyword is used in the immediate textual context, we would select "context" from the choices on top as shown in Figure

When we use concordance lines, we look for individual (or a group of) pre-selected keywords to see them in their immediate context. The most common form to display a keyword in context (KWIC) is through concordance lines. As mentioned above, concordance lines highlight the word you pick and provide additional text around it. You can set the size of the text by selecting the number of characters (see our discussion on AntConc in Chapter 5) in the window around the keyword, and the output lists the examples selected from the text. Each time you run the search, even with the same keyword (by choosing the button "KWIC" in the top left corner of COCA), it will display a different set of randomly selected examples.

Why would the textual environment be interesting? Because you can see the word in context now, you will be able to see patterns surrounding the When you access COCA, the different colors denote different part of speech categories (here everything is black and white only). You can get the full list of what the colors mean via the website, but for now, let's pick a few: verbs are pink, prepositions, as in "say in the paper", are marked as yellow; nouns are turquoise, as in "I heard him say abstinence"; and adverbs and adverbials are in orange, as in "They all say basically the same".

Project 3.1: "Say" Followed by What Part of Speech?

Given the 100 randomly selected examples that you see, answer the research question "What is the distribution of the different part of speech categories following the keyword 'say' when it is a verb?". To answer this question, create a table (see Table

"SAY" NOUN PRONOUN ADJECTIVE PUNCTUATION "THAT" OTHER Total Project 3.2: "Say", POS, and Register

For this project, your new research questions are: "What is the distribution of the different part of speech categories following the keyword 'say' (when it is a verb) in spoken discourse versus academic prose?" and "Is there a difference in use?"

In this case, since you are more interested in how your keyword is used in different registers, you would want 100 sample concordance lines randomly selected from spoken discourse, and 100 concordance lines randomly selected from academic prose. To get those concordance lines for your sample, go to the main page of COCA, click on "chart" and type in "sa*_v" in the search box. This will select only instances of "say" as a verb and show you the distributional pattern across several registers. Click on the bar for spoken discourse (SPOK) and that will give you 100 sample cases when the verb "say" occurs in spoken discourse. Type in "say_spoken" under "Create new list" above the sample concordance lines and save the list. Then click on the academic prose bar to get your second 100 samples just from that register; save it as "say_academic". Create a table like Table

Project 3.3: "Say" and "State"

You also know that "say" and "state" are synonyms, so they could be used interchangeably when they are verbs -at least in terms of syntax and semantics. For simplicity, let's just pick the past tense/participle forms of these two verbs: "stated" and "said". You may predict that different registers probably use these two verbs differently because one is considered more "formal" than the other. Your research question is: "What is the frequency and distributional pattern of 'say' and 'state' (as verbs in past tense and participial forms) in three registers: spoken discourse, newspaper, and academic prose?" Create a table suggested in Table

• To state an action in the past, as in "She said that he had gone to the party" • To state a completed action with a result relevant in the present (present perfect), as in "He has stated his rights already" • To state action completed in the past prior to a past action (past perfect), as in "I had said that before" • To modify a noun (adjectival function), as in "the stated example"? Fill in Table

Keyword Analysis

Through a keyword analysis, you can identify words that exist in one text but not in another text. In other words, a keyword analysis helps you identify unique words in a text (keywords) when you compare that text with another text. You can also compare two corpora that are made up of several texts. In this case, you compare words in one corpus, called target corpus, with words in another, called reference corpus. Keyness defined this way was first promoted by

We list a few studies next where scholars have used keyword analysis, so you can have an idea of what kinds of questions they were interested in:

1. Words used by different characters in classic literary works have been a very popular topic for keyword analyses. For example,

Another example of tracking keywords in telecinematic discourse

leads to conclusions about changing gender roles through time. This work is exemplified by

With these studies in mind, the next project will compare two corpora already available through AntConc. Let's say, we are interested in finding out how American press reporting is different from British press reports in terms of vocabulary. You probably know the answer already that there are many words that would be different, but this project will look for some specific examples to show how you can support a perspective with empirical data. Our research question is: "How does vocabulary in American press reporting differ from words used in British press reporting?" Before we start, please watch the tutorial on how to use the keyword feature in AntConc: www.youtube.com/watch?v=SludW4FHatI&list=PL iRIDpYmiC0SjJeT2FuysOkLa45HG_tIu&index=10. Since we are using already existing corpora in AntConc, there is no need to upload any texts from your own corpus, but as the tutorial says, you are more than welcome to do that as well for other projects (including the related projects described below).

Step 1. Following the online tutorial on YouTube referenced above, identify the press reports sub-corpus in both the American and the British collections (AmE06_PressRep and BE-06_PressRep) and feed them into the tool. The British press reporting is your target corpus and the American press reporting is your reference corpus.

a) Go to File → Open corpus manager b) Download the required sub-corpora into the program; you have successfully downloaded the texts if the little diamond shape becomes green c) Highlight the British press reporting text (the line becomes blue) and press "choose", making sure that the tab on the right-hand window is on "target corpus"; then repeat the same for the American press reporting but now your tab will be on "reference corpus".

Step 2. Once you have downloaded the appropriate sub-corpora, run the analysis by clicking on the "start" button. The higher the keyness value for the words, the more likely that they appear in the target versus the reference corpus. The words in Figure

Another example is the word "blair" ranked as sixth on the list. This word clearly refers to Tony Blair, a former Prime Minister of Great Britain, and will be more prominent in British than in American press reporting (64 times in nine texts versus one time in one text, respectively).

5: Keywords in British versus American Press Reporting

After running the texts as described above, what other words can you identify on the list that are clearly more likely to be used in the British press? For example, UK, Britain, London … Can you group them based on some category that is contextually driven (like the analysis above may suggest)? For example, references to Great Britain as a context (its institutions, cities, etc.).

Can you think of any other groupings that you could come up with? (Hint: Do you see anything that may be special from a linguistic point of view?) Now click twice on the word "labour". Let's predict why this word is more likely to be used in British over American press reportage. What is the meaning/synonym of "labour"? Yes, it has something to do with work. Click on the word twice to see how it is used in context. What do you notice? Read some of the concordance lines and try to answer the question: Why do you think it is used with a capital "L"?

Project 3.7: Keywords in Your Corpora

If you want to work with your own texts, for example, from the Michigan Corpus of Academic Spoken Discourse (MICASE), you will need to download the texts first, save them as text files, and then feed them into AntConc (see YouTube tutorial).

If you need to convert webtext, i.e., from html to text, follow the steps on this website: www.computerhope.com/issues/ch001877.htm#text. If you need to convert pdf files to a text format, the best tool to use is another free program by Laurence Anthony, and it is called AntFileConverter: www.laurenceanthony.net/software/antfileconverter/. Once you have your texts in .txt format, you can go back to the tutorial and learn how to feed them into the AntConc program.

The one thing to keep in mind when you compare corpora for a keyword analysis is to choose corpora that are approximately the same size in terms of the number of words in them. A corpus with more or longer texts will allow more words in them. With more words, the frequency of each word increases and since the keyword analysis is based on frequency in one corpus over another (see tutorial), this may be problematic if you have different-sized corpora.

If you use MICASE, you can compare two different disciplinary areas to see whether they use certain words in one discipline versus another. To do so, select two disciplinary areas you are interested in -for us, it is history and natural sciences (natural resources). If you select these areas, you will be given multiple texts to choose from. Select the file titled "History Review Discussion Section" (DIS315JU101) and "Ecological Agriculture Colloquium" (COL 425MX075). We will use two areas because they have about the same number of words in the files. Download the files and make sure they are saved in text format (see the website referenced above on how to do that). Run the keyword analysis and then determine what sort of groupings you can identify for the types of words one session is using over the other.

Collocates

As mentioned in Chapter 1, collocates are two words that occur together more often than we would expect by chance. More specifically, Webster's dictionary defines collocate as a word that is "habitually juxtaposed with another with a frequency greater than chance".

Project 3.8: Collocates of "Spend"

Go to the main page of COCA, type in "spend", and click on "Word" among the list of options. On this page, you have all kinds of information about the word spend, including what words they collocate with. Look at what collocates this word has. You will see that the collocates are, for example, spend time, money, day, etc. If you click on the word "time" from this collocate listing and scroll down, you will be able to bring up all the examples where these two words occur together in texts. Click on "money", and examine the first 100 samples provided to you, answering the following research question: "In which register does the collocate spend (v) + time occur most frequently (in your sample)?" Another way of looking at collocates is if you click on the link called "Collocates" in the top right, you will see all the collocates with the word "spend" and for each, you can see their frequency counts as well.

N-Grams

Most often, n-grams in linguistics are sequences of words explored as a unit, where the value of n denotes how many words there are in that unit. If the basic unit of analysis is a word, then we can call that a uni-gram (1-gram). If we have two words to consider as a unit, they are bi-grams (2-grams), and if we have three words as a sequence in a unit, it will be a tri-gram (3-gram), and so on.

A special computer program is often designed to process the text and to look at sequences of words in a window of text as they emerge in a corpus. Depending on how "big" your unit is (i.e., how many words in a sequence you want to trace at a given point in time), the window size is set accordingly. That is, if you want to identify bi-grams, you will capture each two-word sequence in the corpus. If you are looking for tri-grams, you will capture each three-word sequence, and so on. As you are doing so, the already identified sequences are tracked and the new sequences are constantly checked against what the program has already found. Each time the same word sequence is found, the program counts the frequencies of that sequence. We can explore how frequently particular word combinations (n-grams) occur together in a corpus or how they are distributed across different registers.

If you know ahead of time what sequences you are looking for, you can just type the sequence in the search engine. In this case, you are not looking for the kinds of n-grams that may be emerging in that corpus; instead, you are just looking for a pre-defined sequence of words (that may, or may not, have been extracted from a previous corpus). For example, if you type in the word baby, you will see that it occurs over 60,000 times in the COCA corpus. But you picked the word ahead of time, so you knew what to look for. If you are interested in the sequence a baby, it occurs more than 10,000 times in the same corpus, and if you are interested in the sequence like a baby, you will see that it occurs more than 600 times in COCA. At the same time, the four-word sequence sleep like a baby only appears 25 times in the same corpus. In all of these instances, however, you have typed in the words that you were interested in.

In contrast, if you don't know what you want to look for ahead of time, but you are interested in the possibilities a corpus may have, you can either design a new computer program for yourself and run your own data, or run the n-gram program already available to you (e.g., through AntConc, a software that we discuss more in Chapter 5). The COCA site actually provides the lists for you, including bi-, tri-, four-, and five-grams, and their frequencies in COCA.

One-Grams or Uni-Grams and Individual Words

When you search for uni-grams, you are basically interested in individual words. When you know ahead of time what word you are interested in, they are often referred to as "keywords". As we have discussed earlier in this chapter, in the KWIC section, you could then see your keyword in a textual context. The purpose of your search for keywords could be that you analyze a) what kinds of words surround your keyword, b) how they are positioned in the sentence or in the phrase, or c) what the frequencies are of your keywords. We have illustrated how this may work in the previous chapter.

Project 3.9: "WAY"

Go to COCA and search for the word (or 1-gram) way. Click on the chart button. Report on the overall frequency of the keyword "way", its normed frequency, and its distributional patterns across all seven registers (as well as their use through time) noted in COCA -namely, spoken discourse, fiction, magazine, TV shows and movies, newspaper, and academic prose.

Project 3.10: Frequency Rank of "A" Versus "Juxtaposition"

Let's have a look at the vocabulary characteristics of a text. This way we can investigate patterns in larger units such as a text. Each word in the COCA corpus is classified into frequency bands. That is, each word is ranked depending on how often it occurs in COCA. For example, the third person pronoun he is ranked as the 15th most frequently occurring word in the corpus with a frequency of 6,468,335. The word way, when a noun is ranked 82nd in COCA with a total frequency of 1,260,011. The most frequently occurring words in general are function words -for example, articles (a, an, and the) or prepositions or particles (in, on, etc.). In fact, the definite article the is ranked #1 in the corpus with a frequency of 50,017,617. Now compare the rank and frequency of the word juxtaposition (as a noun) to the indefinite article that you have just searched. What is the rank and what is the frequency for this word? Is it a frequent word in the corpus?

Project 3.11: Vocabulary Characteristics of a Text

Go to COCA's main page and click on the icon that looks like a page of writing or a page of typed text (Figure

As you see, there are three frequency bands. Words marked with the color blue are among the top 500 most frequently occurring words in the corpus. The green ones rank as the top 501-3,000 most frequently occurring words in the corpus, and the yellow ones are marked for those that are in the rank of less commonly used vocabulary in the corpus (beyond the rank of 3,000). Be careful not to interpret these as frequencies because these are rank numbers. You can see the actual words from the text in these three bands on the right-hand side. Their frequency in that text is also shown. For example, the word "thief" is a low-frequency word (in band three marked with yellow) and it occurs three times in this text. If we jump to the frequency band, we see that the word "the" occurs 22 times in this text sample.

While Figure

If you click on any of the words in the list, it gives you information about that one word in terms of its distributional patterns across the different registers, provides a definition of the word and its collocates, and also provides examples from the COCA corpus in concordance lines. For example, if you select "thief", you will see the window shown in Figure

Project 3.12: Vocabulary and Academic Prose

In our example above, we used a sample from fiction as a register and looked at different kinds of words in the text. Following the same steps outlined above, choose a sample from another register -for example, academic prose (marked as ACADEMIC on the list). Follow the same steps to look at how many and what kinds of words you see in this text sample. Do you see any difference between the two registers in terms of the percentage of frequent (or not frequent) words in the text? Report your results.

If you want to check any text of your choice, all you have to do is copy and paste the text in the textbox and the words will be marked up for your text in the same way as it is done in the examples. The frequencies, once again, were identified in COCA. It is a great baseline corpus for your own research as well. This is a very powerful and useful tool to determine the vocabulary characteristics of a text. Can you think of ways you may be able to use this tool for your own writing?

Bi-Grams and Collocates

Bi-grams are two words in sequence. The difference between bi-grams and collocations is the fact that bi-grams are identified based on two words that happen to be next to each other in a corpus while collocations are two words co-occurring more frequently than by chance. Collocates are always two-word combinations, are statistically determined, and are also called 2-grams (or bi-grams), as mentioned before. All collocates are bi-grams but not all bi-grams are collocates.

Project 3.13: "Way" and Its Collocates

Look for the prepositions that are found with the word way. In COCA, type in way, click on "collocates" and type in the following:

As part of your qualitative analysis, you may want to see some of the meaning differences when you use different types of prepositions, or how a second particle or preposition is attached.

Tri-Grams

Any three words co-occurring together in the same sequence are known as tri-grams. Some are complete structural or semantic units such as by the way or in a way, and can be treated as fixed expressions with a specific meaning. Others may be a semi-structural unit such as by way of, which is

Project 3.14: "By the Way" in Registers

Let's look at COCA again. It is a common belief that by the way is used only in spoken discourse, and never in academic prose. First, let's search for by the way in two registers: spoken and academic prose. Make a frequency table as we did before and report on their distributional patterns. Do you see anything interesting to report or discuss?

Project 3.15: "By the Way" and "In a Way"

It is believed that both by the way and in a way are used as sentence adverbials. Make a general search for each and see whether this could be supported by the 100 examples that you have found. Another aspect of this would be to look at where these tri-grams appear in a sentence or utterance. Look at your 100-item sample in each of the two registers and determine whether they are always at the beginning of a sentence or utterance. Is there a difference between the two registers from this perspective?

Project 3.16: "By Way of " What?

Type by way of into the top and the command [nn*]. This string by way of [nn*] will allow you to search for nouns that come after the tri-gram by way of. This command will give you the strings and will list the nouns following the string.

Step 1: Click on "chart" and the button below where you typed in your string above. This will give you the distributional patterns of this construction (with any nouns) following the tri-gram. Then click on the SPOK (spoken discourse) bar. Is there a difference between spoken and academic discourse in the kinds of nouns that are used after by way of? When tri-grams occur with a particular frequency (e.g., 20 times) and in one specific register in a corpus, they are often called lexical bundles. However, four-grams are most generally investigated as lexical bundles and are discussed in the following section.

Four-Grams and Lexical Bundles

Four-grams are sequences of four words occurring together in a corpus. When we call them 4-grams, it does not matter how often they occur in a corpus; they are still called 4-grams. That is, every four-word sequence in a corpus is a 4-gram. However, similar to tri-grams, when these four-word combinations occur at least 10 or 20 or more times in a million words (depending on how conservative we want to be) and appear in at least five different texts (to avoid idiosyncratic -that is, individualistic -use) in a register, they are referred to as "lexical bundles" in the literature (see the background on lexical bundles in

Lexical bundles, a special type of four-word sequences, are defined by the number of times they occur in a million words in a register. As mentioned above, they are not necessarily structurally complete units, e.g., in the case of. But sometimes, they happen to be units that we recognize and know quite well, such as if you look at in classroom discourse or on the other hand in academic prose. The latter is a semantically and functionally complete unit even though that is not a typical characteristic of bundles by definition, and, therefore, it is not common to find these among bundles. Given that earlier studies (especially

Report on what you see in terms of spoken/academic usage of the lexical bundle on the other hand in the COCA corpus. Then click on the column under spoken. This way all your examples will be from the spoken subregisters in COCA. (See Figure

Take each one of the 100 samples you get this way and classify each bundle according to its position in the sentence. You can use the following categories: a) sentence initially; that is, when the bundle is the first four words in the sentence or utterance (for spoken); b) sentence finally; that is, when the bundle is the last four words in the sentence or utterance (for spoken); and c) neither sentence initially nor sentence finally; that is, when neither a) or b) applies. Use Table

As a next step, do the same with academic prose to get your samples from that register. Then classify each of the 100 examples as one of the three categories above. Finally, calculate the percent value for each. (Note: Since you had 100 observations for each register, the percent value and the raw counts are the same.) Reset the sample size to 200, run it again, and see whether your results are similar. As a final note to this section, the longer the n-gram, or the word sequence, the less frequently it will occur simply because n-grams are embedded in one another. For example, in the four-word sequence on the other hand, the three-word sequences of on the other and the other hand are both present. These would be counted as two separate three-word sequences.

Five-and More-Grams

When you are extracting n-grams from a corpus, you can imagine that your sequences could be endless. However, that is not true. First, all threeword sequences will contain all the two-word sequences in them. Similarly, if you look at four-word sequences, they will have all the three-word sequences in them (and the two-word ones as well). Therefore, the higher the n is in your n-gram, the less frequency you will get for each sequence. This is particularly relevant in lexical bundles where it is not just the sequence that is part of the definition of a bundle but the cut-off point counts as well. To illustrate this, we ran a lexical bundle search in a corpus of webtexts. This was a 1-million-word corpus of cybertexts collected from five internet registers: pop culture news, advertising, forum requests for advice, blogs, and tweets (Connor-Linton, 2012).

Project 3:18: Webtext Bundles

GlobWbe is an additional corpus on the website that contains texts from the internet. Check whether the webtext bundles reported above are present in all and with what frequency. In COCA, you can download the n-grams found up to 5-grams at their site with a specific n-grams website here: www.ngrams.info/samples_words.asp

POS Tags

Marking the words with part of speech (POS) tags [n, v, j, r] can lead us to different types of analyses than those we have seen so far. For example, you can look for specific words and their associated grammatical patterns or you can look for (co-occurring) grammatical patterns independent of the actual words. On the one hand, POS tags can help you be more specific about the words you are searching if you are going through an already existing search engine and if you are searching a corpus that has been tagged for part of speech. On the other hand, POS tags can also give you more options and more flexibility in your search. Below are examples of each. There are many other part of speech categories that could be potentially interesting for any linguistic study, but before going into some analyses we can do when we work with tags, let's clarify some basic grammar.

Each part of speech belongs to one of two basic classes: open or closed. Those part of speech categories that belong to the open class contain an unlimited number of members in them. That is, there is not a set number of members for an open class POS. There can be as many as there are in a language and we do not know how many there are. In contrast, those POSs that belong to the closed class have the characteristic to contain a limited number of members and we know exactly what they are. As another way to understand this distinction, note that we frequently have new open class words coming into the language, but the same is not the case with closed class words -new closed class words are quite rare.

Examples of POS belonging to the open category are the four main parts of speech: nouns, adjectives, verbs, and adverbs. Typically, nouns [n] are modified by adjectives [j], as in, for example, big red car where big is a characteristic of the noun in terms of its size, and red is a characteristic of the noun, telling us the color of the noun, and car is a common noun. Verbs

Whether a POS belongs to the open or closed class, there are endless possibilities to search for association patterns, as shown in Chapter 2. As we have also seen in that chapter, the co-occurring patterns of these categories are the most interesting types of studies from the perspective of register variation because they are able to provide us with more comprehensive and detailed analyses of texts. Now, some corpora (e.g., COCA) have POS tags attached to each word, and some corpora (e.g., Michigan Corpus of Academic Spoken English [MICASE], and Michigan Corpus of Undergraduate Student Papers [MICUSP]) do not have that feature in addition to the actual words in a corpus. Some scholars find it more difficult to do a search on POS tags, and others write their own computer programs to process and count the different grammatical patterns through those tags.

Specifying POS for a Word

First, let's see how specifying part of speech categories with POS tags can help you be more specific about your search words. In the previous section, we compared the use of state and say in spoken and written registers. Go to COCA, hit "Browse" and type in say in the word box. Select all four part of speech categories. As you will see (also in Figure

Using POS tags can broaden your options of looking for patterns -that is, how this will give you more options and more flexibility in your search. These main POS categories identify the word as you type it into the search box. Through the tags, however, we are able to look for variation within POS categories as well. The tags, for example, allow you to look for a given word in different word classes, such as can as a noun and can as a modal verb. The third possibility is to see how often a specific word comes about in texts with different word forms. In the previous examples, we looked at say as a verb. In COCA, you would be typing the following string into the search box: say.[v*]. This indicates that you want to search and see the word say in this form and when it is a verb. What if you want to find out how often the verb say is used in past tense because you would like to make a comparison between the past tense use of say across four registers or see the syntactic position of the past tense use of say. Type the following into the search box:

The fourth option could be that you have an n-gram (e.g., a lexical bundle) and you want to see what kinds of words precede or follow that sequence. Let's try this with if you look at.

Project 3.19: "If You Look at" What?

Go through the following steps in completing this task:

Step 1: Go to COCA. This chapter provides you with an opportunity to use readily available corpora to conduct corpus linguistics projects. In this chapter, you will gain practical experience in using corpus linguistic methodologies and interpreting results. There is a total of ten projects in this chapter with some comments for each project that can guide your analysis. Because there are different ways to conduct analyses with respect to search procedures and analyses, the projects do not have a single correct answer. At the end of each project, we provide commentary for those who may seek more guidance on how to search and interpret findings.

The projects in this chapter use four different corpora that are found on English-Corpora.org (www.english-corpora.org/), an online resource for corpora that is updated regularly and is an excellent resource for getting started working with corpora. Our decision to use this resource is related to cost, accessibility, and coverage. Although you need to register in order to use the corpus, as we mentioned in the previous chapter, access to the corpora is free of charge. Every 10-15 searches you will receive a message asking you to subscribe but subscription is optional. If you choose to subscribe, the current price (2023) is 30.00 USD which is quite cost-effective for the available corpora and data. Institutional licenses are also available so check with your school whether they have an institutional subscription. In addition to being reasonably priced, these corpora have a number of advantages for researchers, teachers, and students of language studies. The corpora include different varieties of English, including American (Corpus of Contemporary American English), British (British National Corpus), and Canadian (Strathy Corpus). There are also corpora focusing specifically on news found on the web (iWeb, GloWbE, Core) as well as television shows, movies, and soap operas. All of the corpora are in English but many of them include different varieties of English. For example, GloWbE contains Chapter 4

Projects Using Publicly Available Corpora

4.1 Word-and Phrase-Based Projects 4.2 Grammar-Based Projects web news from 20 different countries which permits investigations related to variation across different varieties of English. All of the corpora also use the same search interface so that once you learn how to "ask" for information in one corpus, you can conduct searches in all of the available corpora. The home page also includes many helpful resources and videos to guide you in the use of the corpus and use of search terms. There is even a link to training videos for people who are unable to access YouTube.

A list of the corpora we will use for the corpus projects in this chapter is provided in Table

There are a few important points to remember about these corpora that are especially true when conducting projects that compare features across different corpora or when looking at register differences. First, the corpus sizes range from the 1.9-billion-word GloWbE corpus to the 50-millionword Strathy corpus. This means that if one is comparing a different feature across different corpora, frequency comparisons refer to normalized counts. Another important aspect to keep in mind relates to different registers in the corpora. Some corpora (GloWbE and COHA) are comprised of a single register; other corpora (COCA and BNC) contain multiple registers. Even within a single register, there are situational differences that need to be carefully considered. For example, in the BNC, the spoken data include contexts such as oral histories, meetings, lectures, and doctor-patient interactions. In COCA, the spoken examples are all taken from television and radio news and information shows. These different situational variables mean that the term "spoken language" may mean (i.e., represent) different things in different corpora. It is important to understand the situational characteristics of the texts when representing a register (as we discussed in Chapter 2). These potential differences need to be considered when comparing and interpreting language features across corpora. Even though both COCA and the BNC contain a "spoken" component, there are many situational differences between news shows and face-to-face conversation (for one, news shows display grammatical features that are characteristic of more informational types of discourse while face-to-face conversation displays features that have been associated with more involved or interactional types of discourse; cf.

Word-and Phrase-Based Projects Project 4.1: Lexical Change Over Time

The Corpus of Contemporary American English (COCA) corpus is divided into different time periods. Find five words that are recent (have a higher frequency in the most recent time period) and five examples of words that are more common in the earliest time period. For the more recent words, is there a steady growth curve or does the word gain popularity fairly rapidly (i.e., in a single time period)? For the declining words, is there a steady decline or a fairly rapid decline? What are some possible reasons for these tendencies you have found? Comment: For this exercise, you may want to think of words related to technology, political events, or social trends. For example, if you were to look at "text" as a verb, you would see that this verb has increased from 1990 to 2020 while "fax" has moved in the opposite direction. In addition to the frequency of the word over time, you may also consider how the word is used. Do the connotations of the word remain the same over time? Are there any new meanings associated with these words?

Project 4.2: Meanings of "Literally"

The word literally was originally used to mean something similar to exactly, precisely, or actually. In this sense of the word, the meaning is closely related to the concept of something being free from metaphor. This meaning is understood in a sentence such as: I've literally spent more holidays with the animals than I have with my own family.

(example taken from COCA)

In addition to this meaning of the word literally, there is another sense of the word that shows emphasis (with a meaning similar to really). This sense of the word is sometimes the opposite of "being free from metaphor" as in a sentence such as:

There's a story about this in this book and it blows my mind, literally.

(example taken from COCA)

Using COCA, determine the variation in use of both the literal and figurative sense of the word literally by completing the following steps:

Step 1: Develop and state your method for determining whether the word falls into the "literal" category or the "non-literal" category. Do you find any other senses of the word that do not fit into your two categories? If so, describe the extra category or categories you have found and provide examples to support each category. Then, determine whether one sense of the word is more frequent.

Step 2: Register differences: Using the chart function, describe the distribution of literally across the different registers. Are there some registers where this word is used more than other registers? What are some potential reasons for any differences? Using the method you have developed in Step 1, do you find any register differences in the meanings of literally across registers?

Step 3: Looking at the distribution of literally across time periods in COCA, do you find any historical trends in the frequency of the word? Is there an increase in the different senses of the word literally over time?

Comment: Using the word search function will provide many examples (over 39,000) that may be difficult to interpret without some systematic way of analyzing the word. To narrow your search and make an analysis more manageable, you can choose a smaller number of texts to analyze or even create a smaller, virtual corpus. These are both explained in the "KWIC → analyze texts" option under the "guides" tab on the home page.

For Steps 1 and 2, you can get a general overview of the word by choosing "search" and then "word". This provides information such as register frequency, meanings, topics, collocates by word class, frequent clusters containing literally, concordance lines, and references to entire texts that contain the word. For Step 3, make sure that you are still using the COCA corpus and have selected "chart" in the search.

Project 4.3: "Gate" as Post-Fix for Any Accepted Problem After the Term "Watergate"

In addition to its contribution to the demise of the presidency of Richard Nixon, the so-called Watergate Scandal has contributed to the American lexicon through the use of the suffix -gate added to describe controversial political or social incidents. In this project, you will explore the different ways this suffix has been used in American English and look at other language varieties to see if this suffix is also found in other varieties of English.

Complete the following steps:

Step 1: Using COCA, identify at least six cases where this suffix is used with a noun. For each, note its first appearance; then note when each term was most frequently used and whether it is still used today.

Step 2: Use GloWbE to determine whether this suffix is also found in other varieties of English.

Step 3: Interpret your results: What are some possible reasons for your findings? Are there other examples of prefixes or suffixes that are specific to varieties of English?

Comment: A search using the wildcard * + gate will yield a list of words containing -gate. These include the word gate as well as words such as investigate and promulgate. You will also find words such as travelgate and pizzagate that are more relevant. From this list, you can then select specific instances of the word that use this suffix. When using other corpora, you may either search for the exact words you found in COCA or use the same wildcard search to see if there are specific uses of -gate in the other corpora you have used. Using the latter approach will identify any creative uses of the suffix.

Project 4.4: Clichés

Owen Hargraves has written a book titled It's Been Said Before: A Guide to the Use and Abuse of Clichés (2014). In the introduction of this book, Hargraves states:

While it is true that a vast number of expressions have become tired through overuse, there is an opportunity to make even the most worn expressions striking and powerful. How do we decide which among many expressions might be just right for the occasion, or just wrong? (p. xi)

In this project, we will take a close look at some of the clichés Hargraves mentions in his book. Complete the following steps:

Step 1: Come up with a "working definition" of a cliché. How has the concept of a cliché been defined? Is there general agreement on what a cliché is? What differences in definitions do you note?

Step 2: Use the "search" and "word" functions to inform your working definition. Specifically, how does the topic, collocate (what nouns, verbs, adjectives, and adverbs occur most frequently with cliché?) and cluster information help in defining your definition? Step 3: Using both COCA and the BNC, provide the normalized frequency of each of the phrases below:

What do these frequencies tell you about the relationship between these clichés and American and British English?

1. Are there specific registers that use these phrases more frequently than other registers? What might be some reasons for any differences you find? 2. Given your working definition of a cliché, would you define any or all of the six examples as clichés? Are there other examples that are more representative of clichés?

Comment: In addition to deciding if these six phrases are all clichés, you may also want to consider the extent to which these phrases are fixed. For example, you can search for the most frequent collocate that occurs after dizzying or the most frequent collocate that occurs before array (make sure to choose the first slot to the right or left of these words). Doing this for many of these phrases can provide information on how frequently these words collocate. This approach is easier to do with phrases that do not have function words (touch and go; point the finger) since there will be many more words that precede and follow function words such as the function words and/the.

Project 4.5: Collocation of Modifying Elements

In this project, you will look at the most common words that follow the modifiers below.

You will use COCA, the BNC, and GloWbE. Complete the following steps in the project.

Step 1: For both COCA and the BNC, determine the most common word that follows each of the terms above. You can do this by using the dizzying array meteoric rise point the finger perfect storm touch and go totally awesome categorically deeply entirely far-reaching massively "COLLOCATES" function; set the span to "0" on the left and "1" on the right.

Step 2: What similarities and differences in the two language varieties do you find in the types of words that follow the modifiers?

Step 3: Use GloWbE to determine the most common collocate of the five modifiers above. Using the patterns you found for both American and British English, try to find a language variety that patterns like American English and a language variety that patterns like British English for each of the five modifiers. What factors might influence these language varieties to pattern like American or British English? Do you find any patterns that are unlike both American and British English? What are some possible reasons for any new patterns that you find?

Comment: When searching for these modifiers, note the frequency of each word. What are the most common and least common modifiers? Do the frequency counts for these words stay the same across language varieties? Additionally, when looking at collocates of these words, pay attention to the types of words that are found to the right. Are all the words adjectives or are there other word types (e.g., verbs) found as a right collocate?

Project 4.6: Sustainability

According to the Oxford English Dictionary (www.oed.com), the adjective sustainable originally referred to the ability to endure something. In this definition it was synonymous with the adjective bearable. Although this use of the term is now quite rare, there are other meanings of sustainable that are more commonly used in English. These definitions are provided below (definitions are quoted from www.oed.com):

1. Capable of being upheld or defended as valid, correct, or true 2a. Capable of being maintained or continued at a certain rate or level 2b. Designating forms of human activity (esp. of an economic nature) in which environmental degradation is minimized, esp. by avoiding the long-term depletion of natural resources; of or relating to activity of this type. Also: designating a natural resource which is exploited in such a way as to avoid its long-term depletion.

In this project, you will use both COHA (Corpus of Historical American English) and COCA to investigate these different meanings of the word sustainable (and its noun counterpart, sustainability) over time and across registers. Complete the following steps:

Step 1: Using COHA, note the first 50 occurrences of the adjective sustainable. For each use, provide the date of occurrence and note which of the three definitions provided above best fit with the occurrence of the word. Make sure to provide examples from the corpus to support your analysis of their meanings. Is one use of sustainable more prevalent than other uses of it? Is there a tendency for the meaning to change over time?

Step 2: Using COCA, note the register distribution of the adjective sustainable. In which registers is sustainable most common? In which registers is sustainable less common? Are there specific meanings of sustainable that are representative of specific registers? Provide some reasons for any register or meaning differences that you find. Make sure to support your analysis with examples from the corpus.

Step 3: This part of the project asks you to look at the meanings and register distribution of the noun sustainability. According to the online site "Environmental Leader":

Sustainability includes sustainable building, design and operations. Sustainability is the collection of policies and strategies employed by companies to minimize their environmental impact on future generations.

Ecological concerns, such as the environmental impact of pollutants, are balanced with socio-economic concerns such as minimizing the consumption of limited natural resources to maintain their availability for the future. (www.environmentalleader.com/category/sustainability/)

Using COHA, note the first 20 occurrences of the word sustainability. For each use, provide the date of occurrence and note which of the three definitions provided above best fit with the occurrence of the word. Make sure to provide examples from the corpus to support your analysis of their meanings. In which registers is sustainability most common? In which registers is sustainability less common? Provide some reasons for any register distribution differences that you find. Do the meanings of sustainability all relate to environmental or ecological issues or are there other senses of the word that are found in COCA? Comment: It is possible to search for the most common nouns following sustainable using <sustainable NOUN+>. For sustainability, using the first left collocate function will provide a list of the most common adjectives preceding sustainability. It may also be helpful to use the "compare" function to investigate any synonyms of sustainable/sustainability. Do sustainable/sustainability have any viable synonyms? Project 4.7: "Frugal", "Cheap", and "Thrifty"

In this project, we will consider different connotations of the adjectives cheap, frugal, and thrifty. We will also look at how these words may differ in their syntactic positions. There are a group of adjectives in English that can occur in both pre-noun and post-verbal positions. For example, the adjective little can be used in the sentence The little house is painted blue as well as in the sentence The house is little. In the first sentence, the adjective little is called an "attributive" adjective (i.e., it occurs in the attributive [pre-noun] position); in the second sentence, the adjective is called a "predicative" adjective (i.e., it occurs in the predicative [post-verbal] position). Not all adjectives have such a freedom of movement to these different positions. For example, the adjective upset generally is found in the predicative position (The man is upset) and may sound odd in the attributive position (The upset man left the library).

This project will consider the connotations of a group of adjectives that can occur in both attributive and predicative positions. We will start this project by considering the following letter that appeared in "Dear Abby" on

Step 1: Using COCA, report on the frequency and distribution of the adjectives cheap, frugal, and thrifty. Which of these words is the most frequent, and which of these words are less frequent? Are there any register differences in the distribution of these words? If so, what are some possible reasons for any register differences? Step 2: Using the "KWIC" and "COLLOCATES" functions, explain any differences in meaning among these three words. Do some words have a more positive or negative connotation than other words? If so, what evidence can you provide to support your answer? Make sure to use examples to back up your analysis.

Step 3: Using the "POS" function in COCA, determine whether each of these three adjectives is more common in attributive or predicative position. Do all three adjectives have similar syntactic distributions? Are there differences in meaning when the same word is in a different syntactic position? Make sure that you use examples to support your analysis. Also, make sure that you include all of the search terms that you have used.

Step 4: Given what you now know about the different meanings and syntactic positions of the adjectives cheap, frugal, and thrifty, write a response to "Thrifty in Texas" that might provide some helpful advice for how to address his problem.

Comments: In Step 2, you can use the "compare" function and look at the most common words to the left and right of the adjectives. This option allows you to control the number of words to the left and right. Choosing the words directly before or after the adjective will tell you about any potential collocations; choosing a larger span will tell you more about the other types of words that are semantically associated with each adjective. In Step 3, one possible search string for the attributive position is looking for all verbs before the adjective (e.g., <VERB thrifty>) or all nouns following the adjective (e.g., <NOUN cheap>).

Grammar-Based Projects Project 4.8: Variation in the Passive Voice

There are two main "voices" in English. In grammar terms, voice refers to the relationship between the verb of a sentence and the other participants, such as the subject and the object of a sentence. In a sentence such as The boy saw the ghost (an active voice sentence), we know who sees the ghost (the subject of the sentence, the boy) and who is seen (the object of the sentence, the ghost). In this sense, the subject noun phrase the boy serves as the "actor" of the sentence (the one doing the action of the verb) and the noun phrase object the ghost serves as the "patient" or "recipient" of the action (the receiver of the action). The second type of voice in English is called the passive voice. For example, in a sentence such as The boy is seen by the ghost, even though the boy is still in the subject position of the sentence, it is the ghost who is doing the seeing in this type of sentence. Consequently, the voice of the sentence provides information on who or what is doing the action and who or what is affected by the action expressed by the verb. The passive voice is formed by adding an auxiliary verb (be) to the passive voice sentence and substituting the "regular" form of the verb with the past participle. You will be asked to look at this rule a bit closer in the project below.

Another noteworthy aspect of the passive voice includes variation in the extent to which the original actor of the sentence (the subject of the active voice sentence) is present in the passive voice sentence. For example, compare the active voice The girl broke the window with its passive voice counterpart, The window was broken (by the girl). In this passive voice sentence, there are two options, one that states who broke the window and another that doesn't state the subject by the deletion of the entire "by phrase" (e.g., The window was broken). We can call the first of these passive types, the long passive (a passive sentence that includes the "by phrase"), and the second type, the short passive (a passive sentence without the "by phrase").

In addition to the verb be, the passive voice can also be expressed by the auxiliary verb get. See Table

Using COCA, compare the two forms of the passive, initially concentrating on the two auxiliary verbs was and got by using the search strings was _v?n and got _v?n).

1. Which of these two types of passive is more common overall? Are there register differences between the two passive types? What possible reasons might there be for any differences you have found? 2. Is there a difference in the verbs used in was passives versus got passives? How would you describe these differences? 3. Choose five different was passives and five different got passives. For each type, determine whether there is a preference for the "by phrase". Does your data suggest that the "by phrase" is dependent on the verb, on the auxiliary verb, or is it due to some other factor?

Comment: In addition to passive voice differences between was and got, it is also possible to include different forms of the auxiliary verb in your analysis. This can be done by replacing was with is/are/ being or got with gets/getting. Alternatively, the lemma of each verb can be used by using the search terms BE _v?n or

Project 4.9: "Going to" as a Modal Verb

In the two sentences below, the underlined words going to are different types of grammatical constructions. In the first, going is the main verb of the clause and to is a preposition that is followed by a noun phrase (the loo); in the second sentence, going to precedes the verb go. One way to show this difference is to determine when going to can be contracted or simplified to gonna. It is possible to use gonna in the second sentence but not the first sentence.

(a) My father is up all night going to the loo which keeps both him and my mother awake. (b) Yet not everyone is going to go to college, develop software skills, or become an entrepreneur.

(examples from COCA)

The difference between these two examples illustrates the descriptive observation that in cases where a contraction is permissible, the going to is functioning as a modal verb; in cases where gonna is not possible, the construction is comprised of a main verb (going) followed by a prepositional phrase. In this project, you will examine the modal verb going to in detail. Complete the following steps in your analysis of the modal verb going to (gonna).

Step 1: Determine the search term(s) you will use to find examples of going to followed by a verb (e.g., going to VERB) Step 2: Use COHA and determine when this use came into the language.

What patterns of development do you find? Are there certain verbs that tend to occur with going to? Have these verbs changed over time?

Step 3: Are there differences in the way the two forms of going to and gonna are used in COHA? What are some possible reasons for any differences you may find?

Step 4: Using COCA, do you see any register differences in the use of going to and gonna as a modal verb? If so, what are the differences you see? Try to provide some explanation for any register differences you may find.

Comment: Trying different search terms can be helpful in this project. Some possible search terms are: <BE going to> which provides all different forms of the verb be before going to (e.g., are/is/were going to). It is also possible to search for both <VERB going to> to find all verbs preceding going to and <going to NOUN> to find all nouns following going to.

Similar searches can also be done with gon na (<VERB gon na> and <gon na NOUN> which can provide some very interesting results. Doing similar searches in GloWbE will also illustrate potential differences across different language varieties.

Project 4.10: Grammatical Constructions Following "Begin", "Continue", and "Start"

In English, there is a good deal of variation in the forms of non-finite grammatical clauses that follow (i. Note that the infinitive clause is not a possible complement of miss (*… missed to go to regionals) and the gerund clause is not a possible complement of ask (*… asked estimating the surface normal at many points on the drawings). There are other verbs that allow both gerund and infinitive clauses as complements. The verb start, for example, allows both, as seen in (

Step 1: Using COCA, report on the complementation patterns of the three verbs (begin, continue, and start). How do the three verbs compare in their complementation patterns?

Step 2: For each of the three verbs, determine whether there are register differences in the patterns.

Step 3: What reasons account for the variation of complementation patterns in these three verbs?

Comment This chapter will take you through the steps to complete a corpus project. By reference to a specific research question, you will learn how to build your own corpus and then analyze it using both the register functional analysis approach covered in Chapter 2 and the corpus software programs covered in Chapter 3. You will also learn how to use AntConc in this chapter.

Do-It-Yourself Corpora

In the previous chapter, you were exposed to readily available corpora through the projects using the suite of corpora at English-corpora.org. These corpora can be used to explore language variation by reference to different situations of use, such as newspaper writing, fiction, and spoken language from news talk shows. These corpora are not, however, designed to understand language variation in other contexts that may also be of interest. For example, there is no easy way to determine information about the gender or age of those who produced the texts. If you were interested in looking at gender or age differences in language use, these corpora would not be of much use. Certain research questions require "specialized" corpora that are built for specific purposes. Sometimes researchers need to build their own corpora. Corpus building not only allows you to answer a specific research question, but it also gives you experience in corpus construction. There is likely no better way to learn about the issues in corpus design and to appreciate the larger corpora built by other researchers than to build one on your own. Constructing a useful corpus involves a number of steps that are described below. Before covering the steps in corpus building, we should acknowledge potential copyright issues. In some cases, you may use the internet for the texts to include in your corpus. In order to do this, you will need to carefully consider your selection of materials and the potential copyright infringement issues that relate to compiling and storing digital texts. Additionally, it is important to take into account the country in which the corpus materials are used. Different countries have different copyright rules. What might be considered a copyright infringement in one country may not be considered so in another country. If you are using the corpus for educational purposes and do not plan on selling the corpus or any information that would result from an analysis of the corpus (e.g., in publications), the likelihood of being prosecuted as a copyright violator is usually small. Nevertheless, you should take into account the following guidelines when building your own corpora:

• Make sure that your corpus is used for private study and research for a class or in some other educational context. • Research presentations or papers that result from the research should not contain large amounts of text from the corpus. Concordance lines and short language samples (e.g., fewer than 25 words) are preferable over larger stretches of text. • When compiling a corpus using resources from the internet, only use texts that are available to the public at no additional cost. • Make sure that your corpus is not used for any commercial purposes.

• Make sure to acknowledge the sources of the texts that are in the corpus.

For those interested in more information on corpus building and copyright laws, there are some sources to consult at the end of this chapter.

Deciding on a Corpus Project

Corpus research projects take a good deal of time commitment to complete. A worthy research project has a number of different components, including providing a motivation of the significance of the topic, a clear description of the corpus and the methods used in the study, presentation of results, a discussion of the results, and a conclusion that provides a summary and "takeaway message" of the research. In Chapter 8, you will learn more about how to present your research as both a written report as well as an oral presentation. However, before embarking on this project, it is valuable to spend some time thinking seriously about what you want to research and the reasons for conducting the research; i.e., the research goal of the study. Selecting an appropriate research issue is not This list is not intended to be exhaustive. Each of the topics and subtopics described above address issues that are not specific to the field of corpus linguistics but lend themselves to corpus research quite well. For example, a topic examining gender differences could involve creating a corpus of fitness articles with females as the intended audience and comparing this to a corpus of fitness articles with males as the intended audience. A project looking at the language of social media posts could be achieved by creating a corpus of Twitter posts and comparing the language used in the posts with written or spoken language found in existing corpora such as the CORE corpus found English-corpus.org.

In addition to identifying a research goal, you should also write a research question or set of research questions that you seek to answer in your research. Because this book uses register analysis as a framework for interpreting your research, the research questions in your projects all share the similarity of investigating the extent to which situational variables result in different linguistic features for some functional reason. In this sense, all research questions are framed from a particular perspective by reference to a specific methodology (corpus linguistics). The research question of an individual study depends on the specific variables under investigation. Note that all of the research issues described above are in the form of questions. Each research topic has a corresponding question (or set of questions) or a hypothesis that will be answered in the research study.

Whatever issue you select, you should have a convincing explanation of your reason for conducting the research. The first questions you can ask about your project are: "What is my research goal?" and "Why was it important to conduct the research?" If you find, for example, that song lyrics have different linguistic characteristics in different types of music, what is the relevance of this finding? Does it say something about the possible socio-cultural aspects of the consumers of the music or does it say something about the music genre in general? Clear and convincing reasons for choosing a research topic will not only help you in motivating your research, but it will also help you in interpreting the results of your research. Worthy research topics do not need particular outcomes to be interesting or relevant. To use the example of song lyrics and musical varieties again, it would be just as interesting to find little difference in the linguistic characteristics of musical varieties as it would be to find strong differences.

A final consideration relates to the type of corpus that you will build to conduct your project. A vital part of your corpus project is, obviously, the corpus itself! Before deciding on a final topic, you should determine the availability of texts that will enable you to address the issue you propose to research. You will need to make sure that the types of texts you need to carry out your project are available free of charge (so as to decrease the chance of a copyright infringement).

Giving careful thought and consideration to the importance and relevance of your research topic (including a strong justification for your selection of a research topic, i.e., the motivation for your study) is more likely to result in a project that you are proud of and that contributes to an understanding of language variation. Taking the time to consider the significance of your project and its potential application to the field of applied linguistics (or other fields of study such as sociology, business, or art and music) is time well spent.

Building a Corpus

Once you have selected an adequate research goal and corresponding research question (or set of questions), the next step is to build a relevant corpus. The corpus that you will be building for your project will likely not be a large general corpus but will be a smaller, "specialized" corpus that is designed to answer the specific research question(s) you are investigating. One difference between specialized corpora and larger, more general corpora relates to their purpose: Specialized corpora are normally designed to address specific research questions while general corpora are intended for a larger audience and are designed to answer a larger set of research questions posed by multiple researchers. This is not to say that specialized corpora are never used to answer different research questions, but they generally are designed to investigate a restricted set of questions, and therefore, are less likely a representative of language use in general terms. As you will see further in the next chapters, with smaller, specialized corpora, you are only able to draw conclusions in your dataset rather than generalize the results to larger contexts. Even though smaller, specialized corpora are used for more restricted research purposes than general corpora, adopting a sound set of guidelines to build the corpus is still important. A welldesigned corpus includes texts that are relevant to the research goals of the study; are saved into a file format that allows different software programs to analyze the texts; and are labeled with enough relevant contextual material so that the different contexts are easily identifiable in the corpus. We will take a closer look at each of these below.

The selection of the texts to include in your corpus depends on their suitability and their availability. Clearly, the texts need to share relevant characteristics (or variables) that meet your selection criteria for inclusion in the corpus. A project that considers how news writing changes in different time periods would, obviously, require a corpus that includes newspaper articles written at different periods of time. In order to build a corpus to address this issue, you would need to make sure that there is an adequate number of newspaper articles that have been written at different time periods. Additionally, a corpus addressing this issue would need to have sub-corpora of relatively equal size. As illustrated in some of the corpus projects that compared COCA with the BYU-BNC corpus, the unequal sizes of these two corpora did not allow for straight frequency comparisons between the two corpora. Thus, corpus "balance" is a key aspect of reliable corpus building. Note that the balance should consider not only the number of texts in each sub-corpus but should also consider the word count of the sub-corpora.

Frequency comparisons are done on the basis of the number of words, not by the number of texts. If your specialized corpus also contains subcorpora, then you should ensure the sub-corpora is of fairly equal sizes. Another issue related to corpus balance in your corpus relates to text types. A news writing corpus would need to include the various types of news textssports and lifestyle news as well as state, local, national, and international news. If only one of these text types is included then the sample might not account for variation in the different types of news texts. A balanced news writing corpus would either include texts of sports, lifestyle, and general news texts or would select only one of these text types for analysis.

Table

Once you have located relevant texts that can be used to build a balanced specialized corpus, you will need to prepare the text to be read by a software program such as AntConc, a popular and powerful program available free of charge. Different types of texts have different types of character encoding associated with them. If you use texts from the internet, the texts will likely be in Hypertext Mark-Up Language (HTML). A text that is read on a web browser such as Google Chrome or Safari looks like this: You are not very good at parking. You're just not. Unless you happen to be a middle-aged gentleman from China called Han Yue. If you are indeed Mr Yue, then (a) welcome to

This coding scheme would allow you to clearly identify the text with "010001" being the first text in time period A, "010002" being the second text in time period A, and so on. In order to ensure that the text numbers relate to the characteristics of each text, each text will also have the relevant header information described above. Note that this entire corpus -let us call it A Historical Corpus of American News Writing -would consist of three sub-corpora related to each of the three time periods. All of the files in a single time period would be available in a single folder so that each sub-corpus could be loaded separately. Depending on different research questions, the corpus could also be loaded with all three time periods. Note that if the files followed a consistent labeling practice, you would be able to determine the time periods by reference to the file name easily.

An alternative way to name files would be to use transparent file names with "word strings" instead of numbers. This way, the file names are transparent immediately, and information about the extra-textual features of the files can be accessed easily. If you choose to do this, you will need to make sure that the filename length is the same even though you may not have information in a particular category (for easier processing). For example, "news_15_election_00" would mean a news text from 2015 about an election in 2000.

Depending on your research goal and the design of your corpus, you may also want to include specific information on the situational characteristics of each text in individual text files in your corpus. Specific information such as the length of each text (by number of words), the topic, or the type of each text (if you included different types within a specific genre) can also be included in individual text files. This type of information is not a part of the text analysis but it can provide important interpretive information used in the functional analysis of your results. In a corpus of general song lyrics, you would want to include lyrics from different types of music (rock, rap, country, popular music, etc.) in order to achieve balance in your corpus. To be able to identify these different types of song lyrics, you could either come up with a system of naming each file (as described above) or you could include some of this information in each text file. Because you do not want this information counted as part of the linguistic characteristics of your text, you can put this or any other relevant information that you do not want to be a part of the linguistic analysis into angled brackets (< >). This type of information is included in the "headers" of the text but will not be read by the concordance software. Thus, each individual text file can include a relevant header and other extra-textual information as well as the text itself.

Figure

However you go about constructing your corpus, you should use the following questions to guide the process:

1. Do the texts in my corpus allow for an investigation of a specific research issue? 2. Is the corpus constructed in a balanced manner? 3. Are the texts in a file format that will allow for analysis by the corpus software you will use in your analysis?  4. Does each text have a specific code and/or header information so that specific information in each file is identifiable? 5. If relevant to your research goal, is the corpus constructed so that specific sub-corpora are readily retrievable?

Software Programs and Your Corpus

As we mentioned in previous chapters, Laurence Anthony works at Waseda University in Japan (www.laurenceanthony.net/software.html). He develops software programs that are extremely useful for corpus linguistic analyses and he makes them freely available (although you are able to make a donation should you choose to do so). To date, there are 17 software programs available for PCs, Macs, and LINUX. While it is worth finding out about each one of the programs on Anthony's webpage as they are very useful, we will mainly focus on two here, AntWordProfiler for lexical analyses and AntConc for lexical as well as grammatical analyses, as the most pertinent for your use when analyzing your corpus.

AntWordProfiler

The function of Anthony's word profiler is very similar to what we saw with WordandPhrase, except for two main differences: 1) You can use as many texts as you want at once for an analysis; and 2) instead of using COCA as the background or monitor corpus, this one uses two other word lists (General Service List by Michael West, 1953, and Nation's academic word list) on which vocabulary frequency bands are based. (See Figure

1. What kind of information can you get to know about your text(s) through the Vocabulary Profile Tool? 2. What kind of activities can you do through the File Viewer and Editor tool? 3. What do the different menu options do?

Project 5.1: Vocabulary Comparison

Let's say you are interested in finding out about the differences in the way vocabulary is used in a Wikipedia page and your own term paper on the same topic. Take one of the papers that you have written for another class and save it as a text file. Then search for the same topical area on Wikipedia, and copy the text, saving it into a text file. Read both texts into the AntWord Profiler, and run the program twice, once on each individual file.

AntConc

The function of Anthony's concordance program is similar to what we saw at the main interface of COCA. Using your own corpus, you should be able to do KWIC searches through the concordance lines, and other types of lexical as much as grammatical analyses in your own texts. Once again, download the "Help" file to get an overview of what is possible with this particular program

Clearly, this program is capable of facilitating some of the same kinds of analyses COCA did but with your own texts. Among those analyses are: KWIC (keyword in context), n-grams, collocates in a particular text, and word lists. In addition, this program is able to show you how the word (or collocate or lexical bundle or any n-gram) is distributed within each of your texts (a concordance plot) as well as how many texts include examples of your search term.

Read in (i.e., upload) your corpus through the "File" menu ("Open files") and type any search word in the search box that you would like to find out about in your text(s) and hit the "start" button to get a KWIC concordance line. (See Figure

If you press the "Sort" button, the words following the search term will be in alphabetical order. It is important to keep in mind that the colors in AntConc do not denote part of speech categories as they do in COCA; they simply show first and second and third place after the search term. (See Figure

As mentioned above, if you click on the "Concordance plot" tab, you will get a view of the spread of your search term in each of the files you uploaded as part of your corpus (see Figure

It is also possible to identify collocates of your search term. In Figure

If you click on any word on the list, it will bring you to a concordance line listing the instances of that collocate; by clicking on the word, you can take it from here for a larger textual span, as you have seen above. (See Figure

You can also generate an n-gram list based on the texts you have. Click on the "Clusters/N-grams" tab on the top and click on "N-grams" under the search term on the bottom, and also specify how big the window size should be under "N-gram size". If you are interested in lexical bundles, you should also specify what the minimum cut-off is under "minimum frequency" and "minimum range" just below. In Figure

Why Do Statistical Analyses?

When doing register analyses, researchers look for patterns of language use and their associations with the texts' situational characteristics. We need empirical measures to see what these associations are, and we need quantitative measures (e.g., the frequency of a particular language feature) to see how commonly these patterns occur. We can then look at how the frequency of that measure is distributed across the two or more situations that we are interested in.

Descriptive statistics will give us averages through which we can compare typical uses of the features in question. However, this only gives us an impressionistic view of the difference for our dataset. If we rely solely on descriptive statistics, we cannot tell how generalizable those differences may be. To be able to generalize about the "typicality" of patterns of use, we need to use other statistical procedures.

Generalizability means that the results in our sample can be predicted to be true, with a high level of certainty, to samples outside of our own dataset as well. That is, if we were to conduct a new study under the same conditions we are reporting on, we could be 95% or 99% certain to get the same results. In order to have generalizable results, we need to make sure that a set of assumptions about our data is met (see later in this chapter).

Basic Terms, Concepts, and Assumptions

In this section, we outline the basic terms and concepts that are used when doing any kind of statistical analysis. First, we discuss variable types and Chapter 6

Basic Statistics

6.1 Why Do Statistical Analyses? 6.2 Basic Terms, Concepts, and Assumptions 6.3 How to Go About Getting the Statistical Results 6.4 End of Chapter Exercises levels that are critical to know before any test can be done. Second, we introduce measures of central tendency ("typicality" in a dataset) and measures of variability or dispersion ("spread").

Variables and Observations

Variables are typically classified based on a) the type of variable and how they function in the research design, and b) the range of values and levels they can have. It is crucial to think about this ahead of time because the validity and reliability of our research depends on how we define our variables and observations. Also, the variable scale and type determines the types of statistical analyses that can be done.

Variable Types and Functions

"Regular variables" are variables that you either manipulate or want to see change in your design. They can have a range of values (numeric) or levels (non-numeric). Numeric values are relevant if you have frequencies of a linguistic feature (e.g., the number of personal pronouns in a text). Non-numeric values (or levels) are relevant when you refer to a variable in terms of categories, e.g., class size ("small" or "large"). Instead of calling them small or large, you could also give numbers to these categories (e.g., small = 1 and large = 2); however, they are not values, just numeric codes. That is, there is nothing inherently primary or secondary in the number they are assigned to.

Further classification of regular variables is based on the function they have in the design. We distinguish between two types: dependent variables and independent variables. Dependent variables are the variables that you are most interested in for your research because you think that the values of the variable (e.g., frequency) will change (or not) as you are manipulating some external factors around it. The change that will occur (or not) depends on your manipulation of other variables around it. Independent variables are the variables that you manipulate in order to see whether there is a change in the dependent variable. Dependent variables are often called "outcomes" and independent variables are often called "predictors" (of change).

EXAMPLE

You read in an educational journal article that lectures in small classes are more "personable" than in large classes. As there is no linguistic evidence provided in the article for this claim, you want to find out yourself. You decide that you will use first person pronouns (I, we) as a measure of "personable". You are interested in whether the frequency of first person pronouns (I, we -and all their variants) changes at all when you are attending a lecture in a large class with 200 students or in a small, seminarlike class with 20 students. You hope to see that the frequency of first person pronouns will change depending on which class you attend. That is, the use of first person pronouns will depend on the class size (lecture versus seminar-type). The dependent variable in this design is first person pronouns (the frequency of which will change) and the independent variable is class size (the one that you manipulate to see the change). So, your predictor for change in the outcome (pronoun use) is class size.

"Moderator variables" are referred to as other predictors or other independent variables in your design (if you have more than one). Moderator variables are viewed as independent variables potentially interacting with other independent variables. In our example, let's say you want to see whether the instructor's gender also has an effect on the use of first person pronouns in small or large classes. Your independent variable is class size and the moderator (or other independent variable) is gender. In this design, you may be interested in whether it really is class size alone, or gender alone, or the two independent variables together (class size moderated by gender) that cause a change in the use of first person pronouns.

"Control variables" are not real variables in the way we have been describing variables so far. You are not able to measure a control variable in your study; instead, it is just something you control for.

"Intervening variables" are variables that you should have measured in your study but you realize later that you didn't. Typically, these are the variables that are mentioned in the discussion section of an article or report when calling for further research.

Variable Scales

"Nominal scales" (also called categorical, discrete, discontinuous scales) are variables measuring categories. They are used in naming and categorizing data in a variable, usually in the form of identity groups, or memberships. The variable could occur naturally (e.g., sex, nationality) or artificially (experimental, control groups), or any other way, but in all cases, it is a limited number of categories. They represent non-numeric categories (e.g., religion, L1, ethnicity). When they are assigned to numbers, they carry no numeric value. Instead, they are only a category identifier (e.g., there are two sexes: 1 = male, and 2 = female).

"Ordinal scales" are used to order or rank data. There is no fixed interval, or numeric relationship in the data other than one is "greater than" or "lesser than" the other. No fixed interval means that we don't know whether the difference between 1 and 2 is the same as between 4 and 5 (i.e., no fixed interval between values as is the case for interval scales).

Examples of ordinal scales are holistic scoring, Likert scales, and questionnaires. They are numeric in that the numbers represent one being more -or less -than the other, but they do not say how much more.

"Interval scales" reflect the interval or distance between points of ranking. They are numeric, continuous scales, and are the same as ordinal but with fixed intervals. That is, while with ordinal scales we do not know whether the difference between 2 and 3 is the same as between 4 and 5, with interval scales we do. For example, the difference between 18 and 19 milliseconds is the same as between 22 and 23 -that is, one millisecond. The difference between 2 and 3 meters is the same as between 4 and 5 meters -that is, one meter, 100 centimeters, 1,000 millimeters (no matter how we measure it, the difference is exactly the same). This means that we always know how much more or less distance there is between the two measures. Sometimes, frequencies, test grades, or evaluation are considered interval variables; however, it is not really fixed. The best way to deal with frequencies, for instance, is to put them under a scale, at which point they become interval scores. We can do this by norming 1 frequency counts, for example, or by calculating percentages.

"Ratio" only tells us about the relationship between two measures. It is not a very good measure for register studies. Let's say we want to compare two texts to see which one has more nouns.

Text 1: noun/verb ratio =.27 Text 2: noun/verb ratio =.32

We are unable to tell which text has more nouns because it is only in relation to the verbs that we might have more nouns. That is, ratios measure how common one thing is but only in relation to a potentially unrelated other thing.

Variable Values (Levels)

Variables can have multiple values or levels. For example, if participant age is a variable, the (numerical) values can be counted between 0 and 120. If ethnicity is a variable, we can list what ethnicities we would want to include and give each a nominal value. For example, African American = 1, Native American = 2, Asian American = 3, etc.

Observations

Observations are individual objects that you are characterizing. They provide the unit of analysis that will make up your data. For register studies, an observation is typically each text that you enter into your database.

For other linguistic studies, it could be the individual linguistic feature you are considering or the individual test-taker whose language you are characterizing.

EXAMPLE

Let's assume you are interested in how complement clauses are used by younger and older generations and also how they are used by people with different educational backgrounds. You are using a corpus to look for patterns. Take 100 instances of complement clauses and mark each for who uses them in terms of age and educational background (hopefully, this information will be available in the corpus). You can use other contextual variables as well, but the focus should be on the two variables you identified. Instead of listing your findings in a table exemplified by Table

Measures of Central Tendency and Measures of Variability

Central tendency describes typical values for a variable; that is, it is the central point in the distribution of values in the data. Dispersion, on the other hand, is how much variation you get within your data. Both of these are important measures to see patterns.

Measures of Central Tendency

Measures of central tendency tell us about the most typical score for a dataset. There are three types: mode, median, and mean. "Mode" works for any variable scale (nominal, ordinal, or interval). It is the most frequent/common value (whatever value occurs with highest

• If there is not one most frequent score (but more than one -for instance, two, just like two and four above occur with the same frequency, so if those two were the most frequent scores, we could not tell what the mode is), there is no mode. • If each ranked score in the dataset only occurs once, i.e., no score receives a frequency higher than one (i.e., every score in the dataset occurs just once), there is no mode. • The mode is too sensitive to chance scores (when a mistake is made in entering the scores).

"Median" works for any numeric variable (ordinal or interval). It is the 50th percentile (i.e., the middle observation). To calculate the median, rank order all scores and the observation in the middle is the median. If you have an even number of scores, the median will be in between the two middle scores; if you have an odd number of scores, the median is the middle score. The quartiles are located as well in the ranking, and the number of observations that go with each score with the same number of observations from both sides. Let's say we have the average scores for the use of hedges in our corpus of nine texts.  The quartile gives us distributional patterns in the data. In this example, the 25th percentile means that 25% of the texts display a score of 18.5 or less; the 50th percentile means that half of the texts display a score of 20 or more and half of the texts display a score of 20 or less, and finally, the 75th percentile means that 75% of the texts display a score of 21.5 or less.

Median is often used as a measure of central tendency when:

• the number of scores is relatively small • the data have been obtained by rank order measurement (e.g., a Likert scale) • the mean is not appropriate (because the variable is not interval -see below)

Boxplots are typically used as visuals to show the range of scores (minimum and maximum), the 25th, the 50th (median), and the 75th percentile.

A boxplot is also able to show outliers in the dataset. In the example below, we display the use of nouns by teachers and students in the corpus. As you can see, the mode (most frequent scores) and median (the central score after rank ordering all scores) are the same for all three groups. However, the mean becomes vastly different depending on the actual scores in the dataset. In Group A, the scores vary a great deal. Social sciences students use hedges in an idiosyncratic way; that is, it really depends on the individual. Some students use none or very few, and some use a lot! When this is true, the mean is relatively high (especially in comparison with the others). In Group B, the scores are not going into extremes. Instead, they are pretty evenly distributed. That is, the students in this group more or less use hedges the same way, or at least very similarly. In Group C, students overall use hedges similarly but there is one student who hedges a lot. That one student changes the mean score dramatically. The two scores in

The characteristic of a normal distribution is that the mode, the median, and the mean are identical. Group B above has that example. And if you look at the variability of the scores, you can see that it is steadily in order. There are no outliers or extreme scores in the dataset. The scores are simply normally distributed.

Measures of Variability and Dispersion

Measures of variability and dispersion only work with interval scale type data. While range looks at the scores at each end of the distribution, variance and standard deviation measures look at the distance of every score from the mean and average them. More specifically, range only takes the highest and the lowest scores into the computation, and variance and standard deviation take each score into account.

"Range" tells us the spread of scores. We compute the range by subtracting the lowest score from the highest score.

Range x x highest lowest

= -

For example, the range of scores for Group A in the example above is 99, for Group B it is 6, and for Group C it is 82. The problem here is the same as with the mean scores, as it changes drastically when you have more extreme scores (as you see in the examples). Since it is unstable, it is rarely used for statistical reporting, but calculating the range could be informative as a piece of additional information (e.g., to see whether there is an outlier). "Quartile" (interquartile) or percentile measures tell us how the scores are spread in different intervals in the dataset. As outlined above, the median is a measure of central tendency, and by adding the interquartile figures (the percentile figures), we are able to see the spread as well. Once again, the 25th percentile tells us what scores we would get for a quarter of our data, the 50th percentile tells us what score we would get for half of the data and the 75th percentile refers to the score we would get for threequarters of the data.

"Variance" summarizes the distance (i.e., how far) individual scores are from the mean. Let's say our mean is 93.5 (X = 93.5). If we have a score of 89 (x = 89), that means our score is 4.5 points away from the mean, and that is the deviation (value) away from the mean. In this instance, we just discussed one score only. However, what we want is a measure that takes the distribution and deviation of all scores in the dataset into account. This is the variance.

To compute variance, take the deviation of the individual scores from the mean, square each deviation, add them up (oftentimes called the "sum of squares") and average them for the dataset dividing it by the number of observations minus one. As a formula, it looks like this:

"Standard deviation" is a measure of variability in the data from the point of central tendency. Standard deviation tells us the variability of the scores -i.e., the spread of the scores from the central point -and is most often used as a measure of dispersion in studies of a variety of fields, including corpus linguistic studies. But why is this important? Let's take an example that illustrates why it is important to know the spread.

With another example, from

EXAMPLE

Imagine you would like to find out whether one class is more interactive than another. As

Lecture #1: 5 turns, a total of 150 words, average turn length 30 words, each turn is of equal length. Turn 1: 30 words Turn 2: 30 words Turn 3: 30 words Turn 4: 30 words Turn 5: 30 words Total = 5 turns, 150 words Average turn length: 30

Lecture #2: 5 turns, a total of 150 words, average turn length 30 words, turn length varies for each turn.

Turn 1: 2 words Turn 2: 140 words Turn 3: 2 words Turn 4: 3 words Turn 5: 3 words Total = 5 turns, 150 words Average turn length: 30

In both instances, the average (mean) turn length is 30 words, which is the measure of central tendency. But it is clear that one lecture is very different from another in terms of turn length measures. By calculating the standard deviation for each, we are able to tell the spread in the scores; that is, whether the scores are close to each other or they vary, and if the latter, how much they vary (in terms of magnitude measured by a single number).

For Lecture #1, the standard deviation is 0, and for Lecture #2, it is 61.49.

A zero standard deviation says that there is no variation in the scores at all (clearly), and 61.49, being very high, tells us that there is a great variation in the scores. Does this tell us which lecture is more interactive? If we think that relatively shorter turns are making the class more interactive, then Lecture #1 is more interactive. If we think that longer stretches of turns coupled with two-or three-word turns is more interactive, then Lecture #2 it is. Lecture #1 looks to be the best candidate simply because the number of turns and the turn length measure together tell us that people would have more opportunity to express actual ideas rather than just agree to what is happening with one or two words at a time (see Csomay, 2012 for short turn content).

In sum, the larger the standard deviation, the wider the distribution of scores is away from the measure of central tendency (the mean). The smaller the standard deviation, the more similar the scores are, and the more tightly the values are clustered around the mean.

To calculate the standard deviation, all you need to do is to square root the variance (explained above). With this, we are able to see that two groups could be very similar in terms of their means but they can be very different because the distribution of scores away from the mean may be quite different.

Parametric and Non-Parametric Tests, Research

Questions, and Hypotheses

Parametric and Non-Parametric Tests

Non-parametric tests do not require strong assumptions about the distribution of the data. The observations can be frequencies (nominal scores) or ordinal scales and can be rank-ordered. They can be used with interval scales, too, when we are unable to meet the assumptions of parametric tests (e.g., normal distribution in the data). Non-parametric test results can only be interpreted in relation to the dataset in question. That is, no projections or predictions could be made about the population it was drawn from, and the interpretation can only relate to the dataset investigated. A nonparametric test, for example, is Chi-square (see details on this in Chapter 7). Parametric tests, however, do require strong assumptions about the nature and the distribution of the data. These assumptions are:

1. Dependent variables are interval scales (where means and standard deviations are the measures of central tendency and dispersion, respectively) and not frequencies or ordinal data. If you are using corpus data from COCA, for example, you may not want to use the frequency data but the normed score (frequency per million words) to make sure your values are interval. 2. Dependent variables are strongly continuous (rather than discrete as ordinal scores are). That is, we know exactly how much difference there is between two scores and they are always at the same distance. 3. We can estimate the distribution in the population from which the respective samples are taken. That is, the distribution in the "sample" could be projected to the distribution of the "population". A small sample size will make it problematic to do this -a minimum of 30 observations for each variable is needed. If you compare two registers, for example, you will need values for your dependent variable from at least 30 texts (observations) for each register. 4. Data are normally distributed (sometimes we use fewer than 30 observations -remember that is the minimum to assume normality in the distribution -the larger the size, the better, of course). 5. Observations are independent; otherwise, research is confounded, as discussed before -that is, there is no relationship between the observations, or cases.

Why do parametric tests? The reason parametric tests are more powerful than non-parametric tests is because a) they have predictive power (i.e., we can predict that if we followed the same procedures, and did the study the same way, we will gain the same results) and therefore, b) the results are generalizable (i.e., we can generalize that the results are true to the larger population the samples are drawn from -that is, if we repeat the study with the same parameters, we would get the same results). Therefore, they are very powerful!

Research Questions and Hypotheses

According to

(a) They are phrased in the form of statements (rather than questions). (b) Their statements show specific outcomes. (c) They need to be testable.

In other words, a "hypothesis is a statement of possible outcome of research"

Typically, we are looking for either differences between two or more groups or we are looking for relationships between two groups (see Chapter 7 for further explanation).

In looking for differences, our null hypothesis will be stating that there is no difference between two or more groups (independent variables) with respect to some measure (dependent variable). (These are typically parametric tests.)

For example, we may have the following null hypothesis:

H 0 There is no difference in the use of nouns across disciplines.

The alternative hypothesis would be:

H 1 There is a difference in the use of nouns across disciplines.

In looking for relationships between two or more variables our null hypothesis will be stating that there is no relationship between two or more measures.

H 0 There is no relationship between the use of nouns and first person pronouns in university classroom talk.

Alternative hypotheses:

H 1 There is a relationship between the use of nouns and first person pronouns in university classroom talk. H 2 There is a positive relationship between the use of nouns and first person pronouns in university classroom talk. (That is, when nouns occur, first person pronouns will as well.) H 3 There is a negative relationship between the use of nouns and first person pronouns in university classroom talk. (That is, when nouns occur, first person pronouns will not occur.)

We look to reject the null hypothesis of "no difference" or "no relationship". A p <.05 (probability of 5%) means that we have a 95% chance of being right in rejecting the null hypothesis. A p <.01 means that we have a 99% chance of being right in doing so, and a p <.001 means that we have a 99.9% chance of being right in rejecting the null hypothesis. There are two types of errors that we can commit in rejecting the hypothesis: Type 1 and Type 2. See their description below.

When we reject the null hypothesis, we want the probability (p) to be very low that we are wrong. If, on the other hand, we must accept the null hypothesis, we still want the probability to be very low that we are wrong in doing so.

The probability value (alpha, or p) basically tells you how certain you can be that you are not committing a Type 1 error. When the probability level is very low (p <.001), we can feel confident that we are not committing a Type 1 error described above, and that our sample group of students differs from other groups who may have taken the test in the past or who might take it in the future (population). We test whether the data from that sample "fit" with that of the population. A p <.05 tells us that there are fewer than five chances in 100 that we are wrong in rejecting the H 0 . That is, we can have confidence in rejecting the H 0 .

Two-Tailed Test/Hypothesis

In two-tailed tests, we specify no direction for the null hypothesis ahead of time (that is, whether our scores will be higher or lower than more typical scores). We just say that they will not be different (and then reject that if significant). (See first example above.)

One-Tailed Test/Hypothesis

We have a good reason to believe that we will find a difference between the means based on previous findings. The one-tailed tests will specify the direction of the predicted difference. In a positive directional hypothesis, we expect the group to perform better than the population. (See second example above.) In a negative directional hypothesis, the sample group will perform worse than the population. One crucial remark: We cannot repeat tests as often as we may want to. The statistical tests that we introduce in this book are not exploratory statistics, but they follow experimental designs, and test hypotheses. Onetime deal only. Steps for hypothesis testing:

Step 1: State null hypothesis.

Step 2: Decide whether to test it as a one-or two-tailed hypothesis. Question: Is there research evidence on the issue? a. NO: Select two-tailed → will allow rejection of null hypothesis in favor of an alternative hypothesis. b. YES: Select one-tailed → will allow rejection of null hypothesis in favor of directional.

Step 3: Set the probability level (typically p <.05 or lower). Justify your choice based on the literature.

Step 4: Select appropriate statistical test.

Step 5: Collect data -apply statistical test.

Step 6: Report the results and interpret them correctly.

How to Go About Getting the Statistical Results

Several statistical programs are commercially available and are potentially cheaper at a student price, and there are others that are free. While SAS, STATA, and R, for example, are powerful statistical software programs, we will be showing you how to do descriptive statistics and the basic statistical methods we outlined above with SPSS (Statistical Package for the Social Sciences). We consider this program the most user-friendly, as the other three mentioned above require some programming abilities. Also, SPSS is still the most frequently used program at university campuses and is typically available for the students at computer labs free of charge through a university license. In this section, we will show you how to organize your data in SPSS (very different from Excel!) and how to access descriptive statistical results.

Preparing the Data and Descriptive Statistics

In SPSS, the way we organize the data is very different from the way data could be entered in Excel. Therefore, we would like you to completely forget Excel while you are using SPSS. As a start, there are two views you can have in SPSS: the variable view and the data view. Before we explain each view a bit more in detail, let's review one more time the dependent versus independent variables and what the basic unit of analysis is (observations) in the example we use.

When we characterize registers based on one or more linguistic features, the unit of analysis is a text. That means that each observation is a text in which we look for the particular variable that we hope to see variation in (i.e., the dependent variable). In our examples, it has been an individual linguistic feature, such as nouns, or pronouns, etc. Each text then will have other, "extra-textual" features as well. An extra-textual feature is, for example, what register it comes from -that is, whether it is news, or face-to-face conversation, etc. Another extra-textual feature can be the time the text was produced: whether the text comes from the year 1920 or 2015. These are your independent variables, and depending on your research question, you will manipulate these to see if there is variation in the dependent variable.

When we characterize individual speakers' way of using certain language features, the unit of analysis is the text produced by those speakers. The unit of analysis is still the text (because the language was produced and transcribed), but it may not be obviously understood in the same sense as the text above because each text is more associated with individual speakers who would have certain characteristics. Yet, it is the text produced by them, and that will be the basis for comparison.

Finally, when we look at characteristics of individual linguistic features (e.g., article type in subject and object positions), our unit of analysis is each instance of that feature. Then, we characterize each observation for its features, which in this case would be syntactic position and type of article.

Preparing the Data: Entering Data into SPSS

We will now show you the basics in each view, and tell you how to organize your data in these two settings. Let's start with the variable view (see the highlighted tab at the bottom left-hand corner). Here, you will enter the names and characteristics of both your dependent and independent variables. Let's take the example we discussed earlier in this chapter when explaining the mean. Here's the text again: You are interested in finding out whether undergraduate students majoring in natural sciences use fewer "hedges" than students in the humanities or in the social sciences. You look at a corpus of student presentations that includes nine presentations from each area (a total of 27 texts). The data below shows the normed scores for "hedges" for each of the presenters in each of the three areas. Your dependent variable is "hedges" (interval scores, as it is normed to, let's say, 1,000 words) and your independent variable is discipline (nominal) with three levels (the three disciplinary areas). Because SPSS is not good at processing "string" data, i.e., text, for its variables, we need to give a nominal numeric value to each discipline. We name Social Sciences 1, Natural Sciences 2, and Humanities 3. There is no numeric value across these categories.

Your SPSS Variable view will look like Figure

We need to focus on some of these headings, but not all. For example, those that seem less important are "Width", which determines how wide the cell is in your data view, and "Columns", which determines how many columns there are. "Align" is also less important as it sets how you would like to see the text aligned in the data view (to the left, the middle, or to the right), and "Role" is what role you assign this variable in the dataset (it will all be input for us). We really do not need to worry much about these tabs. However, we do need to know more about all the others: "Type", "Decimals", "Label", "Values", "Missing", and "Measure". We will go through each of these one by one:

Type: Numeric (whether it has a numeric value or not -see nominal independent variables above).

Decimals: You can set the number of decimals you want to see. For interval scores, we typically use two decimal points and for nominal scores, we use zero decimal points (since they have no numeric value, they do not and will not have any fractions).

Label: SPSS takes very short names for variable names and only in oneword strings. Labels, then, provide you with the opportunity to give longer names that could be used as the labels for your output results as well.

Values: These are the values that you can assign to the levels. For hedges, we will not have any values assigned. But for the nominal variables, as we mentioned above, we have 1 = Social Sciences, 2 = Natural Sciences, and 3 = Humanities. As you enter each one, make sure you hit the "Add", or else it will not be added to the list. (See Figure

Measure: In this area, you will need to determine what kind of variable you have. In our example, since hedges are interval variables, we will choose "Scale", and since discipline is a nominal variable, we will choose "Nominal". (See Figure

Before we turn to our data view, let's add one more variable, so we can keep track of our observations. The filenames will be portrayed as a string variable called "text_number" (we really are not including this as a variable in any calculations; it is more like a reference for us to know which text file the data is coming from). So it will be string, and it will be a nominal type of data (all strings are nominal). (See Figure

Now that this is all set, let's turn to our "Data View" to see how the data will need to be entered. First, as we see in Figure

Now we can start running some descriptive statistics.

Descriptive Statistics

In order to get information in a stratified manner for your levels, you want to give the following command. On the top bar with "File", "Edit", "View", choose the following set: Analyze → Descriptive Statistics → Explore to get to the window shown in Figure

Following our case study, as you see, your dependent variable (hedges) needs to be under "Dependent List" and your independent variable (discipline) needs to be under "Factor List". This way, your descriptive statistics will be calculated for each level (i.e., for each of your disciplines) versus giving just one mean score of the entire dataset you have. Run the statistics, and see to what extent the results by SPSS match the descriptive statistics we calculated earlier (they really should!). If you only want the numbers, click on "statistics"; if you want a boxplot (described in this chapter) and the numbers, click on "both". Explore what option you may have further by clicking on the "Options" button at the upper right-hand side.

If you only wanted the numbers, it should look like the details in Table

We believe the numbers generated by SPSS match the hand calculations we made in this chapter. In the next chapter, we will look at four different statistical tests that we can apply to our datasets. Before we do that, why don't you test your knowledge based on this chapter? 2. population and sample and their relationship to each other 3. observations 4. standard deviation, variance (include calculation for each) 5. mean, median, mode 6. interval, nominal, ordinal scales 7. normed counts (importance and how to calculate it) 8. parametric versus non-parametric tests and inferential statistics

Variables

For each of the following research designs, determine what the dependent and the independent variables are, and whether they are interval, nominal, or ordinal scores. State the research question(s) and the null and alternative hypotheses.

1. Mary was curious to find out whether the instructor's gender or the course's level of instruction has a greater effect on informational focus in university classroom talk. She built a small but balanced corpus of randomly selected 30 class sessions from multiple corpora (MICASE, BASE, and others) where male and female instructors and levels of instruction (undergraduate, graduate) were both represented. She tagged the texts with a grammatical tagger, counted the appropriate part of speech tags (see Chapter 9 about tagging), and normed the feature counts to 1,000 words each. She defined informational focus by adding up the number of normed counts for nouns, attributive adjectives, nominalizations, and prepositions in each of the courses she included in her corpus. 2. Allen was interested in finding out what kinds of reduced forms correlate (if at all) in highly interactive university classes. He was particularly interested to see whether phrasal types of reduction (pronoun it, demonstrative pronouns, and indefinite pronouns) and clausal types of reduction (that deletion, contraction, do as a pro-verb) have any connection to one another. Examples of the different types of features are as follows:

pronoun it: I read it and graded it. demonstrative pronoun: Look at this here. indefinite pronouns: Does anyone have an answer? that deletion:

I believe [that] you are right. contraction:

I'll do that next week. do as pro-verb:

I like pies and he does too.

Note

multiply it with a number (typically 1,000) for each observation. That is, let's say Text 1 has 45 first person pronouns, and it is 1,553 words long. Then we will calculate the normed count to 1,000 words (as if the text were that long) by (

Difference Tests

When doing difference tests (e.g., One-Way ANOVA or Two-Way ANOVA), we test whether there is a statistically significant difference in the average scores, i.e., mean, between two or more variables. The goal of difference tests is to see the extent to which the independent variable(s) is/ are responsible for the variability in the dependent variable. That is, we are interested in how one or more variables affect another variable. We can make claims about cause and effect, i.e., one variable changes because another variable has a profound (statistically significant) effect on it. We cannot talk about the results in terms of more or less significant, however.

Once the results are statistically significant, we can investigate where the differences are with post-hoc tests and how strong the association between the dependent and independent variable is with Cohen's d measures, or more typically with R 2 .

One-Way ANOVA (Analysis of Variance)

We can do a One-Way ANOVA test when we have one dependent variable and one independent variable, the latter with more than two levels (see example below). The dependent variable has to be an interval score, and the independent variable has to be nominal. With parametric tests, like One-Way ANOVA, we can generalize to the population that the sample was drawn from. Conceptually, with a One-Way ANOVA we are interested in identifying the change in the dependent variable and associate the change with the manipulation of the independent variable. More specifically, we seek to find out whether the variability in the dependent variable is due to the variability of the scores within each level of the independent variable or across the levels (or groups) we are comparing. That is, One-Way ANOVA assesses whether the differences in mean scores are attributed to the variability within the groups or across the groups. If the ratio of these two measures is small -that is, if the "across-group" variation is small relative to the "within-group" variation -there is no statistical difference. If, however, the "across-group" variation is large relative to the "withingroup" variation, there is a statistically significant difference across the groups. That is, the larger this ratio between the "within-group" variability measures and the "across-group" variability measures (F score), the more likely that the difference between the means across the groups is significant. Assumptions and requirements with ANOVA:

1. We have one dependent and one independent variable, the latter with three or more levels. 2. The dependent variable must be reported in interval scores (e.g., normed counts for linguistic features) and must be continuous, and the independent variable must be nominal. 3. Measured variables must be independent (not repeated). 4. Normal distribution of scores is expected in each group. 5. Number of observations is equal in each group (a balanced design), although it is only necessary when we do calculations by hand. The statistical package (e.g., SPSS 1 ) accounts for an imbalance. 6. Values/categories on independent and dependent variables must be mutually exclusive and exhaustive. 7. Cell values cannot be too small. A minimum of five observations per cell is necessary for each level of the independent variable.

EXAMPLE

As an example, let's say you are investigating university classrooms as your context. You analyze your context for all the situational variables outlined in Chapter 2, and you realize that discipline may be a situational variable in the academic context that may have an effect on how language is used in the classrooms. In fact, you have read earlier that the use of pronouns may vary depending on the discipline. Based on your readings, you also know that first person pronouns are more apparent in spoken discourse, and have been associated with situations where the discourse is produced under more involved production circumstances (e.g., where the participants share the same physical space, allowing for the potential of immediate involvement in interaction). Knowing all of this, you isolate this one pronoun type because you are interested in the use of first person pronouns (I, me, we, us). More specifically, you would like to find out whether there is a significant difference in the use of first person pronouns in different disciplines (more than two).

You formulate your research question in one of two ways:

1. How does the use of first person pronouns differ across disciplines? Or 2. Is there a difference in first person pronoun use across disciplines?

The dependent variable is the composite normed score for the first person pronouns as listed above (instead of using frequency scores, which are nominal, use normed counts -an interval score), and the one independent variable is discipline with three levels (nominal score with no numeric value). The three levels are the three disciplinary areas: Business, Humanities, and Natural Sciences.

You formulate your hypothesis:

H 0 : There is no statistically significant difference in the use of first person pronouns across the three disciplines. H 1 : There is a statistically significant difference in the use of first person pronouns across the three disciplines.

The statistical test to use is One-Way ANOVA (one dependent variable [first person pronouns] with interval scores [normed to a thousand words] and one independent variable [discipline] with nominal scores and with multiple levels, in this case, three disciplines). The significance level is set at the .05 level, and to locate where the differences are in case the ANOVA results in a significant difference, we will use a Scheffe post-hoc test.

To illustrate how the statistical program calculates the F score, we will do a step-by-step demonstration. Table

= = = =

(Section 6.3.1), these numbers are entered into SPSS, as illustrated in Table 7.1 below. Each observation (i.e., each text with each normed count) will be in a different row. The two variables are: First person pronoun use in each text normed to 1,000 words (interval variable) and Discipline (nominal with three levels: 1 = Business; 2 = Humanities; 3 = Natural Sciences). But in this chapter, we will go through the steps of calculating the One-Way ANOVA by hand. For this reason, and for hand-counting the ANOVA, we will use a different type of organization, as it is easier to see what is happening within the groups when listed by group (Table

In calculating the F score (the ratio for the mean sum of squares between and across groups), we need to take several steps. Conceptually, we are looking for the mean score for each group and then the variation as to how the scores are dispersed or spread (i.e., how far away each score is from the mean). This way, we can tell whether the variance can be attributed to variation inside each group or across the groups.

We take eight steps to do the calculations and determine where the differences lie. The following are the eight steps:

Step 1: Calculate the mean score for each group and for the entire dataset.

Step 2: Calculate distances across scores (and square them).

Step 3: Calculate degrees of freedom.

Step 4: Calculate mean sum of squares.

Step 5: Calculate F score.

Step 6: Determine whether the F score is significant.

Step 7: Calculate strength of association.

Step 8: Locate the differences with post-hoc tests.

Step 1: Calculate the mean score for each group and for the entire dataset. Step 2: Calculate distances across scores. Before we get into details in this area, it is necessary to make the difference between two notions: a) a score being x mean away from another score, and b) a score being x value away from the mean. The following is the explanation for the difference between these two notions:

a) If a score is x mean away from another score, it means that we are measuring the distance in the value of the mean score. Let's assume, for example, that Mean = 4; Score(1) = 2; and Score(2) = 10. In this case, Score(

In our calculations, we will mostly use the second type of distance measure. In looking at how the scores are dispersed, we need to calculate a) the distance between the individual score and its own group's mean, b) the distance between the group mean and the mean for the grand total, and c) the distance between the individual score and the mean for the grand total.

We will work with the following terminology: within sum of squares (SS W ) (the sum of squares within each group), between sum of squares (SS B ) (the sum of squares across groups), total sum of squares (SS T ) (the sum of squares for the entire dataset), degree of freedom within (Df W ) (degree of freedom within each group) and degree of freedom between (Df B ) (degree of freedom across groups). a) Within each group: How far is each score from its own group's mean?

To calculate the within-group sum of squares (SS W or group variance), take each individual score (x) minus the mean for its group (X group), and square it. Add values gained this way for each group; then add each group together. You will get the within sum of squares (SS W ), or group variance (see Table

Between sum of squares:

Total sum of squares

To calculate the total sum of squares, take each score (x) minus the total mean (X), square it and sum it up (see Table

∑ ( )

An easier way to calculate this score is by adding up the "within" and "between" sum of square values calculated before: SS W + SS B = SS T

Df W (degree of freedom within groups) = all observations minus # of groups Df W = N -N group

For our dataset: 18 -3 = 15, so our Df W = 15. Six observations in three disciplinary areas (6 x 3) minus three groups.

Df B (degree of freedom between groups) = # of groups minus 1 Df B = N group -1

For our dataset: 3 -1 = 2, so our Df B = 2. This will be important in looking up whether our F score is significant.

Step 4: Calculate mean sum of squares.

As an intermediary step between the distance calculations and the degree of freedom, we need an average of the squares. We will use the mean squares within-group (MS W ), and the mean squares between groups (MS B ) as a final step before being able to arrive at the F score. The mean square within the group is the within sum of squares divided by within degree of freedom.

For our dataset: 96/15 = 6.4

The mean square is between sum of squares divided by degree of freedom between groups.  In our example, R 2 = 192/288 =.666 R 2 =.666 means that 66% of the variance in the first person pronoun use can be accounted for by the discipline. That is, if you know the discipline, you can predict the use of pronouns more than half the time.

MS SS Df

Or, by knowing the first person pronoun score, we are able to predict which discipline it comes from with quite good certainty -more than half the time.

Step 8: Locate the differences with post-hoc tests.

With identifying the F score's significance, we can only say that there is a statistically significant difference in the use of, in our case, first person pronouns. What we cannot say is where the statistically significant differences are exactly. In order to be able to say that, we can use a range of post-hoc tests, including Scheffe, Tukey, Bonferroni, Duncan, or LSD. We are using Scheffe for the current question and dataset to illustrate how this works. Table

1. Business -Humanities: The Business mean is 4 values lower than the Humanities mean (hence the negative number). Looking at our mean scores, it is true, since the Business mean was 4 and the Humanities mean was 8. Throughout the analysis, this was considered to be a statistically significant difference. 2. Business -Natural Sciences: The Business mean is 8 values lower than the Natural Sciences mean (hence the negative number). Looking at our mean scores, it is true, since the Business mean was 4 and the Natural Sciences mean was 12. Throughout the analysis, this was considered to be a statistically significant difference. 3. Humanities -Natural Sciences: The Humanities mean is 4 values lower than the Natural Sciences mean (hence the negative number). Looking at our mean scores, it is true, since the Humanities mean was 8 and the Natural Sciences mean was 12. Throughout the analysis, this was considered to be a statistically significant difference.

The rest of the information in the table is a repetition of this but with reversed direction. If we look at our original mean scores, it is true that the Business mean was 4, the Humanities mean was 8, and the Natural Sciences mean was 12. And now we know that these differences are, in fact, statistically significant.

INTERPRETATION

Based on previous readings, we know that first person pronouns are typically associated with a communicative context where language is produced in a shared physical space and under involved production circumstances, allowing for the potential of interaction. We also know through the situational analysis that disciplines may differ in the way the material is presented, and so we want to know to what extent first person pronouns would be an indicator of such difference. The statistical results in our mini-study showed that there is a statistically significant difference across disciplines, and that significantly more first person pronouns are used in Natural Sciences than in either one of the other two disciplines. In addition, it also shows that when compared to Humanities, Business also uses significantly fewer first person pronouns. These results indicate that in Natural Sciences classrooms, language features seem to be similar to those in spoken discourse (rather than written), which then is associated with discourse produced under involved production circumstances suggesting interaction. The fact that Business showed the least number of personal pronouns may be attributed to less interaction in the classroom, and more teacher talk perhaps.

Two-Way ANOVA

We do a Two-Way ANOVA test when we have one dependent variable, and at least two independent variables that could have two or more levels each. As with other parametric tests, the dependent variable has to be an interval score (as the mean has to be the best measure of central tendency and the standard deviation has to be the best measure of dispersion), and the independent variables have to be nominal. With parametric tests, like the Two-Way ANOVA, we can generalize to the population that the sample was drawn from. Conceptually, the Two-Way ANOVA helps us identify which one of the two (or both) independent variables is (are) responsible for the variability in the dependent variable. More specifically, the question is whether the variability in the dependent variable is due to one or both independent variables. Again, we are looking at the variability of the scores within each level group versus across the levels but not only for one independent variable as we did with the One-Way ANOVA but two.

That is, the Two-Way ANOVA assesses whether the difference in mean scores is attributed to the variability within or across the level groups when it comes to two different variables and their combinations. Again, if the cross-group variation is large relative to the within-group variation, there is a statistically significant difference across the groups. That is, the larger this ratio between within-group variability measures and across-group variability measures (F score), the more likely that the difference between the means across the groups is significant. Although we would be using the same calculations if we wanted to calculate a Two-Way ANOVA by hand, as we have just done with the One-Way ANOVA, the computation becomes rather complex with two variables; hence, we will not do that by hand. We will, however, rely on the statistical package to give us the results (as we see it is pretty reliable!). What we need to be careful of here is the interpretation of the results.

EXAMPLE

As you have attended classes at the university, you noticed that teachers talk differently in classes not only from different disciplines (as we have seen the example before), but also in classes with different educational levels. Your primary investigation is discipline but it seems that level of instruction may also be a variable that could intervene in the variability of the data, and you are hoping that it does not affect your previous findings. All in all, you do not know whether the language change is attributed to only one of the variables (discipline/level) or the two together (discipline and level). In your situational analysis then, you take discipline as your main variable, and level of instruction as another, intervening variable. As for the teacher "talking differently", you continue to believe that, based on your previous readings, first person pronoun use is what makes the difference. First, you formulate your research question in one of two ways:

1. How does the use of first person pronouns differ across disciplines and levels of instruction? OR 2. Is there a difference in first person pronoun use across disciplines or across levels of instruction? 3. Is there an interaction between discipline and level of instruction in terms of first person pronoun use?

The dependent variable is first person pronouns (use normed counts as discussed before, as it is an interval score), and the two independent variables are discipline (with three levels) and level of instruction (with three levels).

The three levels for the independent variable "discipline" are Business, Education, and Natural Sciences, and the three levels for the independent variable "instruction" are lower-division undergraduate, upper-division undergraduate, and graduate.

Second, you formulate your hypothesis:

H 0 : There is no effect on first person pronoun use for discipline or level instruction and there is no effect for the interaction. H 1 : There is an effect on first person pronoun use for discipline. H 2 : There is an effect on first person pronoun use for level of instruction. H 3 : There is an interaction effect on first person pronoun use.

The number of observations in each cell is summarized in Table

In this dataset, each of the independent variables is significantly marking the variation in the first person pronoun use. At the same time, the interaction measure (Discipline * Level) is also significant with a p <.05. This means that neither discipline nor level of instruction alone is responsible for the variability in the use of first person pronouns. Instead, the two variables together cause the change in the dataset. In other words, we cannot say that, for example, Natural Sciences consistently use more first -person pronouns than Humanities, because their use of pronouns is connected to the level of instruction. Apparently, they use more in their lower and graduate classes, but not in the upper-division undergraduate classes. This variation is also true for the other two disciplines, and so discipline alone is not a factor for the change in the dependent variable. It also depends on the level of education at least as robustly. All in all, the interaction effect, if significant, overrides the effect of the individual independent variables.

Relationship Tests

When doing relationship tests (e.g., Chi-square and Pearson correlation), we test the relationship between two or more variables. That is, we test how well they go together. We are not interested in how one variable affects another one, as that is the goal of a test of difference seen in previous sections. Therefore, we also cannot make claims of cause and effect with relationship tests. We can only talk about the results in terms of a strong or weak relationship between two or across many variables.

Chi-Square

With Chi-square tests, both the dependent and the independent variables can be nominal data. The results of non-parametric tests, like Chi-square, cannot be generalized to the population the sample was drawn from but we can ask questions related to the given dataset. Namely,

• Is there a relationship between two variables in the dataset? • How strong is the relationship in the data? • What is the direction and shape of the relationship in the data?

• Is the relationship due to some intervening variable(s) in the data?

Conceptually, we typically want to know whether there is a relationship between two variables (and their levels). Chi-square compares the actual observed frequencies of some phenomenon with the frequencies we would expect if there were no relationship at all between the two variables in the sampled dataset. That is, Chi-square tests our actual results against the null hypothesis (i.e., no relationship) and assesses whether the actual results are different enough to overcome a certain probability that they are due to sampling error. The further apart the observed and expected values are, the more likely it is to be a significant Chi-square.

Assumptions and requirements:

1. The sample must be randomly drawn from the population. 2. Data must be reported in raw frequencies (not in scales, e.g., as percentages would be). 3. When frequencies of a phenomenon are counted, the frequency of nonoccurrence will also have to be counted. 4. Measured variables must be independent. 5. Values/categories on independent and dependent variables must be mutually exclusive and exhaustive. 6. Observed frequencies cannot be too small; the expected cell frequency has to be at least 5.

We rarely use One-Way designs in our studies, and therefore, we will focus on a Two-Way design. 2

EXAMPLE

Imagine that you would like to find out about the relationship between article type (a, an, the, zero article) and their position (subject or object).

Here are the steps you need to take:

Step 1: Formulate your research question: Is there a relationship between type of article use and clause position?

Step 2: State your null hypothesis: H 0 -There is no relationship between type of article use and clause position. State your alternative hypothesis: H 1 -There is a relationship between type of article use and clause position.

Step 3: Create a cross tab of frequencies of two nominal variables, article and position.

Each cell reports on how many observations produced that combination of independent and dependent values (see Table

Here are some of the rules:

• For a 1x2 or 2x2 table, expected frequency values in each cell must be at least 5. • For a 2x3 table, expected frequencies should be at least 2.

• For a 2x4 or 3x3 table, if all frequencies but one are at least 5 and if the one small cell is at least 1, Chi-square is still a good approximation.

If you are worried about the frequencies in the cells, you could collapse categories that make sense. In the example above, the two types of indefinite articles (a/an) can be collapsed since their use is dependent on the word following them (whether the following word starts with a vowel or a consonant) and will not affect the syntactic position they are in. Table

If the article distribution were the same in subject and object positions, we would get an equal number of them across article types. So the questions are: "How far is this off?" and "Can we say that they are really off, and whether there is a difference, or not?" That is the real question. In other words, if there were no relationship between the article type and the position, we would get an even distribution of the frequencies. Considering this, the question is: "Is there a relationship between the article type and clause position?" We calculate what we would expect if there were no relationship and compare that with the existing dataset. First, we calculate the row and column totals (Tables 7.14 and 7.15). Second, we calculate the expected value for each cell by taking the row total and the column total, multiplying the two, and dividing it by the grand total. Below is the formula.

For example, to calculate the expected value for the first cell ("the" in subject position) take 108 (Row total) times 146 (Column total) divided by 296 (N) = 53.27. Do the same for each cell (Table

INTERPRETATION

What does this mean? It means that we cannot reject the null hypothesis stating that there is no relationship between the article type and position. In other words, any of the articles could pretty much randomly occur in any position, as there is no relationship between the position and the type of article used. While this example may not have direct relevance to register studies, we could follow up with a register study. Instead of looking at the potential relationship between article type and syntactic position, the focus of the investigation would be to see whether one type of article, when in a certain position, occurs more often in one register over another, and versus in another position.

Correlation

Among the three different types of correlations (Pearson, Spearman Rank Order, and Point-Biserial), Pearson correlation is the most frequently used statistical procedure in corpus studies. With Pearson, we need interval data for both the dependent and independent variables. Conceptually, we are looking for relationships between two or more variables in the dataset. Again, as with Chi-square, we do not look at how one variable affects the other but how they relate to each other. Therefore, the research question also aims at looking for relationships (whether strong or weak), and not differences (whether there is an effect or not).

The null hypothesis for difference studies (e.g., One-Way ANOVA) is like this: "There is no difference between the two variables with respect to some measure".

The null hypothesis for relationship studies is like this: "There is no relationship between two measures".

EXAMPLE

You noticed that I mean and ok often come as a package in spoken discourse. You also noticed that both teachers and students use it, but what you don't know is whether it's the same when they are presenting in front of an audience. Let's assume you would like to find out whether there is a relationship between the use of "I mean" and "ok". You have a small corpus of presentations comprising two sub-corpora: teacher presentations and student presentations. Let's say, the mean score for I mean used for teachers is 39.1, and for students, it is 42.5.

There is too much overlap between the two types of presentations in terms of "I mean" use. That is, if we did a difference type of test (e.g., like ANOVA), there would not be a significant difference between teacher and student presentations in terms of the use of "I mean". In other words, we would not be able to predict whose presentation it is by knowing the "I mean" count.

In contrast, the mean score for ok use for teachers is 150, and for students, it is 328. There is no overlap between the two types of presentations in terms of the use of "ok". That is, in a difference type of test (e.g., One-Way ANOVA), this would show a significant difference between teacher and student presentations in terms of "ok" use. That is, we could predict who gives the presentation by knowing the "ok" count.

"I Mean" and "Ok" Use in Two Settings: Teacher Presentation and Student Presentation

If the uses of the two expressions consistently overlapped, seeing a correlation may be nice. That is, if we knew that the count for one feature is high, we could know that the other feature count will also be high. So, the relationship between the two features would be strong.

With two interval variables, you want to see what the strength is between the two variables so you can predict the occurrence of one by knowing the occurrence of the other. If there is no correlation, there is no relationship. If there is a correlation, then that means that there is a relationship between the two variables. The questions to ask then are: a) what kind of relationship it is, and b) how strong the relationship is. There are two kinds of relationships: positive and negative. In positive relationships, if one score is high, the other score is also high. Translated to our question, if the "I mean" score is high, then the "ok" score will also be high. In a negative relationship, if one score is high, the other score is low. In our case, if the "I mean" score is high, the "ok" score would be consistently low.

The correlation coefficient (r) is between 0 and 1 (whether positive or negative depending on the direction of the correlation explained above), where zero means no correlation (i.e., absolutely no relationship), and +1 means perfect correlation with a 100% overlap. In terms of strength, we need to see at what percentage can we predict one over the other. The direct measure of strength is r 2 , and we are looking at the percent overlap between 0 and 100%.

Let's have a visual about a potential dataset. Look at Figure

We can calculate the Person product r through Excel or SPSS. Table

This means that 78% of the time we can predict that if one feature occurs, the other one occurs too. That is, "the magnitude of r 2 indicates the amount of variance in" one variable "which is accounted for by the other variable" or the other way around

What is a strong overlap and what is a weak one is hard to tell without knowing the question. If you wish to show that one text is very similar to another, the higher the overlap the better. What the cut-off point is (i.e., what counts as an acceptably strong correlation) depends on the field of study. In social science research in general, if the overlap is over 25%, it is considered very high. However, since it is genuine continuous data, there is no need for a cut-off point. The degree will depend on how the disciplines regard this as strong or not, and that is why no significance level is necessary.

Some useful hints when doing correlation studies:

1. Forget the groups. Whether there is a relationship between the two variables is the question. 2. Only use words like strong and weak and not significant when talking about correlations. 3. Be aware that there is a relationship between sample and correlation.

Every correlation has a significance part in terms of correlating or not (as we have seen) and a strength part. The closer we get to a 100% relationship, the stronger the correlation is, but that increasing strength does not affect the significance of the correlation.

I mean ok

% overlap

Figure

In this section, we only looked at how two linguistic variables may relate to one another (or what relationship they may have) but we can look at more than two at once. It is almost like going to a party where you try to figure out who is hanging out with whom and what characteristics they have. In any case, if you look at correlations of more linguistic variables at once, you can start characterizing texts for their comprehensive linguistic make-up. We will briefly discuss this and point you in that direction in the last chapter of this book.

INTERPRETATION

The interpretation here is simple: when one language feature occurs, the other one does as well. That is, there is a positive relationship between the two variables and so we can predict that if there is a high number of one, there will be a high number of the other as well. This kind of study becomes more interesting when we look at more than just two linguistic features co-occurring with one another; i.e., when we are able to detect how a number of features, when thrown in the same pot and having an effect on one another, will behave. This will be discussed in Chapter 9 further as we are looking ahead.

How to Go About Getting the Statistical Results

As in the previous chapter, we will show you how to get the results in SPSS for the four different tests you set out to investigate.

Difference Tests

One-Way ANOVA

To run a One-Way ANOVA test in SPSS, from the tabs select Analyze → Compare Means → One-Way ANOVA. Again, your "Dependent List" will contain the dependent variable. Although you could only have one dependent variable to test a One-Way ANOVA, if you want to run the test on more than one dependent variable at the same time (e.g., you want to see variation in hedges and also in noun use), instead of opening the window for each individually, you can list all of them under the dependent list. The program will take them one by one, and run the test on each separately. Your independent variable with multiple levels will go into the "Factor" window.

Two-Way ANOVA

To run a Two-Way ANOVA test in SPSS, from the tabs select Analyze → General Linear Model → Univariate. Again, you will put your dependent variable in the "Dependent Variable" field, and the independent variable will come under "Fixed Factor(s)" and your intervening variable (your second independent variable) will come under "Random factor(s)". You can also determine what "Post-hoc" test you may want to use in case only one variable significantly accounts for the variability of the data.

Relationship Tests

Chi-Square

Effect Size

With each parametric or non-parametric statistical test, a strength of association (effect size) measure is calculated. This could be Eta-square, R-square, Cohen's d, and others. Conceptually, effect size measures point to how strong an association there is between the dependent and the independent variable. The larger the effect size, the stronger the relationship; that is, the more important the connection is between the two variables. In this section, we will focus on Cohen's d only as an effect size measure as this measure has been used more prominently in recent years.

Cohen's d

After determining statistical significance with parametric or non-parametric tests that compare two groups at a time, for example, with an Independent sample T-test (unlike ANOVA, which compares three or more groups), Cohen's d is used more and more frequently in applied linguistic research

Cohen's d "expresses the mean difference between (or within) groups"

Cohen's d effect sizes can take up positive or negative values depending on whether the larger or the smaller mean score enters the equation first.

While there is no exact cut-off point for Cohen's d, researchers in applied linguistics typically consider ±0.40 a small effect size, ±0.70 a medium effect size, and ±1.00 and above a large effect size

EXAMPLE

Let's say, you want to see how important the difference is between students in one class over another in the use of academic vocabulary (adopted from

The larger the effect size measure, the more robust the differences are.

To calculate Cohen's d in Excel, the following steps need to be taken:

Step 1: Enter your data into Excel by creating two columns, one for each of the levels in your independent variable. (In the example above, one would be the use of academic vocabulary [in percentages] for one group and for the other group.)

Step 2: Calculate the means and standard deviations for each group.

Step 3: Calculate the Cohen's d score based on the measures in Step 2. The formula in Excel is: =(mean_gr_1-mean_gr_2)/SQRT((POWER(sd_ gr_1,2)+POWER(sd_gr_2,2))/2) where mean_gr_1 is mean of Group 1, mean_gr_2 is mean of Group 2, SQRT is square root, POWER and 2 is for the second power, i.e., squared, sd_gr_1 is standard deviation for Group 1, and finally, sd_gr_2 is standard deviation for Group 2.

Step 4: Interpret the result.

INTERPRETATION

Let's say your results for Cohen's d were the following: Mean group1 = 5.64, SD group1 = 1.58, and Mean group2 = 9.22, SD group2 = 1.95. Your Cohen's d = -2.02 (adopted from

End of Chapter Exercises

After reviewing your answers to the scenarios in Section 6.4.2 in the previous chapter, determine what statistical test you would use selecting from the possible tests described in this chapter. With the data provided below, review Section 6.3, enter it into SPSS, and run the appropriate statistical test.

1. Data for Question #1 in Section 6.4.2 2 We must be careful as to how we select them in the corpus, though -and the best way to do so is to get a set of randomly selected relative clause sentences and continue our classification based on that.

DOI: 10.4324/9781003363309-11

The previous chapters of this book have discussed 1) how to do a register analysis (Chapter 2); 2) the types of software that you can use in corpus analysis (Chapter 3); 3) how to do corpus projects using existing corpora (Chapter 4); 4) how to build and structure your corpus for analysis (Chapter 5); and, 5) how to apply and interpret basic statistical techniques (Chapters 6-7). This chapter will guide you through the steps and procedures to actually put the corpus to use and to report on your research findings. In the following section, we will provide some guiding principles for how to go about answering your research question(s) using a register analysis framework and corpus methods. Then, we will describe the different parts of a research study and provide some guidelines for writing up and presenting your research project as well as suggest some approaches to how your corpus project can be assessed.

Doing a Register (Functional) Analysis of Your Project

As illustrated in Chapter 2, a register functional approach includes three components: 1) describing situational characteristics of texts; 2) identifying frequent linguistic characteristics of these texts; and 3) providing a functional interpretation of why these frequent linguistic features are found in the texts. In the following sections, we will discuss the steps in more detail. In order to illustrate some of the concepts in doing a situational and linguistic analysis along with a functional interpretation, we will refer to a small corpus of English as Second-Language writers who were asked to produce problem-solution paragraphs in two different conditions. The research question for this study is: "Do collaborative and individual texts differ in terms of their use of lexico-grammatical features?" The texts from this corpus were collected under two different conditions: First, the students were placed into pairs and asked to write a problem-solution paragraph collaboratively. Later in the semester, each student had to write a problem-solution paragraph as part of an in-class examination. The essays were then typed and saved as text files with headers and codes to show the authors and topics of the essays. The corpus was also divided into two sub-corpora: one consisting of collaboratively written essays (N = 51) and another consisting of individual essays (N = 102). A description of the corpus is shown in Table

Situational Analysis

As you recall in our discussion of register analysis in Chapter 2, the first step in a register functional analysis requires a description of the situational characteristics of your corpus. We follow the work of

In one sense, the distinction between corpus-driven and corpus-based research methods can be misleading. At least with respect to a register functional approach, any feature that is identified through a corpus-driven approach will still merit closer scrutiny and analysis in the corpus. If, for example, a word list shows that one particular word is more frequent in one sub-corpus than in another (a corpus-driven method), then the researcher will still need to look at the distribution and use of this feature more closely in the corpus by investigating its use in some more detail. One might argue that this second step can be seen as corpus-based because it identifies features in the corpus that merit further attention. On the other hand, it might also be argued that the feature was not identified by previous corpus-based research so it is, by definition, not corpus-based. We see the merits of both approaches in trying to understand language use and would encourage the use of both methods, especially in the smaller corpora that serve as the basis for your projects.

In Chapter 5, we mentioned the importance of building sub-corpora of fairly equal sizes (see Section 5.3). Sometimes this is not possible, as in the case of the problem-solution corpus described above. Since the design of this study was focused on writing paragraphs, the researchers had no control over the length of the texts. Furthermore, because the same writers produced texts both individually and collaboratively, it was not possible to simply add more collaborative texts to make the corpora equal. If your project involves comparing the frequency findings of sub-corpora in your corpus (or to another existing corpus), you will need to employ a simple statistical procedure known as "normalization" to ensure that your results are comparable (see a brief mention of this in Chapter 6). Normalization allows frequency counts taken from corpora of different sizes to be compared by providing a count of the frequency of the feature in a similar number of words.

Table

In a linguistic analysis, it is also worthy to note not only the potential frequency differences in shared words across the two types of texts but also the use of words that are different in the texts. In this small sample, we not only see differences in nouns ("English" is the third most frequent word in the individual corpus and "students" is the fourth most frequent word in the collaborative corpus) but also differences in function words ("in" is the fourth most frequent word in the individual texts and "of" is the fifth most frequent words in the collaborative texts). Some differences might be related to topic (as with the nouns) but other differences might be related to the production circumstances or relations among participants. Only a closer examination of these features in the corpus can provide us with evidence to support the analysis. Furthermore, Table

In addition to word lists, the n-gram function can show us potential variation in corpora. As discussed in Chapter 3, n-grams are contiguous sequences of words that can vary in length depending on the interest of the researcher. In Table

The following patterns can be observed in this dataset:

1. No 4-grams are shared between the two groups. 2. All of the 4-grams produced by the first group contain verbs. 3. All of the 4-grams produced by the second group contain at least one (at times two) prepositions (of, in, and among). 4. None of the 4-grams in the first group have prepositions. 5. None of the 4-grams in the second group contain verbs. 6. Overall, the second group seems to have used more 4-grams.

There are many different types of searches that you can do with your corpus. As mentioned in Chapter 5, you should feel encouraged to explore the AntConc program (as well as related literature such as the "read me" files on the AntConc website) to learn about the program. New tools are frequently available so you should visit the site from time to time to learn about the new functions and programs. For example, as mentioned above, an important consideration in understanding the use of any feature (including both word lists and n-grams) relates to the distribution (or dispersion) of a feature in the corpus. A given feature may be frequent, but it is important to make sure that the feature is not used in a few texts at a very high frequency. Although not in a statistical sense as we described dispersion in Chapter 7, you can check the visual of the distribution in AntConc by using the "Concordance Plot" option (see Chapter 5 for details). This function will show you how many different files the given feature occurs in as well as how many times the feature occurs in a single file. This is an easy method to provide a visual representation of the distributional patterns of the feature that you are looking at. Distributional patterns like these can be very helpful in interpreting your results. Seeing the distributional patterns can also help in examining whether your findings for a given feature are, in fact, spread in your corpus or are found in a limited number of texts only. If the latter, you may need to be aware that that language feature is probably used in an idiosyncratic way; that is, it is used only by one or two participants or in only a few of the texts (depending on your unit of analysis).

Functional Interpretation

As

For the specific dataset above, we have described some patterns. The fact that no 4-grams are shared by the two groups (i.e., in the two types of texts) could be attributed to certain differences in the situational characteristics; namely the topic, the production circumstances, and relations among participants. Although both groups wrote problem-solution paragraphs, the topics they chose were different; it is possible that these 4-grams are topic-related. However, the n-grams in the individual texts do not seem to be topic-related; they are more focused on providing solutions to the problem. The collaborative group did have 4-grams that mentioned specific problems, but since the individual group did not do this, it is difficult to see how the topic might have influenced the individual writers to mention solutions and the collaborative writers to mention specific problems. Since the time given to write the essays was the same in both the individual and collaborative assignments, the fact that the collaborative group used more types of 4-grams cannot be attributed to the time they may have had to complete the work. The difference might be attributed to other situational variables such as the difference between an exam and an in-class activity or the relationship between participants and the text construction. While the first group wrote the essays individually in an exam condition, the collaborative group completed the work as an in-class activity. This is a possible explanation, but it seems more likely that the differences have to do with the production circumstances related to constructing texts individually as opposed to constructing texts as writers are interacting with another person to produce a single text collaboratively.

The production circumstances, as one of the situational variables, may also be the reason that one group used verbs, while the other used prepositions. In previous register variation studies

Reporting on Your Project

The previous three sections of this chapter have led you through the steps of analyzing your corpus using a register functional perspective. At this point, you have a well-structured corpus that is guided by a well-motivated research question (see Chapter 5, Section 5.2). You also have a clear methodology for searching for and interpreting your results. It is now time to package your research project so that others can learn about your work. Below, we provide a template for your research paper and include some questions that you can use to guide your research. There are five general parts to a research paper: 1) establishing the research context and significance of the study; 2) introduction and explanation of your data and methodology you used in the study; 3) your results; 4) a discussion of your results; and 5) your conclusion. (Sometimes the results and discussion are found in a single section of the paper but we place them in different sections here.) When writing a paper of this type, you may want to think about writing a small paper for each of these five sections and then putting these sections together for the final product. At the end of this chapter, we also provide you with a sample rubric that can guide your project and give you an idea of how your project can be assessed (see Table

Parts of a Research Paper (and Guiding Questions)

Research context and significance

What is the research issue?

• What is the rationale of the current research?

• Why was it important to conduct the research?

• Is the statement of the problem adequate and convincing?

What other research studies were conducted in the same area?

• What were the main findings?  • What do you conclude from the findings?

• How do the findings relate to your research issue?

• What are the implications of the findings?

Conclusion

• Are the results logically drawn from the analysis?

• Are the conclusion, implications, and recommendations justified by the results? • What are the limitations of the study and why do you think that they are limitations?

Research Presentation

In addition to the research paper, you may also be asked to give an oral presentation of your research project. For example, you may be required to present your work in ten minutes, leaving five minutes for questions at the end. Your presentation should be accompanied by a visual aid such as various slide show programs such as PowerPoint or Google Slides and/or a one-page handout. Should you choose to do a slide show presentation, try not to put too much text on your slides. For example, you could try to have no more than six lines with each line containing no more than six words. When giving the presentation, try not to read every word on the slide. The slides serve as an outline for your presentation. You should expect that your presentation will be evaluated using the following criteria:

Description of your problem/research issues: Explanation of why your issue is important/a real-world problem A description of your corpus: Size, number of texts, how it is structured A description of how you analyzed the corpus: Search terms, commands to the software program you have used Some results and analysis A (tentative) conclusion Format and clarity of your visual aid DOI: 10.4324/9781003363309-12

Beyond simply illustrating how searches can be done with a corpus, the purpose of this book is to show how a complete corpus-based project can be carried out, including some of the technical aspects and some basic statistical analyses. As we have discussed many times throughout the book, one can follow a) a corpus-driven or b) a corpus-based (often called corpus-informed) approach to linguistic analysis. In this book, we made an attempt to illustrate both. We showed how we can do a corpus-based study with already identified language features, whether doing a lexical study or searching for the use of particular grammatical patterns. We have also illustrated the notion of a corpus-driven study, as we extracted lexical items (n-grams) from a small corpus and showed what kinds of questions a keyword analysis can answer. While it is relatively easy to carry out lexical studies with corpus-driven approaches (whether you rely on existing corpora or analyze your own corpus), as available tools allow you to extract lexical patterns from the texts, it is quite difficult to apply corpus-driven approaches to do a full lexico-grammatical analysis of texts. The main reason for this difficulty is related to the fact that texts need to be grammatically tagged so that grammatical categories can be extracted from corpora in the same way that specific lexical items are. Tagged corpora cannot only include specific types of grammatical items (such as nouns, verbs and adjectives) but also sub-categories in these different word types (such as concrete or abstract nouns, private and suasive verbs, attributive and predicative adjectives). Some tagging software is available, but there is an increasing need for corpus researchers to gain computational and statistical skills if they want to carry out more in-depth analyses. If you don't have such skills (yet), perhaps the best solution is to continue doing corpus-based studies. You can continue to look for lexico-grammatical patterns that you find interesting, or you can carry out corpus-based studies that rely on the results of previous, corpus-driven studies. For the latter, you would use the findings and apply them to new datasets. We hope that the reference list after each chapter will help you in that endeavor. Without access to tagged corpora, advanced programming and statistical knowledge, your corpusdriven research will be limited to focusing on word lists, keyword analyses, and n-grams.

Should you choose to expand your corpus linguistic skills, we present below some influential register studies that a corpus-driven approach can offer with the goal of providing comprehensive linguistic characterizations of texts and alternative ways to do keyword analysis in different registers. The purpose of the brief description is to point you to a way forward if you become interested in this type of research.

Unlike the corpus-driven approaches illustrated in this book, more advanced corpus-driven register studies have identified co-occurring linguistic features that have emerged through corpus analyses. While researchers do rely on earlier work to identify functional categories and their associated features before a corpus investigation

When we do this kind of research from the beginning (instead of using patterns already identified), the analytical framework applying this empirically based, statistical method to provide comprehensive linguistic descriptions was developed by

Many studies have applied a multidimensional analytical framework as they describe language variation across registers

On the other hand, researchers may also use an already-existing model where the dimensions (and associated linguistic features and communicative functions) have already been identified prior to the given study. The purpose of these studies is to investigate how their own texts place on the existing continuum of variation and in this sense have a corpus-informed component to them. Examples of these types of studies use an existing dimensional framework, most often referring to
Introduction

For several decades now, corpus linguistics has been among the fastest-growing methodological disciplines in linguistics. For instance, in his outgoing column as the editor of Language,

In spite of this welcome development, change in the field of linguistics is slow, and corpus linguistics in particular is limited in two ways: First, in computational ways in the sense that probably the majority of corpus linguists are still relying on a small set of often commercial and proprietary point-and-click kind of corpus search tools (such as WordSmith Tools, MonoConc Pro, or AntConc); given the severe constraints that this results in (see

The second kind of limitation involves statistical methods: While the overall amount of statistical expertise in the field is growing, corpus linguists should both widen and deepen their expertise to go beyond the handful of widely used methods. By that I do not only mean that corpus linguists need to use more different statistical tests (while that is generally true, the choice of a particular test is of course mostly dictated by the particular research question), but also that there needs to be a growing awareness that some choices that corpus linguists traditionally make may be pro blematic and would benefit from a different perspective. In the next section of this paper, I want to exemplify several such problems and survey some solutions to them. Specifically, I shall discuss potentially problematic choices or omissions in the area of general corpus statistics, in particular the choice of association measures for co-occurrence data, that is, measures with which corpus linguists quantify the degree of association between two linguistic expressions (e.g. two words or a word and a syntactic pattern/construction). In addition, I shall briefly comment on the underutilized notion of dispersion, that is, a measure that quantifies how evenly distributed elements are in a corpus, and thus also relates to the notion of corpus homogeneity. Finally, I shall demonstrate how the current typical neglect of the hierarchical structure of corpora poses severe problems. More specialized areas are currently booming, it seems: diachronic corpus linguistics, which needs to deal with the problem of how temporally-ordered corpus data are grouped into temporal stages for subsequent analysis; and learner corpus research, which needs to move on from decontextualized studies of over-and underuse to more comprehensive models of learner language and its differences to native language.

General corpus statistics

1 Co-occurrence information

One of the most fundamental notions in corpus linguistics is the distributional hypothesis, that is, the working assumption that linguistic elements that are similar in terms of their distributional patterning in corpora also exhibit some semantic or functional similarity.

[i]f we consider words or morphemes A and B to be more different in meaning than A and C, then we will often find that the distributions of A and B are more different than the distributions of A and C. In other words, difference of meaning correlates with difference of distribution.

That is, a linguistic expression E-morphemes, words, constructions/patterns, . . .-can be studied by exploring what is co-occurring with E and how often. The simplest possible way to do this would be by raw co-occurrence frequency or, more likely, conditional probabilities such as p(function|E) or p(contextual element(s)|E). Since raw frequencies will be distorted by words that are highly frequent everywhere, a more frequent way is to use association measures (AMs), that is, statistics that quantify the strength of mutual association between two elements such as a function or a contextual element on the one hand and E on the other. Most AMs are based on co-occurrence tables of the kind exemplified in Table

Problems with the quantification of co-occurrence

Problem: multi-word AMs are not conservative enough

Despite their frequency of use, AMs of the above kind are not unproblematic. One smaller problem is the fact that they do not easily generalize to n-grams (uninterrupted strings of n words), or multi-word units (such as according to, in spite of, etc.). At this point, MI for n-gramslog 2 ( obs a / exp a )-is often simply computed on the basis of complete conditional independence, which will tend to underestimate expected frequencies of a and, thus, overestimate the strength of association. If one computes the MI of in spite of in the untagged Brown corpus by comparing the observed frequency of in spite of of 54 against an expected frequency based on complete independence, MI becomes an extremely high value of 12.25. However, if one computes MI by comparing the same observed frequency of in spite of to the one expected from the occurrences of in spite and of, then that MI-value decreases to 4.76. Thus, corpus linguistics needs to explore more adequate and conservative ways to extend AMs to n-grams.

Problem: nearly all AMs are symmetric/bidirectional

An even more important problem is that nearly all AMs are symmetric: the association of expression E to context C is presumed to be symmetric/bidirectional. However, associations in general and associative learning are certainly not (always) symmetric, which is why, ideally, corpus linguistics would explore the use of directional AMs. Some work on this area exists, in particular

First, they explore the correlation of conditional probabilities from adjective-noun collocations with the University of South Florida Association Norms, but find the measure lacking in identifying symmetric associations; in addition, conditional probabilities do not normalize the observed percentage against any baseline.

Second, they explore a measure based on the differences of ranks of AMs (such as chi-squared values). For such rank measures, a collocation x y is explored by -computing all AMs for collocations with x, ranking them, and noting the rank for x y; -computing all AMs for collocations with y, ranking them, and noting the rank for x y; -comparing the difference in ranks. In tests analogous to those of conditional probabilities, this rank measure does not perform well with asymmetric associations but a little better with symmetric ones; in the additional classification task, the rank measure came with an even higher error rate than conditional probabilities. In

T able 1: Schematic co-occurrence frequency table

While this sounds promising, the computational effort that goes into these calculations is immense, since the computation of one AM for the collocation x y requires the computation of all AMs for all collocations with x and then separately for all collocations with y. In addition, in spite of the huge computational effort involved in the thousands of ranked G 2 -values, they do not perform better than conditional probability

( 1) a.

For example, all traditional AMs would return a high value for of course (see Gries 2013:144), but it is ΔP that recognizes that the association between of and course is not symmetric: of is not a good predictor that course would follow whereas course is a strong predictor that of will precede. In fact,

(2) a. apart from, according to, upside down, contrary to, ipso facto, irrespective of b. at least, per annum, status quo, for instance, de facto, vice versa

In sum, ΔP is by design more sensitive than traditional AMs since it can tease apart directionality effects; it is very easy to understand and compute; its computation/interpretation does not require assumptions (such as normality, which is very rare in corpus data); it avoids problems of the Null Hypothesis Significance Testing paradigm because it does not test the observed distributional data against an illusionary null hypothesis distribution; finally, it has received experimental support both in psychology and in linguistic work by

Problem: nearly all AMs involve only token frequencies

The next AM problem to be discussed here is perhaps just as fundamental as the symmetry problem, but even less recognized and explored: namely that the computation of nearly all AMs involves only the four token frequencies represented in Table

Given the importance of type frequencies or entropies for many domains (productivity, language change, language acquisition, . . .), it is amazing how little alternatives to AMs that utilize type frequencies or entropies have been explored in corpus linguistics proper. Studies from neighboring disciplines

Within corpus linguistics,

(3) Grav ity G (w 1 , w 2 ) =

Unfortunately, there has been very little follow-up on this notion. Two exceptions are Gries (2010b) and

The latter study explores an extension of G to the identification of n-grams in different varieties of English. More precisely, it shows how one can use G to identify n-grams, and how a G-based cluster analysis of spoken and written data from four different varieties (British, Hong Kong, Indian, and Singaporean English) perfectly distinguishes speaking from writing.

In sum, there are compelling arguments to include type frequencies from theoretical considerations as well as from neighboring disciplines such as psycholinguistics or computational linguistics, and there are promising first results within corpus linguistics proper, but more exploration is definitely required. In particular, all of the above approaches only deal with the minimal amount of information one should include-the more comprehensive information regarding token and type frequency distributions and entropies still awaits first exploration.

Problems with ignoring distribution in the structure of the corpus

Pro blem: (co-)occurrence may be underdispersed

The next AM problem to be discussed here concerns another important dimension of corpus data that the traditional kind of AM approach based on Table

As for the implications for corpus-linguistic analysis, consider the question of which verbs are likely to be used in imperatives. A perfectly normal traditional corpus-linguistic account could approach that question by computing for each verb lemma in a corpus that occurs in the imperative at least once an AM that quantifies the association between that lemma and the imperative based on tables such as Table

As for the implications for psycholinguistic and more general (theoretical) applications, dispersion has by now been shown to be relevant in domains other than core corpus linguistics, too. For instance, Simpson-Vlach &

Proble m: ignoring the hierarchical structure of the corpus

The final problem to be discussed in this section is concerned with the fact that the vast majority of statistical analyses in corpus linguistics-be they chi-squared tests, simple correlations, generalized linear models (GLM, e.g. binary logistic regressions), . . .-violate a fundamental assumption of these statistical methods: that the data points are independent of each other. Rather, there are three different ways in which many corpus data points can be seen as related to each other, the first two of which are well-known from psycholinguistic work:

-Speakers/writers in corpus data/files often provide more than one data point in a concordance so that all data points from a particular speaker/writer are related to each other (as they may reflect that speaker's idiosyncratic behavioral patterns). In psycholinguistics, this is often addressed with F 1 -or related ANOVA statistics. -For many grammatical patterns, concordance lines will involve the same lexical item so that all data points with that lexical item are related to each other (as they may reflect that lexical item's idiosyncratic patterning). In psycholinguistics, this is often addressed with F 2 -or related ANOVA statistics. -Corpora often come with a hierarchically-nested structure in which speakers are nested into files, which in turn are nested into sub-registers, which in turn are nested into registers, which in turn are nested into modes (e.g. spoken versus written). Thus, there are multiple levels of corpus organization at which effects may be located, but these levels are typically not all tested.

While it is usually freely admitted that corpus data are much more messy/noisy than (often carefully) controlled psycholinguistic experimental data, the massive interrelatedness of corpus data along the above three lines is typically ignored. In this section, I exemplify how this is problematic by comparing an analysis that, as usual, ignores this interrelatedness to one that takes it into consideration. As a small example, whose actual linguistic implications I shall not be concerned with, let us consider the question of who is more likely to use I or you-men or women-and where/when (early/ late in a conversation and/or early/late in a sentence); maybe there is an assumption that women are generally less likely to use I . . . Using an R script (R Core Team 2014), I extracted all instances of I and you (when tagged as PNP) from all 21 files of the British National Corpus World Edition (XML) whose names begin with 'KR'. For each instance, I retrieved/annotated the following variables:

-MATCH: whether the speaker used I or you; -FILE: the name of the file in which a speaker used I or you; -SPEAKER: a unique identifier for the speaker who used I or you; -SEX: the sex of the speaker, female versus male; -SENTENCE: the square root of the ID number (from 1 to n) of the sentence in the files in which a speaker used I or you (the square root transformation was used to make the distribution of SENTENCE more normal);

-DISTANCE: the natural log of the number of characters in the sentence before the I or you in question (after tags etc. had been removed; the log transformation was used to make the distribution of DISTANCE more normal). This is a data set that requires a multifactorial method of analysis such as a binary logistic regression. Let us assume that one decided to begin with a first maximal model that tries to predict MATCH, that is, the choice of I and you on the basis of all fixed-effects predictors-SEX, SENTENCE, and DISTANCE-as well as their pairwise interactions, and that one used a backwards model selection process in which the least significant predictor is deleted till only significant predictors are left. It turns out that this model selection process involves the elimination of the interactions SENTENCE:DISTANCE (p = 0.058) and SEX:DISTANCE (p = 0.05) and results in a highly significant model (L.R chi-squared 881.9; df = 6, p < 0.0001); the coefficients of this model are listed in Table

Note that, while the regression model is highly significant, its predictive power is extremely weak: R 2 = 0.055, C = 0.613, and the classification accuracy is a mere 58.3%, which is not significantly better than chance. The nature of the effects is somewhat clear from Table

When one then does an analogous model selection process by eliminating non-significant fixed effects, once the same interactions are deleted as before-with very different p-values, though: SENTENCE:DISTANCE (p = 0.216) and SEX:DISTANCE (p = 0.224)-and one arrives at a final model with the coefficients represented in Table

What about the classificatory power of this model? While it is still not as good as one would theoretically want it to be, it is much higher than the previous one: marginal R 2 = 0.044 and conditional R 2 = 0.24, C = 0.717, and the classification accuracy is now at 65.7%, which is now highly significantly better than chance. 2 Before we compare the two models, let us again first look at the visualization of the significant highest-order effects, which are shown in Figure

As for the commonalities: both models contain the same fixed effects and in both models the effect of DISTANCE is probably the same. However, there are also many (more) marked differences. The most obvious was already mentioned: the GLMEM achieves a much higher and highly significant classification accuracy. Then, the GLMEM can see that, once file and speaker information is included, SENTENCE is not significant, whereas it is significant in the GLM. Most important, however, are the differences for the crucial interaction most of interest, SEX:SENTENCE. First, the GLM assigns to this interaction a p-value that is 24 orders of magnitude smaller (i.e. more significant) than the GLMEM. Second and more interestingly, the above two models were fitted with user-defined orthogonal contrasts-something else that happens way too rarely in corpus linguistics-to see easily (i) whether the speakers of an unknown sex are different from those where the sex is known, and (ii) whether female and male speakers behave differently. Since the GLM does not take the relatedness of the data points of each speaker into account, it returns results that are quite different from the more precise GLMEM:

-With regard to the contrast of female versus male, the GLM returns a highly significant coefficient that is ≈2 times as high as the non-significant coefficient for female versus male from the GLMEM. In other words, the GLM strongly overestimates this contrast, much of which is in fact due to speaker-specific behaviors. -With regard to the contrast of female versus male, the GLM returns a highly significant coefficient for female versus male that is >2 times as high as the highly significant coefficient for female versus male from the GLMER. Again, while the contrast is significant in both models, the GLM strongly overestimates its strength.

Space does not permit a more detailed discussion of these data or of the specifics of mixedeffects and multi-level modeling here (see Gries forthcoming for some more details in a corpuslinguistic context). It should have become clear, however, that much of what happens in corpus data is a result of word-/speaker-/file-/register-specific random effects rather than of the fixed effects we as corpus linguists are usually interested in. GLMs or any other statistical tool that does not take the relatedness of data points into consideration run the risk of severely overestimating the size and significance of effects. But to make matters worse, it is just as possible that GLMs underestimate the size and significance of effects-the problem is there is no way of knowing the direction of error of GLMs ahead of time. It is therefore imperative that corpus linguists follow the lead of recent developments in psycholinguistics and make mixed-effects/multi-level modeling a central analytical tool: without it, we will never know how much of an effect is interesting, and how much is just due to particular speakers sampled in a corpus.

Interim summary

Given the distributional hypothesis discussed above, the quantitative exploration of cooccurrence data is the most fundamental methodological tool in corpus linguistics and the last few decades have produced a plethora of papers and findings that are based on co-occurrence frequencies, co-occurrence probabilities, association measures, and other statistical approaches (most often regression-analytic methods). While much of that work has, of course, been successful because, for example, high token frequencies in b and c are positively correlated with high type frequencies, and high token frequencies in a are negatively correlated with clumpy distributions, it is unclear how potentially skewed the results are for cases where those correlations do not hold. A study that tries to identify multi-word units while at the same time trying to address all these AM issues mentioned above is Wahl (in progress).

In addition, ignoring the repeated-measurements nature as well as the hierarchical structure of the corpus data not only violates the fundamental assumptions of most statistical methods-the independence of data points-but also distorts our results in unpredictable ways. Thus, most of the approaches above are relatively easy ways in which we can try to make our co-occurrence-based studies more robust; there is no reason not to pursue those strategies if corpus linguistics as a whole wants to evolve in tandem with what happens in other disciplines.

More specialized applications

The three problems discussed above have implications for most corpus-linguistic studies: the issue of underdispersion, or clumpiness in distribution, is a threat to any statistic based on frequency data-because they all involve frequencies of occurrence and of co-occurrence. Likewise, the lack of bidirectionality and of type frequencies and their distributions in the computation of AMs is a threat to virtually all studies based on co-occurrence data. However, at this point in time, quantitative corpus linguistics is becoming more and more established also in specific linguistic subdisciplines, which raise their own, more specialized problems. In this section, I shall discuss one example each from two areas in which corpus research is currently booming. In §3.1, I shall discuss the issue of studying temporally-ordered corpus data in a way that is both bottom-up/exploratory and principled/objective; in §3.2, I shall turn to the field of learner corpus research and the question of how to make the best use of what native and non-native learner corpora have to offer.

Temporally-ordered data and the problem of identifying stages

Temporally-ordered corpus data play an important role in two different areas in linguistics. On the one hand, there is the area of first language acquisition. In that area, corpus data are both longitudinal and cross-sectional and in order: (i) to discern longitudinal trends in the data for one or more children, (ii) to identify children at comparable levels of development for cross-sectional analysis, or (iii) to increase sample sizes and/or filter out outliers, it is often useful to be able to group the temporal data for children into different stages.

On the other hand, there is the area of diachronic historical corpus linguistics, in which corpus data are-given the relevant time spans-usually cross-sectional, covering, for instance, several centuries of the history of a language. Given that historical data are not collected in the carefully controlled ways in which psycholinguists (try to) collect language acquisition corpus data, such historical data are often quite heterogeneous so that here, too, it is useful to be able to group temporal data and at the same time clean the data of outliers in a principled fashion. Figure

The fact that there are overall increasing trends can be easily tested with correlation coefficients such as Kendall's τ or others. However, not only can such data violate the assumptions of frequently used statistical tests such as linear regression, but many frequently used statistics also provide too little information about the data. In particular, such statistics do not necessarily answer questions such as: (i) Are there different stages in the data, and if so, how many?; (ii) Do these different stages exhibit kinds of trends?

A frequent exploratory method to answer the first question, namely to discern sub-structure(s) in corpus data, is hierarchical cluster analysis, a statistical tool that groups data points into clusters on the basis of the points' pairwise similarity (such as the differences between MLU values or differences between percentages of (e)s). However, such cluster analyses cannot straightforwardly be applied to such temporally-ordered data: The computation of the similarity matrix of, say, the percentages of (e)s will return extremely high similarities for data points 150 years or more apart. However, a cluster analysis should not group such distant data points together given that, in historical data, grouping data points that might be 150 or more years apart makes little sense linguistically just as, in language acquisition data, grouping data points that might be 2 or more years apart makes little sense cognitively. Thus, what is required is a modification of the cluster-analytic approach that makes it operate locally, rather than allow it to merge data points that are too far apart.

One such approach is variability-based neighbor clustering (VNC; see

Consider Figure

For example, Figure

-Zero becomes less frequent over time; -P becomes more frequent over time; -N and DP do not change much/markedly. In all of the above, VNC was used on data in which the measured data could be univariate (just one frequency as in the case of just because) or multivariate (several frequencies (of grammatical patterns) as in the language acquisition data), but where the dimension along which the clustering happened and along which VNC restricted it to neighboring elements was onedimensional: time. Another interesting extension is using VNC for the analysis of data where there is more than one dimension, as when one studies geographical data in a quantitative dialectology setting and wants to prevent a regular hierarchical cluster algorithm from merging geographically very distant regions. The VNC algorithm can be adjusted correspondingly. Figure

Figure

-In the left panel, some first smaller clusters have emerged mainly in the south (one in the Cornwall and Devon regions and one in the Kent, East Suffolk, and London regions) as well as one small one involving Dumfriesshire and a larger one around Manchester. -In the center panel most of the south is now interconnected (although Cornwall/Devon remain separate from the rest); not much has changed in the middle area. -In the right panel, most of the country is now inter-connected apart from the very northaround Banffshire, Sutherland, Ross, and the Hebrides. Thus, VNC can contribute to the (methodologically already quite sophisticated) domain of quantitative dialectology by helping to identify structures in corpus-linguistically described regions of a country or other larger regions that can then be interpreted against the background of other empirical or theoretical work. Given the increasing availability of historical corpora and regionally-stratified corpora, this method may therefore be a useful addition to the corpus-linguistic toolkit.

Learner corpus research and the problem of missing/impoverished context

The final corpus-linguistic domain to be discussed here is learner corpus research, that is, the branch of corpus linguistics exploring corpora containing non-native speaker (NNS) speech and/or writing. This field has become increasingly vibrant over the last 15 years or so, given the increasing availability of learner corpora. Much of this work is contrastive in the sense that NNS language is compared to the target of the learner as well as his L1(s), and an increasing amount of work approaches learner corpus data from a cognitively-informed perspective. Unfortunately, many studies in this field are quantitatively quite simplistic and restricted to the description of overand underuses of linguistic elements in NNS language, accompanied by univariate or bivariate chi-squared tests. Examples include: -

Typically, such quantitative analyses are lacking not only because of all the issues raised above, but also because they are not 'comparing/contrasting what non-native and native speakers of a language do in a comparable situation' (Péry-Woodley 1990:143, quoted from Granger 1996:43, our emphasis). This is because many studies reduce the notion of comparable situation to a single co-occurring factor/predictor, such as when

Thus, if the goal of learner corpus research is to determine how native speaker (NS) language and NNS language differ, a more comprehensive definition of comparable situation is needed, which will typically require the annotation of multiple features of the instances of the word/pattern in question. This in turn means that all these multiple features have to be included in the statistical analysis so as to determine which of these features has what kind of effect in the company of all other characteristics. Two main possibilities to do all this are available: both require corpus data on the element E under consideration that come from both NS and NNS data and that have been annotated with regard, ideally, to all the features that one has reason to believe affect the choice of E. Then, first, one can fit a regression in which:

-The dependent variable is either a binary or polytomous choice (for a binary or multinomial logistic regression) or a frequency (for a Poisson regression); for the choice of of-versus s-genitives, this would be the binary variable GENITIVE: of versus s. -The predictors are all the annotated features as well as their statistical interactions (usually only up to the second or third degree); for the choice of of-and s-genitives, these may include the animacy of the possessor and the possessed, the length of the possessor and the possessed, the givenness of the possessor and the possessed, and many more; ideally, this would be a mixed-effects/multi-level model with random effects as required by the data/ question(s). -All the predictors from the previous bullet point are also allowed to interact with a predictor called CORPUS or L1.

What is the rationale for the latter two guidelines? The rationale for the second guideline is that if one does not include the interaction, say, ANIMACYPOSSESSOR:ANIMACYPOSSESSED, then one has no way of finding out whether the preference of animate possessors for s-genitives holds regardless of whether the possessed is concrete or not. The rationale for the third guideline is that if one does not include the interaction, say, ANIMACYPOSSESSOR:L1, then one has no way of finding out whether the preference of animate possessors for s-genitives holds in both NS and one or more NNS groups to the same degree (given the presence of all other (significant) predictors), which is precisely the kind of question that much learner corpus research is interested in but can often not answer because too few relevant predictors have been included (see

There is a second approach (called MuPDAR, for Multifactorial Prediction and Deviation Analysis with Regressions) that is even more promising. It involves the following steps:

(i) Fit a first regression R 1 that conforms to the first two bullet points above, but only to the NS data. (ii) If and only if R 1 results in a good fit and classification accuracy, then apply the regression equation thus obtained from R 1 to the NNS data to obtain for every NNS data point a prediction of what a NS would have done in the very same situation, which will serve as the gold standard. (iii) If and only if R 1 's NS regression equation also results in a relatively good fit with the NNS data, fit a second regression R 2 in which the dependent variable now is either a binary variable specifying whether the NNS made the same choice as a NS (yes versus no) would have made, or a continuous variable quantifying how much of the NNS choice was compared to what an NS was expected to say/write (this variable is 0 if the NNS made the NS choice, and a number other than zero, but between -1 and +1 if not).

It is this regression approach that precisely answers the core question of learner corpus research-in this linguistically and maybe contextually complex situation where the NNS had to make a choice, did he make a nativelike choice, 'Yes or no?'. And it is this regression approach that requires and at the same time guarantees a comprehensive definition of comparable situationa hopefully large number of annotated factors describing the situation in which the NNS had to make a choice.

Gries & Adelman (

(i) Fit a first mixed-effects regression R 1 that models whether Japanese NS realize a subject in a sentence on the basis of whether the referent of the subject is contrastive (a variable called CONTRAST) and how given it is (a variable called GIVENNESS). (ii) Apply the regression equation thus obtained from R 1 to the non-native speakers of Japanese corpus data to obtain for every NNS data point a prediction of whether an NS would have realized the subject there, yes or no. (iii) Fit a second mixed-effects regression R 2 in which the dependent variable is a binary variable specifying whether the NNS made the same choice as an NS (yes versus no).

Using a polynomial to the second degree to model the predictor GIVENNESS, they find that the NNS are on the whole quite close to the NS behavior, but (i) different speakers exhibit quite different degrees of proficiency, and (ii) all NNS struggle most with making nativelike choices with intermediate degrees of givenness and non-contrastive referents:

-When the referent is contrastive, they realize it in the subject position as NS would.

-When the referent is non-contrastive and highly given or completely new, they do not realize it in the subject position or realize it in the subject positions as NS would. -When the referent is non-contrastive and somewhat given, then faced with this middleground degree of givenness, their degree of nativelikeness decreases.

This approach, too, needs to be refined and developed further, however. It goes without saying that it is cognitively and contextually much more realistic and statistically more appropriate than decontextualized frequencies and/or chi-squared tests. So, again, it remains to be hoped that analytical strategies like this one will gain more ground in learner corpus research, the research on varieties, and any other domain where one part of the corpus data can be considered a standard or target with which the others can be meaningfully compared.

Concluding remarks

By way of a brief conclusion, corpus linguistics has made enormous headway in the recent past. To grow from a not particularly widely used method, geographically somewhat restricted to several Northern and Central European countries, to one of the most widely applied methods in linguistics of all sorts of theoretical persuasions worldwide in 15 to 20 years is no small feat. However, this is no time to rest on our laurels-now that corpus linguistics has become mainstream, and that's a good thing, we too must continue to refine our methods just as other fields have to. Many areas in psycholinguistics and computational linguistics have made interesting discoveries, have developed useful tools, have adopted great methods from neighboring fields, but corpus linguistics is unfortunately not leading the pack and must take care not to lose momentum either in terms of its own evolution or in terms of how it helps to shape linguistics as a whole. The present paper is an attempt to provide a snapshot of current problems, both in corpus linguistics in general and in selected hot topic areas, as well as to provide ideas and (first) suggestions about how to cope with these problems; I hope it will succeed as a call to (methodological) arms, and thus trigger developments that will help our field advance once more.
Bnqotr khmfthrshbr

?mcqdv GYqchd

21-0 Bnqotr khmfthrshbr9 'm nudquhdv

What is a corpus, and what is corpus linguistics? Answering either of these questions involves answering the other, as the notion of a 'corpus' in contemporary linguistics is inextricably tied up with the ideas and techniques of corpus linguistics. The term corpus (plural corpora) is simply the normal word in Latin for 'body', as in a body of text. Over time, the term came to be used for any natural-language dataset used by a linguist -so that, for example, a field linguist might refer to a set of sentences elicited from an informant as their 'corpus'. Since the 1960s, a yet more specific meaning for the term has emerged: corpora in this sense are very large, machine-readable collections of spoken and written text that are analysed using computational methods. The set of methods required to approach the quantitative and qualitative analysis of a collection of language data on a scale far larger than any human being could hope to analyse by hand -together with related areas such as the compilation and annotation of these corpora -constitute the modern field of corpus linguistics.

Describing corpus linguistics as a 'field' or subdiscipline of linguistics is a statement which requires qualification, however. Most subfields of linguistics relate to the study of some aspect of the language system or its usage, such as phonology, morphology or sociolinguistics. Corpus linguistics, by contrast, is not concerned uniquely with any single facet of language, but rather is an approach which can be applied to many or all aspects of language. For this reason, many corpus linguists prefer to describe it as a 'methodology'. As a methodology it has certainly been a great success in fields ranging from the history of English to lexicography to language teaching to discourse analysis. But for others in the field, the insights into language that arise from the use of the corpus are so radically different from the traditional understanding of how language works that they consider corpus linguistics to constitute an independent field of study or theory of language. This latter view is associated with the neo-Firthian school of researchers led by the late John

Corpus-based methodologies remained something of a niche approach through the 1960s and 1970s, partly because of the influence of the Chomskyan arguments against the use of corpus data, but perhaps more importantly because few linguists had access to the computer technology without which corpus linguistics is effectively impractical. Through the 1980s, and especially in the 1990s, all this changed. Advances in information technology put computer power sufficient for the analysis of larger and larger bodies of data within the reach of any researcher who cares to use a corpus. As a result, in the twenty-first century, corpus linguistics has become -in the words of Tony McEnery -a 'killer method' in linguistics, applied to a hugely diverse array of types of linguistic research. This is, in short, why corpus linguistics matters. It is not merely an improved method within lexicography, or a method for researching grammar, or an approach to political or media discourse. It is all of the above and more.

There is no room in this chapter to attempt a comprehensive review of the impact that corpus research has had across the field of linguistics. Instead, I will provide an overview of the fundamental ideas of corpus linguistics and some key methods and practices -starting with how we approach corpus design and construction, moving on to a summary of the most widely used corpus methods, and finishing with a very brief survey of some applications and advanced forms of analysis.

21-1 Bnqotr cdrhfm 'mc bnmrsqtbshnm

21-1-0 FdmdqYk bnmrhcdqYshnmr

More linguists work with corpora than actually collect corpus data or work on corpus design. A very large set of questions in corpus linguistics can be answered without going beyond standard, widely available corpora. A good example of such a corpus is the British National Corpus (BNC). The BNC is a 100 million word collection of British English, compiled in the early 1990s, and including samples of a wide range of spoken and written genres. As a broad sample of the English language in general, it is suited to many different research aims. In other cases, a more specialised collection of data may be required. A wide range of such specialised corpora have been constructed and likewise made generally available, covering historical and dialectal data, specific genres, or the spoken language of specific types of people.

But there are other types of research question for which no standard corpus is available. In this case, the first step for the researcher is to build the corpus on which the analysis will be based. For instance, many corpus-based analyses of media discourse are based on examining media texts that address some particular topic. It is unlikely that such a highly specialised dataset would be available in advance. If I am interested in how local newspapers in Britain discussed the issue of crime in the years 2010 to 2013, I will almost certainly need to build a corpus myself for this specific purpose. Such goal-oriented corpora are informally known as 'do-it-yourself' or DIY corpora and very often are not made available to researchers other than the original compiler (often, for copyright reasons, they cannot be).

The virtue of working with a DIY corpus is that the design of the data can be tailored directly to the specific research question at hand. The virtue of working with a standard published corpus is that the corpus serves as a common basis between different researchers -allowing results to be tested and replicated by other scholars, for instance. Both approaches have their place in different kinds of corpus-based study.

However, corpus researchers need an understanding of the issues that arise in corpus construction even if they never build any corpora of their own. This is because we have to understand the contents, design and structure of the corpora that we use, in order to select a corpus appropriate to what we want to research, in order to make appropriate use of that corpus, and in order to treat our results critically in terms of how far they may be extrapolated beyond the specific corpus at hand. The key concepts here are the notions of the corpus as a sample and of balance and representativeness.

21-1-1 Sgd bnqotr Yr Y rYlokd

Any corpus is fundamentally a sample. That is, there is some large phenomenon that we want to know about -a language, or some specified variety of language, as a whole -and since we cannot look at all the possible text within that language, we must select a sample. Borrowing terms from statistics, we can talk about the whole of the language variety we are approaching as the population, and the corpus we are using to look at that language variety as the sample. Our interest is typically on what we can say about the population on the basis of the sample -and how confidently we can say it. The very fact that the corpus is a sample means that we need to think carefully about how our sample relates to the population, which means we need a detailed and critical awareness of the process of sampling that has been used in corpus construction.

It is rarely possible to avoid sampling. As Chomsky pointed out, language is in principle non-finite (it is always possible for a speaker of a language to create a new sentence in that language that has never been used before) and it is -of course -impossible to collect a dataset of infinite size! Only in the case of certain historical data, where the amount of text that has survived from a particular period is finite, can we have a complete corpus of some language or language variety. For example, it would be possible to compile a complete corpus of Old English, because only a limited number of documents in Old English have survived. But even in this case, we are effectively letting the forces of history -the preservation of some documents, and the loss of others -do the sampling for us.

An important difference between sampling in corpus linguistics and in other sciences is that a corpus is never a random sample of the population. In research such as opinion polling or medical trials, where the population is a literal population of people, the sample must be as close as possible to a randomly-selected subset of the population, such that every person has an equal likelihood of being selected to be in the sample. Corpus collection is very different. It is possible to select the texts of a corpus randomly from a population of texts of interest. But we rarely use the whole text as the unit of analysis. Most of the analyses we will want to undertake with our corpus will be centred on a much smaller linguistic unit, such as the sentence, the clause or, perhaps most commonly, the word. However, even if the texts of the corpus have been selected randomly, the sentences and words are not random. For instance, consider the word elephant. It is not especially common. Many texts contain no examples at all of elephant. However, a text that does use the word elephant once is then rather likely to go on and use it several more times (for example, if it is a text about elephants). This means that the probability of a given word occurring at some point in a text is influenced by the words that have occurred before and thus the words in a corpus are not random. Similar logic can be applied to sentences.

What does this mean for corpus analysis? Three things. First, we have to be very careful in applying statistics to corpus data that are unproblematic in other fields, precisely because of this issue. Second, when we analyse a corpus we must always be aware of dispersionlooking out for the difference, for instance, between a word that occurs 100 times in our corpus where all 100 instances are bunched close together in one text, and a word with the same 100 occurrences that are spread out across the whole corpus. Third, in the process of corpus construction we have to think about the relationship between the population and the sample in a somewhat different way to how this is done in fields where sampling can be done on a random basis. The concepts that we normally apply for this purpose are representativeness and balance.

21-1-2 PdoqdrdmsYshudmdrr Ymc aYkYmbd

If the words and sentences of a corpus are not a random sample, then how can we have any confidence that findings we arrive at using a corpus are applicable to the language or variety as whole? We must consider the issue of how representative the corpus is of that language or variety, based on the decisions that were made about what to include and what to exclude in that corpus. As

Balance is a related, and equally problematic, issue. It relates to the relative proportions of different types of data within a corpus. A corpus of general English, for instance, must sample both spoken and written data (of various kinds) in order to be representative; however, what should the relative proportions of speech versus writing be? A corpus is considered balanced if its subsections are correctly sized relative to one another. However, there is no single answer to the question of what the correct proportions are. For instance, in the case of speech versus writing, we might propose a corpus that is 50 per cent spoken and 50 per cent written data (this is, in fact, the design of the International Corpus of English, ICE). However, this assumes that speech and writing are equally important. Is that the case? How do we even determine the parameters of what is 'important' in this context? Most people probably hear more language than they read; does this mean the spoken language should make up more than half the corpus? On the other hand, speech is generally much more ephemeral than writing; does that mean written texts should dominate? As with representativeness, the issues around balance are fraught with difficultly, but the critical thing for the user of any corpus is to be actively aware of how that corpus is balanced, to think carefully about what this means for the generalisability of results based on that corpus.

21-1-3 LdsYcYsY) lYqjto Ymc YmmnsYshnm

A corpus always contains the actual words of one or more texts that have been collected as discussed above. These will typically be collected in the form of plain text files -that is, computer files containing just the actual characters of the text, without any kind of formatting, as found in word-processor files, for instance. The plain-text words of the corpus are sometimes called the raw data of the corpus. However, raw data is not necessarily all that the corpus contains. There are three types of additional information that can be added to a corpus, namely metadata, markup and annotation.

The definitions of (especially) the terms markup and annotation in the context of corpus design and encoding vary quite a lot in the literature; the term tagging is also sometimes used as a synonym for either or both of these terms. However, for present purposes we will treat them as distinct concepts. Markup is information added to a corpus to represent features of the original texts other than the running words themselves -for instance, in the case of a written text, features such as the beginning and end points of sentences or paragraphs, or the position of page breaks, or the position and content of elements such as illustrations which would be omitted in a plain text corpus. Annotation, by contrast, encodes into the corpus the results of some linguistic analysis which we have undertaken on the corpus text.

The third thing which can be added to the raw text of a corpus is metadata. Metadata is data about data, that is, information about the texts that have been included in the corpussuch as whether they are written or spoken, information about genre or domain, bibliographic information on original publication of a written text, or demographic information about the speakers in a spoken text. There are several ways to store metadata for a corpus, but one popular way is to insert a chunk of descriptive information at the beginning of each corpus text in a fixed, specified format: this metadata chunk is called the header of the text.

Critically, markup, metadata and annotation can be processed by computer, just like the actual words of the corpus texts, and therefore they can be exploited in automated corpus analysis. We will return to annotation in the next section; let us deal briefly here with metadata and markup by discussing some examples of how they may be used to enhance a linguistic analysis -and, thus, the reason corpus builders invest effort into adding them into the corpus in the first place! Metadata has two main functions. First, it allows us to trace the corpus evidence we see back to its source. If we are examining a series of examples extracted by searching a corpus, for instance, it is often very useful to be able to see, via the metadata, precisely what text each example comes from: knowing, for example, whether a particular instance comes from a novel or from a newspaper report can be a valuable aid to interpretation. The second function is that metadata allows us to isolate and compare different sections of a corpus. If, for instance, we have a corpus containing both writing and speech, we may on occasion wish to search just within the spoken texts, or to compare the results of some analysis in the written data versus the spoken data.

Markup has similar applications in that it allows analyses to take account of the structure of the corpus texts. For instance, in a spoken corpus, who said what is often critical to an analysis: only the markup of utterance boundaries and speaker identities allows us to know this. But the same can apply to written data -we might be interested in differences in how a word is used paragraph-initially versus medially or finally, for instance, and this can only be automated if the corpus has paragraph breaks marked up.

21-1-4 Bnqotr YmmnsYshnm

To annotate a corpus is to insert codes into the running text to represent a linguistic analysis. In many cases, the analysis is done at the word level, so a single analytic label or tag is assigned to each word of the corpus. However, analyses at higher or lower levels of linguistic structure can also be represented as corpus annotation. Much research has been done to automate the process of adding the most common forms of analysis, so that it is not necessary for a human being to read through the whole corpus and manually insert the tags. Let us consider four of the most widely used kinds of corpus annotation -those often pre-encoded in general-purpose corpora prior to their being distributed.

Part-of-speech (POS) tagging is the assignment to each word of a single label indicating that word's grammatical category membership. It is the longest-established form of corpus annotation, with the first efforts in the direction of automatic taggers going back as far as the early 1960s, and the first practical, high-accuracy tagging software emerging in the 1980s -although even the best POS taggers have a residual error rate of around 3-5 per cent which can only be removed by painstaking manual post-editing. The utility of POS tagging, especially for languages like English, is that many words are ambiguous in terms of their part-of-speech. For instance, walk can be both a noun and a verb, and the only way to know which is to look at it in context. Having the computer evaluate the contexts for us and on that basis disambiguate each instance of the word allows us to treat walk the noun and walk the verb separately, according to what makes most sense for a given research question.

Different POS tagger systems use different tagsets, that is, systems of grammatical (specifically, morphosyntactic) categories. Some simple tagsets only distinguish major word classes such as noun, verb, adjective, preposition and determiner. However, there are many more morphosyntactic distinctions that can be useful to linguistic analysis, and more fine-grained tagsets will try to encode these distinctions. For example, it is common for POS tagsets to distinguish common nouns from proper nouns, or lexical verbs from auxiliary verbs. Inflectional distinctions will also usually be represented: singular versus plural, or past tense versus present tense, for instance.

The underlying principle of semantic tagging is the same as POS tagging, namely, the assignment to every word in a text or corpus of labels representing categories within a scheme of analysis. The difference is that in the case of semantic tagging, the tags represent semantic fields, i.e. categories of meaning -and the tagset as a whole thus represents some ontology, or way of dividing up all possible meanings into various domains or concepts.

Semantic tagging is a harder task for a computer than POS tagging. This is because, while a word can be ambiguous for both grammatical category and semantic field, disambiguating the semantics involves actually understanding the meaning of the context, which computers cannot do. For this reason, we must be prepared to work with a higher error rate when analysing the output of a semantic tagger than when working with POS tags. Nevertheless, semantic tags can be extremely useful, especially for detecting patterns in a corpus that affect groups of semantically-related words which are individually very rare.

A lemma is a group of word-forms that are judged to constitute a single entry in the lexicon of the language -different inflectional forms of a single dictionary headword. Lemmatisation is conceptually the most straightforward of the word-level annotations; every word is, simply, annotated with the lemma to which it belongs (where each lemma is represented by its morphological base form, such as go for go, goes and went). Thus, computer searches of a corpus can be based on grouping together all the separate inflectional forms of a single word. However, even this relatively simple operation can raise issues regarding what is, and what is not, considered to be part of the same lemma. The English word broken, for instance, can be tagged either as a participle or as an adjective. Many lemmatisers will treat it differently depending on how it is tagged. If it is tagged as a participle, it is lemmatised as break -since a participle is considered an inflectional form of a verb base. On the other hand, if it is tagged as an adjective, broken is lemmatised as broken -since the change of category to adjective is deemed to create a new lemma. So the lemmatisation does not always group elements together; in some cases it can draw distinctions between elements that are superficially identical.

The other form of corpus annotation that is often applied automatically is parsing, the annotation of syntactic structures and relations. Unlike the forms of annotation mentioned so far, parsing does not operate strictly at the word level; rather, what is annotated are grammatical phenomena at the phrase, clause and sentence level. As such, parsing represents an implementation within a computerised text of the traditional tree-diagram style of syntactic analysis; for that reason parsed corpora are often called treebanks. There are two major forms of parsing, each corresponding to a different kind of syntactic analysis. Constituency parsing is about identifying the phrases (syntactic constituents) and how they are nested within one another. This is done by introducing codes into the text to represent the beginning and end points of all the phrases. Dependency parsing, by contrast, labels the relationships between words: each word is linked to the grammatical structure by depending on another word in a particular way. For instance, a noun may relate to a verb by being its subject, or its object; an adjective may relate to a noun by being its modifier. Different tags are used to represent different types of dependency relationship.

Like semantic tagging, automated parsing has a much higher error rate than POS tagging, because the task is inherently more difficult. In both cases, the benefits of being able to access the semantic or grammatical analysis make it worth the trouble of working with data that has a known error rate. That said, many of the generally-available treebanks have been parsed manually, or at least have had their automated parsing manually corrected.

There are many other kinds of annotation which a researcher might apply for the specific purposes of their own research questions, such as different kinds of discourse-pragmatic or stylistic annotation. But these are often very different from research project to research project, and therefore must often be applied manually rather than automatically.

When a corpus has been tagged, the fundamental techniques of corpus analysisfrequency counts and concordance searches, which will be introduced at length in the next section -can be undertaken at levels of analysis higher than the orthographic word-form. For instance, lemmatisation allows frequency lists to be compiled at the level of the dictionary headword, which may subsume many distinct word-forms. Conversely, tagging can disambiguate the words: break is both a noun and a verb, and broken is both a participle and an adjective -POS tags allow a computer to distinguish the different grammatical functions.

As I mentioned above, there exists recurring confusion regarding the distinction between annotation and markup. Part of the reason for this is that, as a practical matter, both can be inserted into the corpus using the same encoding system of tags inserted into the running text. The current standard method for adding tags to a corpus is by means of XML (the eXtensible Markup Language). In an XML document, anything inside <angled brackets> is part of a tag; anything outside angled brackets is part of the text itself. The special meaning of the angled brackets < and > means that actual less-than or greater-than signs in the text itself must be represented by the special codes &lt; and &gt;. Different XML tags are used for markup, metadata and annotation. For instance paragraph boundaries might be shown in XML using <p> tags; a <title> tag in the header might contain the original title of a text; and a POS tag on a word could be represented as <w pos='NN1'> (where NN1 is a common tag for a singular common noun).

21-2 ?m'kxshb sdbgmhptdr hm bnqotr khmfthrshbr

21-2-0 Bnqotr YmYkxrhr rnesvYqd

The two most basic forms of data which we can extract from a corpus are the concordance and the frequency list. It is, in theory, possible to generate either of these through hand-andeye analysis of a stack of paper documents. Indeed, in the pre-computer age this was sometimes done; the earliest concordances were compiled manually for the study of the language of the Bible, and in the early twentieth century word frequency lists were often compiled manually to help inform foreign language teaching. However, in practice, all techniques for corpus analysis -these two most basic methods, and all the more complex methods built upon them -are nowadays supported by the use of various pieces of corpus analysis software. Thus, what possibilities we have for the analysis of a corpus is inevitably a function of the affordances of the available software tools.

Nearly all corpus analysis software permits the generation of a concordance -a listing of all the instances of a word or phrase in the corpus, together with some preceding co-text and some following co-text (usually anything from a handful of words to a couple of sentences). As this functionality is so common, the term concordancer is an often-used synonym for 'corpus software tool'. Concordancers vary greatly in terms of what analyses, other than basic concordance searches, they allow. Since the late 1970s and early 1980s, a very wide range of such programs have been created -some very general and some highly specialised. For our present purposes, it suffices to distinguish two broad varieties of concordancer that are used today. Some programs (e.g. WordSmith, AntConc) run from the researcher's desktop computer; this means that the user can analyse any corpus they have available locally using these programs. Other tools operate as client-server systems over the internet: the user accesses a remote computer via a client program, typically a web browser, and uses search software that actually runs on a remote server machine where some set of corpus resources and analysis systems has been made available. Client-server tools currently in wide use include SketchEngine, Wmatrix and CQPweb; some of these allow users to upload their own data to the corpus server, while others restrict users to a static set of available corpora.

In some cases, an analyst may not have much choice in whether to use a desktop concordancer or a client-server system: the choice is determined by what data they wish to use. Many large, standard corpora are only available via the web (such as the Corpus of Contemporary American English (COCA) -see

One other approach exists to corpus software: that is, for the researcher to write ad hoc programs specifically to carry out the particular analyses they wish to undertake. The argument for this practice -which necessarily requires that corpus linguists learn computer programming -is primarily one of flexibility; without the ability to create specialised analysis programs, the researcher is limited to solely those procedures that their concordancer happens to make available (see

21-2-1 Bnqotr eqdptdmbhdr Ymc sgdhq hmsdqoqdsYshnm

One basic output that nearly all concordancers can produce is a frequency list: that is, a listing of all the words in the corpus, together with their frequency. In discussing the corpus frequency of words, it is useful to introduce the distinction between word types and word tokens. A token is a single instance of any word at a particular place in the corpus; a type is an individual word-form, which can occur many times in the corpus: thus one type is usually instantiated by multiple tokens. (The exception, types which occur only once in a corpus, are called hapax legomena -Greek for '(they were) said once'.) So for instance, if we see on a frequency list for an English corpus that the is the most frequent word, occurring 60,000 times, then we could say that the single type the is represented in this corpus by 60,000 tokens. Frequency lists can also be compiled for any annotation that is present in the corpus, and we can talk about types and tokens of annotation tags in the same way.

Any given type will be more frequent in a bigger corpus; for that reason, corpus frequencies are often given in relative terms, as frequencies per thousand words or per million words. Relative frequencies of types are calculated by taking the raw frequency, dividing it by the number of tokens in the corpus overall, and multiplying by the normalisation factor (one thousand, one million or whatever). So, if the occurs 60,000 times in a 1.5 million word corpus, its relative frequency is forty per thousand words, or 40,000 per million words.

Speaking meaningfully about corpus frequencies is not straightforward. Simply stating that the occurs forty times per thousand words in a corpus tells us nothing of linguistic interest, whether about the word the or about the corpus in question. It is only when we compare frequencies that we arrive at interesting linguistic generalisations. We could compare the frequency of the to that of other words on the frequency list, and observe whether it is more or less frequently used than those other words. We could, moreover, compare the frequency lists of two corpora or sub-corpora, to ascertain whether the contents and ordering of their frequency lists are similar, as a means of contrasting the types of language those corpora represent. We would find, for instance, that in a written English corpus the will probably be the most frequent word, but in conversational English the personal pronouns I and you are likely to top the list.

A frequency list is inherently quantitative in nature. To be interpreted meaningfully, then, it must therefore nearly always be combined with some form of qualitative analysis -that is, an analysis that involves the linguist interacting with the actual discourse within the corpus and its structure and/or meaning. This is most straightforwardly accomplished within the context of a concordance analysis.

21-2-2 BnmbnqcYmbd YmYkxrhr

As we saw above, a concordance is the result of a corpus search: all the examples in a corpus matching some specified search pattern, together with some preceding and following context. In raw or unannotated corpora, we can only search for specified words or phrases. However, if tags are present, they can be used instead of or as well as a word pattern -for instance, to search for can as a noun versus can as a verb, or to search for all adverbs in the corpus. Most concordancers allow the use of special characters called wildcards that indicate a position in the pattern where 'any letter' or 'any sequence of letters' may be present, and thus allow searches to be underspecified; for instance, if the * character is a wildcard, then searching for act* will find not just act but also actor, action, active and so on. Some corpus software allows the use of the extremely sophisticated system of wildcards called regular expressions. The use of tags and wildcards together allows wholly abstract grammatical structures to be specified in a search pattern, for example, phrase-level constructions such as the English passive, perfect and progressive.

Whatever form of search is used, the result is the same: a list of examples in context, typically presented so that the word or phrase matching the search term is centred on the screen, and often sorted alphabetically (by the match expression, or by a preceding or following word). How do we handle the analysis of this concordance? There are several different approaches. At one level, analysis can be more or less impressionistic -based on scanning the eye up and down the concordance lines in an attempt to observe features of note that recur in the concordance, or to identify different functions of the word or phrase that was originally searched for. A more careful analysis will often attempt to quantify the number of concordance lines that exemplify a given function or contextual feature, and to make sure that every single concordance line has been inspected individually, to identify exhaustively all possible categories and patterns of usage. The most rigorous form of concordance analysis will systematise the aspects of each example that are considered, building a matrix of different features -grammatical, semantic or pragmatic -and the values these features have for each concordance line. All this can make a full concordance analysis a major undertaking; in compliance with the principle of total accountability, concordances too long to analyse in full should be reduced (or thinned) randomly to a manageable size. It is especially important not to look at just the beginning of a concordance, as these early concordance lines may present examples from the early part of the corpus, which is not necessarily representative of the corpus as a whole.

21-2-3 Jdx hsdlr YmYkxrhr

A key items analysis is based on contrasting frequency lists. As we saw above, frequency lists often become most interesting when we compare lists from different corpora or different texts. However, simply comparing two or more lists by eye is only a very approximate way to do this. A more rigorous way to compare a pair of frequency lists is to use a statistical procedure to identify on a quantitative basis the most important differences between the two lists. Several difference statistics can be used, but the most common is the log-likelihood test of statistical significance, a procedure similar in principle to the better-known chisquared test. A key items analysis is performed as follows: first, frequency lists are created for two texts or corpora -either two corpora which we wish to contrast, or a single text of interest and a generic reference corpus. The frequency list may be of word types, lemmas or any kind of tag -thus, we often talk about keywords and key tags. Second, the frequencies of each item on the two lists are compared by calculating a log-likelihood score from the two frequencies and the total sizes of the two corpora. Third, the list is sorted in descending order of log-likelihood score; those items whose frequencies have the most significant differences between the two corpora under comparison will appear at the top of the list. Key items can be classed as positive (more frequent in the first corpus) or negative (more frequent in the second or reference corpus); both can be of interest, but studies based on keywords tend to focus on the positive items.

The interpretation of a keyword or key tag list, done properly, is an onerous task, as it will typically involve undertaking at least a partial concordance or collocation analysis of a large part of the highest-scoring key items. In fact, for many analysts, a primary virtue of a key items list is that it provides a way in to identifying which words or categories in a corpus will be of interest for more detailed study. Other analytic approaches to the key items list might involve grouping the items according to either an ad hoc or pre-established scheme of classification, to identify patterns or trends of usage across some large fraction of the list.

21-2-4 BnkknbYshnm YmYkxrhr

In the same way that key items analysis is based on, and abstracts away from, two frequency lists, so a collocation analysis is based on, and abstracts away from, a concordance search. Collocation in the broadest sense means simply those aspects of a word's meaning which subsist in its relationship with other words alongside which it tends to occur. The idea that collocation is of high or indeed primary importance to understanding the meaning of a word or other linguistic form originates with J.R. Firth and is very important in the neo-Firthian school of corpus linguistics; but collocation analysis itself is a commonplace of most or all approaches to corpus linguistics. There are many different ways to operationalise the notion of co-occurrence. One is to look at sequences of words which recur in a fixed order with high frequency in the corpus -these sequences are variously referred to as n-grams, clusters or lexical bundles. A more common approach is to begin with a single word (or lemma) of interest, called the node, for which a concordance search has been performed; the surrounding text is then scrutinised to establish what elements occur frequently in the vicinity of the node. Depending on one's methodological preferences and theoretical orientation, there are then a number of ways in which one might proceed. One way is manual and largely qualitative: to undertake a hand-and-eye analysis of the concordance, as explained above, where the focus of analysis is to identify co-occurring elements, the collocates of the node, in the concordance lines. However, given how labour-intensive this approach is, it is very common to automate the process of extracting collocates from the local context around the node. The software compiles a list of words that occur in the context (either all words, or words which stand in some particular relationship to the node) and then applies statistical analysis to identify words which are more common in the vicinity of the node than elsewhere in the corpus. These are then presented as a list of statistical collocates for the researcher to interpret. It should be noted that this is fundamentally rather similar to the procedure of a key items analysis, except that we are comparing the vicinity of the node to the rest of the corpus, rather than comparing two different corpora, and some of the same considerations apply. First, as with key items, a number of different statistical procedures can be used; loglikelihood is one of the more common, but mutual information, z-score, and t-score are also common -among others -and the choice of statistic can result in almost totally different results. Second, as with key items, a rigorous collocation analysis will always go beyond the raw list of statistical collocates to look at actual instances of those collocates in usage alongside the node.

Although we can use collocation as a general covering term for different kinds of cooccurrence phenomena observed in corpora, we can also use a set of more specific terminology, developed largely by John Sinclair and colleagues, to distinguish co-occurrence patterns at different linguistic levels. In this more specific sense, collocation refers to the co-occurrence of particular word-forms with the node, and three other terms are used to refer to grammatical, semantic or discourse-pragmatic or affective co-occurrence pattern: colligation, semantic preference and semantic prosody.

Colligation refers to a recurrent co-occurrence of some node with a particular grammatical category or structure. For example, if we observe that a node word almost always occurs after an adjective -even if no single adjective co-occurs frequently enough with the node to be considered a collocate -we could say that the category of adjective is a colligate of that node. A colligate can also be a syntactic structure that the node tends to occurs within or alongside -such as a sentence position, or a verb complementation pattern. Semantic preference or semantic association refers to a consistent co-occurrence with a set of words which -again, while perhaps not individually significant collocates -are drawn from a recurring semantic field. Finally, semantic prosody or discourse prosody refers to a broad function or meaning which tends to co-occur with the node but which may be variously realised. For instance, the verb cause tends to occur in contexts where the thing that is caused is negatively evaluated -but the linguistic realisation of that negative evaluation can take many different forms both in terms of the words and the grammatical structures used.

21-3 Bnmbktrhnm9 'ookhb'shnmr 'mc eqnmshdqr ne bnqotr khmfthrshbr

In this chapter, we have briefly considered a number of central topics of corpus linguistics. It bears emphasis that any analysis conducted using these techniques will be applied in addressing some specific research question, and the nature of that research question will to some extent dictate the selection of data and method. Unfortunately, there is simply no space here to give a full account of all the areas of linguistics, and of other fields of study, where the methods of corpus linguistics have been productively applied. As previously noted, corpus techniques have acquired the status of a key methodology applicable to nearly all subfields of language study (perhaps the sole exception being those Chomskyan formalist approaches whose opposition to corpus data is a matter of principle). It is not possible to do more here than provide a non-exhaustive list of applications without further commentary. The two initial applications of corpus data were improved grammatical description (see, for instance,

The current frontiers of corpus linguistics are twofold. The first is to extend yet further the range of applications in the field -beyond the concerns of linguistics, to serve the research interests of scholars across the humanities and social sciences. Many such disciplines are focused on one form or another of close analysis of text, for instance history and literary criticism. As more and more texts become available in large electronic archives -especially texts from before the late twentieth century -so there will arise an increasing requirement in humanities and social sciences research for approaches that allow such large amounts of data to be handled. This is exactly what the methodology of corpus linguistics offers to researchers in disciplines beyond linguistics. Not entirely unrelated to this is the second front of current progress in the field -namely, the continuing development of more refined techniques and tools for corpus analysis. In this chapter we introduced four basic analytic methods, but a number of more complex approaches, often based on advanced corpus statistics, have been introduced over the years, such as collocation network analysis, advanced multivariate statistics applied to concordance analysis and corpus frequencies, and so on. Further extension of these methods, and the development of more such complex techniques, is something we can expect to see over the next several years.

Etqsgdq qd'chmf
Introduction

There are two distinguishing features of Corpus Linguistics as a field of research. Firstly, it involves naturally-occurring discourse, and in relatively large quantities. What counts as 'relatively large' depends on the individual study and can be anything from a few hundred thousand words to hundreds of millions of words. Among the assumptions that lie behind Corpus Linguistics, though, is the view that there are aspects of language use that are important but that are invisible to the human reader of texts. In particular, the relative frequency with which words, phrases, and grammatical categories are used is of importance but can be established only with the help of search software. In very basic cases, the language of the corpus is rearranged so that a reader is presented with an altered and focused view. Taking this one or more steps further, quantitative information is given that replaces the human reading process.

Secondly, Corpus Linguistics attempts to make contributions to linguistic theory that are informed by quantitative information. As will be noted below, those contributions may relate to the mechanisms by which language changes over time, or to the nature of the difference between registers, or to the relationship between lexis and grammar. The question of 'what is language like' is one that Corpus Linguistics seeks to answer. In some cases, that answer is very much aligned with other approaches to language; in other cases less so.

Work in Corpus Linguistics has grown exponentially over the last three decades, and the quantitative tools it routinely uses have become more sophisticated. An area of constant exploration is the role that a technical expertise in language structure -lexis, grammar, discourse -plays in relation to expertise in quantification. As the discussion below will indicate there are various types of combination, from 'mainly language with some numbers thrown in' to 'mainly numbers with little regard for language'. This is not simply a matter of finding a balance, but constantly exploring new ways of approaching language so that possibilities of finding new knowledge constantly come into view. Whatever Corpus Linguistics is, it is not static.

In this paper I shall present a view of Corpus Linguistics that conceptualises it in three phases, distinguished by the use made in each phase of quantitative data. I am going to be talking about corpora of English, though much of what I say will be applicable to other languages.

Laying the foundation: quantifying language categories

One of the greatest contributions of corpus linguistics to theoretical linguistics is the opportunity it affords for quantifying the comparative frequency of various linguistic categories that are identified and tagged in corpora. This activity has not been without criticism. Chomsky, for example, has famously offered the view that simple quantity adds nothing to the explanatory function of theory. It is true that establishing comparative frequencies does not change our knowledge of what can be said, but it does alter our understanding of what is typically said, and under what circumstances. It also permits comparison between corpora and as a consequence geographical varieties can be compared, changes in language over time can be observed, and the stages of language development (in children or in learners) can be described.

A key example of this kind of work is the Longman Grammar of Spoken and Written English

An example of comparative work in the same tradition is the paper by

Annotating a corpus for the categories central to SFL (for example, distinguishing process types, or types of Theme) remains a largely manual enterprise, though assisted by mark-up and quantification software such as the UAM Corpus Tool (O'Donnell). Once annotation has been carried out, however, quantification can be carried out. Matthiessen demonstrates that registers are indeed distinguished by the relative frequency of grammatical categories in them, and he also shows the variation of individual texts within the register.

'Quantifying' phraseology: a lexical approach

The second 'phase' I draw attention to adopts a lexical view of language, and is often treated as qualitative rather than quantitative. This approach to Corpus Linguistics is based on the observation of pattern in concordance lines, where a word or short phrase is the node of the line and the few words occurring before and after the phrase are shown for each occurrence. The job of the researcher is to identify patterns of use. The attitude towards quantification is relatively casual and implicit, but is nonetheless of importance. Two examples will be given here, both using the Bank of English corpus (HarperCollins Publishers and the University of Birmingham).

The first example uses the quintessentially British phrase 'cup of tea' (a phrase used as a demonstration by Sinclair and by Danielsson). The Bank of English contains over 2,400 instances of this phrase, of which 100 random examples are selected for this illustration. Preceding 'cup of tea' are a number of different elements:

• Indefinite article: 'a cup of tea' (65 instances) • Indefinite article + adjective: 'a nice/quick/calming cup of tea' (9 instances) • Possessive: 'my/your/Linda's cup of tea' (7 instances) • Other determiners: 'the cup of tea', 'another cup of tea', 'every cup of tea' (6 instances)

The obvious observation here is that 'cup of tea' is used with the indefinite article in nearly 75% of cases. But a further observation that can be made is that 'a cup of tea' and 'my cup of tea' are dissimilar in meaning ('They planned to celebrate with a quiet cup of tea' as opposed to 'She's not my cup of tea'). In other words, 'a cup of tea' is a container with brown liquid in it, whereas 'my cup of tea' refers to one's preference or otherwise for an individual.

To what extent is it true that 'possessive + cup of tea' has this metaphoric use exclusively? To test this, instances of 'my/his/her/their cup of tea' in the BoE are identified (160 in total), and 100 random lines selected. The ones that do not indicate preference are then identified; there are 27 of them. Thus, 73% of instances of 'possessive + cup of tea' have a non-literal interpretation. A further observation is that the non-literal instances tend to include either a negative or a comparator ('not my cup of tea' or 'more/just/entirely my cup of tea'). A further count is then carried out on the 100 lines, identifying those including a negative or comparator and those with literal or non-literal meaning.

Literal

Non-literal Total The usual conclusion from this kind of study is that the non-literal meaning of 'cup of tea' is reliably associated with negative and comparative phraseology, while the literal meaning is rarely used with these words. More significantly, information such as this forms the basis of Sinclair's concept of Idiom

Sinclair argues that much of English operates within this somewhat grey area between the absolute fixed phrase and the entire open choice. It is often exploited in jokes, such 'Chocolate is not my cup of tea'. The second point to make is that the precise numbers involved here are not of particular interest. The difference between the relevant numbers have to be significant, not only in the statistical sense but sufficiently to give confidence that the generalisations drawn from them are accurate.

My second example is a study of verb complementation

that whether decide 7 16

Furthermore, the wordform 'decide' when followed by 'whether' is frequently preceded by indications of obligation, necessity or volition, such as 'will decide', 'has yet to decide', 'was forced to decide' and so on. This was later formalised with a study that looked at 10 verbs, each occurring with both that-clauses and wh-clauses. The conclusion was that non-finite verb-forms co-occur with wh-clauses that construe hypothetical actions whereas finite verbforms co-occur with that-clauses that construe actual situations. In addition, a concept known as 'semantic sequences' was developed -this suggested that concordance lines can be read to identify 'what is often said', thus moving from lexis to grammar to discourse. Further examples are given in

In this section I have drawn on a tradition which, as noted above, is often described as qualitative rather than quantitative. What I have tried to point out, however, is that this qualitative work does rely on measures, sometimes intuitively rather than formally established, of relative and comparative frequency. What is valued in this kind of work is observation or noticing, that is, the identification of classes of object that may not exist as a class outside that context and which have a meaning-or function-related definition rather than a formal one.

Enhancing innovation

All the kinds of quantitative approaches outlined above continue to be used in Corpus Linguistics. If we try to combine the benefits of phases 1 and 2 we might arrive at the following desiderata:

• The statistics should be robust and should be applicable to language data;

• The method of working should rest on as few preconceptions about language as possible, and be as exploratory as possible; • The outcome should offer genuine insight into language and discourse.

I am now going to give three examples of what might be considered to be phase 3 research. I would describe this work as quantity-led. Like the phase 2 work outlined above, there is an attempt to rely on information that emerges from the text ('trust the text', as Sinclair says), rather than on information that is presupposed. Phase 2 work is based on the human observation of the behaviour of large numbers of a single word or phrase ('decide' or 'cup of tea') and builds theory bottom-up from such observations. Concordancing software rearranges the data -the texts -to permit that observation. In phase 3 work, statistical packages take over the role of rearranging the data. So although the approach is quantitative rather than qualitative, and relies on numbers rather than observation, to my mind it has the same bottomup approach that moves from evidence to theory.

One example of this phase comes from many decades ago; the others are more recent.

Example 1: Multi-Dimensional Analysis

and subsequent publications)

The essence of Biber's MDA is the observation that language is different in different contexts. Academic prose is different from newspaper prose; political speeches are different from casual conversation, and so on. The differences are easily recognisable, but rest on a multiplicity of frequency variations. Biber is not the only linguist to observe this (Halliday does this in a more theoretically-informed way, and Matthiessen, as noted, has added statistical rigour to that model), but Biber has carried out more extensive investigations of this type than anyone else. Unlike Halliday, he begins with a 'common sense' notion of register and with an eclectic mix of language features. The mix is intentionally eclectic because unlike Halliday he does not make presuppositions about which features will be significant in distinguishing between registers. The various sub-corpora are then tagged with the language features, and the strength of co-occurrence of those features is calculated. The result is a number of factors, each consisting of a set of features that either attract or repel each other. The factors are then interpreted in terms of what they mean in terms of discourse. 'Informational' versus 'involved' is one factor; 'narrative' versus 'non-narrative' is another. At this point the factors are renamed 'dimensions'. Corpora of texts belonging to two different registers may be alike on one dimension and different on another. Plotting registers as being more or less like each other therefore requires multiple dimensions.

The original five dimensions proposed by Biber have been widely used in subsequent work by him and others, but in other work the dimensions have been worked out anew, permitting the approach to be applied to texts that may not be categorised in traditional register categories. Further refinements to the set of language features have also been made.

A project carried out 2013-15, led by Thompson and advised by Biber ('Interdisciplinary Research Discourse' or IDRD), used the current set of language features from Biber's studies, but attempted two innovations. The first was to generate new dimensions from a corpus of articles from a single academic journal (Global Environmental Change). We identified, for example, 'system-oriented' versus 'action-oriented'; 'explicit argumentation' versus 'implicit argumentation'; greater or lesser degrees of 'spoken-ness'. The second was to avoid a prior division of our corpus into registers. We had deliberately selected an interdisciplinary journal for our project, with the aim of investigating ID discourse. Although we hypothesised in advance that different disciplinary discourse styles would emerge from the journal, we were not able to, and indeed did not want to, divide the articles in the journal between those disciplines in order to arrive at sub-corpora that could then be compared. What we did instead was to assign each article -each text -a value on each of the identified dimensions. We then derived clusters of texts (or 'constellations') that shared values on those dimensions. Depending on how the figures are interpreted -with greater or lesser granularity -we identify 3 or 6 constellations. We are able then to identify the type of research being reported in each, basically on stylistic grounds. Figure

Example 2: Collostructions (Stefanowitsch and Gries, 2003)

My second example uses quantitative information to enhance a linguistic theory. That theory is the theory of Constructions (e.g.

For example, they study the ditransitive construction ('give someone something', 'promise someone something', 'tell someone something'), arguing that the 'agent + recipient + theme'

Stefanowitsch and Gries's work aligns with, though is not derived from, classic phase 2 approaches to language. For example, constructions such as 'accident waiting to happen' look very similar to Sinclair's Units of Meaning. Those such as 'predicative as' or 'causative into' are identifiable as grammar patterns

Where Stefanowitsch and Gries's work is different is that it offers a robust quantitative approach to the question of collostruction, rather than relying on impressions of frequency. The numerical value given to collostructional strength is used to offer an insight into how constructions come to express meaning, as this is said to be derived from the meaning of the collostructions with the highest strength.

Example 3: Co-occurrence measurements as exploratory mechanisms

In this third example I include two research projects carried out recently, both of which involve exploiting quantitative measures to explore corpora. These are 'bottom-up' in the sense that there is no pre-emptive model of language at the outset, but unlike phase 2 studies they have numbers rather than words at their heart. For me, they have the genuine sense of exploring the unknown and of encountering unexpected insights that phase 2 studies also have. The studies are somewhat controversial, in that they treat language as a 'bag of words', that is, without adding linguistic knowledge about structure, meaning and so on. Linguists normally look askance at 'bag of words' studies, but I do think they offer interesting new ways of carrying out 'corpus-driven' research.

In introducing these examples I have used the term 'co-occurrence', meaning that two words frequently co-occur in the same text. Co-occurrence of words within a short span (i.e. 'collocation') is a traditional concept in Corpus Linguistics, as noted at the beginning of this paper. Collocation, as Firth famously almost said, gives us a lot of information about a word: its denotational and connotational meanings, for example. It has been widely accepted with Corpus Linguistics that collocation needs to be measured within a fairly short span: + / -5 words from the node is common. Beyond this, the influence of a given word is negligible. In the studies described in this section, a rather different view of co-occurrence is taken. There is no node word and no directional influence, and the purpose is not to find out more about an individual word. Rather one aim is to gain novel insights into a set of texts by observing the co-occurrence of words within them. The second aim is to gain novel insights into those words by organising them into groups according to the strength of their co-occurrence in given texts.

The first is a project initiated by Neil Millar

My second example under this heading is another study undertaken as part of the IDRD project outlined above and used the same corpus. One of our aims in that project was to find bottom-up and novel ways to explore a corpus whose contents were diverse but not in known ways. To do this we used topic modelling

The output from topic modelling is a set of lists of words that are grouped according to the probability of their co-occurrence within the specified texts. What the lists give us, first of all, is a sense of the 'aboutness' of the corpus. Here are a few of the themes that can be identified (from

• Kinds of natural environment e.g.

There are, of course, other methods of tracking topics through time, or of identifying the focus of a given paper. Simply reading the paper is an obvious method! What is significant about using topic modelling is precisely that it does not rely on pre-existing ideas about what a topic might consist of. It throws open the notion of 'aboutness' and uses a data-driven way of organising the content of a large corpus.

Conclusion

In this paper I have distinguished between a number of approaches to quantification, or phases, in Corpus Linguistics. I have suggested that there tends to be a tension between statistical rigour and a desired objective of using data-driven methods to drive theoretical innovation. However, methods of identifying word co-occurrence provide a way of organising a corpus to lead to new insights.

One of the key questions for Corpus Linguistics is how a corpus might satisfactorily be 'analysed'. In general, the investigation is top-down, in the sense that a question is asked of the corpus and means devised to find the answer to the question. The project relating to 'Rate My Professors', described above, is one such investigation, where the question: 'what categories of individual qualities are discernible from the comments made' is answered using the strength of co-occurrence of adjectives as the research method. A guiding principle in Corpus Linguistics, however, is that one should 'trust the text'
Introduction

This tutorial is a short introduction to what statistics is good for, the basis of statistical thinking, and how some statistical tests can be computed using the program R, cf. R Development Core

2 What is statistics?

Before getting to the heart of the matter, it is perhaps necessary to clear the way by stating what statistics isn't about. First, statistics is not an indicator of how 'true' or 'correct' the obtained results are. Second, statistics is not primarily about mathematical calculations. And third, statistics is not a substitute for informed reasoning.

Rather, statistics is a way of quantifying assumptions, so that they can be applied to large data sets. Thus, statistics is an indicator of how 'correct' your results are, if you have based the calculations on appropriate assumptions and interpreted the results correctly -and this is a big 'if.' This is a matter of careful consideration and experience, not mechanical application of test procedures. Furthermore, the calculation of such tests is now a trivial matter, carried out quickly and accurately with appropriate software. However, the software can never tell you if you have made some erroneous assumption or violated the conditions of a test: the software crunches numbers, and the validity of the results depends on the person who entered those numbers into the program. Finally, statistics can be used in two ways: to describe a data set, or to draw inferences outside of the data set (descriptive and inferential statistics, respectively). The conditions for describing or drawing inferences are obviously not the same, and this means that it is important to define what is being studied, and how the conditions for a given test are met in the data set.

A typology of statistics

Statistics is not one homogenous field, and it is sometimes useful to think of it in terms of three broad paradigms:

i) The frequentist approach: The results of a statistical test are conceived of as part of a very long (but hypothetical) series of repeated experiments or tests.

ii) The Bayesian

iii) The explorative analysis approach: This purely descriptive tradition, as exemplified by for instance correspondence analysis, considers observations as correlations between categories in an n-sized dimension space.

It is probably safe to say that i) has dominated both the practice and teaching of statistics in the 20 th Century for a number of reasons that we will not touch upon here; suffice it to say that this is what is primarily taught in introductory statistics courses in most universities. ii) has recently been getting more and more popular, partially because of the increase in computational power available. However, Bayesian statistics is more properly taught in an intermediate statistics course, see

Why R?

R is a free, open source version of the programming language S-plus, and is becoming the defacto standard statistical toolbox in many academic fields. In addition to being free, R has a number of advantages over commercial statistical packages such as SPSS:

• once you get used to the idea of a command line interface, R is much faster and easier to work with than SPSS.

• R is very flexible and can be used for preparing the data before applying the statistical tests, that is, it is much more than just a statistical software package.

•

• furthermore, R has many linguistics-specific functions contained in packages such as languageR by Harald Baayen; other packages like openNLP are also useful.

• R is more reliable than, say, an online statistics calculator. It is sometimes difficult to check the reliability of such calculators, and there is no way of knowing how long a given web page hosting such a calculator will be available.

• R (like SPSS) produces print quality graphics like figures and charts.

Types of data

The notion of data type is crucial to all branches of statistics. Because all statistical tests make assumptions about types of data (they are quite picky), it is necessary to decide which type the data at hand most closely correspond to, in order to choose the most appropriate test. In corpus linguistics, we are almost always dealing with nominal data.

Nominal data

Most linguistic data are of the nominal kind. As the name implies, it deals with named categories of things, like countries, beer types, or syntactic categories. Such data are unordered, which means that rearranging them does not affect their information value: listing countries by geographical size or alphabetically by name does not affect their properties as data. Nominal data are sometimes referred to as 'count-data,' because no arithmetical operations are allowed on them, they can only be counted (1, 2, 3, . . . , n bottles of beer).

Ordinal data

Ordinal data are ordered categories of things, and classic examples are score lists or race results: The winner was the first across the finishing line, but it is not important by how much he or she beat the competitors; that is, the magnitude of the difference between each category does not affect the information value it has for this kind of data. The important thing is the order the data points

Continuous data

Continuous data are things that can be measured on a continuous scale. This includes anything that can be measured in centimeters, inches, kilos, pounds, years, hours, minutes, seconds etc. The key property which differentiates these kinds of data from the previous ones, is their reducibility. One meter is composed of 100 centimeters, each of those centimeters is 10 millimeters, each of which can be measured in micrometers, nanometers and so on. This means that a number of arithmetic operations can be carried out, such as calculating the mean value. The average height in a population is easy to interpret in relation to the height of each person (i.e., each data point). It is less obvious how to interpret, say, an average number of children (which parts of a whole child are missing in a '0.8 child' ?). In linguistics, continuous data are mostly found in psycholinguistic reaction-time experiments where reaction times to linguistic stimuli are measured in seconds and milliseconds, or in studies where the age of participants is relevant.

A bit of terminology

A 'population' in statistics means a group or collection of entities that we want Population to study. Thus, 'population' could refer to people, but also light bulbs, car accidents, university students, or grammatical constructions. A population is thus not something which occurs naturally -it is defined for the purposes of the research project. A 'sample' is a subset of the population that we want to study. Sometimes Sample the sample is carefully collected based on pre-defined criteria. However, we sometimes have to work with the sample we happen to have available, like in historical linguistics.

A 'random sample' is a sample where every member of the population has Random sample equal probability of being included in the sample. This is not always possible to achieve, but most statistical tests assume that the sample is drawn randomly A 'distribution' is a mathematical function which can in some cases serve as a Distribution model fair (but not necessarily perfect) model of the population we wish to study. One such model is the so-called 'normal' or Gauss-distribution (used with continuous data), with a shape more or less like a bell. There are other such distributions, notably the chi-square (or χ 2 ) distribution used to model the population under study in chi-square tests (nominal data).

A 'null hypothesis', or H 0 , is a term used to denote the default assumption Null hypothesis of most statistical test, namely that all the variation in the sample data is due to random variation. The null hypothesis is then tested against an alternative hypothesis, or H 1 , which is typically states that the variation is not due to chance.

Statistical tests

The sections below present some statistical tests as they are implemented in R. For instructions on how to install and use R in general, see the web page

Pearson's chi-square

Pearson's chi-square (often referred to as simply 'chi-square') is a commonly used test in linguistics, because it can handle almost any kinds of nominal data. However, it still assumes that i) the data are a random sample from the population ii) the chi-square distribution is a fair model of how the phenomenon under study is distributed in the population iii) expected observations in each cell larger than five iv) you have actual observed frequencies -never do a chi-square test on percentages!

The Pearson chi-square can be used in two ways, as a test of independence / correlation or as a test of goodness of fit. The goodness of fit test is used to check whether a set of observations are adequately represented by the chi-square distribution, and it will not be discussed further here. The test for independence is based on the following logic: a) Take two sets or more of some observations in a 2 × 2 or larger table

The aim of this is to test whether the observations in the categories that we have divided the data into (i.e. the rows and columns of the table) represent random variation or whether it is caused by the factors represented by the categories.

The underlying assumption is that if the observations in the table (i.e. our categories) are related only by chance, the observations will match well with the chi-square distribution in figure

Genre NP subject Clausal subject Fiction 45 67 Newspaper 34 82

Pearson's chi-square is computed in R the following way, assuming that x is a 2 × 2 table with nominal data, created like this (the < -sign is R's assignment operator which assigns the material on the right hand side to a short-hand variable):

(1)

x < -matrix(c(45, 34, 67, 82), nrow = 2) R code which produces the following output when x is entered into R:

34 82

The chi-square test is then computed like this:

In some circumstances, R will apply the Yates correction for continuity to the Pearson chi-square test. The issue is somewhat complicated, but there are good reasons not to use the Yates corrected chi-square. In order to tell R not to use it, write:

where the argument correct = FALSE turns off the Yates' correction.

How are the results of a Pearson chi-square test to be interpreted? The Interpretation of Pearson's chi-square

Pearson chi-square p-value indicates the probability of obtaining the entire set of observations in the table, provided that the observations are a random sample from the population, and that the null-hypothesis is appropriate. In other words, the p-value indicates whether the null-hypothesis (the set of observations is a random selection from a single, chi-square distributed population) should be rejected (low p, in linguistics and the social sciences often somewhat arbitrarily set to p < 0.05) or whether we should choose to not reject the null hypothesis (p > 0.05). There is often an implicit alternative hypothesis of the form that the set of observed values come from two (or more) different populations.

In the example above, the result of the uncorrected Pearson chi-square was p = 0.0847. Since this is a number which is larger than the threshold of 0.05, the result would normally be considered an example of random variation and thus not significant. That is, in this case we cannot reliably differentiate between random variation (noise) and interaction effects (information). But note that the obtained p-value is also quite close to the conventional 5% threshold.

As pointed out above, the Pearson chi-square assumes that we have a random sample from the entire population we want to generalize to. But what if this is not the case? In this case, we need to interpret the results with more care, and take into consideration the size of the sample in relation to the entire population as well as the effect size (see below), instead of blindly trusting in the chi-square p-value. Note that the p-value does not say anything about the association between the observed values, it refers to the whole set of observations in relation to a larger population (for between-observation association, see the section on effect size below).

The proper way to report the results of a Pearson chi-square test is to include Reporting results all the following information:

• the chi-square value (reported as 'X-squared' in R),

• the df-value (stands for 'degrees of freedom', this is a complicated concept which falls outside the scope of this tutorial),

• whether Yates' correction for continuity was used,

• the p-value (this should be the value as reported by the test, not e.g. p > / < 0.05)

Fisher's exact test

Fisher's exact test

Traditionally, the Fisher exact test is treated as equivalent with the Pearson chi-square, but used in cases where the Pearson chi-square is considered inappropriate, notably with very small sample sizes (n < 20) or in cases where the expected table cell values are smaller than five.

The test has certain advantages and certain limitations. Among its advantages are that it is less conservative than the Pearson chi-square, that is, it can more easily detect a real relationship in the data. Furthermore, the Fisher exact test p-value can be interpreted as a reasonable measure of the size of the observed effect , i.e., the strength of association between the variables for purposes of comparison, cf. footnote 6 in

Like in the Pearson chi-square, the R-format of the Fisher exact test is:

(4) fx < -fisher.test(x) R code

When the fisher exact test is run with the x table above as its argument, we get quite a lot of information from R:

data: x p-value = 0.09575 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 0.9015428 2.9181355 sample estimates: odds ratio 1.61635

For the present purposes we will ignore most of this information, and simply consider the p-value, which in this case is 0.096. As with the Pearson chisquare, this is normally taken to indicate that the result is not significant given the conventional threshold of 0.05.

The Fisher Exact p-value can be interpreted as the likelihood of obtaining Interpretation of Fisher's exact test the observed table, or a table with 'more extreme' (essentially larger differences) observations. Additionally, the p-value gives a relative effect size adjusted for the observed frequencies in the table. In the context of corpus linguistics, the most obvious role for the Fisher exact test is to measure dependencies between collocations, or in the case of Stefanowitsch and Gries, dependencies between words and constructions. Note that it is not given that the results of a Fisher exact test can be extended beyond the corpus, due to the mathematical assumptions it is based on. Stefanowitsch and Gries do so anyway, but through an explicitly psychological, or psycholinguistic interpretation of their object of study, thus illustrating both the limitations of the test and how to overcome them.

Rank tests

This handout is primarily directed towards corpus linguistics, but as mentioned in section 5 above, we sometimes deal with ordinal data in linguistics, typically in the context of an experimental or sociolinguistic study.

(5) m < -wilcox.test() R code

Mann-Whitney U

Consider the following situation, adapted from the example in

x < -c(43,34,14,62) R code y < -c

The result is p = 0.8714, which would usually be taken to indicate that there is no real difference between the two areas in their judgments -in fact, they are almost identical.

Wilcoxon

Now consider a slightly different scenario, adapted from the example in

x1

The result is p = 0.025, suggesting (again based on the conventional threshold of 0.05) that the subjects have a systematic preference for one construction over the other (i.e., there is a real difference in the subjects' rating of the two constructions). Judging by the differences in rank sums, it seems that the subjects find construction 1 more acceptable than construction 2.

Student's t-test, ANOVA, and parametric rank tests

These tests are so-called parametric tests designed for continuous data, and fall outside the scope of the present tutorial. How and when to use them is taught in all introductory statistics courses, such as the ones listed in section 12. In the context of corpus linguistics, their use is somewhat questionable, and the reader should be aware that it is regrettably not unusual to find these tests employed in ways which do not fit well with their assumptions.

Effect size

What is the importance of effect size, or association strength? Generally, the p-value of a statistical test says nothing about the size of the observed effect in the data, that is, the association between variables in the data. Rather, the p-value tests the hypothesis that the distribution in the data is a random sample from a population which has the properties of some mathematical distribution (e.g. the chi-square). That is, the p-value indicates how likely we would be to observe the data -the full set of data -in this table if we assume that the population follows a chi-square distribution and if the data in our matrix is a random sample from some population.

Whether these assumptions hold or not, is often a question of interpretation. However, the main reason why effect size is important is this:

In corpus linguistics, the chi-square p-value addresses a different question than the one we want to answer ! As

What we need instead is some test or measure which indicates the magnitude of difference when we observe 34 in one cell and 82 in another cell, or which can tell us how much the information in one of the columns contributes to the overall result. Put differently, when when we observe 34 in one table cell and 82 in another table cell, how can we quantify the tendency of the factors involved to go in the same (or opposite) directions? With the possible exception of the Fisher exact test, cf. 7.2 above, the statistical tests we have looked at so far need to be augmented by some kind of effect size measure to give us this kind of information. In this section two such useful measures are introduced, however, there are a lot more such measures around in the social and behavioral sciences, and no 'gold standard' currently exists.

Phi and Cramér V

Phi (or φ) is computed based on the chi-square value. Recall that the chi-square p-value is very sensitive to the sample size (n). Phi and Cramér V 'factor out' the size of the sample, and give the 'average' contribution of rows and columns (that is, the categories in the table and their respective observations in the rows and columns) to the final result. Phi has certain weaknesses when the table gets bigger than 2 × 2, however, and the Cramér V is a generalized version of Phi, cf.

Essentially, Phi is restricted to 2 × 2 tables, whereas Cramér V can be used on larger tables. Note that in the 2 × 2 table case the tests are identical, cf. (

χ 2 is the computed test statistic from the uncorrected Pearson chi-square, n is the total sample size (i.e. the sum off all cells in the matrix), and k is the smaller of either the number of rows or the number of columns. Converted into R code, this can be calculated quite efficiently as follows (assuming the same matrix vector x as above):

It is possible to save some typing by converting this code into a script and loading the script into R. This will not be covered here, though. If you are only working with 2 × 2 tables, Phi is even easier to compute: Phi (8)

(Phi is simpler because the reason for introducing (6) was to test more complex cases, i.e., cases where the table is larger than 2 × 2). Phi can be computed as follows:

Cramér V and Phi

(10)

The result, 0.114 or 11.4 %, indicates a mutual association between rows and columns of approximately eleven and a half percent.

However, other factors should influence the interpretation of the effect size, notably:

• the size of the sample -a small sample is almost always a bad representation of the population. Thus, whether the observed effect can be applied to the entire population needs careful interpretation.

• how much data is missing? If you know that a lot of data is missing, this should influence the interpretation.

• what type of study are you conducting? The interpretation of the Phi/Cramér V should differ in a corpus based syntax study, an experimental situation, or the evaluation of a sociolinguistic survey.

Note that both these measures are symmetric, that is, they give you both the association of rows with columns and columns with rows. Often this is ok, but sometimes we want to measure asymmetric relationships -this is discussed in the section on the Goodman-Kruskal lambda below.

Goodman-Kruskal lambda

Unlike Phi and Cramér V, the Goodman-Kruskal lambda is not on the chi-square statistic. Instead, it is based on the probability of 'guessing' the right result in the table cells if you know something about the categories of the data. See

(11) lx < -matrix

The Goodman-Kruskal lambda (or λ B ) is not implemented as a default test in R, but can be computed as follows:

(12) lb < -(sum(apply(lx, 2, max))-max(rowSums(lx))) R code /(sum(lx)-max(rowSums(lx)))

The R code in (12) above is an implementation of a mathematical formula from

Note that the code above assumes that x again is a matrix of nominal data where rows represent observations and where the columns of the matrix contain the classes, i.e., the independent variable.

Goodman-Kruskal lambda can be interpreted as follows: This test measures Interpretation of lambda how much the potential error of predicting the observed results can be reduced by looking at additional columns (or classes). Put differently, if we are trying to predict the distribution of row observations other than the one with the highest frequency (i.e. the variation), how much would knowing the classes (columns) help us? In the case above, the result is 0.12, or 12 %. That is, in this case information about time period is only moderately helpful in explaining the variation (conversely, if the test is done on the rows instead of the columns, the result is 0.5, indicating that other factors have more explanatory value here). In other words, the Goodman-Kruskal lambda can be used to assess to what extent (measured in percent) each variable in either rows or columns contributes to the effect observed on the other variable. Note that this test is not particularly suited for 2 × 2 tables, or tables where the observations are very evenly distributed.

Effect size measures for ordinal data

There are a number of effect size measures available for ordinal data, examples include Spearman's rho (ρ) and

(13) r < -cor.test(x1, y1, method = 'kendall', alternative = 'two.sided', exact = FALSE) R code

In the code above, method refers to the type of test, alternative = 'two.sided' means that we had no indication before the experiment which construction would be rated highest, and exact = FALSE is necessary because there are ties in the rank sums. The output is as follows:

Kendall's rank correlation tau data: x1 and y1 z = -0.9223, p-value = 0.3564 alternative hypothesis: true tau is not equal to 0 sample estimates: tau -0.2411214

An in-depth discussion of all the output above falls outside the scope of this tutorial, and we will only consider the tau value.

The Kendall tau is always a number between -1 and 1, where -1 indicates Interpretation of Kendall's tau negative association (i.e. disagreement), 1 indicates positive association (i.e. agreement), and 0 indicates no association. Formally, Kendall's tau is the difference between the sum of actual rank scores and potential maximum rank scores, which makes this a good measure of the size of the observed effect. The value obtained above, -0.24 or 24 %, indicates a weak to moderate negative association or difference in acceptability, between the two constructions.

P -values and research questions

It is crucial to keep in mind that the result of a statistical test cannot answer your research questions for you: you need to interpret the statistical results, see

i) Are the research questions well operationalized -i.e., have you spelled out how you think your hypothesis relates to the data in terms of frequencies or magnitudes?

ii) Do you have all the relevant information (i.e., are there other factors that could influence the outcome)?

iii) How well do your data match the assumptions of the statistical test?

Basically, i) is your responsibility -the researcher conducting a study is responsible for clarifying how the empirical and statistical results can be interpreted as having explanatory value with regards to a research question.

ii) is obviously a matter of interpretation -what is 'enough' information about the sample, the population, any missing data etc? As a rule of thumb, the information should be sufficient to let you make good operationalizations.

iii) is a very difficult problem to handle, and very often a statistical test is used in a way which does not match its assumptions well. It is important to keep in mind, however, that even when the assumptions match almost perfectly, you still need to (or ought to) explain your reasons for using a specific statistical test -'everyone else does it' is not an acceptable reason! How badly your data violate the assumptions of a given test and how this will influence your interpretations of the results in relation to the research questions is a factor of uncertainty which must be dealt with in any case.

It might then be tempting to ask what is the point of doing a statistical analysis at all? The answer is simple: there is a world of difference between interpreting the result of a statistical test and interpreting raw frequencies. The human mind is not particularly well equipped to process complex frequency data in a reliable, unbiased way. Consequently, an appropriate statistical testwhatever its shortcomings -is in most cases preferable over raw frequencies as the basis for quantitative, scientific analysis.

10 What is not covered in this tutorial?

As mentioned previously, this tutorial is restricted to a few nonparametric tests and measures within the frequentist tradition. For a particular research project, there might be useful tests and measures to be found among the parametric tests, as well as in the Bayesian and correspondence analysis traditions. In most cases it would be advisable to follow a formal course in statistics such as one of the courses listed in section 12 below. Below are some examples of important concepts that were omitted for reasons of space, but this is in no way an exhaustive list:

There is a lot more to be said about data types than the brief exposition in section 5. For instance, the problem of data source -as opposed to data typehas not been touched upon at all, but is nevertheless important.

Furthermore, the question of sample and population is often quite complex in most real research projects and requires a lot more attention than what was given to it in section 6. Yet another important -but omitted -aspect of statistical testing is one-directional versus two-directional tests.

All of these concepts typically require more attention than they could possibly receive in a short workshop. Again, the best solution would be to follow a regular course where these issues can be treated with the attention they require.

Relevant literature

For a gentle, non-numerical introduction to statistical thinking,

For an in-depth understanding of some of the issues pertaining to the interpretation of statistics, it is necessary to go beyond introductory books. Articles such as

Finally,

12 Statistics courses at the faculty of humanities

• dasp 106 "Statistikk og kognisjonsforskning" -5 credits: A brief introduction to parametric and nonparametric frequentist tests.

• dasp 302 "Statistisk metode" -10 credits: A more in-depth introduction to parametric and nonparametric frequentist tests.

• huin 308 "Statistikk for HF-fag" -15 credits: Identical to dasp 302, but with added coursework on correspondence analysis and Bayesian statistics.

For an updated list of statistics courses offered at the faculty of humanities, see the course listings at
From fallacies and pitfalls to solutions and future directions

Navigating the evolving terrain of corpus linguistics

In a short but important paper published thirty-five years ago in the ICAME Journal,

The insightfulness of Rissanen's article has been generally recognized by corpus linguists (see, e.g.,

As solutions to the issues raised,

In general, critical self-assessment is unquestionably an essential part in the evolution of any new methodological approach into language study, and corpus linguists have consistently recognized the theoretical and practical challenges inherent in the compilation, design, organization, and analysis of data. Over the years, scholars have noted and discussed the occasionally elusive nature of the challenges and suggested solutions to the problems, tackling broader and abstract notions such as representativeness and balance of corpora, but also more concrete questions like sampling, various levels of annotation, precision and recall of corpus queries, dispersion of search hits across a corpus, statistical evaluation, among other points.

The attitudes of corpus linguists seem to include both a healthy sense of humility as regards what is possible to achieve, as well as an understanding of the need to continuously remind the scholarly community of the core concerns in the field. As illustrations of the former idea, we can consider the often-quoted statements that "

Having noted that it is only sensible to be aware of the impossibility to create perfect corpora and analytical methods which are entirely free of pitfalls, it would feel wrong not to take note of the increase of the levels of analytical sophistication and efficiency in the field. The toolbox that a corpus linguist has today is immensely superior to a corresponding one twenty or thirty years ago. However, some problems persist, and it is safe to assume that the number of pitfalls that a corpus linguist needs to try to avoid has not necessarily decreased, quite the contrary. But it is not difficult to imagine that for novice users of corpora, the numerous functions readily available in concordancers and online corpus interfaces have created another fallacy that might be called a "fallacy of sophisticated technology": with such state-of-the-art systems, what could go wrong? In a way this would be related to Rissanen's "God's truth fallacy", but instead of a false sense of security arising from the seemingly representative datasets, the sheer impressiveness of the software and tools of analysis may suggest that pitfalls do not exist. Improvements in some areas do not mean that all of the old problems have been solved.

In many ways we can then say that the challenges do not only relate to corpora and their use themselves, but also to how we educate new corpus users and inform them about the fundamental concepts and concerns. There are undoubtedly many types of persistent problems, common frustrations, and messiness in corpus data that seasoned scholars have encountered and know about, but which are seldom specifically addressed. Yet beginning corpus users might benefit from learning about what may be regarded as tacit knowledge in corpus linguistics, and even the more advanced scholar may encounter issues new to them that have been addressed earlier.

This is also the main motivating factor behind the present volume. Comprising eight chapters, the book discusses the nature of these problems and seek solutions to the perplexities faced by themselves and other scholars. While the chapters mostly concentrate on English corpora, many of the themes are relevant from a general corpus-linguistic point of view. The topics range from addressing issues relating to grammatical annotation

It deserves to be mentioned that Rissanen's concerns on the use of corpora were explicitly presented from the point of view of historical linguistics, and while many of the issues he raises are also relevant to the study of contemporary forms of languages, historical corpus linguistics faces significant challenges of its own. The chapter by Turo Vartiainen and Tanja Säily (Chapter 2) raises a number of practical points which still pose problems in the analysis of historical corpora. The authors examine a set of case studies, highlighting issues like part-of-speech annotation, miscategorization challenges, and problems related to metadata and digitized texts in historical databases, such as the 10.5-billion-word Eighteenth Century Collections Online, a database which was originally not compiled for linguistic purposes. Reporting on a number of instances where initial findings from corpora turned out to be misleading or inconclusive upon closer inspection of the data, the authors advocate for methodological improvements, user feedback channels, and balancing subgenres. They stress the significance of "knowing one's data" for researchers dealing with vast corpora, emphasizing collaboration with historians, and systematic text examination for qualitative analyses and data interpretation.

The impact of insufficient metadata is also the topic of Chapter 3 by Mark Kaunisto, who examines the problems seen in the annotation of named entities in corpora, delving into the problems that arise in interpreting corpus data. Despite the long-recognized importance of considering proper nouns and names in corpus annotation schemes, many contemporary linguistic corpora lack the capability to exclude these items effectively when setting up queries. The presence of names in corpora introduces a layer of complexity, potentially including items that may not reflect the active linguistic choices of the represented writers or speakers. Through small-scale case studies utilizing prominent English language corpora like the British National Corpus and the Corpus of Contemporary American English, the chapter underscores the necessity for careful post-processing of search results. The occurrence of searched items within named entities poses challenges in analyzing word frequencies and collocational behavior. The chapter advocates for more detailed annotation of named entities in large linguistic corpora that are already available. Notable improvements could also be gained by the increased collaboration between traditional corpus linguists and scholars in the fields of computational linguistics and NLP, of which there are already encouraging examples.

Chapter 4 by Marcus Callies discusses challenges related to the special characteristics of learner corpus data, emphasizing the importance of valid data representation for studying L2 production and development. Learner corpora, particularly those containing academic texts, often include instances of multilingual practices, code-switching, and borrowed content, presenting difficulties for annotation and analysis. Specific attention is called for in tagging elements like expert terminology, metalinguistic language use, and quoted passages to ensure the accuracy of word counts and concordance analyses. Compilers and users of learner corpora also grapple with the issue of unwanted lexical bias, stemming from task topics, writing prompts, or other input materials. Identifying and addressing lexical bias is crucial, as it can impact the validity of research findings, especially given the consideration of lexical variation as a proxy for L2 proficiency. Methods to tackle lexical bias involve treating biased words as stopwords or excluding L2 structures likely induced by bias. Additionally, task and prompt materials may influence the recurrent use of specific grammatical constructions, highlighting the need for careful analysis. Callies concludes his chapter by addressing potential bias in annotation methods within Learner Corpus Research (LCR) and related disciplines. Overall, the chapter emphasizes the need to move beyond a monolingual native-speaker norm in assessing learner data, acknowledging the significance of diverse linguistic expressions. Callies also raises an important point about the challenges that the use of AI or other writing tools may pose in the compilation of learner corpora in the future -a point that will also be relevant to the compilation of many different types of corpora -as one needs to make sure that the samples compiled truly reflect the writers' own language choices.

In Chapter 5, the question of the useability of databases for corpus linguistic purposes is addressed by Turo Hiltunen, who discusses issues relating to the use of materials in the British Library Newspapers database. Perspectives on the matter vary between scholarly approaches: the criteria for determining what is "good data" -for example, considerations on the accessibility and processing of the data -can be quite different in Digital Humanities and corpus linguistics. For a corpus linguist, it is important to have detailed knowledge of how the data has been compiled, what editorial or reformatting practices have been applied to the data, and with what tools the data can be examined. Hiltunen highlights ways in which the useability of databases can be assessed, also noting the different conceptualizations of notions such as register in different scholarly disciplines as posing a challenge. As a way forward in trying to find solutions to improve the adaptability of databases for linguistic research, Hiltunen calls for more interdisciplinary discussions and collaboration between corpus linguistics, Digital Humanities, and related disciplines, which are still too far from each other because of the disparities in their research agendas.

The subject of accessibility of data and its repercussions in corpus study is also covered by Stefan Hartmann (Chapter 6), who brings up the problem of replicability of corpus studies, often resulting from the limited accessibility of the corpora studied, as some corpora are only available behind a paywall. Seeing connections between these problems and those outlined by

With the rise of social media and web data, the question of how texts of distinctly different shapes and forms, compiled together into a corpus, can reliably be examined becomes increasingly pertinent. Aatu Liimatta's chapter (Chapter 7) focusses on the challenges posed by variation in text length and the specific issue of short texts, an issue which so far has not received much attention from quantitative corpus linguists. Largely the reason for this is how previously short texts have not been regarded as being a major problem, but with the advent of new forms in contemporary digital communication, the 'problem of text length' , as Liimatta calls it, needs to be addressed. Although solutions have been proposed, they often appear to be limited as regards their suitability in different kinds of studies. Liimatta proposes potential avenues for improvement and new method development, including the exploration of resampling methods for estimating distribution and approaches utilizing the large size of datasets. While perfect solutions may not yet exist, Liimatta offers useful insights to the study of linguistic questions within the context of varying text lengths.

In Chapter 8, Daniel Ocic Ihrmark explores the challenges of categorizing fiction genres in corpus compilation, especially when catering to both linguistic and literary research fields. General-purpose linguistic corpora have traditionally aimed to include works of fiction; however, as Ihrmark notes, the practices of categorizing (and subcategorizing) literary genres in the disciplines tend to differ, which may result in difficulties in trying to make use of corpora in literary studies. The differences between how literary genres are categorized largely reflect the different viewpoints and goals between the fields, with corpus linguistic categorizations aiming to focus on the different communicative purposes of texts, whereas the descriptions of genres by literature scholars are based more on content and stylistic concerns. Ihrmark examines various methods employed in corpus stylistics, such as keyword analysis and n-gram searches, and their reliance on genre categorization for comparative studies. Overall, Ihrmark suggests adopting broader genre categorizations at higher levels for wider applicability, while allowing for additional granularity as needed.

As observed in other chapters in the volume, corpus linguistics stands to benefit from the innovative ideas, methods, and expanded possibilities developed in related fields such as natural language processing and computational linguistics, enriching its analytical toolkit and enhancing its capacity for nuanced linguistic analysis. In the concluding chapter of the volume, Filip Miletić, Anne

Introduction

When Professor Matti Rissanen wrote a short article entitled "Three problems connected with the use of diachronic corpora" for the ICAME Journal in 1989, the landscape of corpus linguistics looked quite different from today. At the time, Rissanen was leading a group of scholars responsible for the compilation of the first diachronic corpus of English, the Helsinki Corpus of English Texts. The Helsinki Corpus, which was to be published in 1991, would revolutionize the historical research of English for decades to come, and its importance for historical corpus linguistics cannot be overstated. Against this background, it is interesting to note that Rissanen felt compelled to discuss some potential pitfalls related to historical corpus linguistics two years prior to the publication of the Helsinki Corpus.

From a present-day perspective, some of the issues raised by Rissanen have been addressed, while others remain a source for concern. Furthermore, there are some new trends in corpus-based research that Rissanen could not have anticipated in his article, and these introduce challenges of their own. In this paper, we revisit Rissanen's ideas in order to identify some of the present-day pitfalls in historical corpus linguistics.

We pay particular attention to what

However, it is not our intention to advise against the use of big-data corpora or sophisticated statistical methods in historical corpus linguistics. Having larger corpora at our disposal is arguably one of the most important advances of the recent decades, and the more statistically-oriented research projects have permitted the analysis of highly complex questions pertaining to language change with increased quantitative rigour

Accordingly, we propose that the changing focus from detailed analyses of small datasets to more abstract and statistically-oriented analyses of big data requires a reconceptualization of the philologist's dilemma. While the original formulation of the philologist's dilemma is no less relevant for scholars engaged in, say, historical sociolinguistics, we argue that in more grammatically oriented studies that make use of big data, increasing attention should be paid to the corpus itself: to its sampling principles and layers of annotation. Indeed, while the handling of linguistic big data always includes a degree of uncertainty, there is no excuse for ignoring the potential problems related to these resources. We acknowledge that some of the issues that we discuss in this paper may not be specific to historical corpora (see, e.g,

After this short introduction, we proceed directly to our examples of presentday pitfalls in historical corpus-based research. Each of the following sections is intended to illustrate one general problem associated with historical corpus linguistics by discussing individual studies that we have carried out previously. In Section 2, the focus is on part-of-speech (POS) annotation, which can be regarded as one of the most useful kinds of linguistic annotation in contemporary research, but which, as we shall see, does not come without problems. Section 3 focuses on some pitfalls related to big corpora, and our examples in this section concern both the reliability of the semi-automated sampling of such resources and the comparability of the research results when new genres are introduced to the corpus. In Section 4, the focus is on linguistic databases, and on Eighteenth Century Collections Online, in particular. Here, we discuss issues related to the reliability of the database with respect to OCR (Optical Character Recognition) accuracy as well as problems pertaining to the balance of the database and the metadata associated with it. Section 5 brings the chapter to a close with a discussion of the main topics and some final conclusions.

POS annotation in diachronic datasets

Part-of-speech (POS) annotation arguably provides one of the most useful layers of linguistic annotation to assist the researcher in the retrieval of relevant constructions from corpus data. Having each word in the corpus tagged according to its part of speech (i.e., word class) permits, for instance, the study of partiallyfilled constructions (e.g., N-PROP of N-PROP; "the Einstein of Italy") and of lexical items of a specific part of speech (e.g., love_V; "I love you").

However, applying software developed for Present-day English to historical data is somewhat problematic

Accounting for category change

One of the areas of research where POS annotation might fail is category change, that is, a process where a word of one word class gradually begins to be used like a word from another class (see, e.g.,

(1) I think that the role of the Ambassador in the Soviet Union is a very key one.

Both key and fun in the above examples are correctly tagged as adjectives in COHA. Whether or not the tag has been probabilistically assigned by the tagger or manually inserted as a rule is not obvious, but at this stage we can note that the tagger can correctly identify at least some usages of these items as adjectival. Figure

(3)

(4)

(5)

(6)

The situation becomes even more complicated when the research focuses on items that are attested with a lower frequency, or on items whose adjectival use has not yet become conventionalized to the same degree as key and fun. In (7) to (11), taken from COHA's sister corpus, the Corpus of Contemporary American English (COCA), all the "nouns" in boldface are used in a descriptive and non-referential function (i.e., similarly to key and fun in Examples (1) to (4) above), and yet none of them has been annotated as an adjective in the corpus. (11) The construction project was mammoth by the standards of the day.

(12) (COCA, Fic, 2008) He wasn't just killer good-looking. He was to die for.

(13) (COCA, Fic, 2002) Business meeting: Claudia Lester was textbook perfect.

To summarize, the pitfall related to POS annotation in the case of category change is that even though word classes are treated as static entities in corpus annotation, they are in fact dynamic; word classes exhibit both category-internal and category-external gradience, and this gradience may be a result of ongoing language change. This problem is of course not limited to historical corpora but also affects present-day corpora when the process of change has not reached its conclusion. While software like CLAWS also provide probabilistic information about word class, in practice this information is typically not available to the end users of the corpus. A possible solution to these kinds of problems is to make use of queries that not only combine lexical items with POS tags (e.g., key_j, fun_j) but also target the surrounding context of the item under study.

Theoretical choices in the design of the annotation scheme

There are also cases where it may be impossible to make use of POS annotation because of the idiosyncratic tagging of the corpus. For example, in

However, our approach was partly unsuccessful because of the theoretical choices made in the tagging of the corpus (see

Whether or not Huddleston and Pullum's analysis is ideal from the perspective of word class theory is not relevant in this case; the important thing is that the PCEEC is annotated in a way that makes it impossible to study colloquialization by measuring the frequency of prepositions in different time periods. As pointed out in

Annotation tailored to specific research questions

Our third example of a potential POS-related pitfall pertains to a case where the corpus compilers were themselves working on the long diachrony of English and used a conservative annotation scheme to facilitate comparability over time. The PCEEC is one of the parsed corpora of historical English produced in collaboration between the universities of Penn, Helsinki, and York, among others. These corpora are intended to cover all stages of the history of English, and as such, the annotation scheme has been designed to be backwards compatible all the way to Old English. This is of course a laudable aim and makes the corpora invaluable for research on historical syntax. However, it severely complicates the study of certain types of research questions that rely on POS annotation, as will become apparent below.

In our case, we were interested in examining changes in the POS frequencies in the PCEEC over time. In our first explorations, we focused on the frequency of nouns and personal pronouns in the corpus

We initially assumed that any compound tag ending in a noun tag could be counted as a noun. Sometimes this was indeed the case, as in sixpence_NUM+N. At other times, however, these represented other parts of speech that had grammaticalized from nouns (see, e.g.,

To resolve these issues, we ended up retagging the entire corpus in order to move the grammaticalized items from nouns to their current parts of speech. This was highly labour-intensive, especially as some of the compound tags were ambiguous in that they included both genuine instances of nouns, as in gentle-man_ADJ+N, as well as adverbs, as in likewise_ADJ+N; there were 1,199 instances of ADJ+N alone. Moreover, because the corpus contains private letters from a period before English was standardized, there is a great deal of spelling variation in the corpus. Consequently, it was not enough to simply list all adverbs etc. that could have been tagged as nouns; instead, we had to identify all their spelling variants as well, including those that had been written as two words (e.g., like_ADJ wise_N). These tokens were combined and reclassified, so the retagging involved changes in tokenization, too. Time-consuming though this process was, it enabled us to gain interesting and reliable results, which we refined and augmented in our study discussed above

Large corpora

Inaccuracies in text sampling

In this section, we give an example of a potential pitfall related to the semi-automated sampling of corpus texts. Here, it is not our intention to argue that the texts included in big-data corpora should be sampled manually; indeed, when the corpus includes samples from tens or even hundreds of thousands of individual texts, it is obviously not feasible to check all the data sources individually. Consequently, big-data corpora are likely to contain errors pertaining to the dating of some of the texts, their genre, and even the language variety, which the researcher must be aware of. Needless to say, even though some amount of data-related noise may be tolerable in the analysis, sampling errors like these can sometimes lead to disastrous results if one does not exercise sufficient care in data collection and analysis.

As an example of a pitfall related to text sampling, we discuss a particular usage of the complex adverb as well, which caught our attention some time ago. While as well is commonly used in a clause-final position in cases like (

(14) Not only are we stranded on this dreadful planet, but we are starving as well.

(COHA, TV/Mov, 1968) (15) This interaction with readers continues not only through the mail but on-line (COHA, NF/Acad, 1995) as well.

(16) These communities have significant African-American and Hispanic-American populations, among others. As well, they are largely low-income (COHA, NF/Acad, 1994) communities.

We consequently decided to investigate the frequency of sentence-initial as well in COHA, a corpus that provided us with enough material to examine its diachronic development in recent American English. According to our corpus queries, the frequency of the usage was generally very low in the corpus, but it was clearly becoming more common in the late twentieth century (Figure

Importantly for our preliminary analysis, the frequency of these potential bridging contexts and the frequency of the sentence-initial connective uses converged in an interesting way in the corpus: we see the frequency of the bridging contexts decrease as the frequency of the sentence-initial connective uses increases (Figure

(18) An attempt is made to bind up and fetter this country's expanding energies and prospects with Old World theories and methods. As well attempt to put (COHA, Mag, 1874) baby-clothes upon the statue of Hercules. Considering all this evidence, we were optimistic that the phenomenon was indeed real and that we were studying a usage that had thus far been ignored in the literature. We were only concerned about the low frequency of the form: in the last period studied , there were 32 tokens of the sentence-initial usage in COHA, which meant that our results might have been tarnished by errors made in the sampling of the corpus. Unfortunately, this is exactly what had happened. Indeed, our initial enthusiasm was quickly dampened by the observation that the connective use of as well was dispersed very unevenly in the corpus. On closer inspection, we were able to establish that 13 of the 32 tokens, all from the 1990s and the 2000s, were in fact produced by Canadian authors. Six additional tokens in the 1980s sample were produced by an author from Portland, Oregon, and one from Seattle, Washington, which raised strong suspicion of Canadian influence. Finally, the author of one sentence-initial usage turned out to be Irish. When these data are excluded from the results, the frequency of the form is so negligible that the argument for an incoming connective use of as well in American English can no longer be maintained. Now that we knew that the connective use was probably due to sampling errors in COHA, we conducted a new literature survey with a focus on Canadian English (CanE). We promptly found brief mentions of the sentence-initial usage in

Changes in the balance of subgenres

Our second example pertains to genre balance, and in particular to the effect of including new subgenres in a corpus over time. In

(20) (COHA, News, 2017) We are very pleased with the court's ruling.

Because we were interested in the potential impact of gender on the change, we chose as our dataset the fiction section of COHA, which contains named authors for whom gender metadata has been generated by

However, when we began to look for potential reasons for this lag, we noticed that the internal balance of the fiction subcorpus changes over time. While most of the corpus consists of novels, the most recent periods in the corpus include increasing amounts of drama, short stories and movie scripts, for example. When we restricted our corpus queries to novels alone, which is possible if one uses the downloadable version of COHA rather than the online interface, the gender difference disappeared.

This example serves as another illustration of the pitfalls of mega-corpora that are not as carefully sampled and balanced as smaller corpora. Of course, the issue of genre balance also affects smaller diachronic corpora like the Helsinki Corpus, because some genres may disappear and new ones appear over time, and compromises must be made in the compilation process in terms of representativeness vs. diachronic comparability. In the same vein, it is completely justified to include movie scripts in the fiction section of COHA as movies begin to be made in the twentieth century. On the other hand, the history of many fiction genres, including drama and short stories, is certainly longer than the coverage of COHA, so their proportions could have been more optimally balanced in the corpus; considering the automated sampling of COHA, their better representation in the most recent periods probably reflects their increased availability in digital form.

Historical databases

Issues with balance and metadata

In recent years, corpus linguists have been increasingly interested in making use of massive historical databases, such as the Eighteenth Century Collections Online (ECCO), in their research. ECCO consists of more than half of all known British publications in the eighteenth century, or about 200,000 texts and 10.5 billion running words

Fortunately, there is ongoing research that aims to shed more light on what exactly the database contains. For example,

With the help of the ESTC metadata, harmonized and augmented by the Helsinki Computational History Group (COMHIS), the ECCO dataset can be narrowed down to first editions only, which greatly facilitates linguistic research. The metadata also provides opportunities for generating principled subcorpora of ECCO, such as economic literature

OCR errors

Crucially, the digitized texts in ECCO do not even represent "God's truth" about the original publications on which they are based, owing to severe errors in optical character recognition (OCR) in the digitization process. These errors, the rate of which varies between Parts I and II

Hapax legomena

To give a concrete example, many studies of lexical and morphological productivity rely on accurate type counts, particularly of hapax legomena, that is, words that only occur once in the corpus. Here, the small subset of ECCO that has been keyed in manually by the Text Creation Partnership (ECCO-TCP), and which is hence free from OCR errors, provides an illustrative example. Comparing the texts shared by ECCO and ECCO-TCP, we find that more than 90% of the hapax legomena in the ECCO sample (henceforth ECCO-OCR) are not found in ECCO-TCP. This means that they are in fact spurious OCR errors and that the precision of queries based on rarity is abysmal (Figure

Historical lexis

A recent study of economic vocabulary in ECCO

The second issue relates to the fact that some characters are more likely to result in OCR errors than others: in particular, the long "s" and ligatures are more likely to be incorrectly identified. For instance, we aimed to study the spelling variants of economy to analyse the diachrony of the transition from the Latinate variants, "oeconomy" and "oeconomy", to "economy". However, by taking a small sample of the instances and going back to the document images, we discovered that about a third of the instances of "economy" were in fact OCR errors for "oeconomy". This made accurate timing impossible, unless we were to take a more representative sample and check all the document images manually, which would have been a very labour-intensive task. Moreover, in our multi-dimensional analysis of economic vocabulary where we mapped the words in a subset of ECCO to the "trade and finance" section of the Historical Thesaurus of the Oxford English Dictionary, none of the words that emerged as significant had an "s" in them, suggesting that these might have been missed owing to the problem with the long "s". Here one solution would be fuzzy searching, or at least adding variants with "f " or "l" for "s" to the queries.

The texts in ECCO were digitized and OCR' d in the 1990s and 2000s. Since then, OCR methods have vastly improved, so it is to be hoped that the document images -poorly scanned as they may sometimes be -could be re-OCR' d in the future, with much better results. These methods rely on deep learning and neural networks, which are becoming increasingly common in other linguistic applications as well, for instance word embeddings, which are used for lexical semantics. Deep learning is an area of computer science that is highly resource intensive, which means that the restrictions of hardware and software, already mentioned by

Discussion and conclusion

In this paper, we have discussed pitfalls related to some of the widely used historical corpora and databases in the context of current corpus-linguistic research. Our main argument was that the recent trends and advances in the field, such as the publication of big-data corpora, the increased reliance on statistical approaches to linguistic data, and the exploitation of various kinds of metadata, require that the linguist should have a detailed understanding of the structure of the corpus, its sampling procedure, and the principles followed in the construction of the metadata. In other words, the increased use of big-data resources in historical research has resulted in fundamental changes in research methods, data analysis and research questions, and while these changes have provided researchers with exciting new possibilities, they have also presented new challenges.

We also suggest that these challenges are ultimately not so different from the pitfalls discussed by Rissanen in his "philologist's dilemma": just like in the early days of historical corpus linguistics, today's challenges pertain to the linguist's knowledge and understanding of their data. However, because the focus of research has in many cases shifted from small, carefully compiled corpora to bigdata resources, the nature of the "data" and methods with which historical corpus linguists typically work has changed. Consequently, we suggest that it would be prudent to think about the principle of "knowing one's data" from a new perspective: as the sheer size of the modern mega-corpora prevents scholars from engaging with either all or the majority of the original source texts in as much detail as in the early days of historical corpus linguistics, they should strive for an intimate understanding of the historical corpora as mediators of the original texts.

Admittedly, this perspective signifies a trade-off between two types of knowledge. On the one hand, the linguist must accept that while sociocultural contextualization remains as important as ever, it is not always possible to check every corpus text or concordance line manually. On the other hand, the linguist can compensate for this to some degree by acquiring a thorough understanding of the corpus and its description. This knowledge is more abstract in nature, and it is partly a reflection of the more abstract kinds of research questions explored in corpus-based research today, as well as of a shifting focus towards an increasingly statistical orientation in research design. This shift also means that researchers must be willing to accept a certain degree of data-related uncertainty or "noise", which cannot always be controlled as well as when one works with smaller corpora. In the following, we discuss these issues and potential solutions to them in light of our case studies.

Our first examples concerned problems related to part-of-speech annotation, and these challenges are hard to overcome entirely. Even when the coding schemes are intended to be theoretically as neutral as possible, the POS tags always add an analytical layer to the corpus, which reflects a particular theoretical stance towards word class categorization. Because of this analytical interference,

In our case study on as well in Section 3.1, the confounding influence of miscategorized Canadian sources could be controlled because of the low frequency of the form, but if the frequency of the form had been higher, the likelihood of our spotting the miscategorized texts would have been lower. However, based on the proportion of the Canadian texts that yielded a number of hits of as well in our query out of all the texts sampled in COHA, the effect of CanE on most research questions studied with COHA is likely to be negligible. Indeed, we wish to emphasize that resources like COHA are invaluable for diachronic linguistic research, and that these resources are even more useful when one knows exactly how they are constructed and annotated. Furthermore, the biases can be combatted methodologically, for example, by using "dispersion-aware" methods

In the case of changes in the balance of subgenres (Section 3.2), one solution is for corpus compilers to provide detailed, or at least basic metadata, that enables users to zoom in on maximally comparable subcorpora or to take balanced samples of their own. As also suggested by

In addition to problems with metadata, the noise present in the digitized texts in historical databases (Section 4) may lead to compromises in accuracy in corpus-linguistic research. While the precision of queries can always be increased by going through the hits manually or semi-automatically (taking a smaller sample of the full dataset if needed), striving for perfect recall in data afflicted by OCR errors may prove to be a doomed endeavour. As noted in Section 4.2.1, however, if the OCR errors are distributed relatively equally across the corpus data, settling for lower recall may be justified, as there is so much data that missing some of it does not significantly impact the results. Whether or not lower recall is acceptable ultimately depends on the research question.

The principle of knowing one's data and being "on really intimate terms" with the data

To conclude, we argue that all linguistic research conducted on historical corpora and text databases benefits from an understanding of not only the texts and the historical language variety but also from the sociocultural contexts in which the texts were produced. From the corpus compiler's perspective, one way of contextualizing the texts is to provide relevant metadata to the end users (e.g.,

Named entities as potentially problematic items in corpora

Mark Kaunisto

Tampere University

This chapter discusses problems in the interpretation of corpus data arising from the insufficiencies in the annotation of named entities. Many corpora nowadays still do not adequately enable corpus users to set up queries that would exclude items appearing in names when needed to improve precision of the searches. Through an examination of case studies in major English language corpora, the chapter highlights the need to carefully post-process the search results, as irrelevant occurrences of named entities may pose challenges in the analyses of word frequencies and their collocational behaviour. The chapter calls for more detailed annotation of named entities in already available large linguistic corpora and reminds of the importance of close inspection of the search hits.

Keywords: named entities, proper names, annotation, corpus linguistics

Introduction

As we have reached the 2020s, corpus linguistics as a branch of linguistic study has progressed in many ways from its early days, nowadays offering a multitude of possibilities of examining various facets of language use. Not only have new corpora become available that are massive in size compared to those compiled in the 1980s and 1990s, but search engines or online interfaces have also become technically more and more sophisticated, giving users the opportunity to observe statistically organised search results that go far beyond the presentation of mere concordance lines. In concrete terms, the changes are noticeable on the level of educating students about the basics of corpus linguistics: with the evolving nature of the field, there is a constant need to update course materials to provide novices with an overview of both the possibilities and challenges relevant to corpus study.

With new corpora containing billions of words and allowing complex searches by making use of various search functions and different types of linguistic anno-tation, novice corpus users may find it difficult to question the relevance of their search results adequately and carefully. In line with the "God's truth fallacy" noted by

There are many types of items commonly found in corpora which, while they are perfectly representative of language use, may present problems when investigating patterns of language use. For example,

There are also items which, because of their high frequency, pose other, quite serious challenges in corpus analyses, and the ubiquitous references to named entities can be identified as one such issue. Corpus users are probably well aware that items found in proper names -for example, words found in titles of books, articles, films, etc. -may be regarded as frozen items and would normally be excluded from further analysis because the choice of the exact words in such instances was usually made by someone else than the authors or speakers themselves. The identification of multi-word units functioning as proper names is a task that has received a great deal of attention from scholars to solve problems relating to different purposes -for example, data mining, automatic translation, named entity recognition -generally in the field of Natural Language Processing (NLP). Much work has gone into developing efficient annotation strategies for differentiating items belonging to named entities from those representing regular uses of the words. The task is obviously not straightforward, as names differ greatly as regards their structural complexity, and in some cases, they can be rather long, as in Example (1) from a review of a music album.

(1) Originally titled 'The Southern Harmony And Musical Companion Featuring A Choice Collection Of Tomes, Songs, Odes And Anthems From The Most Eminent Authors In The United States' , this is the record that was struggling to be (BNC, CHA 4244) heard inside the more studio-constricted 'Money Maker' .

In this example, we can see that the title of the album is a lengthy one, and it can be observed that although the mention of the title in practice constitutes a single reference to only one entity, the 25 words of the name are similar to a quote in that none of the words and their combinations were originally produced by the author of the review.

Proper names and multi-word proper names in particular pose challenges to the study of language use, and many currently available linguistic corpora are lacking in this kind of annotation. In automated collocational analyses, for instance, it may not always be possible to make use of part-of-speech tags to distinguish between instances of words where the word has been a part of a proper name or not. The situation is somewhat easier if we need to distinguish between proper nouns and common nouns, as far as we can rely on the accuracy of the tagging. But as regards, for example, the occurrences of adjectives within proper names -as in magical in Magical Mystery Tour -no annotation is necessarily available for us to separate such instances from regular uses of the adjective, and close manual inspection is therefore needed to exclude such items from the analysis.

In this chapter, I will examine through different types of examples how the frequencies of English proper name uses can distort studies focussing on word frequency and collocational behaviour, and how the occurrences of proper name use may show different degrees of prominence of words in different genres and regional varieties. Section 2 provides the broader background in relation to English proper nouns, proper names, and the strategies of annotating names. Section 3 presents small-scale studies of corpus searches involving words that may be frequently used as named entities, or parts of named entities. Section 4 provides further discussion and concluding remarks. Overall, the main argument is that due caution must be given to such items and sufficient manual inspection of concordance lines is needed to avoid the possibility of misinterpreting initial findings from corpus data.

Background

The concepts of proper nouns and proper names

Many studies on the annotation of named entities start off with descriptions on how the concept of "name", or "proper name" has been described and defined, going back to the theoretical postulations by philosophers such as John Stuart Mill, Bertrand Russell, and Ludwig Wittgenstein (see, e.g.,

Some grammarians (e.g.,

In the domain of NLP, the term 'named entities' is often used as a broader umbrella term, including temporal and numerical expressions in addition to conceptually more straightforward 'proper nouns' (see, e.g.,

Annotation of named entities

As regards the task of annotating corpus data, the identification and annotation of named entities has been an area of interest from the very beginning, both in terms of manual and automated annotation strategies. There have been different ways of annotating named entities in corpus data, and the strategies tend to reflect both the various interests (e.g., translation and information retrieval, alongside general language study) as well as the practical possibilities of doing so. The guidelines used for the earliest English corpora compiled for linguistic analysis such as the Brown Corpus already observed the problems in the tagging of proper names. The annotation of multi-word proper names was also recommended in the corresponding guidelines for the Penn Treebank Project, in which

Considerably more in-depth work on the development of automatic algorithms to identify and annotate named entities in digital texts has been done in the realm of NLP and Computational Linguistics, with the aim of increasing accuracy and precision related to tasks such as data mining for named entities and machine translation. Within NLP, Named Entity Recognition (NER) is now seen as one of the major tasks under the broader field of Information Extraction (IE). The challenges in such tasks include, for example, taking into consideration the differences between languages in terms of how the concepts of proper nouns and proper names are understood and how such items manifest themselves on a grammatical level. The use of capital initials is not a fully reliable marker of the status of a proper noun even in English (see, e.g.,

To study annotated corpora efficiently, it is important to be aware of what kinds of things have been annotated and how, as this ultimately affects our understanding of the expected levels of precision and accuracy of our search hits. In addition, it is also true, as stated by

Case studies

This section looks into a number of lexical items whose analyses based on corpus searches are complicated by a high number of named entity instances found among the search results. What is the extent of potential noise, and why is the only option often to manually inspect the concordance lines in order to exclude irrelevant items from the analysis? Some of the items examined here are selected based on observations of individual problematic instances made over the years, while some near-synonymous word pairs chosen for closer study have one member of the pair fairly frequently occurring in different types of named entities, and the aim is to see how much such cases influence the studies of the near-synonyms.

In the 100-million-word British National Corpus, (for the purpose of the searches for this chapter, I have used the BNCweb interface),

Common nouns used as (parts of ) proper nouns: Lifespan and samurai

If one performs a frequency list search of the singular nouns in the BNCweb with the _NN1 tag and begins to browse through the most frequently occurring nouns on the list, paying attention to the structural make-up of the words, it is probably unsurprising to see that the most common singular nouns are monomorphemic ones. Nouns such as time, way, year, day, and man appear at the top of the list. Not much further down the list, we also find nouns formed with a root and an affix (e.g., government, business, and development). The most frequent singular noun consisting of two roots on the list is chairman, and other nouns of this type include football, newspaper, background, and bedroom. Among the highest-ranking nouns of this type is lifespan, followed by words such as database, policeman and classroom.

Considering the most common two-root singular nouns, we might be surprised to find lifespan featuring as one of the most frequent ones. With a total frequency of 3,700, and a normalized frequency of 37.6 instances per million words, it can be regarded as a high-frequency word in the corpus. However, a closer look into the instances of lifespan informs us that all the 3,700 hits are found in only 83 texts, which suggests the word probably has a very uneven dispersion across the corpus.

(2) Customers who have purchased a LIFESPAN maintenance agreement are enti-(BNC, HWF 15266) tled to call upon the services of the LIFESPAN help desk.

The case of lifespan is an example of the problem of accurate annotation in cases where common nouns are used as proper nouns, as has been observed in previous studies by, for example,

Another interesting common word that is often found in proper names is the loan word samurai. In her MA dissertation on the occurrences of Japanese borrowings in six regional varieties in the Corpus of Global Web-Based English (GloWbE; Davies 2013),

All instances of samurai in GloWbE are tagged as singular common nouns,

(3) a. Inside he said he saw Mr Fisher holding a samurai sword.

(GloWbE, Great Britain, General, www.thisisstaffordshire.co.uk) b. Believing he was carrying a Samurai sword, the officer called for backup and the police helicopter was scrambled […] (GloWbE, Great Britain, General, www.southwalesargus.co.uk) Table

The differences seen in the rates of samurai in named entities potentially also reflect some cultural differences and the interests of the writers represented in the corpus: the named entities including samurai in the GB and US sections included more references to drama films (The Seven Samurai, The Last Samurai), the Asian sections featured references to food-related items, computer software (Market Samurai, a keyword research tool), action toy figures (Samurai Predator AC-01). The popularity of the word in the names of video games or board games (The Way of the Samurai 4, Samurai Warriors 3, Samurai Spirit) was seen in all sections examined. 5 3.2 Near-synonymous adjectives in named entities: Limited/restricted, royal/regal and fantastic/fabulous

In addition to difficulties caused by common nouns used as parts of names, words representing other parts of speech may also be problematic when they are included in a name. For instance, in popular culture, the names of entities such as films, songs, books, etc. may feature a high number of adjectives, and some adjectives may appear more commonly in them than others. The tagging of some of the most well-known named entities in the BNCweb and the Corpus of Contemporary American English (COCA; Davies 2008-; which both use the CLAWS tagger), such as the White House (the official residence of the president of the United States), seems to take account of the use of capital initials, and mostly White in that name is tagged as a proper noun (_np or _NP0). Although for the most part, white in references to the White House is tagged as a proper noun, there are also some instances in BNCweb and COCA where it is tagged as an adjective; particularly in COCA, there are altogether 1,497 hits (searched August 27th, 2022) with this tagging, showing various combinations of capital or lowercase spelling of initial letters of white and house (mostly referring to the building in Washington D.C.). Interestingly, the House part in the name is usually tagged as a common noun. Lists of high-frequency names are often used when designing automatic taggers, which might account for the fact that with names of lower frequencies, the tagging of white in names appears less systematic. 6

5.

Considering the fashionableness of the concept of samurai in popular culture today, it is quite possible that the occurrences of the word in named entities would show changing trends in diachronic corpora, and the proportions of hits of the word in names as opposed to common noun uses vary from one period to another. 6. As noted previously, in the BNC2 manual,

One kind of implication of difficulties in the tagging of named entities can be seen in the corpus-based analysis of near-synonyms, and here we will take a look at some examples of adjective pairs to illustrate the types of problems that can be posed by the occurrences of adjectives in proper names. On a purely intuitive basis, we might predict that the comparison between the adjectives limited and restricted could be affected by the fact that limited appears in names of

In the BNCweb, there are no instances of limited which are tagged as a proper noun, regardless of the use of a capital or lowercase initial. There is variation in how limited and restricted are tagged, but it concerns the identification of the words as either adjectives or participial forms of their underlying verbs. To keep the analyses simple, the words were searched in BNCweb with the tag query _AJ*. The wildcard * allows one to target instances where the words were either assigned an adjectival tag or the portmanteau tag (sometimes also referred to as an ambiguity tag) _AJ0-VVN. The order of the two tags in a portmanteau tag is significant: in this case, the automatic tagger has found insufficient evidence to determine the accurate part of speech, but that it was more likely an adjective -the first element of the tag -than a past participial form of a lexical verb -identified by the tag _VVN. In order to assess the frequencies of the adjectives limited and restricted, and then to manually inspect the frequencies with which they occur in names, searches were made in three written sections of the BNC -Academic prose, Non-academic prose and biography, and Newspapers, which also allows for comparisons of the frequencies between written domains. The frequencies of the adjectival uses of the words, starting with total numbers of hits and then providing separate figures for nonnamed entity and named entity uses, are presented in Table

As can be seen in Table

7.

The adjective restricted was rarely used in named entities; for example, the few instances of the word in newspaper texts related to the name of a horse racing category. In other words, the potential of finding unwanted or irrelevant items may in some cases vary between different sections of the corpus. Similar problems are encountered if we examine the adjective pair royal and regal. In the BNCweb, there are 56 instances where royal with a capital initial is tagged as a proper noun (NP0), as in Royal Exchange Square (as a part of an address), Palais Royal, Park Royal, and Musée Royal des Beaux-Arts, and in these cases all elements of the names are applied with the NP0 tag. However, out of the total number of hits of royal in the corpus -14,628 hits in 1,825 texts -it appears with a capital initial as often as 10,237 times in 1,603 texts). In the vast majority of instances with an capital initial royal is tagged as an adjective, as in the Royal Academy, the Royal Shakespeare Company, the Royal Exchange Theatre, the Royal Commission on Environmental Pollution, the Royal Mile, the Royal Albert Hall, the Royal Navy, and so on. 8 The adjective regal is overall considerably less frequent 8. Considering the names in which royal with a capital initial is tagged as an adjective instead of a proper noun, the types of entities in question is reflected in the tagging, as noted in the BNC2 manual; in the names of institutions or charters, royal is an adjective, in the names of locations it is treated as a proper name.

than royal in the BNCweb, with 304 instances found in 152 texts, all of which are tagged as adjectives, although in a number of cases the word appears in names, as in the Regal Scottish Masters (a snooker tournament), the Regal (a cinema theatre), the Regal Arms (a hotel), and Regal Trophy (a rugby league). There are altogether 194 hits of regal spelled with a capital initial, although not all instances are proper names. In other words, in the case of both royal and regal, substantial proportions of the occurrences of the words in the corpus appear in proper names, and it would be crucial to exclude such instances if we wanted to study of the use of the words as non-frozen elements.

In lexical studies, near-synonyms are investigated for various purposes and interests: to reveal fine-grained distinctions and principles behind word choices, to examine variation between dialects and sociolects, or to provide assistance to translators and language learners, to name but a few. When we examine the characteristics of near-synonyms with corpus data and attempt to analyze the differences in the uses of the words, we typically examine their collocational behaviour. Many corpus interfaces and tools allow for the analysis of the strongest collocates of words based on different methods of assessment: Mutual information, log-likelihood, T-scores, raw frequency ranking, and so on. In such analyses, it might be beneficial to be able to exclude those tokens which appear as a part of a name. If the taggings of the corpus data are not helpful in this regard, one can try to perform case-sensitive searches; in the BNCweb, the searches could be restricted to all-lowercase spellings of royal and regal, for example. However, this would compromise those cases where the words appear at the beginning of a sentence. Conversely, particularly in British English, the adjective royal is sometimes spelled with a capital initial, perhaps out of respect to the monarchy, even in cases where it is not a part of a name, as in "The old Royal adage of never complain, never explain just won't do any longer" (BNC, CH6 6584) and "Princess Diana got the star billing at a Royal banquet on board the Britannia last night" (CBF 9645). In addition, in some collocations there are variant types of spelling of royal: for example, we find instances of royal family, Royal family, and Royal Family, and the form of spelling does not necessarily make it clear whether the authors are using the phrase as a name. All such considerations potentially add to the problems of analysing of collocational strength, which in the case of royal and regal may be more complex with British English data.

Some corpus interfaces such as the one at English-Corpora.org allow users to compare the collocates of two words, and the collocates which are more typically used with the compared words are listed in two separate columns in a ranked order, based on a score that takes into account the frequencies and comparative ratios of the two words in the corpus, the frequencies and ratios of different combinations of the words and their collocates, as well as the frequencies of the individual collocating words across the corpus. The listings are very useful in highlighting the differences between near-synonyms which are reflected in their collocational behaviour. However, we can note that analyses of collocations can also be affected by occurrences of the compared words in names; for example, we can consider the adjectives fantastic and fabulous.

Adjectives sometimes differ as regards their adverbial modification, and adverbs have also been seen to differ as regards their degrees of use in different regional varieties of English (see, e.g.,

None of the instances of absolutely fantastic in the US, GB, and IE sections were names or parts of names. So, when we examine the adverb modifiers of adjectives, we know that the collocation scores probably exaggerate the association between the adverb absolutely and the adjective fabulous, as the scores are skewed by the frequent occurrences of the combination of those two words in names. If we tried to adjust and recalculate the scores of collocational ratios of absolutely fantastic and absolutely fabulous by removing those instances of absolutely fabulous found in names, we would also need to subtract the corresponding instances from the numbers of instances of fabulous and absolutely fabulous. The recalculated scores for collocational strength for absolutely fantastic and absolutely fabulous would then be reversed in the Great Britain and Ireland data (in the GB section, the scores for absolutely fantastic and for absolutely fabulous are 1.7 and 0.6, and in the IE section, the corresponding scores are 1.2 and 0.8), indicating a tendency for absolutely to be used more frequently as a modifier to fantastic than fabulous, although the difference between the scores would not be high enough for absolutely to be identified as collocating strongly with fantastic in comparison with fabulous (i.e., absolutely would not be highlighted in the results list with green colour). But such recalculations, of course, are not entirely accurate, as we would be assuming that the scores for all the other adverbs modifying the two adjectives would be unaffected. Subtracting the instances of absolutely fabulous might not affect the rankings of other collocations to a notable degree, but for things to be ideal, we would need information on the named entity uses of all combinations, and for that purpose manual inspection is practically impossible. Nevertheless, based on these observations, it would be highly advisable to inspect concordance lines of the highest-ranking collocations to check for possible skewing effects by names.

Discussion and conclusion

Considering the occurrences of names in the corpora examined, it can be argued that it would be beneficial to have the option of building a search query that would enable one to include or exclude items if they constitute a part of a name. Ultimately names also contribute to the word count of a corpus, and it might also be beneficial to exclude named entities from automated calculations in cases where names -or other types of items in a corpus which may be irrelevant to what one is trying to assess by the calculations -do not reflect the active linguistic choices of the writers or speakers represented in the corpus. Of course, the uses of words in different types of names is an interesting question in its own right, and the words found in names definitely carry meanings beyond the moment of giving an entity a name. As mentioned, the question of exclusion arises from the idea that a person referring to the entity by its conventionalized name is bound by this convention and is not at liberty to use other words than the ones in the name, expect for any understood or established variants of the name.

By drawing attention to the question of words appearing in names, the present chapter has made the point that it is not always necessarily clear from the outset how much and what kind of post-processing of the search results is needed. As all users of corpora are not necessarily aware of the variety of things to watch out for, a set of examples was presented of cases where occasionally large proportions of search hits can turn out to be -depending on the point of view of the study -instances that one might want to exclude from the study, which may not always be a straightforward matter. As has been observed in instances such as the noun lifespan in the British National Corpus, and the adjectival phrase absolutely fabulous in British and Irish sections of the News on the Web corpus, sometimes the irrelevant tokens may far outnumber the relevant ones.

The concerns and challenges addressed in the present chapter are by no means new ones, and many existing corpora are frequently updated and improved to make the search interfaces more user-friendly as well as to increase the reliability of the findings. As a lot of work has been done on named entity recognition in the field of computational linguistics, a key to address the problems raised in commonly used, large corpora in corpus linguistics would be to increase the collaboration between the areas, and to integrate the systems and practices developed in NLP and computational linguistics into corpora such as the BNC. An encouraging example of such collaboration is the Clean Corpus of Historical American English

Thinking back to the problems relating to the use of corpora as outlined by

Challenges in the compilation, annotation, and analysis of learner corpus data

Marcus Callies

University of Bremen

This chapter highlights and discusses the special characteristics of learner corpus data and the challenges they may present for corpus compilation, annotation, and analysis. Because learner corpus and SLA researchers use their data to study L2 production and development, it is of utmost importance that the data are valid, that is, they represent "authentic" L2 production, which means that the data must stem from the studied learners' own language production. I discuss challenges in three areas:

Introduction and general remarks

Learner Corpus Research (LCR) is a relative newcomer to the scene of research paradigms and methodologies within applied linguistics and second language acquisition (SLA) research. LCR as a field only emerged and became visible in the course of the 1990s in the context of the popularization of corpus linguistics at large but has rapidly evolved and grown in scope and sophistication over the past four decades. Two handbooks that survey the field and discuss its links to the major neighbouring disciplines of corpus linguistics, SLA and language teaching bear witness to this rapid development

Challenges and how to respond to them

Multilingual practices and metalinguistic language use

Texts contained in learner corpora have typically been produced by bi-or even multilingual individuals, and learner data are rich in multilingual practices and phenomena induced by language contact, such as code-switching, foreignizing (the morphophonological modification of an L1 form to adapt to the structure of the L2) or calquing (literal translations of expressions from the L1 into the L2). Importantly, they are indicative of interlanguage development. In SLA, the term and concept of 'interlanguage' refers to a systematic and independent developing learner grammar that should be studied in its own right and contains elements of the learner's L1 and L2 but also independent structures

Moreover, learner corpora, especially those of academic texts, contain expert terminology, metalinguistic language use such as contextualised examples of language use, citations or mentioned items, and sometimes even whole abstracts or thematic summaries from other languages. Such instances usually do not represent the respective learners' own written production as they are typically taken over or copied from secondary sources. On the one hand, from the point of view of quantitative L2 analysis, these could thus be considered unwanted items or 'false positives' as their inclusion in word counts and concordance analyses will affect findings on what structures learners may have acquired and are able to produce. Thus, such cases have to be treated separately so that they can be excluded from search results and word counts to not distort the data in learner corpus studies. On the other hand, they are part of the text, they cannot simply be removed, as this may reduce the context or even render the text illegible. Additionally, their textual embedding and glossing may be of interest for future analyses, for instance in studies of intertextuality and referencing in academic writing, so that it seems desirable to preserve them in their original form and syntactic function.

Response

Data of the kind discussed above should be specifically annotated/tagged. From a practical point of view, the annotation of instances of multilingual practices in learner corpora facilitates their automatic search and identification through corpus software such as concordancers. It also allows for their exclusion from analysis and frequency counts if necessary.

Despite the pervasiveness and importance of instances of multilingual language use in SLA, learner corpora are not commonly annotated for such features. In some corpora, however, specific types of multilingual language use have explicitly been annotated. One example is the Louvain International Database of Spoken English Interlanguage (LINDSEI;

(1) I don't know the the . the name in English but (eh) here we call it <foreign> (LINDSEI; SP025) Traducción e Interpretación </foreign> A similar practice has been followed in the annotation of the Spanish Learner Language Oral Corpora (SPLLOC), a set of corpora of L2 Spanish that were transcribed using the CHAT system developed by the CHILDES project.

(2) a. *P63: y cómo se dice scuba@s:d diving@s:v ?

b. *P51: no están en el sol están en shade@s:d.

(3) Additionally, learners' use of indeterminate forms and idiosyncratic neologisms (referred to as "invented words", see SPLLOC Transcription Guidelines 2008: 18), apparently mostly cases of foreignizing or calquing, were marked with the code @n at the end of the word as shown in (

(4) *P54: um ehm detrás de lo eh pictura@n eh hay [/] hay un número de turistas.

The transcription guidelines also provide for codes to mark the use of a third language different from Spanish or English (see

From the perspective of the corpus user, the annotation of instances of multilingual strategies such as codeswitching, foreignizing, and calquing may prove useful as it makes it possible to systematically retrieve and study them in a learner corpus. Analyzing such passages can provide new insights into interlanguage development, specifically in terms of the multilingual strategies and communicative competencies of the learners.

As for academic texts that contain expert terminology and instances of metalinguistic language use,

Task effects

A further challenge that compilers and users of learner corpora have to deal with is unwanted lexical or constructional bias which results in an accidental overrepresentation of certain structures in the data. This bias can be introduced either indirectly by the topic of the task in that, for example, a large batch of essays on the topic of friendship in a corpus will lead to a disproportionally high frequencies of words and phrases that belong to the lexical field of friendship. Lexical or constructional bias can also occur when learners use words, phrases or syntactic constructions from the task description, the writing prompt or other input material. This may include patterns that are copied as unanalysed sequences by the learners to complete the task, a 'play-it-safe' strategy without necessarily having acquired the respective structure. Lexical material in the task instructions or the writing prompt may even trigger the recurrent use of a whole grammatical construction. For example,

-Explicit encouragement to include a specific structure (e.g., a gerund) or certain type of structure (e.g., a relative clause) in a specific writing task, leading to a higher frequency of use in that particular task; -A task elicits a certain type of structure implicitly. A prompt may elicit a high number of occurrences of a particular structure (e.g., temporal clauses, pronouns) as a natural consequence of language required to meet functional communicative requirements of the task (e.g., past tense narrative) -A task is neutral with regard to the elicitation of a specific structure.

-Copying directly from input prompt.

Finally, as

Response

It is important that researchers detect and control for the effects of lexical and/ or constructional bias but identifying it can be challenging. Essay topics, task prompts, and writing instructions should be checked carefully if related and recurrent patterns are noticed in the data. Words identified to cause lexical bias are either treated as stopwords and thus excluded from corpus queries, and likewise, L2 structures that are likely to have been brought about by lexical or constructional bias (and thus may have been copied as unanalysed chunks by the writer) are excluded from the analysis (as in

"Discourse of deficit" and learner corpus annotation

Finally, I would like to discuss the potential bias of certain annotation methods used in LCR and in other disciplines. LCR has been influenced by the "discourse of deficit"

A special kind of annotation in learner corpora is error annotation. This is typically carried out on the basis of an error tagset, which usually derives from error taxonomies based on structural linguistic categories (see, e.g.,

Creative and innovative but 'non-standard' interlanguage forms are often contact-induced or formed on the basis of semantic or structural analogy to frequent and recurrent patterns identified in the L2 input. An often-discussed example is particle verbs like discuss about, enter into, return back (see

Response

As for error annotation,

By contrast, instances of multilingual language use and lexical innovations should not be annotated as errors. By innovations I refer to forms that are unattested or infrequent in the main reference varieties of British and American English, but products of morphological regularity or creativity in that they are formed by either adapting L1 elements to fit L2 forms, or by recombining L2 elements

Summary and conclusion

In this chapter I have argued that in LCR and SLA the occurrence and contextual use of particular structure(s) in learner data is taken as evidence for the (process of ) acquisition and productive use, hence L2 data must be valid in that they represent language actually produced by the respective learners under study. Since texts compiled in learner corpora have been produced by multilingual individuals, learner data are rich in phenomena induced by multilingualism and language con-5. In addition, ICE uses two more tags relevant to the present discussion: <foreign> (for a "word or sequence of words that is foreign and non-naturalised") and <indig> (marking words which are "non-English but are indigenous to the country in which the corpus is being compiled"). 6. In fact, as ICE components have been compiled over three decades, annotation practices seem to vary from component to component.

tact which have a hybrid nature and are challenging to classify. Learner corpora of academic writing, on the other hand, contain various kinds of metalinguistic language use and language taken over or copied from secondary sources.

I have suggested that multilingual practices, metalinguistic language use and instances of intertextual reference should be identified and annotated so that they can be dealt with in or be excluded from corpus analysis. Researchers also need to pay attention to recurrent patterns in the task instructions, writing prompts and other input material (provided this has been documented!) and if these may induce lexical or constructional bias. When compiling a new corpus, the task instructions and the input material should be neutral with regard to the elicitation of specific structure(s). Finally, annotation and analysis should be separated from normative, oftentimes (over)prescriptive interpretation.

Conceptual and methodological reform and innovation remain a strong desideratum in a young discipline like LCR. To provide a certain incentive for researchers to document and share their methodological practices and tools, and to give researchers engaged in advancing the methodological expertise of the field the recognition they deserve, the International Journal of Learner Corpus Research decided to introduce new publication formats (see

Issues of standardisation and best practices have frequently been addressed (see, for instance, Gilquin 2015), but when it comes to corpus compilation and annotation, such guidelines for best practice do not seem to be available yet when compared to corpus linguistics in general (see, for instance,

Finally, given that LCR is still biased towards written learner corpora that typically contain only the final product of the writing process, it is important to consider that more and more electronic writing tools have become available to help the learner. More recently, the field has seen some initiatives and innovative research that seeks to explore the actual writing process, for example by means of keystroke logging to examine textual revisions

Early newspapers as data for corpus linguistics (and Digital Humanities) 1. Introduction

The availability of massive text archives holds great promise for corpus linguistic work, but at the same time they also present considerable methodological challenges for users (see, e.g.,

The recent proliferation of large digital archives as part of Digital Humanities research projects has raised the question of whether they could be readily used as linguistic corpora, which would allow scholars to spend less time on the compilation and pre-processing of data, and more time on actual research. While this prospect is extremely attractive, anecdotal evidence suggests in many cases this is far less straightforward than what might seem to be the case at the outset (as also noted by Vartiainen & Säily in Chapter 2 in the present volume) and that there are many pitfalls that could limit the effectiveness of this approach, and even undermine the validity of the results that are obtained.

The aim of this chapter is to reflect on some of the pitfalls involved in using the British Library Newspapers database as a corpus and consider some ways in which they can be avoided. I argue that the specific issues encountered with this database are due to fundamentally different ideas of what might constitute "good data" in the fields of Digital Humanities and corpus linguistics, respectively, and this in turn leads to different expectations regarding how the data is made available to researchers (e.g., Mehl 2021). To use large, digitised archives as data for corpus linguistic research, it is essential to critically think about the ways in which they have been collected and processed, as well as what kinds of software tools are available to search them. In practical terms, this may involve different processes of "remediation"

Section 2 of this chapter provides a brief outline of different approaches to digital text analysis. Section 3 identifies three pitfalls specific to the British Library Newspapers database and reviews some possible solutions to them. In Section 4 I summarise the main findings and discuss their implications to the corpus linguistic study of historical newspaper discourse.

Digital text analysis in the humanities

There is a high degree of consensus across humanities disciplines that the emergence of digitised materials and techniques for analysing them is a major advantage for scholarship. For example, in recent Digital Humanities literature, a great deal has been written on the nature of these advantages over "traditional" ones. The argument is that owing to the availability of new sources of data, digital approaches have great potential for increased productivity, enabling scholars to automate many of the tasks that previously have taken up a lot of time. Another widely recognised advantage of the proliferation of data is the possibility to obtain new perspectives on old questions which simply would not have been feasible previously, and this is seen as conducive to significant new epistemological advances. These views are nicely captured in the two quotations by

The phrase [a telescope for the mind?] is Margaret Masterman's; the question mark is mine. […] She used the phrase to suggest computing's potential to transform our conception of the human world just as in the seventeenth century the optical telescope set in motion a fundamental rethink of our relation to the physical one. The question mark denotes my own and others' anxious interrogation of research in the digital humanities for signs that her vision, or something like it, is (McCarty 2012: 113) being realized or that demonstrable progress has been made.

(Gale 2023)

Gale Digital Scholar Lab provides a new lens to explore history.

Very similar discourse is often found in corpus linguistic literature in support of a corpus-based approach. For example, in their widely used textbook Corpus Linguistics: Theory, Method and Practice,

It may refine and redefine a range of theories of language. It may also enable us to use theories of language which were at best difficult to explore prior to the development of corpora of suitable size and machines of sufficient power to exploit (McEnery & Hardie 2011: 1) them.

However, to discuss possible pitfalls associated with research at the interface of Digital Humanities and corpus linguistics, it is necessary to briefly review some basic assumptions associated with these respective areas of study.

Digital Humanities

Central to the field of Digital Humanities are computers and their application to the analysis of large masses of text, but beyond that, this broad field can be defined in different ways. In a recent article,

Given the breadth of the field of Digital Humanities, sub-disciplinary specialisms naturally exist within the field.

1. Digitised humanities (research broadly relying on digitized data) 2. Numerical humanities (which focuses specifically on numerical and, more broadly, formal models, e.g., computational social science or social informatics) 3. Humanities of the digital (i.e., the study of Computer-mediated interaction) He further argues that (

As previously indicated, the role of computing in Digital Humanities is often framed with the help of a visual metaphor as being that of an instrument that enables scholars to get a new perspective on the data that would not be possible through any other means. Irrespective of the orientation, it is generally agreed that advantages of a DH approach are said to be epistemological: by making available large amounts of data, DH provides new ways of looking at data and enables scholars to ask new questions and to be more productive.

Corpus linguistics

It is clear that many of the points made in the previous section apply equally well to corpus linguistics, where research is based on large amounts of language data which have been processed and turned into linguistic corpora. Even the same metaphors are used: Such similarities are of course not surprising, given that both Digital Humanities and corpus linguistics deal with textual data. In fact, the field of corpus linguistics is often conceptualised as being part of the Digital Humanities (e.g., Roth 2018; Zottola 2020; Mehl 2021) despite the fact that it has a longer history. In the modern sense, the term "corpus linguistics" became established early in the second half of the 20th century, with such milestones as the publication of the Brown Corpus (1964) and the formation of The International Computer Archive of Modern English (ICAME) in the early 1970s

However, there is one major difference that sets corpus linguistics apart from the Digital Humanities, and this relates to the definition of the term corpus. Corpus linguistics typically adopts a narrow definition: a corpus is not just any collection of texts, but a collection that has been selected to represent a language or a sublanguage following an extensional view of language (e.g.,

Towards a useful synergy

The differences between Digital Humanities and corpus linguistics should of course not be overstated, given that the two fields share many research goals and practices. For example, the use of unstructured archives as data is not exclusive to Digital Humanities, but they are also made use of by corpus linguists as "opportunistic corpora"

The argument that I want to make in this chapter is that in order to take advantage of the synergy between corpus linguistics and Digital Humanities, it is often necessary to critically reflect on the digital materials, how they have been collected, processed, and made available. In particular, it is often the case that the default workflow for accessing a specific archive is not ideal to tackle corpus linguistic research questions, which may make it difficult to compare results to previous work. 1 To avoid such pitfalls, adjustments are clearly needed, and this often requires carefully considering the influence of register, which is a crucial notion in most types of corpus linguistics. If this is possible -and in particular if the availability of data is not tied to a particular platform or infrastructurewe can improve the quality of opportunistic corpora and ultimately obtain results that are more accurate and are more easily situated within previous research. My aim here is to illustrate this argument by identifying and describing issues that are specific to the British Library Newspapers database, an archive that has clearly not been put together with corpus linguistic research designs in mind, presenting some possible solutions to them, and reflecting on the overall feasibility of these solutions.

1. For a fuller discussion of replicability and reproducibility specifically in the context of corpus linguistics, see

Historical newspaper prose and the British Library Newspapers database

The To peruse larger masses of historical newspaper language, scholars can nowadays turn their attention to the British Library Newspapers database, which is a massive digital archive of newspaper writing archive hosted by Gale. It contains around 5.5 million pages from national and regional newspapers from Britain between the 18th and the 20th centuries. To corpus linguists, however, the database also presents multiple issues, which need to be addressed to make full use of it. In the following sections, I will focus on four such issues:

1. Problem with available search tools 2. Sampling, balance, representativeness 3. Register/sub-register considerations 4. Quality of Optical Character Recognition (OCR)

Problems with available search tools

The British Library Newspapers database can be accessed through the Gale Primary Sources platform,

To a corpus linguist, the main shortcoming of Gale Primary Sources is the fact that the searches produce lists of documents, effectively suggesting that the next logical step in the analysis would be to focus on individual texts and their particularities. While such an "individual-document approach" (McEnery & Hardie 2011: 232) might indeed be appropriate for some purposes, corpus linguists would usually be interested in looking at a concordance to identify patterns in the use of the search term or calculate and compare its normalised frequency across different sections of the database. With Primary Sources this is only possible if the documents are individually downloaded and analysed using some other corpus tool.

To some extent, these shortcomings are addressed by Gale's Digital Scholar Lab, 5 a text analytics platform through which the British Library Newspapers database can also be accessed. It enables users to build "content sets" based on search terms and apply different Digital Humanities tools on them, including document clustering, named entity recognition, n-grams, parts-of-speech, sentiment analysis, and topic modelling.

One clear advantage of Digital Scholar Lab over Primary Sources is that it allows entire content sets to be downloaded by one action. However, it should be noted that the number of documents to download is currently limited to 10,000, which for many sets is insufficient for a comprehensive analysis. The usefulness of the available text mining tools is also seriously limited by their lack of customisability. For example, the part-of-speech tool (using spaCy) only offers visualisations of POS-frequencies but does not give direct access to the tagged texts or include them in the downloadable content sets, which effectively means that the morpho-syntactic annotation provided by Digital Scholar Lab is of no use for corpus research.

Fortunately, there are alternative ways of accessing the textual data. The massive full-text archive of British Library Newspapers is available to subscribing institutions as plain text XML files, and with the help of Octavo, a tool for Digital Humanities research created by Eetu Mäkelä

To sum up, as many workflows in corpus linguistics rely on the availability of data as plain text, it is clear that the possibility to access the data in the British Library Newspapers database using Octavo addresses many of the concerns that 5. 〈https

Sampling, balance, and representativeness

While a Digital Humanities research project might start by mining the entire digital archive in a bottom-up fashion, this approach is often seen as problematic in corpus linguistics, as lack of structure in the dataset makes it difficult to interpret the findings (see, e.g.,

Representativeness and balance of data are known to be tricky issues in corpus linguistics in general: while

Although the database includes articles from both 18th and 20th century, my focus here is exclusively on the 19th century, which is far more comprehensively represented in terms of the newspaper issues available. Even so, it is immediately clear that the distribution of texts is not even: as can be seen in Figure

Figure 1. Overall distribution of 19C issues

Choosing the appropriate sampling strategy depends on the aims of the research, and the chosen method should also take into account the contents of the database itself. For example, the most basic sampling method, a simple random sampling, is not sensitive to the increase in available issues and might result in the second half of the 19th century being overrepresented in the sample. If this is deemed problematic, then a stratified sampling strategy, where the database is first divided into smaller, temporally-defined strata (e.g., periods of 1, 5, or 10 years) before extracting the samples, is likely to be a better choice.

Stratified sampling is also more appropriate if the aim is to investigate differences between different newspapers and their linguistic characteristics, both synchronically and diachronically. To achieve this, it is necessary to first identify newspapers like Leeds Mercury, which are sufficiently well-represented in the database and meaningfully cover the entire century, and use them as the strata

Registers and subregisters

The importance accorded to register in contemporary corpus linguistics can hardly be overstated. Register has been established as a major determinant of linguistic variation

The database contains potentially relevant information about text categories under two headings: Document type contains information about "the format, genre, or other characteristics of the document", and Publication section allows users to limit searches to a specific section of the publication. Of these, the latter indicates whether the text belongs to section A, B, etc., and as such is of limited use to register analysis. Document type is much more useful: it categorises each text as belonging to one of the seven types listed in Table

With that said, the adoption of the text categories in Table

But even if the existing text categorisation is accepted as a basis for corpus compilation, it is still necessary to consider the implications from the perspective of representativeness. The register categories (like newspaper titles in the previous section) are not evenly distributed across the database, and different choices will inevitably result in different representations of the text-external reality, and as before, this information is only accessible to users systematically looking into this. This issue, too, can be illustrated with the help of the same sample corpus from Hiltunen (2021), Corpus A, which comprises complete issues of the four newspapers sampled at 10-year intervals. Out of the 3,475 texts, approximately 88% (3,081) represented a single text category, namely the category News; this is shown in Figure

Optical Character Recognition (OCR)

The third major pitfall in the context of the British Library Newspapers database is the poor accuracy of Optical Character Recognition (OCR). OCR quality is a well-known issue in the Digital Humanities in general, and specifically for digitised newspapers (e.g.,

(3) The circumstance of Mr. Addington being ■shout to be appointed Speaker of the House ©f Lou!-., lias given rise to a report of the following] changes, which we mention without vouching the ' avi-uracy ef any part of the statement :I It is evident that the accuracy of the text depends crucially on the quality of the image used as the basis of the digitised text. Contemporary OCR software is able to reach a high accuracy with clean, present-day English text

Intuitively, the usefulness of digitised texts depends crucially on the overall frequency and distribution of the OCR errors that they contain, and if this is the case, then there indeed appears to be reason for some concern: Tanner, Munoz and Hemy Ros (2009) have estimated the average OCR accuracy across the British Library Newspapers database to be 83.6% for characters and 78% for words, and suggest that if word accuracy is higher than 80%, a fuzzy search engine would nonetheless be able to reach a high search accuracy. However, achieving this across the entire database appears unrealistic, given that this level of word accuracy was only reached by a quarter of the texts in their sample, and this impression is borne out by the trial searches reported by

On a more positive note, it has been shown that not all corpus linguistic research tasks are equally sensitive to OCR errors, with for example, the identification of frequent collocations providing robust results even with texts containing relatively large numbers of errors

To do this, we can make use of the figures for OCR confidence (0-99.99%), which are available for each text in the British Library Newspapers database and "[represent] the OCR engine's confidence in the accuracy of the conversion from image to text". The OCR confidence value corresponding to each text can be conveniently accessed with Octavo, like any other metadata. While it is unclear how the values for OCR confidence are determined,

After evaluating a number of files representing different levels of OCR confidence, it was decided that texts reaching 90% were sufficiently clean to provide accurate research, and this criterion was adopted for Corpus A. This ensures the extracted sample corpus is reasonably clean and can therefore be expected to provide reliable output for standard corpus linguistic tasks. However, the disadvantage is that we also discard an enormous amount of potentially interesting lin-7. See 〈https

To sum up, removing low-quality texts is essential for accurate corpus linguistic work, and it can be accomplished by filtering out texts that do not meet a pre-determined OCR confidence value. The flipside is that this may potentially compromise the representativeness and balance of the filtered sample, and this needs to be evaluated on a case-by-case basis due to the uneven and erratic distribution of OCR errors in the database.

Discussion

After identifying and reviewing four major pitfalls and suggesting possible ways of avoiding them, it is possible to offer some preliminary conclusions about the usefulness of the British Library Newspapers database for corpus linguistic research. Starting with the positives, an important advantage over traditional, relatively small linguistic corpora is that the database enables an exploratory data-driven approach with reasonable effort. In other words, as data extraction can easily be automated, it becomes straightforward to create multiple corpora with the search parameters and frequency thresholds and assess their suitability for different research tasks. The structure of the database also allows researchers to readily incorporate a register perspective into the study design. Finally, as it is possible to automatically discard low-quality samples, this workflow also enables the creation of reasonably tidy corpora. As a result, the British Library Newspapers database is an attractive alternative for the corpus-linguistic analysis of historical newspaper prose.

However, there are also obvious caveats to consider. Compared to carefully constructed traditional corpora with often hand-picked text samples, there is obviously much less control over individual choices, and the fact that the exact 8. Including the text in Figure

basis of text categorisation is unclear is likewise not optimal. Yet the by far most serious issue is the presence of errors in the source material, which introduces errors to analyses, and, in the worst case, may compromise the representativeness of corpora. How problematic these issues are depends on the goal of the individual research project. As the database is large, the omission of some publications or volumes due to low OCR quality might not matter too much for the analysis of general trends in language and discourse, whereas it may effectively preclude the study of specific questions or narrower time periods.

Given these caveats, corpus linguistics arguably still has a place for "small and tidy"

Open Corpus Linguistics -or How to overcome common problems in dealing with corpus data by adopting open research practices

Stefan Hartmann Heinrich Heine University Düsseldorf

In recent years, many researchers have called attention to the fact that research results very often cannot be replicated -a phenomenon that has been called replication crisis. The replication crisis in linguistics is highly relevant to corpus-based research: Many corpus studies are not directly replicable as the data on which they are based are not readily available. Especially in English linguistics, the full versions of many widely used corpora are still behind paywalls, which means that they are not accessible to parts of the global research community, and even when parts of the data are freely accessible, this presents problems for state-of-the-art methods of data analysis. In this paper, I discuss the challenges that have led to this situation and address some possible solutions. In particular, I argue for using smaller but openly available corpora whenever possible and for adopting open research practices as far as possible even when using commercial corpora.

Introduction

In a seminal paper,

More than thirty years later, corpus linguists still struggle with some of the issues that Rissanen has identified. But in recent years, additional issues have emerged. Perhaps most importantly for the purposes of the present chapter, the "replication crisis" that has permeated various quantitatively oriented disciplines in recent years and decades has also had a significant impact on methodological discussions in (corpus) linguistics

The term "replication crisis" refers to the observation that many scientific findings have been found to be much less replicable than many believe they should be

Regardless of the exact approach to replication, it has become clear that the lack of replicability is also a topic in linguistics. To mention only one prominent example from experimental linguistics, a recent multi-lab effort to replicate the seminal study by

Sönning and Werner (2021: 1182) mention the following list of problems that have been identified as potential causes of non-replicability:

-a lack of transparency in methodology and data analysis, -the non-reproducibility of scholarly work, as, for example, original data and analysis procedures are not accessible, -reluctance to undertake replication studies as purportedly "unoriginal" (and unprestigious) despite their potential to put previous findings in perspective, and -concerns about high rates of false-positive findings in the published scientific literature.

At first glance, it might seem quite far-fetched to link Rissanen's problems to the issues related to the "replication crisis". In this paper, however, I will argue that there are important connections between the different issues mentioned above. And more importantly, I will argue that the measures that have been proposed to help overcome the replication crisis can also solve Rissanen's problems -at least partly. Specifically, I will make a case for what I call Open Corpus Linguistics. This entails putting into practice principles of open research at various levels and at various (ideally, all) stages of the research process. In a best-case scenario, it involves the open availability of the entire corpus the researcher draws on, as well as sharing of concordances, annotations, and analysis scripts (if applicable). This also helps other researchers to put one's findings into perspective, which may be seen as the main overarching issue underlying Rissanen's problems. The remainder of this contribution is structured as follows: In Section 2, I explicate Rissanen's problems in more detail, relating each of them to specific issues raised in the replication debate. In Section 3, I discuss the main principles of Open Corpus Linguistics, taking potential challenges and pitfalls into account. Section 4 concludes the paper by bringing the two strands of the discussion together by showing how Open Corpus Linguistics can contribute to overcome several widely discussed problems in corpus linguistics. While my focus in this paper is on English corpora, many considerations brought forward here of course apply to corpora of all languages.

Revisiting Rissanen's problems

In many cases, it can therefore make sense to look for alternative corpora that can be considered equally representative for the language the researcher wants to investigate, or even to compile one's own corpus -possibly by drawing on existing (open) corpora and using relevant subcorpora of each corpus. This can also be advantageous with regard to the "mystery of vanishing reliability", that is, the phenomenon that each datapoint tends to become less reliable the more parameters (in corpus-linguistic terms, annotations) one adds. We can think about this in terms of a simple spreadsheet: The number of data points (rows) remains the same, the number of columns, however, increases. The problem now is of course not that more parameters are added but rather that the number of "cells" (in relation to the number of datapoints) increases and, as such, the potential for error. The obvious solution, then, is not to reduce the number of columns 1 but, ideally, to increase the number of datapoints so that the individual errors weigh in less. As such, the "mystery of vanishing reliability" can be reframed in terms of a lack of extensibility: Being in control over the compilation of a corpus allows us to easily extend the database if necessary. The problem, after all, is not so much that each data point becomes less reliable if we add more annotation categories, but rather that we often do not have enough data points to obtain a truly informative picture when addressing research questions that require us to take many categories into account simultaneously. But there is another dimension to extensibility: The reliability of a particular annotation can also "vanish" because it turns out to be misguided, for whatever reason. For example, it could turn out that an annotation set used for a corpus is based on false assumptions. Thus, it is tremendously helpful if a corpus is extensible, that is, existing annotations can be amended or improved and new ones can be added, both by the original creators and by people who reuse the data. As

Open Corpus Linguistics: Perspectives and challenges

In the previous section, I argued that open research practices can provide (partial) solutions to common corpus-linguistic problems. This raises the question of how exactly these open research principles can and should be put into practice, and which challenges this entails.

There exist several standards and guidelines that can provide orientation. After all, the problems discussed here are not specifically corpus-linguistic ones.

In terms of open data, the FAIR guiding principles

In an ideal world, then, all linguistic corpora would be available for free in re-usable and interoperable formats under a Creative Commons license. In practice, however, there are some obstacles. One obvious problem is that corpora are usually themselves derivative works in the broadest sense, that is, they draw on existing material. And in the default case, the existing material is subject to copyright. In the case of, say, newspaper texts, the copyright holders are usually easy to find but hard to convince to make their content available for free; in the case of web data, by contrast, the copyright situation is often unclear, which can make the redistribution of data crawled from the web problematic

Apart from copyright, personality rights can of course also be an issue. In the case of spoken corpora or child language corpora, for example, we are usually dealing with elicited data, requiring the participants' (or their parents') informed consent. Especially in the case of child language data, the recordings can contain sensitive information such as the child's or the parents' name or the place where they live. Thus, it is important to anonymize or pseudonymize the data. But data crawled from the web can also contain sensitive information. Given that the information was public at the time of crawling, one could make a case that including it in the corpus is unproblematic, but it is quite easy to imagine scenarios in which the publication of data crawled from the web can lead to legally or ethically challenging situations. In some cases, it can therefore be useful to publish a corpus in password-protected form, even though generally, the ideal should of course be maximal accessibility.

How should we ideally approach the copyright problem now if we want to follow the principles of Open Corpus Linguistics? If we do not need full texts to address our research questions, the problem is quite negligible. In that case, we can work with concordances, and to the best of my knowledge, nothing usually speaks against sharing concordances via dedicated repositories like OSF 〈https

If we need full texts, my suggestion is that we should always consider using an openly available corpus like the BNC or the Open American National Corpus first. In a broader sense, the aforementioned COW can be considered open corpora, too -for legal reasons, they are released under a relatively restrictive license, but they are freely available for academic purposes. Naturally, there will be situations in which we cannot use such open corpora because we need more or different data. In such cases, we might have to either fall back on commercial corpora or compile our own corpus. Thanks to the availability of powerful programming languages such as R or Python, this is easier than ever before; even novice users can quite easily get familiar with a tool like

Also, thanks to relatively permissive new legislation at least in (parts of ) the European Union, 4 many data mining activities that used to take place in a legal grey zone are explicitly legal now (see, e.g.,

Conclusion: Open Corpus Linguistics in practice

In the preceding sections, I argued for adopting open research practices in corpus linguistics and examined a number of potential problems that such an endeavor entails. In this section, I discuss how Open Corpus Linguistics can work in practice, and how it contributes to overcoming the pertinent problems addressed in Section 2.

In Section 1, I argued that adopting open research practices -and in the ideal case, using openly available corpora -helps us to overcome "Rissanen's problems": In the best-case scenario, we can use corpora whose full texts are readily available, which can contribute to overcoming "the philologist's dilemma". Such a scenario also provides some flexibility in working with pre-compiled corpora, as we are not at the mercy of the corpus creators with regard to the composition of the corpus. Instead, we can work with custom subcorpora or work with custom compilations of subcorpora from different corpora, which can help to solve the problem of "God's truth fallacy". And finally, such an approach ensures replicability and reproducibility, which partly solves the "mystery of vanishing reliability".

The latter is even true if we work with commercial corpora but make our concordances and analysis scripts available, as is increasingly common in corpus linguistics. For this purpose, platforms like the Open Science Framework (OSF) or the Tromsø Repository of Language and Linguistics (TroLLing) can be used (see the FAQ in Table

In the previous section, I addressed some potential problems, many of which are, in my view, not insurmountable. Some of them, though, call for creative solutions such as Schäfer and Bildhauer's decision to work with sentence shuffles in the case of COW. Needless to say, it would be desirable to have more legal certainty when using copyright-protected content for linguistic purposes -the aforementioned EU directive might be seen as a promising sign that (some) political stakeholders are aware of the need to reconcile research and copyright interests. For the time being, following the ideals of Open Corpus Linguistics might in some cases require entering legal grey zones. The risk of having to tread uncertain legal ground can, however, be minimized by keeping the question of how the data should be published in mind from the earliest design phase on, and by choosing open corpora wherever possible, as discussed above. In other words, following the principles of Open Corpus Linguistics requires us to invest considerable time in Research Data Management (RDM). While this can be time-consuming, it is very likely that it will save others and ourselves much time later on.

One topic that I have not addressed yet is open-access publication. As one reviewer correctly points out, open research practices and open-access publication ideally go in tandem, even though they can be treated as separate topics.

Many of the arguments in favor of open research practices mentioned above also apply to open-access publication, ideally in the form of "gold open access" (i.e., the final publication is available free of charge) or alternatively in the form of "green open access" (i.e., a preprint is published on a pertinent repository; see Eve 2014 for more details and discussion).

While linguistic research questions and the corpus-linguistic scenarios required to address them are too diverse to provide anything like a "cookbook" for Open Corpus Linguistics, this chapter has hopefully provided some helpful guidelines, some of which are summarized in the form of Frequently Asked Questions in Table

Table 1. Answers to some frequently asked questions about open research practices in corpus linguistics

Category

Questions and answers

Existing corpora

Which corpora should I use to follow the ideal of Open Corpus Linguistics?

The choice of corpus has to be guided, first and foremost, by the research question. But in many cases, there are open alternatives to the widely-used default choices.

Examples for synchronic English data include the Open American National Corpus for spoken and written American English, as well as the ENCOW corpus for World Englishes as used on the web. The BNC, which is a paradigm example of an open corpus, hardly needs to be mentioned as it is already widely used. There are also a number of multilingual corpora, for example, the COW family of corpora to which the above-mentioned ENCOW belongs, or the WaCky corpora

Corpus compilation

Which principles should I follow when compiling new corpora?

If possible, try to create a corpus that can be published freely under an open license.

To do so, it is very important to address legal questions at the very beginning of a project. If you cannot make the full texts freely available for copyright reasons, try to make the corpus as accessible as possible, for example, by allowing queries via flexible search engines such as NoSketchEngine or CQPweb and by publishing word and lemma lists (and ideally, n-gram lists), or by publishing it in password-protected form via a repository that allows for closed-access corpora (e.g., CLARIN). Data repositories that fit your needs can be found via 〈https

Repositories Where can I publish my research data?

There are dedicated repositories such as osf.io, zenodo.org, the TroLLing Dataverse (

Table 1. (continued)

Category

Questions and answers

Can I publish my paper (draft) along with my data?

In most cases, this shouldn't be a problem, even if you submit the paper to a commercial journal. The Sherpa-Romeo database gives a good overview of different journals' and publishers' open-access policies 〈https

To what should I pay attention when publishing my research data in repositories?

Make sure that everything is well-documented and self-explanatory. (This is harder than it sounds, which is why I'm not referring to any of my own repositories here as a best-practice example.) Make sure that the repository is actually public when your paper is published (OSF, for instance, offers private and public repositories). Make sure that your analysis scripts are extensively commented (formats like R Markdown or Jupyter Notebooks invite extensive comments, but plain text scripts can also be used, of course). Make use of the possibility to assign a DOI to the dataset(s).

I would like to share my dataset and analysis scripts with reviewers. How can I do this without compromising the anonymity of peer-review?

OSF offers "view-only" links that you can share with reviewers. Note that the nonanonymous repository can easily be retrieved from the view-only link as soon as it is public -if you want to make sure that you remain anonymous, keep it private (the view-only link will still work). This is probably quite common -not only in corpus linguistics but also in other domains, for example, when it comes to publishing programming scripts. But the thing is: Nobody expects us to be perfect. Everybody who has ever worked in corpus linguistics knows that no corpus will ever be perfect, and that the quality of a corpus depends less on the competence of the researchers involved than on the resources they were able to put into it. As such, nobody will blame you for releasing a corpus that is still more of a raw diamond. Publishing your "raw diamond" will give other people the opportunity to build on it, or to work with it while you are still continuing to develop it.

* Thanks to a reviewer for bringing this up!

To sum up, Open Corpus Linguistics can be a challenging endeavor, but given the "replication crisis", it is a necessary one. In the long term, adopting open research practices can also help us to focus on actual linguistic research questions, rather than spending hours and hours of work on things that other people have done before, without making the results publicly available. Adopting open research practices is ethically a good choice, and it is in our own best interest, both as individual researchers and as an empirical discipline.

Introduction

It is a well-known fact that variation in text length is an unavoidable source of nonuniformity in corpora and can cause issues in quantitative corpus-linguistic analyses. At the very basic level, the confounding effect of variation in text length is obvious. Since a longer text by definition contains more words, there are also more opportunities for any given linguistic item or feature to appear. In other words, a longer text will, on average, contain more instances of any item or feature simply because it is longer. This is a problem particularly for text-analytic corpuslinguistic studies, which are interested in comparing how many of these items appear in different types of texts: if two texts have a different number of occurrences of the feature of interest simply because they are of different lengths, it can be very difficult to compare texts of different lengths with each other. 1  1. Variationist corpus linguistics, which focuses on the proportions of variant items or constructions, is not as heavily affected by the issue. However, even variationist analyses may be affected by the distribution of text lengths in their dataset.

Fortunately, this basic problem has a simple mathematical solution, which commonly forms the basis of typical quantitative corpus-linguistic inquiries. The number of occurrences of the feature of interest in a text can be divided by the number of words

Normalization is a tried-and-true method of comparing texts of different lengths with each other. However, while it is a working solution to a huge potential problem in a large number of cases, normalization is not without problems itself. One problem with normalization becomes particularly salient when applying the method to short texts. The problem is based on the mathematical fact that the smaller the divisor becomes, the larger the result of the division will be. In other words, the fewer words there are in a text, the larger the normalized value is. This is of course the very basis on which the normalization method is built to enable comparisons of texts of different lengths. However, when the divisor becomes very small, the effect gets magnified and the result of the calculation inflates to meaningless levels. For instance, consider a short text of only five words (such as a tweet, a postcard, or a sticky note) which contains one instance of a feature, for example, a single first-person pronoun. If we calculate the normalized frequency of first-person pronouns in this text, we get as the result 200 first-person pronouns per 1,000 words. This is the mathematical solution to the formula, but the result is quite useless in terms of comparing texts of different lengths with each other. While not every five-word text contains a first-person pronoun, it is also not that unusual if one does. But the same normalized rate of occurrence value also applies to a text of 1,000 words which contains 200 first person pronouns. While both the five-word text and the 1,000-word text have the same rate of occurrence of first-person pronouns, surely the 1,000-word text with 200 first-person pronouns is much more unusual than the five-word text with one first-person pronoun. Clearly, these calculated rates of occurrence are not meaningful measures for the comparison of short and longer texts in linguistics.

In other words, there are two related problems caused by text length. First, texts of different lengths cannot be directly compared because they have different numbers of everything simply due to their difference in length. I call this the problem of text length. Second, extremely short texts cannot be easily compared with other texts using many typical quantitative corpus-linguistic methods, such as normalization, because the normalization results become meaningless as the length of the text becomes increasingly short. I call this the problem of short texts. In order to talk about these problems, in this chapter, I use the words "long" and "short" to refer to texts which are and are not long enough for typical quantitative corpus-linguistic analysis, respectively, though the line between the two is of course fuzzy.

To combat these two problems, a number of solutions and workarounds of varying sophistication have been devised. However, many of these solutions have problems of their own. Despite the ubiquity of the problem, and the oftensuboptimal nature of its solutions, the problem is not that often discussed in much depth. In this chapter, my goal is to bring more attention to the problems of text length and short texts, and to encourage the development and application of new and improved approaches to the problem. In Section 2, I describe how the problem of text length was historically less of an issue but is coming to the forefront with the rise of research into the language of social media. I also refer to and summarize results from earlier studies, which suggest that texts of all lengths are of interest. In Section 3, I cover various methods which have been used to either solve or work around the issues caused by the problem of short texts, the problem of text length, and related problems, and discuss their upsides and downsides, as well as suggest best practices and propose potential improvements to these methods. Furthermore, I will briefly discuss the related topic of the effect of text length on measures of lexical diversity, which has been studied in more detail. Finally, in Section 4, I will conclude this chapter with some final thoughts on the problems caused by text length and their solutions.

Background

Text length, corpora, and social media

The problem of text length and short texts is caused by a simple mathematical relationship, and as such it has been known of since the beginning of quantitative corpus linguistics. However, historically, the practical problems it has resulted in have arguably been of relatively little actual consequence. Most genres traditionally studied in quantitative corpus linguistics tend to comprise of longer texts, compared to the extremely short text lengths of up to only a few dozen words which are overwhelmingly common on, for example, social media. Because of the comparatively long text length in such genres, the normalization method works reasonably well with them.

The reasons for the focus on genres with "longer" texts have been manifold. A major reason has of course been, and still is, availability of data. Researchers have to use whatever data they have access to. Historically, this has largely been published corpora compiled by teams of researchers. But the compilers of such corpora have also been working with the data they can get access to in large enough quantities. These include genres such as newspaper articles, fiction writing, academic papers, and countless others. Many of these genres have editorial guidelines or genre conventions which place certain requirements for the length of the piece of writing.

Another reason for the focus on longer texts is what has been considered important and influential enough to study. The impact of, for example, newspaper articles, academic writing, casual conversations and personal letters on people, society, and language has been evident to all, and as such it is only natural that texts in such genres are of interest to anyone studying language. But these texts also tend to be long enough for reasonable quantitative corpus-linguistic analysis. It is often easier to overlook the societal and linguistic impact of genres with mainly shorter texts. For instance, personal notes, post cards, and shopping lists are also something written and read regularly, but we might not even think to consider them and other similar genres as research subjects.

The question of influence and significance also comes back to the question of availability, since it is of course more difficult to collect a large and representative corpus of post cards or shopping lists than of newspaper articles. However, it is also, to an extent, a chicken-and-egg situation. With more interest in such genres, it is possible that more such data would be collected into corpora; and with more such corpora available, there might be more interest in such genres.

A similar vicious circle has also developed when it comes to the development of analysis methods which would allow us to better approach genres with short texts. If genres with longer texts are easier to quantify and the available methods work better with them, it is only natural to focus on such genres; and if the focus typically is on genres with longer texts, there is no particular need to develop methods and approaches which could help analyze genres with shorter texts in more detail.

However, over the past decades, many of these paradigms have been gradually but firmly upended. A central catalyst for the change has been the spread of the internet. The web and other means of computer-mediated communication have become easily accessible sources of linguistic data, which has greatly facilitated the building of new corpora and datasets to match the needs of the researcher and to enable the study of entirely new kinds of genres and registers. Of course, published corpora are still being compiled to this day, and they are a valuable tool used in a wide variety of linguistic research. But the easy access to textual data online has allowed quantitative corpus linguists to cast a wider net in terms of their research topics than ever before.

At the same time, the rise of web and CMC texts has brought many of the issues with text length to the forefront. In contrast to the more "traditional" genres which make up many compiled corpora, texts from many internet genres tend to be less bound by word count limits or guidelines. While many online genres have a highly variable text length and a large proportion of shorter texts, such as blog posts or Wikipedia articles, this is particularly true for computer-mediated communication and social language use on the internet, such as postings on various social media platforms. Most social media platforms, such as Facebook or Reddit, do not limit the length of their postings to a meaningful degree. Platforms which do limit posting length, such as Twitter (now X), usually limit the maximum length, confining all of their content into the range of short texts, which is mathematically difficult to work with in quantitative corpus linguistics. Few online platforms require a minimum length for postings or even recommend postings to be of a specific length, whereas such requirements are commonplace within the publishing industry and for many of the genres included in typical published corpora, such as newspaper articles or academic writing. At the same time, online data has brought even the shortest texts to the center stage, making their societal and linguistic importance much more evident in comparison with the more traditional short genres. In other words, the free nature of internet writing has brought texts with a wide variety of lengths into the corpora of many linguists, and consequently made the problem of text length and, particularly, the problem of short texts more central than ever.

The importance of text length

It is clear that variation in text length, particularly the very shortest texts, causes mathematical issues in quantitative corpus-linguistic analyses. But do we actually have to care about text length? Can't we simply ignore the problematic cases when conducting quantitative linguistic studies? Or should we work towards finding more ways to make it possible to include texts of all lengths in our analyses?

In order to specifically focus on the functional variation taking place across text lengths,

This kind of analysis requires a very large dataset, as there need to be enough texts of every length in the dataset for meaningful results. Consequently, social media is a good source of data for this method. Reddit in particular is arguably a very fruitful source of material for quantitative linguistic analyses overall, and especially for the analysis of the effects of variation in text length. First of all, Reddit enables access to large amounts of publicly available textual data. Some other social media platforms, such as Facebook, theoretically also have a lot of data available, but in practice a large portion of it is visible only to one's friends on the platform or those who have joined any specific discussion group. In other cases, the data may be public but difficult to access in large quantities in practice. Furthermore, since Reddit is divided into topic-based subforums called subreddits, the data is naturally subdivided into subcategories by topic and by register (see, e.g.,

The analysis conducted by

Liimatta (2022b) performs a similar analysis but zooms in further to focus on a number of popular subreddits to find out whether all subreddits follow similar patterns, or if the same text length can have different functions in different subreddits. In the analysis, most subreddits analyzed are shown to follow similar patterns with each other. For example, the short comments in most subreddits contain more features which are more casual and involved, and longer comments contain more informational features. Similarly, comments of all lengths appear to be roughly equally narrative in all of the subreddits included in the analysis. However, a handful of the analyzed subreddits, which are more focused in terms of their topic in comparison with the very relative topics of most of the included subreddits, often differ greatly from both the general patterns and from each other. For instance, in the AskReddit subreddit, longer comments are much more narrative than the shorter ones, whereas for some other subreddits the opposite is the case. Figure

These results show that not only does text length play a role in linguistic variation, but that text length can be associated with functions differently within different register categories.

Solutions and workarounds

As the problems of text length and short texts have been recognized, a number of solutions and workarounds for it have also been devised. In this section, I will cover some of these approaches, and some related methods. Some of these approaches help solve or work around the problems caused by variation in text length, some the problem of short texts, and some can help alleviate the effects of both. Additionally, I will describe a closely related problem, that of measures of lexical diversity. While the solutions to the problems with lexical diversity measures are not directly applicable to the problem of text length, they may still provide inspiration and starting points for new ways of approaching the problem of text length.

I have divided the solutions and workarounds to the problem of text length and short texts into two main categories. In the first group of approaches, the original set of texts is manipulated in some way, after which standard methods are applied. These could also be considered the more "traditional" approaches to the problem. Their advantage is that they are simpler to implement, but this means that their downsides are often greater. Conversely, the approaches in the second group make use of various statistical and/or computational methods to see the existing data in a new light. These approaches are more complicated to implement and often only work for specific kinds of analyses, but they are much more powerful in the situations for which they are well-suited. These two groups of course overlap in practice, and methods within and between the groups can even be used together.

Manipulation of the data

Exclusion

A commonly used workaround for the problem of short texts is to simply exclude all texts shorter than some threshold from the analysis. For instance, we might simply choose to remove all texts shorter than, say, 400 words, 500 words, or 1,000 words from our dataset. If the aim is to be able to include as much of the data in our analysis as possible, this approach is at its most reasonable when there are only a small number of outliers under the chosen length limit, as the exclusion of a handful of outliers do not affect the overall results from a good-sized corpus very much. It could even be argued that clear outliers do not even represent the varieties of interest in the corpus particularly well, and that therefore it would even be beneficial to exclude them.

However, the larger the proportion of the texts in the corpus which fall under the chosen length limit, the more problematic the exclusion method becomes. Particularly when typical texts from the shorter end of the length range start to be excluded from the analysis in addition to obvious outliers, it is clear that the dataset starts to lose some of the information potentially available within the data. Of course, it is not obvious where the line should be drawn when considering whether a text of a certain length should be considered an outlier, but from the point of view of the data, the best practice would be to have the length limit as low as possible. The optimal cutoff length when using the exclusion approach would be low enough that as many texts as possible are included in the analysis, but high enough that the desired analysis is still possible to conduct reliably. For a slightly more statistically-based approach than simply choosing some round number such as 400 or 500 as the limit, it is also possible to define the cutoff point as, for example, the 1% quantile of the length distribution, or whichever percentage gives a length limit which is workable with the chosen methods and the research questions being investigated, since this helps quantify the amount of data which has been left out.

However, there are datasets for which the exclusion approach is utterly unsuitable. For instance, most social media postings are very short, and therefore would need to be excluded from the analysis under any commonly used cutoff length which would allow analysis of the data using typical analysis methods. Consequently, different solutions and workarounds to the problems of short texts and variation in text length need to be used when dealing with such data.

Combining

In situations where discarding any data is undesirable, another workaround for the problem of short texts is available. In many studies working with, for example, social media data and other genres which have a relatively large proportion of shorter texts, texts deemed too short to comfortably conduct the intended analysis on are combined to create new "texts" which are sufficiently long for the analysis. For instance, we could decide to combine texts so that each of the combined texts is over some length limit, such as 500 or 1,000 words. As with the exclusion approach, the desirable length depends on the methods being used for the analysis and the research questions being investigated.

The main upside of the combining method, when compared to the exclusion method, is that no data is completely ignored: all text available for the analysis is included in the analysis. However, the downside is that by combining texts together, the texts lose their individual nature. For example, if one text is highly edited in style, and another one is highly casual, combining them together results in a loss of a lot of this information, and makes the combined text look somewhat average on both counts. In this way, the combining approach to the problem of short texts may very easily blur out some of the variation in the data. On the other hand, it is also possible that texts may end up combined in such a way that the resulting dataset overstates the importance of some feature which is actually quite rare overall, for instance, if a feature is highly frequent in a small number of texts. The combining approach also easily results in a violation of the "independence assumption" inherent in various statistical procedures, including those commonly used by corpus linguists, such as Chi-square testing

The issue of blurring out or overstating variation can in some situations be mitigated by the choice of the basis of combination. If the texts which are combined are chosen in an essentially random manner, as is often the case, these potential obscuring effects cannot be reduced. However, in many cases it is possible to use a more principled basis for the combining. In the simplest case, the texts are combined based on some metadata in which we are interested in our analysis. For instance, if the analysis focuses on texts written by different sociolinguistic groups, any combining of texts needs to be done by the sociolinguistic groups in question. This, however, is done out of necessity, and it does not really help to reduce the blurring of variation taking place within the groups. For example, if we are comparing personal letters and official letters, we of course need to combine the shorter texts separately within the two categories, personal letters and official letters. But even in this case there may be variation within these two categories which gets either blurred out or overstated. In order to lessen the blurring and overstating effect, it might be useful to consider combining texts which are as similar as possible in their production circumstances, as far as reasonably possible. What texts exactly are considered "similar" is however a question which depends on the dataset and research questions. As a rule of thumb, however, the highest number of matching or similar metadata field values might be a good starting point.

Raising the level of analysis might be considered a special case of the principled metadata-based combining approach. For instance, instead of studying individual classified advertisements, we might consider the entire classified advertisements section a single text for the purposes of our analysis. Or instead of focusing on individual social media comments, we might choose to focus on full comment threads. The line between raising the level of analysis and the more general approach of principled metadata-based combining becomes blurred, however, in the case of, for example, combining Twitter tweets with their replies together to form individual texts.

Chunking

A different, slightly less-used approach to dealing with variation in text length is the opposite of combining shorter texts together: to cut longer texts into shorter pieces of (near) equal length. For instance, Hiltunen and Tyrkkö (2019) make use of this approach when studying Wikipedia articles, which are extremely variable in length, by dividing the articles into 200-word pieces for their analysis.

When using this method, the fact that all texts included in the analysis are of (roughly) the same length facilitates their comparison using feature counts or rates of occurrence, since the confounding effects of variation in text length have been diminished. At the same time, all of the textual information is included in the analysis and not discarded, even if it has been cut into smaller pieces.

Texts can be split up in various ways. A straightforward approach is to simply split a text into chunks of a certain number of words. However, since sentences are a basic structural unit of language, placing chunk boundaries at sentence boundaries, making sure that every chunk includes enough words, is likely to be a more desirable solution in many cases. Another solution, which keeps the structural and discourse units of a text together even more, is to divide the text into its paragraphs, or multi-paragraph chunks.

In addition to the simple chunking options above, chunking can also make use of various computational methods to create chunks which are meaningful in terms of the discourse structure. For example,

The chunking approach may or may not help with the problem of short texts. It would be difficult to meaningfully divide the longer texts into chunks of equivalent length if the shortest texts in the dataset are very short, such as on social media. On the other hand, if the shortest texts are still of reasonable length, dividing the longer texts into chunks of similar length might actually make them more easily comparable.

Computational and statistical approaches

Lengthwise analysis

In order to make feature frequencies more comparable across text lengths, Liimatta (2020) proposes a family of methods called lengthwise scaling. Closely related to the lengthwise analysis described above, this family of methods is also based on the idea that it is trivial to compare texts which are the exact same length. In lengthwise scaling methods, feature counts in each text are first compared against texts of the exact same length ("intra-length comparison") using some suitable method of comparison. Based on the results of this comparison, each text receives a new, scaled value, which is a representation of how typical the text is in terms of the range of variation seen in all texts of the exact same length. These scaled values can then be compared between text lengths like normalized frequencies would be, such as by visual exploration of graphs or by using some further statistical or computational analysis.

While the idea behind lengthwise scaling can be applied in various ways, Liimatta (2020) demonstrates the method family with two specific implementations, lengthwise rarity scaling and lengthwise quantile scaling. In lengthwise rarity scaling, when computing the scaled value for a feature count, each feature count is compared against the full set of feature counts in texts of the same length. Each feature count is then replaced with the percentage of smaller feature counts in texts of the same length. In other words, the following question is asked for each text: "what percentage of all of the texts of the same length as this text has fewer instances of this feature?"

Of the two implementations, lengthwise rarity scaling is noted by Liimatta (2020) to be particularly useful for visual exploration of data. The advantages of this scaling method include the fact that it particularly highlights smaller differences in rates of occurrence within the data, making it easier to pick up on subtler differences between groups of texts, and that it scales the observed variation into a constrained range between 0% and 100%, facilitating the graphing and interpretation of the results.

Figures

Since lengthwise quantile scaling is based on the median and certain quantiles of the data, the -1 and 1 lines are particularly useful for the interpretation of the data in terms of recognizing texts with uncharacteristically high or low feature counts for any given text length. On the other hand, in contrast with lengthwise rarity scaling, lengthwise quantile scaling does not confine the scaled values into any particular range. It also does not highlight smaller differences as well as lengthwise rarity scaling. However, thanks to its basis on common statistical measures, lengthwise quantile scaling may be the better of the two methods to use as a preprocessing step for further statistical or computational analysis.

The main downside to both lengthwise rarity scaling and lengthwise quantile scaling is that they require a very large dataset, so that there are enough texts of every individual length to make it possible to compare texts of the same length. Such datasets are most readily based on social media and other online sources. However, if the dataset mostly contains longer texts, even a slightly smaller dataset will do if texts of adjacent lengths are binned together. If the dataset is even smaller still, and/or includes shorter texts as well, the two methods may not work too well. But these two methods are only two potential implementations of the lengthwise scaling method family. In situations where the dataset is relatively small and includes a large number of shorter texts, other kinds of implementations of the lengthwise scaling method family may work better. For instance,

However, even this method is unlikely to work with the smallest corpora and the shortest texts, which do not have enough text for each text range to estimate the population parameters with any reliability.

Multiple Correspondence Analysis

There also exist methods for specific purposes which can be used with shorter texts. For instance, factor analysis methods, such as those used in the multidimensional method of register analysis, rely on feature frequencies, and as such the methodology is difficult to apply to genres which include a large proportion of short texts. In order to get around this issue in their multi-dimensional analyses of Twitter tweets,

However, while MCA works well with genres with only short texts, it cannot be used with datasets which include longer texts. This is because the longer a text becomes, the more likely it is to include any given feature. As the texts get longer, more and more of the features of interest start appearing in every text. Due to this, the co-occurrence patterns end up saturated when analyzing longer texts, rendering the method unusable with such texts.

Resampling methods

Resampling methods are powerful statistical methods which "make the best use of the available data"

Resampling methods have been used in various studies of linguistic variation. They can be used simply to estimate the rate of occurrence together with its confidence intervals, or to enable analysis in situations where using the standard method of normalization is difficult (e.g.,

A related problem: Lexical diversity

While the effects of text length have generally speaking not been studied very much in corpus-linguistic research, there is a group of measures, whose relationship with text length has received some more attention: the type-token ratio and other measures of lexical diversity (or "lexical richness"). While the type-token ratio differs as a measure from the typical calculated normalized frequencies, its relationship to text length still bears discussing in this context.

Like its name suggests, the type-token ratio is the ratio of the number of different words in a text (types) to the number of all words in the text (tokens). This ratio is notoriously sensitive to variation in the length of the text it is calculated for. Due to this sensitivity, for the results to be comparable, the ratio should be calculated for texts of almost the exact same length. However, since all texts in a normal-sized corpus are rarely close enough to each other in length, as a typical workaround, the ratio is calculated for a set number of words (such as 400 words) taken from the beginning of each text. While this workaround has been used for a long time to good effect, it is also not optimal, since in many cases it excludes a large majority of the text from the calculation. The solution is a lot less optimal still for datasets with a lot of variation in text length, since the 400-word sample covers a different fraction of each text, which means that every text is represented differently by the sampling.

Due to these problems, and the fact that being able to measure lexical diversity in a meaningful way would be very desirable for many linguistic questions, the question of whether a method which is less sensitive to text length could be devised has received a decent amount of attention from corpus linguists and others.

The problem of lexical diversity measures is closely related to the problem of text length and short texts in focus in the present study. While the efforts to develop a measure of lexical diversity which is less affected by text length do not directly target the problem of text length and short texts, the implication of these efforts is clear: methods which lessen the confounding effects of variation in text length can be developed. Maybe some method created for the purpose of measuring lexical diversity could even be adapted to help with the problem of text length in feature frequencies.

Conclusion

The present chapter has discussed two related problems, the more general problem of variation in text length and the more specific problem of short texts. While these problems have not received as much attention than they could have from quantitative corpus linguists (as evidenced by, e.g., the body of research on measures of lexical diversity), the difficulties caused by the confounding effects of text length are only going to become more central to many studies, as more and more research is being done on social media and web data.

A number of solutions and workarounds to remedy the problems have been devised, all with their own advantages and disadvantages. These solutions can be used to good effect in many kinds of linguistic investigations. However, there still is no one-size-fits-all solution to the problems caused by text length and short texts in quantitative text-analytic corpus-linguistic studies. Some potential avenues for improvements and new method development have been proposed in the present chapter.

Since resampling methods are very powerful for estimating the distribution based on smaller datasets, they appear as a potentially useful avenue for the development of new methods for the analysis of texts across text lengths. At the same time, larger datasets contain more information about the variation inside them, so various approaches making use of the large size of the data, such as those developed by

Even if a perfect all-encompassing solution does not exist yet, or is not possible at all, the solutions mentioned in this chapter can still be used to study many linguistic questions, given that one is aware of the potential implications of their use. There certainly exist many other approaches not mentioned here, particularly various more advanced statistical and computational methods, which are less affected by variation in text length. Nevertheless, there is still a lot of room left for the development of new ways to analyze datasets with a wide range of text lengths, and particularly datasets which contain extremely short texts, which are more common today than ever.

Introduction

Fiction is inherently messy to work with. This is not due to the material itself, but rather due to the field that surrounds it and the needs of different theoretical approaches to the reading of the materials. Since

The concept of special corpora, defined by Tognini-Bonelli (2010: 13) as corpora where the selection is not made to be representative of a language but of a specific use-case, such as the learner corpora available in the International Corpus of Learner English (ICLE)

As the interpretation of data begins, further questions regarding the categorizations arise, often to do with genre and style, and one must consider whether the American author active during the 1920s to 1960s was a modernist and so forth. Consequently, comparing the fiction of one author to that of any other author matching the temporal and spatial categorization becomes problematic. It becomes more of an issue when presenting to an audience mainly engaged with other aspects of the author's work rather than the "when and where" (as attempted in

As the intersection between linguistics and literature becomes more popular and more populated with resources, it becomes important to discuss how these uses of corpora as contrastive, or comparative resources beyond language variants and variation could look. How could this new arena influence our categorization habits, and what are the consequences of deeper categorization of fictional texts? This chapter highlights genre categorizations as a pitfall at the intersection of corpus linguistics and literature and problematizes the use of the genre category tag from the perspectives afforded by both fields. The chapter aims to contribute towards a more explicit communication of our genre categorization practices, and avoidance of miscommunication and confusion due to the genre term being understood differently within different disciplines and backgrounds.

Looking up from the pit

I have presented papers using reference corpora to distinguish author-specific traits on several occasions (for instance, Ihrmark 2018 and Ihrmark 2019) and regularly received questions specifically about the issue of the comparative aspects. One paper was presented to the Hemingway Society in 2018, and compared sentence lengths, noun distribution and lexical density in Ernest Hemingway's writing, using the COHA fiction category as reference. The reference corpus was further limited to include only materials produced between 1900 and 1960 to correctly match the time period. One line of questioning after the presentation was especially interesting for the current paper: First, was my selection of reference materials appropriate? And, second, what should be considered appropriate materials for distinguishing features specific to Hemingway?

Regarding the first question about whether my selection was appropriate or not, I had approached it from an admittedly simplistic perspective. My line of thought had been that the texts belonged within the same category as they were all fiction, and that they were produced during the same time period. In retrospect, the temporal aspect had likely played too large of a role in my justification, and more time should instead have been spent considering the nature of the content. While the reference corpus did offer sub-categorization of the fiction component of the corpus, aligning those sub-categories with Hemingway's oeuvre is not a straightforward task. Partially this has to do with the sub-categorization of the reference corpus in question, but a more prominent issue is the fluctuating idea of Hemingway's genre belonging throughout his work, such as his short stories

Moving on to the second question: what should be considered appropriate materials for distinguishing features specific to Hemingway's writing? The way we end up in the pit, to me, begins with what we consider as "specific to an author's writing". From a general standpoint, one could do as I did and simply use fiction in general as the reference, and asking the question of what makes this author stand out amongst a sizeable sample of other authors active during the same period. However, this leaves the comparison vulnerable for criticisms regarding the oversimplification of fiction, as distinguishing what is specific to an author from what is specific to their peers can be done on many levels. Continuing with Hemingway as an example, it could for instance be operationalized as asking what the differences are between Hemingway and other authors belonging in the Lost Generation of post-war American expatriates, other authors who write about war, other authors sharing his journalistic background, or other authors connected to Gertrude Stein or to Sherwood

The initial rationale for applying these two categories was that they were easily applied with an acceptably high consensus for a majority of the authors based on other descriptions of the Lost Generation, and that they provided a clear, functional separation of texts. This allows for the corpus to be used for comparisons between the authors short stories and novels, as well as comparisons between different authors considered to belong within the same group. However, the corpus can only be used for questions based on comparisons within the Lost Generation in a broad sense, and it does not allow the user to perform comparisons along other lines of inquiry, for instance, carrying out comparisons between different genres of fiction. So far, the categorization is fairly intuitive and creates no overlapping tags, that is, a text is never both a short story and a novel.

From a qualitative perspective and using a close-reading method, as is often the case in literature research, the use of overlapping genre categorizations can provide a way of connecting the content and style of the text being read to different movements or periods from which the author might have drawn their inspiration or within which they might have been an active participant. The genre features found in a piece of fiction can also be used to extend an argument regarding the intertextual properties of a text in such a way that the intellectual context of the writing can be connected to the written text. An example of this could be the use of genre tropes to create connection through allusions to other texts or even direct references to previous works that add to the narrative being conveyed, for instance a character coming across a different text within the story and having it influence the narrative. Genre plays a role in these examples as it draws on the expected previous reading of the audience, their context, and readers of one genre can more often be expected to be familiar with other texts of the same genre. 1  This is a very different use-case than that of genre categorization as a sorting mechanism for achieving portioning of large datasets, which would often be the purpose of genre categorization of literary materials or fiction in the distantreading approach commonly used within corpus stylistics or digital humanities

Turning briefly to the world of fiction in film, combinations of genres have been shown to be viable from a commercial perspective. Star

the intuitive taxonomy inferred by genre labelling in fiction could be when applying genre as a sorting mechanism.

Several novels are highlighted as examples by

Text genre categorization in literature

Let us first start by discussing genre as a literary concept.

Todorov's description emphasizes the influence of conventional choices on the structure of a piece, in this case a sonnet, but the concept of choices made conventional by society is an interesting one for the discussion carried in this chapter.

Seeing the literary text as a social event adhering to (and understood through) the expectations of the receiving audience does connect well to intertextual ideas of literature, for instance, the work of Barthes proclaiming the death of the author as the birth of the reader

Genre being a theoretical minefield does not begin in the 1990s, however, as Levin points out in his 1984 review of Fowler's Kinds of Literature: An Introduction to the Theory of

Text genre categorization in linguistics

It is important to start out with a distinction between text type and genre, as the terms occasionally fulfill a similar function

The use of genres as a sorting mechanism also has practical roots amongst linguists, as

Genres have also been defined from a more theoretical perspective within linguistics, some of which see genre as being dependent on the intended social context of meaning-making. Based on that conceptualization and the earlier work of

Paltridge explores both text type and genre in the classroom setting and indicates a division according to generic structures and text structures, with the former being tied to genre and the latter to text type

Turning to corpora, text categorization often takes place first at a higher level, where genre can, for instance, be defined as "Fiction", as is seen in the widely used COHA or the COCA corpora. The two corpora then provide further granularity by introducing sub-categorization. In the case of COHA, these sub-categories are explicitly tied to the Library of Congress taxonomy of non-fiction and academic materials (Figure

The genre category pitfall

To approach the potential genre category pitfall, this paper will first consider the position of corpus linguists and literature scholars based on the previous sections. In general, their positions on genre could be described to as defined by their relationship to the text as an object.

The components highlighted as interesting from a linguistic perspective are features such as style, intended readership, intended function and the context in which the text is intended to fulfill a communicative purpose. The different purposes of the text are also of interest, especially when seen in relation to the other features. The position could be oversimplified as being one that is interested in the purpose and form of a text, and often focused on the intersection between the two. The occasional overlap between the idea of a genre and that of a text type serves to further muddy the waters in linguistic usage of the genre term.

The perspective afforded by the field of literature, on the other hand, seems more interested in the intersection between style and content. Style is close to the idea of form as used in the definitions from the linguistics side of things, but it brings with it quite different connotations (see Aquilina 2014 for a thorough discussion). While style in linguistics can be discussed as having to do with formality, register and fitness-for-purpose, style in literature takes on an intertextual meaning connected to the idea of a genre. In addition, genres are often defined by their content to a higher degree, as the setting is used for genre descriptions in genres such as science fiction or westerns, whereas style and plot can play a larger defining role in genres such as horror or fantasy.

Returning to the Star Wars dilemma, or the issue of expectations tied to the genre term amongst different audiences raised by

Genres as a mode of categorization in corpus resource creation vis-à-vis use as descriptive tags used for fiction should also be considered an issue in the same pit, as the expected rules governing the use are different in nature. Overlapping genre descriptions in literature or fiction are not considered an issue. In fact, the combination of genre tropes and features within a piece can often be seen as a strength, as indicated by

Turning from the issues to do with the positions of the audiences to the practice-oriented issues of the intersection between corpus linguistics and literature, there must first be an understanding of which tasks are performed therein.

However, the projects in which these tasks are performed often deal with determining what is especially characteristic for a specific author or a specific text

From a distant-reading linguistic perspective, the categorizations relying on text-internal features, such as the one presented by

From a close-reading literature perspective, the resulting category "Novels" becomes too broad to be useful. It does not influence the close-reading method in a meaningful way and does not indicate information that could tie the text(s) to a broader intellectual context. Instead, a more useful taxonomy would be tied to fiction genres, allowing the researcher to connect the texts to their contexts and content more clearly.

3.

As pointed out by one of the reviewers, this could also lead to circularity if the texts are later explored for linguistic features tied to the genre. For example, if the genre "Reports" is defined by text-internal features, those features would be very frequent within the resulting category.

For those stuck between linguistics and literature, it becomes a decision regarding the intended audience of the corpus. In the case of a resource created for the linguistics community it would make sense to include both the text type and the broader genre, as both have a theoretical background through which results could be connected to previous and future research, as well as inform about the nature of the item in an objective way.

It is also important to consider which methods are going to be applied, as the methods relying on comparisons exemplified by

Returning to my own pit for a moment, the use of the "Novel" and "Short story" categorization in the Lost Generation Corpus starts to appear more palatable to me than it was earlier. While very broad categories, they still provide some clear idea of their content in terms of it being fiction of a certain length, albeit with a large helping of internal variation as exemplified by

Conclusion

The main conclusion drawn is that fiction is difficult to categorize according to genre for corpus compilation, and especially so when trying to communicate useful information to two distinct fields of research with complex theoretical backgrounds connected to the term. In practice, this means that work taking place at the intersection between literature and linguistics must be explicit about how the term "genre" is being used, and what exactly is meant by it. The previous research referred to in this chapter could be said to highlight the communication taking place implicitly through the use of the term "genre" as the main culprit through its setting of different expectations depending on the audience.

However, the connection between the approach to genre labelling and the methods being applied provides a concrete bridge across that rift. Considering genre categorization of fiction as a part of the methodology in a study could open up for both an explicit argumentation regarding why one has decided on the labelling being implemented, as well as a clear communication to the reader about what is actually intended by the labels. Moving the act of genre labelling for increased granularity to the methodologies of individual studies according to their specific needs, the broader categorizations make a lot of sense due to the corpora then having a wider applicability. In summary,

Modeling fine-grained sociolinguistic variation

The promises and pitfalls of Twitter corpora and neural word embeddings

Stuttgart

This chapter examines the use of recent data sources and computational methods to study fine-grained sociolinguistic phenomena. We deploy a custom-built corpus of tweets

Introduction

In this chapter, we deploy a novel corpus-based approach to the study of a complex type of language variation. Our focus is on contact-induced semantic shifts in Quebec English, that is, preexisting English words used with a meaning typical of a similar French word. Consider the following example taken from a tweet posted by a speaker from Montreal:

(1) I really want to go to an art museum or an art exposition.

Here, the word exposition refers to what is usually known as an art exhibition. This meaning is not conventionally used in English; it is instead associated with the homographic French word exposition.

This phenomenon is explained by the local sociolinguistic context: Quebec is the only predominantly French-speaking Canadian province. As of 2021, 74.8% of its inhabitants -close to 6.3 million people -report that their mother tongue is French. Ten times fewer Quebecers -7.6% of the population, or just under 640,000 individuals -are native speakers of English

Our analyses rely on a particular type of linguistic data -a large, custombuilt corpus of tweets -as well as a recent computational approach to modeling lexical semantic phenomena -neural word embeddings. Large-scale computational studies of language variation increasingly rely on both of these methodological choices

More specifically, we analyze patterns of regional variation in our corpus of tweets; drawing on underlying demographic distinctions, we aim to isolate instances of contact-induced semantic shifts and further characterize their use. For a given lexical item, our method produces computational representations of its occurrences in the corpus and splits them into groups based on semantic similarity, prioritizing those that are expected to reflect the influence of French. Despite extensive data filtering and a carefully adapted implementation of a recent neural language model, our approach highlights not only contact-induced semantic shifts, but also a range of noise-related phenomena. While this means that it cannot provide reliable results in an unsupervised manner, we show that a coarse manual annotation -conducted on the automatically identified clusters of tweets rather than individual occurrences -provides an efficient way of eliminating false positives. Complementing an earlier analysis of the technical impact of these issues

The remainder of this chapter is organized as follows. We first introduce a summary of related work (Section 2) and a more detailed description of the data and methods we deployed (Section 3). We then present the key results of our analysis (Section 4) and conclude with a discussion and main takeaways (Section 5).

Theoretical and methodological background

In this section, we contextualize our work with respect to research on semantic shifts in Quebec English, which represent our descriptive focus. We further discuss our key methodological choices, thus addressing the use of Twitter corpora and neural word embeddings.

Semantic shifts in Quebec English: The need for corpus studies

The use of English in Quebec is influenced by ongoing contact with French, particularly on the lexical level. Evidence for this claim comes from sociolinguistic studies (e.g.,

These observations, however, are mainly based on studies of French loanwords; descriptions of other types of contact-related lexical influence are considerably more limited. This is particularly the case for the previously mentioned issue of semantic shifts, on which we focus in this paper. We understand this phenomenon as the presence of a sense in a preexisting English word that is explained by the presence of the equivalent sense in a formally and/or semantically similar French word. We know from

Most of these studies rely on traditional sociolinguistic methods, which involve recording the speech production of carefully sampled speakers using face-to-face interviews

Twitter-based corpora for language variation

The development of social media has enabled large-scale analyses of language variation relying on publicly available posts from these websites. This is particularly true of Twitter, a social network created in 2006, where users can post 280-character messages known as tweets.

The sheer amount of data available on Twitter is routinely presented as a key advantage compared to traditional sociolinguistic studies. However, this is counterbalanced by issues such as a lack of reliable demographic informationa mainstay of sociolinguistic research -as well as sources of bias inherent to the platform, affecting key information such as user location

Vector space models for lexical semantic variation

As previously suggested, the persistent challenges in systematic analyses of lexical semantic variation -in sociolinguistic studies in general

Methods such as these constitute the cornerstone of recent computational approaches to semantic change. Starting from a diachronic corpus, different VSMs have been used to quantify the change in meaning of all words in a corpus (or any subset of them) over time (see

However, most of these studies focus on computational issues. Existing descriptive applications include assessing longstanding hypotheses on semantic change

Data and method

Our approach relies on contrasting synchronic data from different Canadian regions, under the assumption that linguistic behaviors that are specific to Quebec but absent from areas where the use of French is limited, are likely to reflect the influence of language contact. This is inspired by the comparative sociolinguistic approach

A corpus of tweets

We use a previously created corpus of Canadian English tweets published by users from Montreal, Toronto, and Vancouver

The data were collected from January to November 2019. We initially used Twitter's Search API to look up tweets tagged as written in English and associated with the geographic area of one of the target cities. The users identified in this way were narrowed down to those whose free-text profile location strictly corresponded to one of the three target cities. We then crawled their profiles, collecting up to 3,200 most recent tweets per user; this allowed us to increase the amount of data and obtain basic sociolinguistic information. For instance, we stored the distribution of language tags in the users' tweet production and subsequently used it as a rough estimate of their degree of bilingualism. Finally, we only retained the tweets tagged by Twitter as written in English, and we automatically removed near-duplicates posted by individual users. Exploratory analyses have shown that the retained data are both specific to the target cities and comparable across them (for more details, see Miletić et al. 2020). 3  In addition to the preprocessing decisions applied to the original corpus, we introduced additional filtering for the experiments presented in this paper. First, we removed the content posted before 2016 in order to limit the likelihood of picking up diachronic effects; the tweets in the original corpus date back to 2006. In determining the cut-off point, our aim was to find a reasonable tradeoff between a reduction in time span and the remaining amount of data. We then 3. In accordance with Twitter's developer terms, the corpus is released in the form of tweet IDs, which can be used with off-the-shelf software to collect the underlying data: 〈http

In identifying the semantic shifts, we relied on descriptions provided in the literature on Quebec English

ically identify occurrences used in similar contexts and quantify sense distributions, focusing on both regional and user-level patterns.

Neural word embeddings

For each of the 40 lexical items from Section 3.2, we first produced word embeddings for their individual occurrences. Each corresponds to a slightly different vector, which incorporates general distributional information captured during model pretraining and is further informed by the target item's immediate linguistic context. We then used these representations to automatically group the occurrences into clusters, which were expected to reflect similar contexts (and thereby similar uses of the target lexical item). This allows for a more efficient subsequent analysis of the full range of uses exhibited by a lexical item: for instance, the fact that similar occurrences are grouped together means that it is not necessary to disambiguate them one at a time.

Word embeddings were produced using the previously discussed BERT model, and specifically the Hugging Face implementation

For each analyzed lexical item, we extracted the tweets in which it appears in all three regional subcorpora. In order to limit processing and memory requirements, we retained no more than 1,000 total occurrences per word and used a random sample for more frequent items. We fed each tweet in its raw text form as a single sequence into BERT, which then produced context-informed vectors for each token in the tweet. The model outputs multiple vector representations per token, each corresponding to a different hidden layer in the neural network architecture. Similarly to other recent studies (e.g.,

Clustering and annotating the uses of a lexical item

Similar uses of a lexical item were automatically identified by clustering its tokenlevel vectors using affinity propagation, an algorithm which performed well in other semantic change studies (e.g.,

In analyzing the output of the analysis, we considered the clusters containing at least five tweets, and retained them if more than half of the tweets were from the Montreal subcorpus. This is because of the focus on the uses which are clearly 7. Figure inspired by Jay Alammar's illustrations available at 〈http

More specifically, a target item's use in a cluster was annotated as contactrelated if it was regionally specific to Montreal and potentially explained by the influence of a formally and/or semantically related French word. This determination relied on the same evidence used to select the target set of semantic shifts, that is, previous sociolinguistic studies and lexicographic sources (see Section 3.2). Recurrent phenomena that were not annotated as contact-related included a range of noise-related issues; these will be discussed in more detail below. A 15-word sample (91 clusters) was annotated by two annotators in order to test the reliability of the general procedure, obtaining a reasonably high interannotator agreement (Cohen's kappa coefficient of 0.55).

On average, 8 clusters per word (min = 3, max = 10) were retained for annotation. The mean number of tweets per cluster stands at 13 (averaged over the means for individual lexical items; min = 8, max = 20). As shown by the examples discussed below, the clusters are largely homogeneous; although some are occasionally difficult to interpret, this is overall rare. Our analysis also allows for a degree of uncertainty, as the annotation targets the predominant use in a cluster. The utility of this approach is confirmed by the fact that it led to the identification of at least one contact-related cluster for each of the 40 target items. From a practical standpoint, using cluster-level annotations was an order of magnitude faster than analyzing individual tweets. This is due to the lower number of required decisions and the comparative ease in determining the meaning of a larger number of similar examples appearing together.

Results

This section discusses the results derived from the annotated data. It first presents a general overview of cluster types across lexical items; it then illustrates a range of true and false positives observed in the data; and it concludes with a case study examining the link of contact-induced semantic shifts with bilingualism.

An overview of regionally specific clusters

A global overview of the analysis (Figure

Types of variation captured by the analysis

This section discusses examples of tweets extracted from our corpus using the clustering analysis described above. Sample clusters of tweets are presented in the keyword-in-context format, for ease of reading as well as to illustrate the effect of this approach on manual perusal of corpus data, as observed during the manual annotation. Each sample cluster contains three representative tweets published in Montreal and occurring in a single original cluster output by our analysis. Further information on the size and regional composition of the clusters is also provided.

In order to protect the privacy of tweet authors, we only reproduce textual content without any metadata. For the same reason, usernames, hashtags, URLs, and names of individuals are redacted from the tweets, except for widely known public figures or if necessary to interpret the meaning of the tweet.

True positives

We begin by examining positive contributions of our computational system, focusing on the help it provided in distinguishing between conventional and contact-related uses based on documented patterns from the corpus. This was beneficial across different semantic mechanisms and degrees of granularity of contact-related influence.

A clear-cut distinction

Perhaps the prototypical mechanism underlying contact-induced semantic shifts involves using an English lexical item to denote a referent conventionally designated by a formally similar French lexical item. One such example is manifestation, which is generally used to signify 'a display of the existence of something' , but is also attested in Quebec English with the sense of 'protest, demonstration' , typical of the homographic French lexical item manifestation. This sense is absent from the Canadian Oxford Dictionary (COD), but it is anecdotally reported by

False positives

We have so far focused on the informativeness of our semi-automated analysis in understanding often fine-grained patterns of contact-related language use, but this process was complicated by different types of false positives. This section provides a detailed analysis of the most frequent patterns that we encountered. We distinguish between the following types of locally-specific word usage which do not constitute contact-induced semantic shifts:

-cultural effects, where word usage is related to the local cultural context of Montreal; -the use of common nouns as proper names denoting locally specific referents; -French homographs of English words, attested in codeswitched tweets; -structural patterns, such as the position of the target item across tweets, which accidentally affect model performance.

For each category that we discuss, sample clusters of tweets are provided in order to illustrate the contrast between the contact-related use -the target of our analysis -and the noise that we identified along the way.

Cultural effects

The regionally specific character of some clusters output by our analysis is not related to the use of the target lexical items with a French-related sense, but rather to the local cultural context of Montreal. Take for example formation, whose English senses include 'the action or process of forming' and 'arrangement or disposition' . In our data, it is also attested with the sense of 'course, training program' , typical of the French homograph formation. This sense is absent from the COD and the OED, but its existence is noted in the sociolinguistic literature

A different but related effect was observed in the case of animator. This lexical item is generally used with the sense of 'creator of animated films' , whereas the formally similar French equivalent animateur also includes the sense 'group leader; organizer; facilitator' . The use of animator with the former senses is attested in the sociolinguistic literature

The goalie formation is solid . even if Price gets injured , Montoya the contact-related use, but these were limited to a single cluster; the remaining eight regionally-specific clusters reflected the conventional sense (Table

Proper names

The regional specificity of some clusters is explained by the target lexical item being used as a proper name, generally to denote a regionally-specific referent. Take for example deception, which in English refers to 'the action of misleading someone' , but whose French homograph also means 'disappointment' . This use is not recorded in the OED or the COD, nor is it described in the sociolinguistic literature we reviewed.

Deception Bay , the title track from @milknbone's

The new song Deception Bay , from Milk & Bone's second album , is Deception Bay on repeat !! Can't wait for the whole

French homographs in codeswitched tweets

The performance of our computational system is occasionally affected by crosslingual homographs of the target lexical items. They are generally used in a span of French text within a codeswitched tweet where most tokens are in English; this explains why the tweets were tagged as written in English and retained in the corpus. Codeswitching is overall rare in our corpus, but its relative frequency is considerably higher in Montreal, as can be expected given the prevalence of bilingual speakers in the city.

The practical implications are illustrated by the case of souvenir. Our analysis focused on the conventional English sense 'keepsake, memento' and the potential presence of the more abstract sense 'memory' , typical of the formally identical French equivalent. The use of the English lexical item with the French-associated sense is not recorded in the COD. It is however attested in the OED, though only as "chiefly literary", as well as in the sociolinguistic literature

Structural patterns affecting model performance

A final recurrent issue is that of clusters where tweets appear to be grouped together based solely on structural regularities. This was observed in the case of trio, which conventionally means 'a group of three' , but is also used with the Francois Fournier a partagé un souvenir . 1 h • 7 years ago , i played my third gig with Old memories . Very old . Oh , les vieux souvenirs! <url> sense of its Quebec French homograph, denoting a 'sandwich-fries-soda special, combo' . This specific use is not attested in the lexicographic sources we consulted, but it is described in the sociolinguistic literature

Deploying coarsely annotated data for linguistic description

The structure of the clusters output by our analysis shows that lexical items differ in terms of the diffusion of contact-related usage (how many tweets are related to contact, out of all those retained in the regionally-specific clusters) as well as its regional specificity (how many tweets in contact-related clusters come from Montreal). These patterns may be indicative of different degrees and factors of diffusion of semantic shifts within the local speech community.

To explore the descriptive relevance of this information, we calculated scores reflecting the two points raised above for each of the 40 manually annotated lexical items: a diffusion score, corresponding to the proportion of tweets tagged as contact-related, out of all manually annotated tweets; and a regionality score, corresponding to the proportion of tweets posted in Montreal, out of all tweets tagged as contact-related. In order to explore the potential impact of the degree of bilingualism on the use of semantic shifts, for each lexical item we also calculated a bilingualism score, corresponding to the mean proportion of tweets in English (out of tweets in English in French) posted by users who used the contact-related sense in the clusters tagged as such. It ranges from 0 for users tweeting only in French to 1 for users tweeting only in English, with intermediate values indicating a production of tweets in both languages.

We first checked the relationship between the three scores by calculating Spearman's rank correlation coefficient. The diffusion score is uncorrelated with both the regionality score (ρ = -0.13, p = 0.42) and the bilingualism score (ρ = 0.02, p = 0.90). However, the regionality and bilingualism scores exhibit a moderate negative correlation (ρ = -0.53, p < 0.001); this link is explored in more detail in Figure

The plotted results indicate that contact-related semantic shifts which are more regionally-specific (i.e., attested in Montreal to a higher extent) are also more directly related to the effects of bilingualism (i.e., a lower proportion of English, and hence a higher proportion of French, tweets). A typical example (bottom right) is the case of circulation, attested in the Quebec English data with the sense of 'traffic' , which is associated with the corresponding French homograph. All of the tweets from clusters tagged as contact-related come from Montreal; moreover, the mean proportion of English tweets stands at 0.75 per user. This may appear to be a relatively high value, but it is in fact just above the 10th percentile for all users in the corpus (0.73); at least within this dataset, this is suggestive of a comparatively and the bilingualism score (y-axis). Dotted lines show the 10th and 20th percentile for the bilingualism score, for all users in the corpus important influence of bilingualism. Patterns at the other end of the spectrum (upper left) are illustrated by the verb remark; we focused on the sense 'notice' , with which the French verb remarquer is widely used. It is less regionally-specific (62% of contact-related tweets posted in Montreal) and less strongly associated with bilingualism (higher mean proportion of English tweets per user, at 0.99). Unlike in the previous example, however, the contact-related sense is attested in dictionaries, but the OED marks it as rare in some syntactic contexts. While it is likely accessible to most English speakers, cross-linguistic influence might facilitate its wider use; this scenario is consistent with our data.

It is also relevant to look at the outliers from the general trend. For instance, in the previously mentioned case of trio 'sandwich-fries-soda special, combo' (upper right in the plot above) all contact-related tweets similarly come from Montreal. However, the mean proportion of English tweets is higher, at 0.99 per user. This is indicative of a use which is regionally-specific, but is widespread in the local linguistic community, including among monolingual speakers. This is further sup-ported by existing descriptions which have shown it to be typical of the speech of native English-speaking Quebecers

These observations indicate that, barring some exceptions, the more region specific the contact-related use is, the more strongly it is associated with use of French. Once again, it is important to note that the manual annotation was conducted on the level of clusters, rather than individual tweets, meaning that some non-contact-related occurrences may have been included in the counts. Moreover, the information on the use of French has the benefit of being empirically grounded in the attested use of languages by individual Twitter users, but it is only a very rough approximation of their linguistic profiles; for instance, there is no reliable way to determine their native language. That said, our analysis identified clear trends regarding the use of semantic shifts based on a large amount of data, further confirming the potential that corpus-based analyses have in understanding the patterns behind complex linguistic behaviors. It also constituted the basis of a face-to-face sociolinguistic survey we conducted in January 2022, whose initial results confirm the overall relevance of our approach, but also highlight the distinct -and complementary -nature of corpus-based and in-person estimates of semantic change

Discussion and conclusion

We have presented an analysis of a fine-grained sociolinguistic phenomenoncontact-induced semantic shifts in Quebec English -using a large, custom-built corpus of tweets and a recent pretrained language model relying on a deep neural network architecture. This approach has paved the way for a more detailed account of previously reported semantic shifts, contributing extensive empirical evidence where original descriptions often consisted in a single anecdotal mention of a lexical item of interest; our approach was also beneficial in more comprehensively characterizing previously undescribed semantic shifts, initially observed in isolated tweets. The computational tools we used facilitated manual inspection of vast amounts of data, directing our attention to the most relevant subsets of occurrences; they also enabled broad quantitative estimates of the use of semantic shifts, highlighting possible interpretations and informing the design of subsequent studies. The results have also broadly confirmed our highlevel assumption that regional variation in synchrony can be used as a proxy for detecting contact-induced phenomena. More generally, the overall setup -data extraction, clustering based on semantic similarity, and analysis of the distribution of occurrences over an explanatory factor -can be generalized to other descriptive issues.

However, we cannot gloss over the fact that our computational system provided actionable results only once it was complemented with extensive manual analyses. The challenges that we encountered are related to several distinct issues: (i) a strong assumption on regional variation underpinning the methodological design -while some language use specific to Montreal is related to language contact, not all is; (ii) inherent limitations of the methods we used, with BERT occasionally capturing phenomena unrelated to lexical semantics; (iii) inherent limitations of the data we used, with a carefully filtered Twitter corpus representing an improvement on highly generic datasets, but still suffering from the 280-character limit and the limited ability to validate user descriptions, among other issues; (iv) the complexity of the phenomenon under study, which often involves very subtle -but nevertheless perceptible and socially meaningful -differences in language use. Some of the described false positives, such as French codeswitching and referents typical of Montreal, are specific to our corpus; however, they echo the observation that semantic change models capture different types of variation in word usage, also raised in other recent studies

Despite these challenges, data-intensive computational approaches to lexical semantic phenomena, and to language variation in general, have an important role to play in descriptive linguistic research. They can provide meaningful quantitative accounts of lexical phenomena, including for the whole vocabulary, based on data obtained in an unobtrusive way; this is clearly complementary to traditional sociolinguistic methods. While methods such as those we implemented still require adaptations to the task at hand as well as some manual analysis, they simplify the tasks required of the linguist. One example of this approach is our analysis based on coarse cluster-level annotations; its relevance is confirmed by the fact that, together with the results presented in

Funding

The computational analyses presented in this paper were carried out using the OSIRIM computing platform, administered by the IRIT research laboratory and supported by the CNRS, the Région Occitanie, the French Government and the European Regional Development Fund (see 〈https

D

M
Introduction

The marriage of corpus linguistics and social science seems, initially, straightforward. Much work in corpus linguistics has oriented itself towards real world problems, including in areas such as climate change research (e.g.

This paper explores these issues, looking, at an abstract level but illustrated through examples, at the interaction between corpus linguistics and the social sciences. We will begin by looking at epistemology, arguing that this is a major driver of corpus linguistics' integration with, or separation from, the social sciences. In doing so, we will outline our own epistemology and suggest a route that corpus linguistics may take in debates around epistemology in the social sciences. The route proposed should, in our view, maximise corpus linguistics' engagement with disciplines across the social sciences. However, we will also explore the varied nature of the social sciences, which is such that even a single discipline within the social sciences may exhibit significant internal variation in focus, theory and epistemology. We will see that such variation can militate for, or against, interaction with corpus linguistics. Throughout our discussion of epistemological concerns, we will note debates within corpus linguistics that echo these debates in the social sciences.

The paper then narrows in focus to look at a group of related areas in the social sciences that might have much to offer corpus linguistics. Following from that, we consider how data processing procedures in corpus linguisticsin terms of corpus mark-up, annotation and exploitationappear to be converging, to an extent, with those in (especially qualitative) social science. In terms of quantitative analyses, we observe how there is much in common between corpus linguistics and the social sciences anyway. Likewise, we also consider how social science theory is exerting influence on studies in corpus linguistics. We conclude by reflecting on the nature of evidence, falsification and corroboration in corpus use in the social sciences.

Epistemological fit

Since the mid-nineteenth century at least, the idea of dealing with the social in the same way as the physical has been pursued

There is, at least superficially, clear potential for alignment between corpus linguistics and naturalism. The approaches taken, for example, by researchers working in the field of social physics (see

A further reason we will turn from the approach taken by naturalism is that the results have been, with regard to the study of language at least, often produced in the absence of input from linguists. As a consequence, such results have been, at times, naïve and insupportable from the perspective of linguistics. For example, a team of Danish and Japanese physicists analysed dialect maps developed in Japan to explore the diffusion of swearwords across Japan over time

Studies in social physics, such as that by

In the third feature of naturalism, we find another stumbling block for the corpus linguist engaging with the social sciences. The call for methods which permit a positivist approach is where quantification comes in as, regarding a values free approach, it 'is commonly believed that this is achieved in natural science by the use of quantitative methods, so social science should, as far as possible, follow the same path'

The forced 'equivalence to quantitative criteria of "good" research practice'

Late nineteenth century Germany provided the arena in which the social sciences split between positivism, as we have explored, and conventionalism.

We might ask, what is the principal anti-positivist objection? Put simply, this rests on an examination of one of the features of positivism already discussedthat positivists generally believed that the world existed independently of the observer and that through observation one could come to know that world. Through valuefree, objective observations subsequently analysed using the scientific method, positivists believed one could come to certain knowledge of the world. The critique of positivism runs along predictable linescertainty in the social world is not a possibility; the social observer cannot be separated from the social object of inquiry, and this, in turn, gives rise to questions regarding the objectivity of the observations made.

While the terminology of the philosophy of science may be slightly alien to corpus linguists, then, the concepts are not. Moreover, it is advisable for corpus linguists to be cognisant of such concepts and the debates surrounding them, as these are likely to colour the perception that social scientists form of corpus linguistics. Indeed, in the literature we can see objections to corpus linguistics which follow the anti-positivist critique, most notably with reference to context and subjectivity. To begin with context, it is unsurprising to see this critique, as it is precisely the critique which spawned the qualitative/quantitative divide in the first place (i.e. the belief that the social can only be studied in a broad, potentially amorphous context of which the observer is a part). It is context that forms the thrust of

So how a corpus linguist positions themself in the epistemological debate is important in interacting with the social sciences. Not all approaches to using corpus data stray towards positivism to the extent that the critique expressed by the likes of Zoldan holds. One such position, adopted here from

with its emphasis upon argument and experience, with its device 'I may be wrong and you may be right, and by an effort we may get nearer to the truth', is […] akin to the scientific attitude. It is bound up with the idea that everybody is liable to make mistakes, which may be found out by himself, or by others, or by himself with the assistance of the criticism of others. It, therefore, taken by corpus linguistics, position and that of the anti-positivists. The alignment is not complete; howeverwhile the critique in favour of introspection denies the utility of corpus linguistics on similar grounds to those of the anti-positivists, the scientia rationalis approach of the Chomskyans is clearly strongly aligned to a logical form of positivism. See

Corpus linguistics and social sciences

suggests the idea that nobody should be his own judge, and it suggests the idea of impartiality

Disciplinary scope and interaction

It would be ideal to be able to align corpus linguistics and the social sciences and identify those areas which align and those which do not. However, such an approach would be naïve because the scope of the social sciences is uncertain. Also, within any subject within the social sciences, the orientation towards the study of language may vary. In addition, for any research area or even, perhaps, individual researcher, epistemological choices may differ. All of this complicates the formation of bridges between corpus linguistics and the social sciences.

The scope of the social sciences is both broad and indistinct. As such, deciding precisely what 'counts' as a social science may be a vexing task. Inevitably, a degree of arbitrary choice is involved in deciding exactly which subjects to include under the label, and likewise in deciding which to exclude from it. However, no matter what parameters are used, the field is wide and varied. By way of example, the UK Economic and Social Research Council (ESRC) includes the following subjects in its definition of the social sciences: area and development studies; demography; economics; economic and social history; education; environmental planning; human geography; linguistics; management and business studies; politics and international studies; psychology; science and technology studies; social anthropology; social policy; social work; social statistics (including methods and computing); socio-legal studies; and sociology. Some of these subjects engage with the study of language to a degree (psychology and sociology, for example), some might conceivably have research questions to which linguists could contribute (social work and education, for example), while others may be focused so far away from language that the likely interaction with linguistics is marginal at best (social statistics, for example).

In one sub-field of corpus linguistics, CADS, alone we see substantial engagement with the social sciences, including with areas such as Business and Administration, Education, Health, Law, Politics and Religion

Sub-area may also impact not just on the question of engagement with corpus linguistics, it may even influence whether we view the subject as a whole as being part of the social sciences, or whether only parts of it truly are. For example, in linguistics, applied linguistics is clearly a part of the social sciences while the study of phonetics and phonology is less clearly so. That is not to say that there is no role for phoneticians within the social sciences. Rather, the claim is that the scale and intensity of the interaction of linguistics with the social sciences varies as the subfields of linguistics and other social sciences interact. Thus, an engagement with the work of syntacticians and phoneticians, for example, is probably strongest in an area like developmental psychology and weak-to-non-existent across the rest of the social sciences. By contrast, the work of applied linguists, especially those working in discourse analysis, interacts quite strongly across the social sciences by comparison, with areas of particular intensity being education, management and legal studies, sociology and socio-legal studies. This has consequences for corpus linguisticsthose areas which routinely draw upon corpus approaches, for example CADS (see Nartey and Mwinlaaru 2019, for an overview), the broad area of teaching and language corpora (e.g. Flowerdew and Brezina 2017) and corpus approaches to language and cognition (e.g. Gries and Stefanowitsch 2006) may find their work more broadly engaged with across the social sciences.

Importantly, the degree to which the engagement of social scientists with corpus linguistic research will occur varies, once more, according to epistemology.

To focus once more upon discourse analysis, while work situated within critical discourse analysis, rooted largely in an anti-positivist tradition, is widespread across sociology, anti-positivists undertaking critical discourse analyses are unlikely to accept the relevance of CADS. Indeed, within linguistics, Fairclough presents a defence of a critical discourse analysis which largely eschews corpusbased approaches

The area of the social sciences that arguably comes closest to some of the methodological concerns of corpus linguistics is demographic studies and the related area of social surveys. The problems faced by such researchers are similar to those faced by corpus linguiststhey often wish to characterise a population which is far too large to encompass fully. This leads to modelling of the population via sampling regimes. Demographers, for example, want to see how social and cultural factors impact upon that population. The variables are too many to define, leading to models of the population, and its interactions, being developed based on a sub-set of characteristics. Within the population, even with the limited set of characteristics observed, intersections between the characteristics give rise to further complexity. That complexity is, in turn, compounded because the interaction of the population with social and cultural variables also varies through space and time, leading to questions about how to measure change in the observed population across space (see

If one were to conceive of language as one of the cultural and social variables that interacts with a population, then the link between the methodological concerns of demography and social surveys on the one hand, and corpus linguistics on the other, becomes clear. It is therefore surprising that interaction between these areas has been fleeting at best. This is in part because demographers in particular are not directly interested in language but also because many linguists show minimal interest in some of the broader questions that demographers and survey-based social researchers ask and, hence, do not necessarily see the value of their datasets. However, for corpus linguists, such datasets can provide some of the crucial context that would allow them to contextualise their observations. Some work in corpus linguistics has drawn on such resources, including panel survey data (e.g. Baker 2005; Blinder and Allen 2014)

Even where such data might be of use to a corpus linguist, the concerns of the demographer or social survey researcher are unlikely to be a direct point of contact for the corpus linguist. The demographer is typically interested in questions relating to patterns of births, deaths and marriages. These are approached largely through numeric data, often gathered either from public records or social surveys. While it might be conceivable that such information could be of importance to linguistsfor example, those looking for cohort effects in language change might conceivably be interested in varying patterns of birth and deathmost linguists would have no use for such data. Likewise, most social demographers have no use for data about language, except in so far as, perhaps, it represents a variable which might explain a feature of a social process they are examining, such as looking for alignments between inequality and language spoken by migrants (see

When looking to a subject like demography, it is possible that corpus linguists, seeing also the scale of investment in that area, may assume that some problems they have may be answered once and for all. For example, that demographers will have the answer to how to build a perfectly representative spoken corpus. However, a striking feature of demography research is that it is built upon the type of pragmatism that

have, called for:

There are certain properties that demographers would like their data to possess. Taken literally these desiderata are of the nature of ideals in that even in the most advanced countries they are never fully attained. Nonetheless, they are goals that should be kept in view.

Data processing

An obvious area where a fruitful cross-fertilisation can occur between corpus linguistics and the social sciences relates to data processing and theory. There have almost been shadow developments occurring between corpus linguistics and the social sciences in this area. The most obvious area in which the social sciences have shadowed developments in corpus linguistics is corpus markup.

Corpus linguistics has long championed, and pioneered, markup schemes to permit the systematic encoding of metadata and interpretative analyses within corpora. Starting from a disparate range of mark-up schemes that were almost bespoke to individual corpus research centres (see

A key area where this interoperability is beginning to encourage crossfertilisation is in software packages used to annotate corpora. Nowadays, many concordance packages, such as SketchEngine and LancsBox, have annotation packages built in, enabling automated annotation of features such as parts-of-speech, parsing and even semantic annotation. Some packages help analysts introduce manual annotations, but these are fewer and less well developed, generally, that the automated systems. For example, in CQPweb, it is possible to categorise examples according to an annotation scheme and then to use the scheme to explore the data. The emphasis of corpus software packages tends to be on quantitative exploration and automated annotation.

By contrast, in many ways, software tools in the social sciences that facilitate textual analysis are oriented towards qualitative researchers and focus squarely on providing support for manual text annotation. Packages in the social sciences such as NVivo and Altas.ti are, arguably, very helpful packages for corpus linguists to use, especially where they are introducing manual annotations to corpus data, as this process is analogous to some of the typical uses of these packages (e.g. adding interpretative labels to interview data). The cross-fertilisation of corpus linguistics and the social sciences via the use of packages like NVivo is now quite marked; at the time of writing, Google Scholar lists over 20,000 academic outputs that mention corpus and NVivo, covering research in areas as disparate as anthropology

Nonetheless, what one can do with a corpus using a package such as NVivo is limited from the perspective of corpus linguistics, and this has consequences for studies in which only a package like this is used to analyse a corpus, of whatever size. NVivo has some strengths that corpus linguists should consider seriously. For example, it is a good environment for adding annotations to a text, it has excellent multimedia capabilities, and it is XML-compatible. However, it also has limitations. Notably, NVivo is not a good source of frequency data, as the types of frequency lists that are common in corpus analysis packages are absent. Likewise, a host of techniques that many corpus linguists would want to use are absent, including collocation analysis and keyword analysis. This, of course, is not to say that NVivo is flawed; for what it was designed fornamely, coding in the context of relatively small-scale qualitative studies -NVivo is excellent. By the same token, where it exceeds the abilities of some corpus tools, it is not necessarily the case that those corpus tools are lacking; they were simply developed for a different set of users.

Used together, standard corpus tools and packages such as NVivo could represent a powerful combination for users interested in building, manually annotating and exploiting corpora. This is especially the case for multimedia corpora, as shown in

While NVivo or other such tools can be used to input annotations, the question of what is annotated is more important than how that annotation is carried out. Put simply, this question goes something along the lines of, 'what is our analytical scheme and what value does it have in aiding the process of interpretation?'. This is a question that is shared by linguistics and other areas of the social sciences, so it is perhaps understandable that annotation schemes are another area where there has been a flow of ideas between corpus linguistics and the social sciences. It is important to note that those schemes often arise from theory, so theories from the social sciences have become, at least indirectly, influential in corpus linguistics: corpus linguistics has provided insights into texts, which have been interpreted through various theories, including cultural theory

There is a reverse flow from this trend, too. Each time the epistemological position of a researcher guides them to explore a theory through a corpus, the corpus plays a crucial role. More specifically, the corpus holds the theory accountable to quasi-reality and permits the possibility of falsification or corroboration

Corpus users need, of course, to be mindful of what corroboration in particular means. It does not guarantee that the theory is 'right', nor does it mean that this one theory alone will fit the data observed. The theoretical under-determination of corpus data ensures that this is the case

At this point, we also need to accept that in science as well as social science, the choice of theories which our data supports may be more sociological than scientific. Here, the ideas of philosophers of science such as

Perhaps there are special virtues that all and only true theories can be expected to have, such as simplicity, coherence and explanatory power. Or perhaps one theory is chosen over another because it has special advantages, maybe it solves problems that are particularly pressing at the moment, Maybe a worldview dictates. Maybe scientists get excited by the new ideas of the most recent theory or by the newest methods and concepts from other disciplines. Or perhaps adopting a particular theory serves some special interest groups over other or fits better with our view of prejudices

Conclusions

The engagement between the social sciences and corpus linguistics is Proteanin some areas it is dynamic and growing, in others etiolated, and yet in others it is nonexistent and unlikely to grow. The varying linkage between the two can, at times, be explained simply by circumstancethe two areas have yet to gainfully interact, though they may in principle. However, there are also active barriers to interaction rooted in epistemology that are as intransigent and, in fairness, as principled as some of those that exist within linguistics which have stopped some linguists from using corpus methods. Over the years, corpus linguistics has made progress in its home discipline by being results-focused and by spawning new theories that use the data made available by corpus analyses. By showing positive results and new, productive theories, users of corpus data in linguistics have been able to tilt the scales in favour of corpus use. The same is possible in the social sciences.

However, in both linguistics and the social sciences, other methods are also competing for attention. In particular, data science or 'big data' methods are producing results and demanding the attention of social scientists more broadly. The work produced by such researchers often aligns much more strongly with naturalism than corpus linguistics does. While this may, in light of what has been outlined in this paper, prove to be an opportunity for corpus linguists, allowing them to appeal to social science researchers who wish to engage with corpus data but not to shift towards positivism, it will only be so if two conditions are met. Firstly, corpus linguists need to be clear about their own epistemologies. If they are not, it is very easy to bracket corpus linguistics together with approaches to language data which, very often, are free of any serious reflection upon the nature of language in the social world. Secondly, corpus linguists need to be clear when marking this distinction. These two conditions have another, valuable, consequence. One thing that corpus linguists should be clear aboutas should researchers using any method or set of methodsis that while a corpus can answer a range of questions worth asking, it cannot answer all questions that a researcher may reasonably have. Accordingly, corpus linguistics is bound to be, and has been, used as one methodological approach amongst many in studies which use mixed methods and orient to triangulation. In being clear about what makes corpus linguistics distinct and what it has to offer, the role of corpus linguistics as offering one further methodological tool in the researcher's toolbox in the social sciences Corpus linguistics and social sciences will be easier to make, and the engagement of corpus linguistics with the social sciences will be more easily facilitated.
The forms of apposition (raw frequencies) 1.5 Forms of nominal appositions (raw frequencies) 1.6 The form of appositions with proper nouns in one unit (raw frequencies) 1.7 The distribution of APNs across registers (raw frequencies) 1.8 APN pattern 3.1 Sample directory structure 3.2 Parse tree from ICE-GB 4.1 Search results for all forms of talk 4.2 Search results for clusters and N-grams 4.3 Search for all forms of the verb walk 4.4 Search results for all forms of the verb walk 4.5 Examples of forms of walk retrieved 4.6 Search results for words ending in the suffix -ion 4.7 Example parsed sentence in ICE-GB 4.8 An FTF that will find all instances of proper nouns with the feature "appo" Linguistic diversity can be found in other types of corpora too. Parallel corpora contain two languages with one language translated into another language, and the two corpora aligned at the level of the sentence. Many such corpora contain English as one of the languages. For instance, the Europarl Corpus (Release V7) consists of transcriptions of 21 European languages taken from meetings of the European Parliament that were translated into English. Sentences are aligned so that English translations can be directly compared to the original sentences on which they are based. There are many learner corpora, which contain samples of English written by speakers for whom English is a second or foreign language. The ICLE Corpus (The International Corpus of Learner English) contains samples of written English produced by advanced learners of English as a foreign language from 25 different language backgrounds. Learner corpora can be used to study language acquisition, and to develop pedagogical tools and strategies for teaching English as a second or foreign language.

Many new corpora have been created in the area of language change. One of the earlier historical corpora, The Helsinki Corpus, contains 1.5 million words representing Old English to Early Modern English. Parsed versions of part of this corpus are now available and included in the Penn-Helsinki Parsed Corpus of Middle English and the Penn-Helsinki Parsed Corpus of Early Modern English. Historical corpora of English span many periods and include different types of English. The Dictionary of Old English Corpus is a three-millionword corpus containing all surviving Old English texts. The Corpus of Early English Correspondence consists of a collection of corpora containing various types of correspondence written in the fifteenth to seventeenth centuries.

There have been many changes in the ways corpora are created and analyzed. First generation corpora, such as the Brown and LOB Corpora, were relatively short (one million words in length) and contained brief samples (2,000 words) divided into different types of written English (e.g. learned writing, newspaper articles and editorials, fiction). The limited scope and length of these corpora was largely a consequence of the fact that printed texts had to be manually converted into electronic textsa very time-consuming process. But because most texts are now available in electronic form, corpora (as noted earlier) have become considerably longer. Moreover, when initially created, the Brown Corpus, for instance, had to be loaded on to a mainframe computer for analysis, whereas many corpora such as COCA are now available for analysis over the Web or on a home computer. Thus, the creation and dissemination of corpora have become much easier over time, resulting in a more varied selection of corpora that are considerably longer than earlier corpora.

But while printed texts can be easily included in a corpus, spoken texts still have to be manually transcribed: no voice recognition software can accurately produce a transcription because of the complexities of spoken language, particularly spontaneous conversation with its numerous hesitations, incomplete sentences, and reformulations. This is why corpora such as the Santa Barbara Corpus of Spoken American English, which is approximately 249,000 words in length, required a team of transcribers to create the corpus. Some corpora do contain transcripts of television and radio broadcasts (e.g. talk shows), but because the transcripts were created by particular broadcast agencies, their accuracy is open to question. In fact, research has shown that there can be great variability between an "in-house" transcription and what was actually said

Considerable progress has also been made in the annotation of corpora. For instance, word-class tagging has become much more accurate. The TAGGIT Program

Other corpus annotation is used to mark additional features of texts. In a corpus of spoken dialogues, for instance, it is necessary to include tags that identify who is speaking or which sections of speaker turns contain overlapping speech. Some corpora, such as the Santa Barbara Corpus of Spoken American English, are prosodically transcribed and contain detailed features of intonation, such as pitch contours, pauses, and intonation boundaries (cf. www.linguistics.ucsb .edu/research/transcription for further information on the transcription symbols used). Other corpora have been semantically annotated. The FrameNet Project has created corpora containing various types of semantic tags, which mark features of what are called "semantic frames" (

The main advantage of annotation is that it can greatly enhance the kinds of analyses that can be conducted on a corpus. In a tagged corpus, various kinds of lexical categories, such as proper nouns or modal auxiliaries, can be easily retrieved. In a purely lexical corpus (i.e. a corpus containing just the text), only individual words (or sequences of words) can be searched for. But even in a lexical corpus, the numerous concordancing programs that are currently available can greatly facilitate searches. The program AntConc permits searches of, for instance, the verb walk with all of its verb declensions (walks, walking, walked) (www.laurenceanthony.net/software/antconc//). However, the program ICECUP, which comes with ICE-GB, is much more powerful because it can search for grammatical categories (e.g. noun phrases) and is not restricted to lexical items.

This brief overview of the current state of corpus-based research is explored in detail in this second edition of English Corpus Linguistics. The book is divided into four primary chapters.

Chapter 1: The Empirical Study of Language

This chapter focuses on the empirical basis of corpus linguistics. It describes how linguistic corpora have played an important role in providing corpus linguists with linguistic evidence to support the claims they make in the particular analyses of language that they conduct. The chapter opens with a discussion of how to define a corpus, and then traces the history of corpus linguistics, noting, for instance, that corpus-based research began as early as the fifteenth century, when biblical concordances were created based on passages from the Bible. Current conceptions of corpus linguistics started with the creation of the Quirk Corpus (which contained print samples of spoken and written English) in 1955 at the Survey of English Usage at University College London. This was followed (in the 1960s) by the Brown Corpus (which contains 2,000 word samples of various types of edited written American English).

Major research centers continued the development. At Lancaster University, one of the first major part-of-speech tagging programs, the CLAWS Tagger, automated the insertion of part-of-speech tags (e.g. noun, verb, preposition) into computer corpora. At Birmingham University, John Sinclair oversaw not just the creation of corpora but the development of their use to serve as the basis of dictionaries. But as corpus-based research began to expand during this period, its emergence during the dominance of generative grammar in the 1960s created quite a controversy, since to some linguists, corpus linguistics reeked of behaviorism. But as much research in corpus linguistics has demonstrated, empirical data can both enhance and support the claims made in linguistic analyses.

The chapter concludes with a description of the many different areas of linguistics (e.g. lexicography and sociolinguistics) that have benefited from the use of linguistic corpora, followed by a linguistic analysis illustrating that corpus-based methodology as well as the theory of construction grammar can provide evidence that appositives in English are a type of construction.

Chapter 2: Planning the Construction of a Corpus

This chapter describes both the process of creating a corpus as well as the methodological considerations that guide this process. It opens with a discussion of the planning that went into the building of four different types of corpora: the British National Corpus (BNC), the Corpus of Contemporary American English (COCA), the Corpus of Early English Correspondence (CEEC), and the International Corpus of Learner English (ICLE). The structure of each of these corpora is discussed: their length, the genres that they contain (e.g. academic writing, fiction, press reportage, blogs, spontaneous conversations, scripted speech) as well as other pertinent information.

Subsequent sections discuss other topics relevant to planning the building of a corpus, such as defining exactly what a corpus is. Should, for instance, corpora containing samples taken from the Web be considered legitimate corpora, especially since the content of such corpora is sometimes unknown? Although this is an open question, one section of the chapter contains an analysis of web data that precisely specifies the most common registers found in the webpages analyzed.

Other sections of the chapter focus on additional issues relevant to planning the construction of a corpus, such as how to determine the appropriate size of a corpus and the length of particular texts that the corpus will contain (complete texts versus shorter samples from each text, e.g. 2,000 words); how to select the particular genres to be included in a corpus (e.g. press reportage, technical writing, spontaneous conversations, scripted speech); and how to ensure that the writers or speakers analyzed are balanced for such issues as gender, ethnicity, and age.

Chapter 3: Building and Annotating a Corpus

This chapter focuses on the process of creating and annotating a corpus. This involves not only collecting data (speech and writing) but encoding it: transcribing recorded speech, for instance, as well as adding annotation to it, such as markup indicating in a conversation when one person's speech overlaps another speaker's, and in writing where such features as paragraph boundaries occur in written texts.

While written texts are relatively easy to collectmost writing is readily available in digital formatsspeech, especially spontaneous conversations, has to be transcribed, though, as will be discussed in the chapter, voice recognition software has made some progress in automating the process for certain kinds of speech, such as monologues. Other stages of building a corpus are also discussed, ranging from the administrative (how to keep records of texts that have been collected) to the practical, such as the various ways to transcribe recordings of speech.

The chapter concludes with a detailed description of various kinds of textual markup and linguistic annotation that can be inserted into a text. Topics discussed include how to create a "header" for a particular text. Headers contain various kinds of information about the text. For instance, for written texts, the header would include such information as the title of the text; the author(s); if published, where it was published; and so forth. Other textual markup is internal to the text, and in a spoken text would include such information as speaker IDs, and the beginnings and ends of overlapping speech.

Chapter 4: Analyzing a Corpus

This chapter describes the process of analyzing a completed corpus, with an emphasis on quantitative and qualitative research methodologies along with sample corpus analyses that illustrate the direct application of these methodologies in corpus-based research. The chapter also contains discussions of corpus tools, such as concordancing programs, that can facilitate the analysis of corpora by retrieving relevant examples for a given corpus analysis, indicating their overall frequency, and specifying (depending upon the program) the particular genres in which they occur.

The first section of the chapter contains a discussion of what is termed "Trump Speak," the unique brand of language that Donald Trump uses. In addition to illustrating how a primarily qualitative corpus analysis is conducted, the analysis of Trump's speech provides a basis for describing the various steps involved in conducting a corpus analysis, such as how to frame a research question, find suitable corpora from which empirical evidence can be obtained to address the research question, and so forth. The analysis of Trump Speak also contains a discussion of how to use concordancing programs to obtain, for instance, information on the frequency of specific constructions in a corpus as well as relevant examples that can be used in subsequent research conducted on a particular topic.

The remainder of the chapter focuses on more quantitatively based corpus research, such as Douglas Biber's work on multi-dimensional analysis and the specific statistical analyses he used to determine the register distributions of a range of different linguistic constructions, for instance, the higher frequency of pronouns such as I and you in spontaneous conversations than in scientific writing. Descriptive statistics (e.g. the chi-square statistic) are illustrated with a detailed quantitative analysis of the use of a stigmatized linguistic construction, the pseudo-title (e.g. former president Bill Clinton), which is derived from an equivalent or appositive (a former president, Bill Clinton) and found mainly in newspapers. Its usage is analyzed in various regional newspapers included in various components of ICE, such as the United States, Great Britain, and Singapore.

At the end of each of these chapters are exercises that allow for the practical application of the various topics covered in the chapters. In addition, there is an Appendix that contains a listing of all corpora discussed in the text with brief descriptions of their content as well as links to where further information on the corpora can be obtained.

In short, the goal of the second edition of English Corpus Linguistics is to provide a comprehensive description of all aspects of corpus linguistics, ranging from how to build a corpus to how to analyze a finished corpus. In addition, sample corpus analyses are included to illustrate the many theoretical and practical applications that linguistic corpora have.

xvi Preface

As any writer knows, there are many people behind the scenes without whose help a book would never have been written. Such is the case with this book.

I am particularly grateful to Merja Kytö, who read an entire draft and whose insightful and judicious comments greatly improved this book.

I would also like to thank Helen Barton and Isabel Collins of Cambridge University Press for promptly replying to my many questions and for expertly taking me through the production process for the book. Their help was immeasurable.

Rachel La Russo and Minh Nghia Nguyen, doctoral students in the Applied Linguistics Department at the University of Massachusetts, Boston, were assiduous in working on the bibliography. I could not have done without their help. Robert Sigley provided me with very helpful comments on the section of chapter 4 dealing with the statistical analysis of pseudo-titles.

Finally, I wish to thank my wife, Elizbeth Fay. She kept me on track with the book through the years, gently reminding me that I needed to work on my book rather than watch yet another sporting event. And although her area of specialty is not linguistics, she helped me immensely with the many questions I had about particular topics for different chapters. Most important was her constant support.

The Empirical Study of Language

In an interview published in 2000, Noam Chomsky was asked "What is your view of Modern Corpus Linguistics?" His reply was, "It doesn't exist"

pretty soon you'll be able to feed the data into the computer and everything will come out. In fact, there's a field now called corpus linguistics which essentially is the same thing except that they put in the word "Bayesian" every few sentences

To explore the role that corpora play as sources of linguistic evidence, this chapter opens with a discussion of how a linguistic corpus is actually defined, and then continues with a historical overview of the development of corpus-based research, beginning with a discussion of pre-electronic corpora; that is, corpora that were manually collected and compiled and that served as the basis of concordances and reference grammars. While pre-electronic corpora were generally based entirely on printed written texts, this changed in established a corpus containing spoken as well as written English, called the "Quirk" or "Survey of English Usage (SEU) Corpus." This corpus established a methodology for corpus creation and analysis that has continued until the present. Ironically, the corpus was created around the time when generative grammar completely changed the shape of linguistics. In particular, introspection replaced the collection of data as the basis of linguistics analyses, pitting the "armchair linguist," as

In 1964, the Brown Corpus, one of the earliest computerized corpora, ushered in the "electronic" era of corpus linguistics. This corpus contained one million words of edited written American English divided into 500 samples representing different types of writing (such as fiction, technical prose, and newspaper reportage). Each sample was 2,000 words in length, enabling valid comparisons between the different registers in the corpus. The Brown Corpus was extremely important because it provided a catalyst for the many computer corpora that will be discussed throughout this book.

But as corpus linguistics developed alongside the dominant paradigm of generative grammar, obvious differences and disputes resulted.

The corpus linguist says to the armchair linguist, "Why should I think what you tell me is true?", and the armchair linguist says to the corpus linguist, "Why should I think what you tell me is interesting?" However, as the example corpus analysis in the final section of the chapter demonstrates, conducting "interesting" linguistic research based on "real" samples of language are not necessarily mutually exclusive activities: a corpus can serve as a test bed for theoretical claims about language; in turn, theoretical claims can help explain patterns and structures found in a corpus.

Defining a Corpus

Although arriving at a definition of a linguistic corpus may seem like a fairly straightforward process, it is actually a more complicated undertaking than it initially appears. For instance, consider the Microsoft Paraphrase Corpus. This corpus contains 5,800 sentence pairs. One sentence of each pair was obtained from various websites covering the news. The second sentence contains a paraphrase of the first sentence by a human annotator. For each sentence being paraphrased, information is given about its author and the particular news source from which the sentence was obtained. While it is quite common for corpora to contain meta-information about the data that they contain, should a collection of unrelated sentences and associated paraphrases be considered a corpus?

One way to answer this question is to examine the guidelines established by the Text Encoding Initiative (TEI) for the encoding of electronic texts, including linguistic corpora (www.tei-c.org/ release/doc/tei-p5-doc/en/html/CC.html). Because corpora are fairly diverse in size, structure, and composition, the TEI provides a general definition of a corpus.

Several points mentioned in the guidelines are directly relevant to whether or not the Microsoft Paraphrase Corpus (MPC) fits the definition of a corpus. First, a corpus needs to be compiled "according to some conscious set of design criteria." The MPC fits this criterion, as the kinds of language samples to be included in the corpus were carefully planned prior to the collection of data. Second, a corpus can contain either complete texts (e.g. a collection of newspaper articles) or parts of texts (e.g. 500-word samples from various newspaper articles). The MPC satisfies this criterion too: each sentence in the corpus was selected from some larger text of which it is a part. The paraphrase of each sentence makes the MPC similar to a parallel corpusa type of corpus that typically contains sentences from a whole text, with each individual sentence translated into a different language. The major difference is that parallel corpora typically contain samples from larger texts (rather than single sentences), with each sentence in the text translated into a particular language.

A third TEI guideline for defining a corpus is more problematic for the MPC. According to this guideline, a corpus is a type of text that is regarded "as composite, that is, consisting of several components which are in some important sense independent of each other" (www.tei-c.org/release/doc/tei-p5-doc/en/html/DS.html). It is certainly the case that the individual sentences in the MPC are individual entities. The sentences are, as the guidelines continue, "a subdivision of some larger object." But can each sentence in the MPC "be considered as a text in its own right"? (www.tei-c.org/release/doc/ tei-p5-doc/en/html/CC.html).

But whether this question is answered yes or no, it is perhaps more useful, as

(1). machine-readable (2). representative (3). balanced (4). the result of communication occurring in a "natural communicative setting."

A "machine-readable corpus" is a corpus that has been encoded in a digital format. For a corpus to be "representative," it must provide an adequate representation of whatever linguistic data is included in the corpus. For instance, a general corpus of newspaper editorials would have to cover all the different types of editorials that exist, such as oped pieces and editorials produced by an editorial board. For a general corpus of editorials to be "balanced," the editorials would have to be taken from the various sources in which the editorials are found, such as newspapers, magazines, perhaps even blogs. Finally, the speech or writing included in a corpus has to have been produced in a "natural communicative setting." A corpus of spontaneous conversations would need to contain unrehearsed conversations between two or more individuals in the types of settings in which such conversations are held, such as the family dinner table, a party, a chat between two friends, and so forth.

The more a corpus satisfies these four criteria, the more prototypical it would be.

Obviously, the MPC would be a less prototypical corpus as well. While it is available in a machine-readable format, it is too short to be representative of the types of sentences occurring in news stories, and whether the random sampling of sentences produced a balanced corpus is questionable. Because the sentences included in the corpus were taken from news stories, the sentences did occur in a natural communicative setting. Arguably, the translations were created in a natural communicative setting too, as a translator is often called upon to translate bits of language.

Corpora vary in size and composition largely because they have been created for very different uses. Balanced corpora like Brown are of most value to individuals whose interests are primarily linguistic and who want to use a corpus for purposes of linguistic description and analysis. For instance,

In designing a corpus such as the Penn Treebank, however, size was a more important consideration than balance. This corpus was created so that linguists with more computationally based interests could conduct research in natural language processing (NLP), an area of study that involves the computational analysis of corpora often (though not exclusively) for purposes of modeling human behavior and cognition. Researchers in this area have done considerable work in developing taggers and parsers: programs that can take text and automatically determine the word class of each word in the text

Cruden's Concordance is lengthier than the Bible because he included entries not just for individual words but for certain collocations as well. In addition, Cruden does not lemmatize any of the entries, instead including separate entries for each form of a word. For instance, he has separate entries for anoint, anointed, and anointing as well as his anointed, Lord's anointed, and mine anointed. For each entry, he lists where in the Bible the entry can be found along with several words preceding and following the entry. Figure

To create the concordance, Cruden had to manually alphabetize each entry by pen on various slips of paperan enormous amount of work. As

The development of biblical concordances was followed in subsequent years by the creation of concordances of more literary texts. For instance,

Corpora served as the basis of many of the descriptively oriented grammars of English written in the late nineteenth and early to midtwentieth centuries by individuals such as George Curme, Otto His ANNOINTED.

1 Sam. 2:10 exalt horn of his a.

12:3 against the L. and his a. 5 the L. and his a. is witness 2 Sam. 22:51 showeth mercy to his a. Ps. 18:50 Ps. 2:2 and against his a. 20:6 the Lord saveth his a.

28:8 saving strength of his a.

Is. 45:1 saith L. to his a. to C.

With regard to my quotations, which I have collected during many years of both systematic and desultory reading, I think that they will be found in many ways more satisfactory than even the best made-up examples, for instance those in Sweet's chapters on syntax. Whenever it was feasible, I selected sentences that gave a striking, and at the same time natural, expression to some characteristic thought; but it is evident that at times I was obliged to quote sentences that presented no special interest apart from their grammatical peculiarities. (p. vi) Jespersen's corpus is extensive and consists of hundreds of books, essays, and poems written by well-and lesser-known authors (Vol.

A typical entry will be preceded by general commentary by Jespersen, with perhaps a few invented sentences included for purposes of illustration, followed by often lengthy lists of examples from his corpus to provide a fuller illustration of the grammatical point being discussed. For instance, in a discussion of using a plural third person pronoun such as they or their to refer back to a singular indefinite pronoun such as anybody or none,

1.2.1

The Transition from Pre-electronic to Electronic Corpora

The most ambitious pre-electronic corpus, the Quirk Corpus, served as a model for the modern-day electronic corpus. It was created at the Survey of English Usage (SEU) as part of other research being done there on the study of the English language.

Although it now exists in digital form, the Quirk Corpus was originally an entirely "print" corpus. Its creation began in 1955, with final and completed digitization occurring in 1985 (www.ucl.ac.uk/eng lish-usage/about/history.htm). The corpus totals one million words in length and contains 200 samples of spoken and written English, with each sample totaling 5,000 words in length

The idea behind the corpus was to provide as broad a representation as possible of the different types of spoken and written English that exist

Because the corpus was completely orthographic, it had to be prepared in a manner making it accessible to researchers. The spoken texts were transcribed and annotated with "a sophisticated marking of prosodic and paralinguistic features," and the entire corpus was "analysed grammatically"

While the Quirk Corpus was an entirely printed corpus, work on the computerization of corpora began during the same time period. Beginning in the 1960s at the University of Edinburgh and later at Birmingham University, John Sinclair created what would become the first computerized corpus of English: a 135,000-word corpus of conversational British English

135,000 words was almost the maximum that could be comfortably stored and processed, using the programs developed at the beginning of the project, given this particular machine's capacity and the time available.

The spoken part of the Quirk Corpus was later computerized at Lund University under the direction of Jan Svartvik. The London-Lund Corpus (LLC), as it is now known, contains a total of 100 spoken texts: 87 from the original Quirk Corpus, plus an additional 13 texts added after the project was moved in 1975 to the Survey of Spoken English at Lund University in Sweden

But of all the early electronic corpora, the first computerized corpus of written English, the Brown Corpus (described earlier), was really the corpus that ushered in the modern-day era of corpus linguistics. Compared with present-day corpora, this corpus is relatively small (one million words). However, for the period when it was created (early 1960s), it was a huge undertaking because of the challenges of computerizing a corpus in an era when mainframe computers were the only computational devices available. Computers during this period were large machines. As

Analyzing early versions of the Brown Corpus was an equally complicated process because these versions were released on magnetic tape. Using the corpus was a two-step process. The tape had to first be read into a mainframe computer and then punch cards prepared to conduct searches. For instance, in his analysis of punctuation usage in the Brown Corpus,

Other work on the Brown Corpus introduced additional innovations. In 1967 Computational Analysis of Present Day American English

The Brown Corpus was also the first corpus to be lexically tagged; that is, each word was assigned a part of speech designation (e.g. the tag DO for the verb do or DOD for the past tense form did). All 77 tags were assigned to each word in the corpus by a computer program designed at Brown University called "TAGGIT"

The early influences on corpus linguistics discussed in this section do not exhaust the many other factors that affected the current state of corpus linguistics. One of the key developments, as McCarthy and O'Keeffe (2010: 5) note, "was the revolution in hardware and software in the 1980s and 1990s that really allowed corpus linguistics as we know it to emerge." Advances in technology have resulted in, for instance, web-based corpora and textual analysis software, such as concordancing programs, that are fast and can be run on desktop and laptop computers. In their overview of the history of corpus linguistics,

Corpus Linguistics in the Era of Generative Grammar

At the time when the Brown Corpus was created in the early 1960s, generative grammar dominated linguistics, and there was little tolerance for approaches to linguistic study that did not adhere to what generative grammarians deemed acceptable linguistic practice. As a consequence, even though the creators of the Brown Corpus, W. Nelson Francis and Henry Kučera, are now regarded as pioneers and visionaries in the Corpus Linguistics community, in the 1960s their efforts to create a machine-readable corpus of English were not warmly accepted by many members of the linguistics community. W. Nelson

This attitude was largely a consequence of the conflict between what Chomskyan and corpus linguists considered "sufficient" evidence for linguistic analysis. In short, corpus linguists were grouped into the same category as the structuralists -"behaviorists"that Chomsky had criticized in the early 1950s as he developed his theory of generative grammar. For Chomsky, the mere "description" of linguistic data was a meaningless enterprise. It was more important, he argued, that grammatical descriptions and linguistic theories be evaluated in terms of three levels of "adequacy": observational adequacy, descriptive adequacy, and explanatory adequacy.

If a theory or description achieves observational adequacy, it is able to describe which sentences in a language are grammatically well formed. Such a description would note that in English, while a sentence such as He studied for the exam is grammatical, a sentence such as *studied for the exam is not. To achieve descriptive adequacy (a higher level of adequacy), the description or theory must not only describe whether individual sentences are well formed but in addition specify the abstract grammatical properties making the sentences well formed. Applied to the previous sentences, a description at this level would note that sentences in English require an explicit subject. Hence, *studied for the exam is ungrammatical and He studied for the exam is grammatical. The highest level of adequacy is explanatory adequacy, which is achieved when the description or theory not only reaches descriptive adequacy but does so using abstract principles which can be applied beyond the language being considered and become a part of "Universal Grammar." At this level of adequacy, one would describe the inability of English to omit subject pronouns as a consequence of the fact that, unlike Spanish or Japanese, English is not a language which permits "pro-drop," that is, the omission of a subject pronoun recoverable from the context or deducible from inflections on the verb marking the case, gender, or number of the subject. Within Chomsky's theory of principles and parameters, pro-drop is a consequence of the "null-subject parameter"

Because generative grammar has placed so much emphasis on universal grammar, explanatory adequacy has always been a high priority in generative grammar, often at the expense of descriptive adequacy. There has never been much emphasis in generative grammar in ensuring that the data upon which analyses are based is representative of the language being discussed, and with the notion of the ideal speaker/hearer firmly entrenched in generative grammar, there has been little concern for variation in a language, which traditionally has been given no consideration in the construction of generative theories of language.

Chomsky also distinguishes between those elements of a language that are part of the "core" and those that are part of the "periphery." The core is comprised of "pure instantiations of UG" and the periphery "marked exceptions" that are a consequence of "historical accident, dialect mixture, personal idiosyncracies, and the like"

This complexity of structure, however, is precisely what the corpus linguist is interested in studying. Unlike generative grammarians, corpus linguists see complexity and variation as inherent in language, and in their discussions of language they place a very high priority on descriptive adequacy, not explanatory adequacy. Consequently, corpus linguists are very skeptical of the highly abstract and decontextualized discussions of language promoted by generative grammarians, largely because such discussions are too far removed from actual language usage.

Corpus linguists also challenge the clear distinction made in generative grammar between competence and performance, and the notion that corpus analyses are overly descriptive rather than theoretical.

Corpus-based studies also expand the range of data considered for analysis beyond the linguist's intuitions, and ensure the accuracy of any generalizations that are made. For instance, in a discussion of

(a) We consider Kim to be an acceptable candidate. (b) *We consider Kim as an acceptable candidate.

However,

(c) The boys consider her as family and she participates in everything we do.

But while corpora are certainly essential linguistic resources, it is important to realize that no corpus, regardless of its size, will contain every relevant example of a particular linguistic construction or be able to provide a fully complete picture of how the construction is used. At best, a corpus can provide only a "snapshot" of language usage. For this reason, many have argued that corpus data should be supplemented with other means of gathering data.

In contrast, judgment tests, such as the "evaluation" test below, require subjects to assess the acceptability of sentences in various ways. In this case, subjects are asked to rank their preferences for the three sentences, from highest to lowest: He doesn't have a car. He hasn't a car. He hasn't got a car.

The purpose of this test was to examine differences in negation and the use of got in British and American English. Thus, while speakers of American English will rank He hasn't got a car higher than He hasn't a car, speakers of British English will reverse the rankings.

Types of Corpora

There are many kinds of corpora that have been created to fulfill the research needs of those doing corpus-based research. These corpora range from multipurpose corpora, which can be studied to carry out many differing types of corpus-based analyses, to learner corpora, which have been designed to study the types of English used by individuals (from many differing first language backgrounds) learning English as a second or foreign language. This section provides an overview of the many different types of corpora that exist. Less detail is given to corpora discussed in greater detail in subsequent chapters.

Multipurpose Corpora

These are corpora like the London-Lund Corpus, the British National Corpus (BNC), and the Corpus of Contemporary American English (COCA). All of these corpora contain numerous registers of speech or writing (such as fiction, press reportage, casual conversations, and spoken monologues or dialogues) representative of a particular variety of English. For instance, the London-Lund Corpus contains different kinds of spoken British English, ranging from casual conversation to course lectures. The BNC and COCA contain various samples of spoken and written British and American English, respectively.

Multipurpose corpora are useful for many different types of analyses. Because the London-Lund Corpus has been prosodically transcribed, it can be used to study various features of British English intonation patterns, such as the relationship between grammar and intonation in English. For instance, in a noun phrase such as the leader of the country and her cabinet, a tone unit boundary will typically occur before the conjunction and, marking the boundary between two coordinated noun phrases. There is also currently in production an updated version of the London-Lund Corpus: LLC-2 (

The BNC and COCA differ in length: the BNC is 100 million words in length, whereas COCA is a monitor corpus; new texts are constantly added to it so that it currently contains one billion words. But despite the difference in length, each corpus contains many of the same registers, such as fiction, press reportage, and academic writing. While the BNC contains spontaneous conversations that have been manually transcribed, COCA is restricted to spoken registers from which published transcriptions are available. Consequently, it contains no spontaneous conversations. The BNC is a fixed corpus: its structure hasn't been changed since its creation in the 1990s, though a successor corpus, BNC2014, is now in progress (cf.

The International Corpus of English (ICE) contains comparable one million-word corpora of spoken and written English representing the major national varieties of English, including English as it is spoken and written in countries such as Ireland, Great Britain, the Philippines, India, and many other varieties as well.

Learner Corpora

This type of corpus contains texts that represent the speech or writing of individuals who are in the process of learning a language as a second or foreign language. For instance, the International Corpus of Learner English contains samples of written English from individuals who speak English as a foreign language and whose native languages include French, German, Portuguese, Arabic, and Hungarian. Learner corpora enable researchers to study such matters as whether one's native language affects their mastery of a foreign language, in this case English. There are other learner corpora representing numerous languages other than English, including Arabic, Hungarian, and Malay, as well as projects providing various ways that learner corpora can be analyzed (for more comprehensive listing of learner corpora, see

Historical Corpora

The corpora discussed thus far contain examples of various types of modern English. However, there are also diachronic or historical corpora, which contain texts from earlier periods of English.

Much of the interest in studying historical corpora began with the creation of the Helsinki Corpus, a 1.5 million-word corpus of English containing texts from the Old English period (beginning in the eighth century) through the early Modern English period (the first part of the eighteenth century). Texts from these periods are further grouped into sub-periods (ranging from 70 to 100 years) to provide what

To fill gaps in the Helsinki Corpus, ARCHER (A Representative Corpus of Historical English Registers) was created (cf.

The Penn Parsed Corpora of Historical English contains components that cover three different periods of the English language: Middle English, Early Modern English, and Modern British English:

The Penn-Helsinki Parsed Corpus of Middle English, second edition The Penn-Helsinki Parsed Corpus of Early Modern English The Penn Parsed Corpus of Modern British English, second edition Each corpus has been both lexically tagged and syntactically parsed. That is, each word has been assigned a lexical tag identifying its part of speech (such as verb, noun, preposition). In addition, the larger corpora have been syntactically parsed, so that both individual words as well as structures, such as noun phrases, can be retrieved. Users of the corpus will therefore be able to study the development over time of both individual words as well as a variety of different syntactic structures.

Although FLOB (The Freiburg LOB Corpus of British English) and FROWN (The Freiburg Brown Corpus of American English) are not historical corpora in the sense that the Helsinki and ARCHER Corpora are, they do permit the study of changes in British and American English between the periods 1961-1991. Moreover, FLOB and FROWN replicate the LOB and Brown Corpora, respectively, but with texts published in the year 1991. Thus, FLOB and FROWN allow for studies of linguistic change in British and American English over a period of 30 years. Although 30 years is not a long time for language to change, studies of FLOB and FROWN have documented changes in the language during this period (cf., for instance,

The two main historical corpora, the Helsinki and ARCHER Corpora, contain many different texts and text fragments covering various periods of English. There are, however, more focused historical corpora covering specific works, authors, genres, or periods. These corpora include an electronic version of Beowulf, "The Electronic Beowulf " (cf. Prescott 1997 and

The Web as Corpus

Because the Web can be searched, it can be regarded as a corpus with infinite boundaries. Initially, as

Chapter 4 will contain a discussion of "Trump Speak": the type of language that Donald Trump used when he was president. The Web was instrumental in locating additional data for the study, since Trump is quoted extensively in newspapers and other print media, making it possible to find additional examples of his usage of language that proved very useful for the study. For instance, when Marco Rubio was running against Trump in the primaries, Trump repeatedly referred to him as "Little Marco." While this usage shows up regularly in Trump's tweets, an example from the Web containing this usage during a debate helped establish this usage as not simply restricted to tweets: "I will. Don't worry about it, Marco. Don't worry about it," Trump said to an applauding crowd. "Don't worry about it, little Marco, I will."

In addition, there are corpora that have been created based solely on data from the Web. For instance, the iweb Corpus is 14 billion words in length and is searchable online. It was systematically created. Using information from alexa.com, one million of the most popular websites were downloaded. These websites were then screened to ensure that they were from countries in which English is spoken as a native language: the United States, the UK, Canada, Ireland, Australia, and New Zealand. Several processes of elimination were developed to further screen the samples, resulting in a corpus based on 94,391 websites that yielded 22,388,141 webpages.

Parallel Corpora

A parallel corpus is a very specialized corpus. As Gatto (2014: 16) notes, "parallel corpora consist of original texts and their translations into one or more languages." For instance, the English-Norwegian parallel corpus contains texts in both English and Norwegian that are then translated into the other language, for instance English into Norwegian and Norwegian into English (

Uses of Corpora

The previous section provided a description of several different types of corpora. This section provides a sampling of the types of analyses that can be conducted on corpora.

Language Variation

Because corpus linguists are interested in studying the contexts in which language is used, modern-day corpora, from their inception, have been purposely designed to permit the study of what is termed "genre variation"; that is, how language usage varies according to the context in which it occurs. The first computer corpus, the Brown Corpus (a general-purpose corpus), contained various kinds of writing, such as press reportage, fiction, learned, and popular writing. In contrast, the MICASE Corpus is a more specialized corpus that contains various types of spoken English used in academic contexts (e.g. advising sessions, colloquia) at the University of Michigan.

While genre or register variation focuses on the particular linguistic constructions associated with differing text types, sociolinguistic variation is more concerned with how various sociolinguistic variables, such as age, gender, and social class, affect the way that individuals use language. One reason that there are not more corpora for studying this kind of variation is that it is tremendously difficult to collect samples of speech, for instance, that are balanced for gender, age, and ethnicity. Moreover, once such a corpus is created, it is less straightforward to study sociolinguistic variables than it is to study genre variation. To study press reportage, for instance, it is only necessary to take from a given corpus all samples of press reportage, and to study within this sub-corpus whatever one wishes to focus on. To study variation by gender in, say, spontaneous dialogues, on the other hand, it becomes necessary to extract from a series of conversations in a corpus what is spoken by males as opposed to femalesa much more complicated undertaking, since a given conversation may consist of speaker turns by males and females distributed randomly throughout a conversation, and separating out who is speaking when is neither a simple nor straightforward computational task. Additionally, the analyst might want to consider not just which utterances are spoken by males and females but whether an individual is speaking to a male or female, since research has shown that how a male or female speaks is very dependent upon the gender of the individual to whom they are speaking.

But despite the difficulties of creating a truly balanced corpus, designers of the BNC made concerted efforts to produce a corpus that was balanced for such variables as age and gender, and that was created so that information on these variables could be extracted by various kinds of software programs. Prior to the collection of spontaneous dialogues in the BNC, calculations were made to ensure that the speech to be collected was drawn from a sample of speakers balanced by gender, age, social class, and dialect region. Included within the spoken part of the BNC is a sub-corpus known as the Corpus of London Teenage English (COLT). This part of the corpus contains a valid sampling of the English spoken by teenagers from various socioeconomic classes living in differing boroughs of London.

To enable the study of sociolinguistic variables in the spoken part of the BNC, each conversation contains a file header, a statement at the start of the sample providing such information as the age and gender of each speaker in a conversation. A software program, SARA (SGML-Aware Retrieval Application), was designed to read the headers and do various analyses of the corpus based on a prespecified selection of sociolinguistic variables. Using SARA,

Other corpora have been designed to permit the study of sociolinguistic variables as well. In the British component of the International Corpus of English (ICE-GB), ethnographic information on speakers and writers is stored in a database, and a text analysis program designed to analyze the corpus, ICECUP (the International Corpus of English Corpus Utility Program) can draw upon information in this database to search by, for instance, age or gender. Even though ICE-GB is not balanced for genderit contains the speech and writing of more males than femalesa search of lovely reveals the same usage trend for this word that was found in the BNC.

Of course, programs such as SARA and ICECUP have their limitations. In calculating how frequently males and females use lovely, both programs can only count the number of times a male or female speaker uses this expression; neither program can produce figures that, for instance, could help determine whether females use the word more commonly when speaking with other females than males. And both programs depend heavily on how accurately and completely sociolinguistic variables have been annotated, and whether the corpora being analyzed provide a representative sample of the variables. In using SARA to gather dialectal information from the BNC, the analyst would want to spot check the ethnographic information on individuals included in the corpus to ensure that this information accurately reflects the dialect group in which the individuals are classified. Even if this is done, however, it is important to realize that individuals will "style-shift": they may speak in a regional dialect to some individuals but a more standard form of the language with others. In studying variation by gender in ICE-GB, the analyst will want to review the results with caution, since this corpus does not contain a balanced sample of males and females. Software such as SARA or ICECUP may automate linguistic analyses, but they cannot deal with the complexity inherent in the classification of sociolinguistic variables. Therefore, it is important to view the results generated by such programs with a degree of caution.

More recent work has investigated sociolinguistic variables in a number of different corpora.

One topic she discussed to illustrate the role that age plays in language usage focused on the use of hedges in constructions containing verbs (I think and I might) and adverbs (maybe and probably). Table

Lexicography

Studies of grammatical constructions can be reliably conducted on corpora of varying length. However, to obtain valid information on vocabulary items for the purpose of creating a dictionary, it is necessary to analyze corpora that are very large. To understand why this is the case, one need only investigate the frequency patterns of vocabulary in corpora, such as the one million-word BNC. In the BNC, the five most frequent lexical items are the function words the, of, and, a, in (

The five least frequent lexical items are not five single words but rather hundreds of different content words that occur 10-15 times each in the corpus. These words include numerous proper nouns, such as Bond and MacDonald, as well as miscellaneous content words such as bladder, dividends, and woodland. These frequencies illustrate a simple fact about English vocabulary (or, for that matter, vocabulary patterns in any language): a relatively small number of words (particularly function words) will occur with great frequency; a relatively large number of words (content words) will occur far less frequently. Obviously, if the goal of a lexical analysis is to create a dictionary, the examination of a small corpus will not give the lexicographer complete information concerning the range of vocabulary that exists in English and the varying meanings that these vocabulary items will have. Because a traditional linguistic corpus, such as the LOB Corpus, "is a mere snapshot of the language at a certain point in time"

To understand why dictionaries are increasingly being based on corpora, it is instructive to review precisely how corpora, and the software designed to analyze them, can not only automate the process of creating a dictionary but improve the information contained in the dictionary. A typical dictionary, as

Prior to the introduction of computer corpora in lexicography, all of this information had to be collected manually. As a consequence, it took years to create a dictionary. For instance, the most comprehensive dictionary of English, the Oxford English Dictionary (originally entitled New English Dictionary), took 50 years to complete, largely because of the many stages of production that the dictionary went through.

Because so much text is now available in computer-readable form, many stages of dictionary creation can be automated. Using a relatively inexpensive piece of software called a concordancing program, the lexicographer can go through the stages of dictionary production described above, and instead of spending hours and weeks obtaining information on words, can obtain this information automatically from a computerized corpus. In a matter of seconds, a concordancing program can count the frequency of words in a corpus and rank them from most frequent to least frequent. Figure

Note the high relative frequency of function words, the modal verb can, and other vocabulary items, such as words or concordancing, that are topics of the discussion in the paragraph.

In addition, some concordancing programs can detect prefixes and suffixes and irregular forms and sort words by "lemmas"; that is, words such as runs, running, and ran will not be counted as separate entries but rather as variable forms of the lemma run. And as Furthermore, if the lexicographer desires a copy of the sentence in which a word occurs, it can be automatically extracted from the text and stored in a file, making obsolete the handwritten citation slip stored in a filing cabinet. If each word in a corpus has been tagged (i.e. assigned a tag designating its word class), the part of speech of each word can be automatically determined. In short, computer corpora and associated software have completely revolutionized the creation of dictionaries.

As

Corpus-Based Research in Linguistics: A Case Study

Linguists from many different theoretical perspectives have discovered that corpora can be very useful resources for pursuing various research agendas. For instance, lexicographers, as noted above, have found that they can more effectively create dictionaries by studying word usage in very large linguistic corpora. Much current work in historical linguistics is now based on corpora containing texts taken from earlier periods of Englishcorpora that permit a systematic study of the development of English and that enable historical linguists to investigate issues that have currency in modern linguistics, such as whether males and females used language differently in earlier periods of English. Corpora have been introduced into other linguistic disciplines as well, and have succeeded in opening up new areas of research or bringing new insights to traditional research questions.

Chapter 4 ("Analyzing a Corpus") describes in detail how to conduct a corpus analysis, covering such topics as how to frame a research question for a particular corpus analysis and select the appropriate corpora for conducting the analysis. But this section demonstrates how a particular theory of language, Construction Grammar, can provide insights into the variable grammatical category of appositions in English. Such linkages between theory and practice are important because they, on the one hand, avoid the oft-made criticism of corpus linguists that they are mainly "bean counters"linguists who fail to make connections between theory and usagewith too little attention paid to the theoretical underpinnings of the data they are describing.

Appositions as Constructions

Apposition has proven to be a problematic grammatical category, largely because treatments of it disagree about which constructions should be considered appositions. For instance, most studies consider a construction such as Geoffrey Plimpton, police commissioner as consisting of two units in apposition. However, if the two units are reversed, a reversal possible with some but not all appositions, a very different construction, police commissioner Geoffrey Plimpton, resultsone that some studies favoring a more expansive view of apposition consider appositional (e.g.

In more recent work,

To illustrate why appositions are constructions, it is helpful to examine the most frequent apposition that

1.6.2

The Frequency and Distributions of APNs in the Corpora

Figure

(1) The first twenty thousand pounds, the original grant, is committed.

(London-Lund Corpus S.1.2 782-3) Occurring far less frequently were three additional types of appositions: (a) non-nominal appositions, such as the two adjective phrases below:

(2) when the patient closed his eyes, he had absolutely no spatial (that is, third dimensional) awareness whatsoever.  One reason these two adjectives are considered in apposition is that the second unit is preceded by the marker of apposition that is, a marker that can precede more "canonical" appositions consisting of units that are noun phrases. (b) a noun phrase in apposition with some kind of clausal structure, such as the wh-question below:

(3) The Sane Society is an ambitious work. Its scope is as broad as the question: What does it mean to live in modern society?  Although the two units in this example are not co-referential (only noun phrases can co-refer), the first unit, the question, referentially points forward to the What-clause. There is thus a referential relationship between the two units. and (c) an apposition requiring an obligatory marker apposition, such as including:

(4) About 40 representatives of Scottish bodies, including the parents of some of the children flown to Corsica, were addressed by an English surgeon and Dr. and by M. Naessens. (Survey of English Usage Corpus W.12.1-40)

In this example, the marker including is obligatory because it indicates that the reference of the noun phrase it precedes is included in the reference of the noun phrase in the first unit, resulting in the referential relationship of "inclusion" between the two units.

Because of their overall frequency, nominal appositions form the class of prototypical appositions. While the other three types of constructions that have been classified as appositions are more diverse in form and function, they are still within the class of appositions and are related through notions such as the two listed below

Within the category of nominal appositions,

As Figure

Each of the six forms (except for "other") are listed in the examples below, which contain identical content words: (5 In context, although each of the examples would have differences in focus and meaning, they nevertheless are very closely interrelated. Within construction grammar, APNs can be viewed as a type of idiom, specifically as two of the types of idioms described in

Moreover, APNs can also fit into a second class of idioms that

The Structure of APNs

The structure of the APN construction is described in the pattern in Figure

This pattern describes the prototypical apposition containing one unit that is a proper noun. The examples in (

(8) Tory leader Cllr Bob Blackman (ICE-GB:W2C-009 #54:3) However, there was a statistically significant tendency for the first units (highlighted in the following) to be five words or longer in New Zealand and Philippine English: Thus, the actual length of the first unit in a pseudo-title is determined by both intonational limits on the length of the first unit as well as editorial practices on length dictated by the editorial practices of a newspaper that differ regionally.

The Communicative Functions of APNs

The form of APNs is very well suited to the genrepress reportagein which they predominantly occur. Consequently, in other contexts, they sound rather odd. For instance, you might say to someone in a casual conversation:

(14) Jack Smith is a distinguished linguist.

However, you probably would not say:

(15) A distinguished linguist, Jack Smith, is having a drink with me later.

And you definitely would not say:

(16) Distinguished linguist Jack Smith is having a drink with me later.

Examples (

(17) Jessica Seinfeld's broccoli-spiked chicken nuggets recipes are all hers, a federal judge ruled Thursday. Ms. Seinfeld. . .did not copy from another author in her cookbook about sneaking vegetables into children's food, the judge said when she threw out a copyright infringement case brought by a competing author, Missy Chase Lapine.

In example (

Conclusions

This chapter described the role that corpus linguistics has played in the study of language from the past to the present, from pre-electronic corpora to the many differing electronic corpora that currently exist. These corpora can be used to conduct linguistic researchboth theoretical and appliedin many different areas, from genre variation (e.g. how language usage varies in spontaneous conversations versus public speeches) to research that is more applied and pedagogical (e.g. how the study of learner corpora can lead to insights for the teaching of English to individuals learning English as a second or foreign language).

Finally, a case study was presented to demonstrate that corpus analyses and various linguistic theories go hand in hand, and that such studies can do more than simply provide examples of constructions and document their frequency of occurrence. If this is the only information that a corpus analysis could provide, then corpus linguistics would have at best a marginal role in the field of linguistics. Instead, linguistic theories can be used to explain why particular frequencies and examples actually exist in a corpus: in other words, to discuss how theory and data interact. This is why corpus linguistics has grown as an area of linguistics and why many people now are using linguistic corpora for many different purposes.

Planning the Construction of a Corpus

The first step in building a corpus is to decide what the ultimate purpose of the corpus will be. This decision is important because it will determine exactly what the corpus will look like: what types of texts will be included in it (e.g. spoken, written, both), how long the individual texts will be (e.g. full length, text excerpts), how detailed the annotation will need to be (e.g. word-class tagging or a purely lexical corpus with minimal annotation), and so forth. For instance, if the corpus is to be used primarily for grammatical analysis (e.g. the analysis of relative clauses or the structure of noun phrases), the corpus can consist simply of text excerpts rather than complete texts, and will minimally need part-of-speech tags. On the other hand, if the corpus is intended to permit the study of discourse features, then it will have to contain lengthier texts, and some system of tagging to describe the various features of discourse structure.

To explore the process of planning a corpus, this chapter first provides an overview of the methodological assumptions that guided the compilation of two general-purpose corporathe British National Corpus (BNC) and the Corpus of Contemporary American English (COCA)and two more specialized corporathe Corpus of Early English Correspondence (CEEC) and the International Corpus of Learner English (ICLE). The remainder of the chapter then focuses more specifically on the individual methodological considerations (e.g. ensuring that a corpus is "balanced") that anyone planning to create a corpus needs to address.

The British National Corpus

At approximately 100 million words in length, the British National Corpus (BNC) (see Table

In planning the collection of texts for the BNC, a number of decisions were made beforehand: (1) Texts included in the corpus were selected according to domain (the categories listed in Table

Currently, there is a newer version of the BNC, BNC2014, that is under development and that will replicate the structure of the original corpus.

The Corpus of Contemporary American English

Like the BNC, the Corpus of Contemporary American English (COCA) contains various samples of different kinds of spoken and written English, with the exception that the corpus represents American rather than British English (see Table

In addition, COCA is a monitor corpus; that is, a corpus to which new texts are added on an ongoing basis. Moreover, COCA is also much lengthier than the BNC: Currently (as of April 2021), it is 1 billion words in length. While the two corpora share several commonalities, they differ in many key areas:

(1) While both COCA and the BNC each contain samples of speech, COCA contains a more restricted sampling of spoken English; only shows broadcast on television or radio as well as TV and movie subtitles.

Unlike the BNC, it contains no spontaneous conversations of individuals having, for example, a casual conversation during breakfast, or a discussion of a current movie that a group of individuals had seen. (

The Corpus of Early English Correspondence (CEEC)

The CEEC is actually a group of historical corpora that were created over the years at the University of Helsinki.

Two of the corpora in Table

(2) But while gender balance is more easily achieved in modern corpora such as the BNC or COCA, in CEEC it was much more difficult, largely because literacy rates were much higher among men during this period than women. Consequently, fewer female writers were included in the corpus than male writers. (3) Most of the letters were digitized by scanning texts from edited books in which they appeared. However, some of the texts had to be typed in manually because they did not appear in edited editions, a very "laborious method of keying in the text" (www.helsinki.fi/varieng/CoRD/corpora/ CEEC/generalintro.html).

The International Corpus of Learner English (ICLE)

The ICLE (version 2) is a learner corpus: a type of corpus containing samples of speech or writing produced by individuals learning English as a foreign or second language. The ICLE is restricted to writing and contains samples of written English produced by individuals at an advanced level of proficiency learning written English as a foreign language. The samples of written English included in the corpus were obtained from native speakers of 12 European languages and 4 non-European languages:

Bulgarian, Chinese, Czech, Dutch, Finnish, French, German, Italian, Japanese, Norwegian, Polish, Russian, Spanish, Swedish, Tswana, Turkish

The International Corpus of Learner English

Some of the design principles of ICLE are similar to those discussed in previous sections with the differences mainly relating to the unique features of learner corpora:

(1) At 3,753,030 words in length, ICLE is smaller than either the BNC or COCA. But like these two corpora, it is divided into samples, ranging in length from 500 to 1,000 words. However, the type of writing included in the corpus is restricted to primarily argumentative writing

(2) Detailed ethnographic information was recorded for each individual whose writing was included in the corpus, including such features as age, gender, mother tongue, the region in which the writer lives (for languages which are spoken in more than one country), and the other languages the writer may speak

(3) Like the BNC and COCA, ICLE is lexically tagged and comes with a concordancing program that has been customized to search for specific tags or combinations of tags included in the corpus and to also study effects on usage such as the age, gender, mother tongue, and so forth of the writers whose texts have been included in the corpus.

The preceding sections provided a brief overview of four different corpora and some of the methodological considerations that guided the development of these corpora. The remainder of the chapter explores these considerations in greater detail, focusing on such topics as the factors determining, for instance, how lengthy a corpus should be, what kinds of texts should be included in a corpus, and other issues relevant to the design of linguistic corpora.

2.5

What Is a Corpus?

The corpora discussed so far have a clearly identifiable structure: we know how lengthy they are, what texts are in them, and which genres the texts represent. Recently, however, the Web itself has increasingly been used as a source of data for linguistic analysis, giving rise to the notion of "the web as corpus." But as

On the one hand, the traditional notion of a linguistic corpus as a body of texts rests on some correlate issues, such as finite size, balance, part-whole relationship and permanence; on the other hand, the very idea of a web of texts brings about notions of non-finiteness, flexibility, decentering/recentering, and provisionality.

With a corpus such as the BNC, we know precisely what kinds and types of English are being analyzed. With web-based material, however, no such certainty exists.

To determine the kinds of texts that exist on the Web,

They found that three registers predominated: narrative (31.2 percent), informational description/explanation (14.5 percent), and opinion (11.2 percent). In the narrative category, general news reports (52.5 percent) and sports reports (16 percent) were most frequent (p. 24). The informational description/explanation category contained, for instance, research articles and technical reports, though interestingly "academic research articlesthe focus of an extensive body of corpus-based researchcomprise less than 3 percent of the general "'informational

Future chapters will explore in greater detail corpus analyses of web data, and how tools such as the WebCorp concordancing program (www.webcorp.org.uk/live/) can be used to extract data directly from the Web. But the remaining sections in this chapter will focus primarily on issues related to the construction of traditional corpora.

Corpus Size

The Brown Corpus, released in 1962, is one million words in length. A more recent corpus, the Corpus of Web-Based Global English, is 1.9 billion words in length. Although there are many reasons for the disparity in the length of these corpora, the primary reasons can be attributed to advances in technology and the easy availability of texts in electronic form.

First generation corpora, such as Brown and LOB, were relatively short (each of one million words in length), mainly because of the logistical difficulties that computerizing a corpus created. For instance, all of the written texts for the Brown Corpus had to be keyed in by hand, a process requiring a tremendous amount of very tedious and time-consuming typing. In contrast, those creating second generation corpora have had the advantage of numerous technological advances that have automated the process of corpus creation. For instance, the availability of optical scanners made it easier to convert printed texts into digital formats. More recently, so many texts are available in electronic formats that with very little work, they can easily be adapted for inclusion in a corpus. As a consequence, second generation corpora are regularly 100 million words in length or even longer.

But while written texts can easily be digitized, technology has not progressed to the point where it can greatly expedite the collection and transcription of speech, especially spontaneous conversations: There is much work involved in recording this type of speech and manually transcribing it. Speech recognition software, which converts speech into writing, has been greatly improved over the years, but it works best with a single speaker who has trained the software to recognize his/her speech characteristics. These logistical realities explain why 90 percent of the BNC (a second-generation corpus) contains written texts and only 10 percent spoken texts.

Other corpora, such as COCA, which contain a significant amount of speech, generally consist of spoken samples, such as broadcast discussions, for which commercially produced transcripts are available. But as will be discussed in a later section, the wide availability of transcription services has made the transcription of speech more financially feasible. In addition, the creation of the spoken BNC2014 and the London-Lund Corpus demonstrate the feasibility of creating corpora with significant amounts of spoken language.

Ultimately, the length of a corpus is best determined by its intended use.

Not surprisingly, Davies found that individual lexical items were better studied in larger corpora than in shorter corpora. For instance, while adjectives such as fun or tender are among the group of adjectives that are most common in COCA, in the Brown Corpus, they occurred five times or less. In contrast, certain types of syntactic structures, such as modal verbs, have more even distributions across the three corpora, thus being one of the few areas "where Brown provides sufficient data" (Davies 2015: 15). Overall, COCA provides many more examples of the 10 types of constructions that Davies studied. However, in some instances, the number of occurrences is so high that even though the BNC may contain lower frequencies, the numbers are still high enough to permit valid studies. For instance, while COCA contains 2,900,000 be passives, the BNC contains 890,000 examples (Davies 2015: 15), a number of occurrences that is certainly sufficient enough to study this type of passive. Moreover, frequencies alone can only be suggestive, since different types of studies may concentrate on texts with lower frequencies, especially if such studies are more qualitative in focus.

Unfortunately, such calculations presuppose that one knows precisely what linguistic constructions will be studied in a corpus. The ultimate length of a corpus might therefore be better determined not by focusing too intently on the overall length of the corpus but by focusing more on the internal structure of the corpus: the range of genres one wishes to include in the corpus, the length and number of individual text samples required to validly represent the genres that will make up the corpus, and the demographic characteristics of the individuals whose speech and writing will be chosen for inclusion in the corpus.

2.7

The Internal Structure of a Corpus

The BNC, as Table

If the BNC is compared with the International Corpus of English (ICE), a collection of comparable corpora representing the major varieties of English spoken worldwide, it turns out that while the two corpora contain the same range of genres, the genres are much more specifically delineated in ICE Corpora (see Table

While the amount of writing in the BNC greatly exceeded the amount of speech, just the opposite is true in the ICE Corpus: only 40 percent of the texts are written. While creative (or imaginative) writing was the most common type of writing in the BNC, in the ICE it is not as prominent. More prominent were learned and popular examples of informational writing: writing from the humanities, social and natural sciences, and technology (40 percent of the written texts). These categories are also represented in the BNC, although the BNC makes a distinction between the natural, applied, and social sciences and, unlike the ICE, does not include equal numbers of texts in each of these categories. The ICE also contains a fairly significant number (25 percent) of nonprinted written genres (such as letters and student writing), while only 5-10 percent of the BNC contains these types of texts.

To summarize, while there are differences in the composition of the ICE and BNC, overall the two corpora represent similar genres of spoken and written English. The selection of these genres raises an important methodological question: why these genres and not others?

The answer is that both the ICE and BNC are multi-purpose corpora; that is, they are intended to be used for a variety of different purposes, ranging from studies of vocabulary, to studies of the differences between various national varieties of English, to studies whose focus is grammatical analysis, to comparisons of the various genres of English. For this reason, each of these corpora contains a broad range of genres. But in striving for breadth of coverage, some compromises had to be made in each corpus. For instance, while the spoken part of the ICE contains legal cross-examinations and legal presentations, the written part of the corpus contains no written legal English. Legal written English was excluded from the ICE on the grounds that it is a highly specialized type of English firmly grounded in a tradition dating back hundreds of years, and thus does not truly represent English as it is written in the 1990s (the years during which texts for the ICE are being collected). The ICE also contains two types of newspaper English: press reportage and press editorials. However, as

While both the ICE and the BNC have a very clearly defined internal structure, the Corpus of Contemporary American English (COCA) differs somewhat in that it contains collections of texts (of varying length) representing five major registers: spoken (transcripts of dialogical speech taken from various television and radio shows), fiction, newspapers, magazines, and academic writing. Each of these registers contains 103-110 million words of text collected during the years 1990 through 2019 with each year containing approximately 20 million words of text (

As will be discussed in subsequent chapters, general-purpose corpora are useful for the analysis of many kinds of grammatical constructions. However, for those wishing to study a particular register, say press reportage, these corpora do not contain enough samples of the individual genres to permit full-scale studies. Consequently, many special-purpose corpora have been developed. These are corpora with a more specific focus. Two such corpora were discussed earlier in the chapter: the CEEC and the ICLE. But there are many additional such corpora as well. For instance, the Michigan Corpus of Academic Spoken English (MICASE) was created to study the type of speech used by individuals conversing in an academic setting: class lectures, class discussions, student presentations, tutoring sessions, dissertation defenses, and many other kinds of academic speech

At the opposite spectrum of special-purpose corpora like MICASE are those which have specialized uses but not for genre studies. Treebank-3 consists of a heterogeneous collection of texts totaling 100 million words, including one million words of text taken from the 1989 Wall Street Journal, as well as tagged and parsed versions of the Brown Corpus. The reason that a carefully selected range of genres is not important in this corpus is that the corpus is not intended to permit genre studies but to, for instance, "train" taggers and parsers to analyze English: to present them with a sufficient amount of data so that they can "learn" the structure of numerous constructions and thus produce a more accurate analysis of the parts of speech and syntactic structures present in English. And to accomplish this goal, all that is important is that a considerable number of texts be available, and less important are the genres from which the texts were taken.

Because of the wide availability of written and spoken material, it is relatively easy to collect material for modern-day corpora such as the BNC and ICE: The real work is in recording and transcribing spoken material, for instance, or obtaining copyright clearance for written material. With historical corpora, however, collecting texts from the various genres existing in earlier periods is a much more complicated undertaking.

In selecting genres for inclusion in historical corpora, the goals are similar to those for modern-day corpora: to find a range of genres representative of the types of English that existed during various historical periods of English. Consequently, there exist multi-purpose corpora, such as the Helsinki Corpus, which contains a range of different genres (sermons, travelogues, fiction, drama, etc.), as well as specialized corpora, such as the previously mentioned Corpus of Early English Correspondence, a corpus of letters written during the middle English period, with the original CEEC containing letters from late middle English and early modern English and a later version of the corpus (CEECE) letters from the eighteenth century. Other corpora, such as the Corpus of English Dialogues (1560-1760) and the Old Bailey

In gathering material for corpora such as these, the corpus compiler must deal with the fact that many of the genres that existed in earlier periods are either unavailable or difficult to find. For instance, even though spontaneous dialogues were as common and prominent in earlier periods as they are today, there were no tape recorders around to record speech. Therefore, there exists no direct record of speech. However, this does not mean that we cannot get at least a sense of what speech was like in earlier periods. In her study of early American English,

In other situations, a given genre may exist but be underrepresented in a given period. In his analysis of personal pronouns across certain periods in the Helsinki Corpus,

The Length of Individual Text Samples to Be Included in a Corpus

Corpora vary in terms of the length of the individual text samples that they contain. First generation corpora, such as the Brown or London-Oslo-Bergen (LOB) corpora, contain 2,000-word samples. An early corpus of spoken British English, the London-Lund Corpus, contains 5,000-word samples. In the Helsinki Corpus, text samples range from 2,000 to 10,000 words in length. And samples within the BNC vary in length, but are no longer than 40,000 words.

While many corpora contain only text samples, others contain entire texts. For instance, the Lampeter Corpus of Early Modern English Tracts, which is c. 1.1 million words in length, consists of complete texts ranging in length from 3,000 to 20,000 words. The Corpus of Global Web-Based English (1.9 billion words) also contains complete texts of varying length.

Ideally, it would be desirable to include complete texts in corpora, since even if one is studying grammatical constructions, it is most natural to study these constructions within the context of a complete text rather than only part of that text. However, there are numerous logistical obstacles that make the inclusion of complete texts in corpora nearly impossible. For instance, many texts, such as books, are quite lengthy, and to include a complete text in a corpus would not only take up a large part of the corpus but require the corpus compiler to obtain permission to use not just a text excerpt, a common practice, but an entire text, a very uncommon practice. In general, it is quite difficult to obtain permission to use copyrighted material. To avoid copyright infringement, those using the BYU corpora (such as the Corpus of Contemporary American English) are only allowed to view "snippets" in search returns of grammatical items in the corpora they are studying.

Of course, just because only text samples are included in a corpus does not mean that sections of texts ought to be randomly selected for inclusion in a corpus. It is possible to take excerpts that themselves form a coherent unit. For instance, many parts of spoken texts form coherent units themselves, containing sections that have their own beginnings, middles, and ends. Likewise, for written texts, one can include the first 2,000 words of an article, which contains the introduction and part of the body of the article, or one can take the middle of an article, which contains a significant amount of text developing the main point made in the article, or even its end. Many samples in the ICE also consist of composite texts: a series of complete short texts that total 2,000 words in length. For instance, personal letters are often less than 2,000 words, and a text sample can be comprised of complete letters totaling 2,000 words. For both the spoken and written parts of the corpus, not all samples are exactly 2,000 words: a sample is not broken off in mid-sentence but at a point (often over or just under the 2,000-word limit) where a natural break occurs. But even though it is possible to include coherent text samples in a corpus, creators and users of corpora simply have to acknowledge that corpora are not always suitable for many types of discourse studies, and that those wishing to carry out such studies will simply have to assemble their own corpora for their own personal use.

In including short samples from many different texts, corpus compilers are assuming that it is better to include more texts from many different speakers and writers than fewer texts from a smaller number of speakers and writers. And there is some evidence to suggest that this is the appropriate approach to take in creating a corpus.

In addition to studying the distribution of word categories, such as nouns or prepositions, Biber (1993: 250) calculated the frequency with which new words are added to a sample as the number of words in the sample increases. He found, for instance, that humanities texts are more lexically diverse than technical prose texts (p. 252); that is, that as a humanities text progresses, there is a higher likelihood that new words will be added as the length of the text increases than there will be in a technical prose text. This is one reason that lexicographers need such large corpora to study vocabulary trends, since so much vocabulary (in particular, open-class items such as nouns and verbs) occurs so rarely. And as more text is considered, there is a greater chance (particularly in humanities texts) that new words will be encountered.

Determining the Number of Texts and Speakers and Writers to Include in a Corpus

Related to the issue of how long text samples should be in a corpus is precisely how many text samples are necessary to provide a representative sampling of a genre, and what types of individuals ought to be selected to supply the speech and writing used to represent a genre. These two issues can be approached from two perspectives: from a purely linguistic perspective, and from the perspective of sampling methodology, a methodology developed by social scientists to enable researchers to determine how many "elements" from a "population" need to be selected to provide a valid representation of the population being studied. For corpus linguists, this involves determining how many text samples need to be included in a corpus to ensure that valid generalizations can be made about the genre, and what range of individuals need to be selected so that the text samples included in a corpus provide a valid representation of the population supplying the texts.

There are linguistic factors that need to be considered in determining the number of samples of a genre to include in a corpus, considerations that are quite independent of general sampling issues. If the number of samples included in the various genres of the BNC and ICE Corpora are surveyed, it is immediately obvious that both of these corpora place a high value on spontaneous dialogues, and thus contain more samples of this type of speech than, say, scripted broadcast news reports. This bias is a simple reflection of the fact that those creating the BNC and ICE Corpora felt that spontaneous dialogues are a very important type of spoken English and should therefore be amply represented. The reason for this sentiment is obvious: while only a small segment of the speakers of English creates scripted broadcast news reports, all speakers of English engage in spontaneous dialogues.

Although it is quite easy to determine the relative importance of spontaneous dialogues in English, it is far more difficult to go through every potential genre to be included in a corpus and rank its relative importance and frequency to determine how much of the genre should be included in the corpus. And if one did take a purely "proportional" approach to creating a corpus,

In general,

Because the BNC is a relatively lengthy corpus, it provides a sufficient number of samples of genres to enable generalizations to be made about the genres. However, with the much shorter ICE (and with other million-word corpora, such as Brown and LOB, as well), it is an open question whether the forty 2,000-word samples of academic prose contained in the ICE, for instance, are enough to adequately represent this genre. And given the range of variation that

The answer is no: While these corpora are too short for some studies, for frequently occurring grammatical constructions they are quite adequate for making generalizations about a genre. For instance,

Pseudo-titles are constructions such as rock vocalist Iggy Pop, which are similar to equivalent appositives (a rock vocalist, Iggy Pop), except that there is no comma pause between the first unit in the appositive and the second unit. While pseudo-titles are very common in American newspapers, they are stigmatized in British newspapers, occurring more frequently in tabloids than in broadsheets.

All told,

Social scientists have developed a sophisticated methodology based on mathematical principles that enables a researcher to determine how many "elements" from a "sampling frame" need to be selected to produce a "representative" and therefore "valid" sample. A sampling frame is determined by identifying a specific population that one wishes to make generalizations about. For instance, in planning the creation of the Santa Barbara Corpus of Spoken American English, it was decided that recordings of spontaneous conversations would include a wide range of speakers from around the United States representing, for instance, different regions of the country, ethnic groups, and genders. In addition, speakers would be recorded using language in a variety of different contexts, such as conversing over the phone, lecturing in a classroom, giving a sermon, and telling a story (www.linguistics.ucsb.edu/research/santa-barbara-corpus). The idea behind this sampling frame was to ensure that the corpus contained samples of speech representing the ways that people speak in different regions of the country and in different contexts.

Social scientists have developed mathematical formulas that enable a researcher to calculate the number of samples they will need to take from a sampling frame to produce a representative sample of the frame.

Sampling methodology can also be used to select the particular individuals whose speech and writing will be included in a corpus. For instance, in planning the collection of demographically sampled speech for the BNC, "random location sampling procedures" were used to select individuals whose speech would be recorded

In using sampling methodology to select texts and speakers and writers for inclusion in a corpus, a researcher can employ two general types of sampling: probability sampling and nonprobability sampling

Although probability sampling is the most reliable type of sampling, leading to the least amount of bias, for those who created first generation in corpora, this kind of sampling presented considerable logistical challenges. The mathematical formulas used in probability sampling often produce very large sample sizes, as the example above with books illustrated. And there were simply not enough resources available to create corpora beyond the million words in length. However, for many second-generation corpora, such as COCA, GloWbE, or many of the corpora accessible through Sketch Engine (www.sketchengine.co.uk/), size is not an issue, since it currently requires fewer resources to create corpora that contain millions, even billions of words. But for smaller corpora, particularly those containing copyrighted material fully accessible to the user, size becomes a more significant issue, since obtaining copyright permission is a timeconsuming and difficult task.

Judgment, or purposive, or expert choice sampling, a second type of sampling, was used to create the Brown Corpus. That is, prior to the creation of the Brown Corpus, it was decided that the writing to be included in the corpus would be randomly selected from collections of edited writing at four locations:

(1) for newspapers, the microfilm files of the New York Public Library;

(2) for detective and romantic fiction, the holdings of the Providence Athenaeum, a private library; (3) for various ephemeral and popular periodical material, the stock of a large secondhand magazine store in New York City; (4) for everything else, the holdings of the Brown University Library.

(quoted

The Time Frame for Selecting Texts

Most corpora contain samples of speech or writing that have been written or recorded within a specific time frame. Synchronic corpora (i.e. corpora containing samples of English as it is presently spoken and written) contain texts created within a relatively narrow time frame.

For instance, the Brown and LOB Corpora contain written texts published in 1961. The written and spoken texts included within the BNC were published/recorded in the late twentieth century (www .natcorp.ox.ac.uk/). The COCA contains texts created between the years 1990 and 2019, with new texts being added on a yearly basis. The Collins Corpus, used as the basis for creating the COBUILD dictionaries, is a monitor corpus that is currently 4.5 billion words in length (

In creating a synchronic corpus, the corpus compiler wants to be sure that the time frame is narrow enough to provide an accurate view of contemporary English undisturbed by language change. However, linguists disagree about whether purely synchronic studies are even possible: New words, for instance, come into the language every day, indicating that language change is a constant process. Moreover, even grammatical constructions can change subtly in a rather short period of time.

With diachronic corpora (i.e. corpora used to study historical periods of English), the time frame for texts is somewhat easier to determine, since the various historical periods of English are fairly well-defined. However, complications can still arise. For instance,

The Linguistic Background of Speakers and Writers Whose English Is Included in a Corpus

The previous sections provided descriptions of various corpora of English, and the many methodological issues that one must address both in the creation and analysis of a corpus. But one issue that has not been discussed so far concerns the linguistic backgrounds of those whose speech or writing has been included in a corpus. If one is creating a corpus of, say, spoken American English, should the speech of only native speakers of American English be included?

And if the answer is yes, how does one determine exactly what a native speaker is?

As

It is therefore quite important that those creating corpora explicitly define the population whose speech will be sampled. For instance, all ICE corpora contain texts representing "the English of adults (age 18 or over) who have been educated through the medium of English to at least the end of secondary schooling" (www.ucl.ac.uk/englishusage/projects/ice.htm). Note that this description does not necessarily exclude bilingual speakers from the corpus. It simply assures that individuals whose speech will be included in the corpus have had exposure to English over a significant period. The BNC is defined as "a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English" (www.natcorp.ox.ac.uk/docs/URG/BNCdes.html#spodes). This restriction allows for a certain degree of flexibility as well, as it would permit a variety of different speakers and writers of British English to be represented in the corpus.

In non-native varieties of English, the level of fluency among speakers will vary considerably: as

To determine whether an individual's speech or writing is appropriate for inclusion in a corpus (and also to elicit the sociolinguistic variables), one can have individuals contributing texts to a corpus fill out a biographical form in which they supply the information necessary for determining whether their native or non-native speaker status meets the criteria for inclusion in the corpus. For instance, individuals can be asked what languages they speak, how long they have spoken them, and in what contexts they have used them, such as in the home, workplace, school, and so forth. If the residence history on the biographical form is unclear, it is also possible to interview the individual afterwards, provided that he or she can be located; if the individual does not fit the criteria for inclusion, his or her text can be discarded.

Determining native or non-native speaker status from authors of published writing can be considerably more difficult, since it is often not possible to locate authors and have them fill out biographical forms. In addition, it can be misleading to use an individual's name alone to determine native speaker status, since someone with a non-English sounding name may have immigrant parents and nevertheless be a native speaker of English, and many individuals with English sounding names may not be native speakers: One of the written samples of the American component of ICE had to be discarded when the author of one of the articles called on the telephone and explained that he was not a native speaker of American English but Australian English.

In spoken dialogues, one may find out that one or more of the speakers in a conversation do not meet the criteria for inclusion because they are not a native speaker of the variety being collected. However, this does not necessarily mean that the text must be excluded from the corpus, since there is annotation that can be included in a corpus indicating that certain sections of a sample are "extra-corpus" material; that is, material not considered part of the corpus for purposes of word counts, generating KWIC (key word in context), and so forth.

Controlling for Sociolinguistic Variables

There are a variety of sociolinguistic variables that will need to be considered before selecting the speakers and writers whose texts are being considered for inclusion in a corpus. Some of these variables apply to the collection of both spoken and written texts; others are more particular to spoken texts. In general, when selecting individuals whose texts will be included in a corpus, it is important to consider the implications that their gender, age, and level of education will have on the ultimate composition of the corpus. For the spoken parts of a corpus, several additional variables need to be considered: the dialects the individuals speak, the contexts in which they speak, and the relationships they have with those they are speaking with. The potential influences that these variables have on a corpus are summarized in the following categories.

Gender Balance

It is relatively easy when collecting speech and writing to keep track of the number of males and females from whom texts are being collected. Information on gender (which is defined biologically in corpus linguistics) can be requested on a biographical form, and in written texts, one can often tell the gender of an individual by his or her first name.

Achieving gender balance in a corpus involves more than simply ensuring that half the speakers and writers in a corpus are female and half male. In certain written genres, such as scientific writing, it is often difficult to achieve gender balance because writers in these genres are predominantly malean unfortunate reality of modern society. To attempt to collect an equal proportion of writing from males and females might actually misrepresent the kind of writing found in these genres. Likewise, in earlier periods, men were more likely to be literate than women and thus to produce more writing than women. To introduce more writing by females into a corpus of an earlier period distorts the linguistic reality of the period. A further complication is that much writing, particularly scientific writing, is co-written, and if males and females collaborate, it will be difficult to determine precisely whose writing is actually represented in a sample. One could collect only articles written by a single author, but this again might lead to a misrepresentation of the type of writing typically found in a genre. Finally, even though an article may be written by a female or a male, there is no way of determining how much an editor has intervened in the writing of an article and thus distorted the effect that the gender of the author has had on the language used in the article.

In speech, other complications concerning gender arise. Research has shown that gender plays a crucial role in language usage. For instance, women will speak differently with other women than they will with men. Consequently, to adequately reflect gender differences in language usage, it is best to include in a corpus a variety of different types of conversations involving males and females: women speaking only with other women, men speaking only with other men, two women speaking with a single man, two women and two men speaking, and so forth.

To summarize, there is no one way to deal with all the variables affecting the gender balance of a corpus. The best that the corpus compiler can do is to be aware of the variables, confront them head on, and deal with them as much as is possible during the construction of a corpus.

Age

There are ranges of age groups that have been included in the many corpora that have been created. For instance, there are special-purpose corpora containing the speech of individuals up to the age of 16. The Child Language Data Exchange System, or CHILDES Corpus, includes transcriptions of children engaging in spontaneous conversations in English and other languages. The Bergen Corpus of London Teenager English (COLT) contains the conversations of adolescents aged 13-17 years. The Polytechnic of Wales Corpus contains transcriptions of conversations between children (aged 6-12 years) and a "friendly" adult concerning their "favourite games or TV programmes."

Overall, though, corpora have tended to contain the speech of adults, largely because to collect the speech of children and adolescents, one often must obtain the permission not just of the individual being recorded but of his or her parents as well, a complicating factor in an already complicated endeavor.

Dialect Variation

It is also important to consider the extent to which a corpus should contain a range of dialects, both social and regional, that exist in any language.

In many respects, those creating historical corpora have been more successful in representing regional variation than those creating modern-day corpora: The regional dialect boundaries in Old and Middle English are fairly well-established, and in the written documents of these periods, variant spellings reflecting differences in pronunciation can be used to posit regional dialect boundaries. For instance,

Because writing is now quite standardized, it no longer contains traces of regional pronunciations. However, even though the modernday corpus linguist has access to individuals speaking many different regional and social varieties of English, it is a significant undertaking to create a spoken corpus that is balanced by region and social class. If one considers only American English, a number of different regional dialects can be identified, and within these major dialect regions, one can isolate numerous sub-dialects (e.g. Boston English within the coastal New England dialect). If social dialects are added to the mix of regional dialects, even more variation can be found, as a social dialect such as African-American Vernacular English can be found in all major urban areas of the United States. In short, there are numerous dialects in the United States, and to attempt to include representative samplings of each of these dialects in the spoken part of a corpus is nothing short of a methodological nightmare.

What does one do, then, to ensure that the spoken part of a corpus contains a balance of different dialects? In selecting speakers for inclusion in the BNC, twelve dialect regions were identified in Great Britain, and from these dialect regions, 100 adults of varying social classes were randomly selected as those whose speech would be included in the corpus

Because it is not logistically feasible in large countries such as the United States or Great Britain to create corpora that are balanced by region and social class, some corpus linguists have devoted their energies to the creation of corpora that focus on smaller dialect regions.

Social Contexts and Social Relationships

Speech takes place in many different social contexts and among speakers between whom many different social relationships exist. When we work, for instance, our conversations take place in a specific and very common social contextthe workplaceand among speakers of varying types: equals (e.g. co-workers), between whom a balance of power exists, and disparates (e.g. an employer and an employee), between whom an imbalance of power exists. Because the employer has more power, he or she is considered a "superordinate" in contrast to the employee, who would be considered a "subordinate." At home (another social context), other social relationships exist: a mother and her child are not simply disparates but intimates as well.

There is a vast amount of research that has documented how the structure of speech is influenced by both the social context in which speech occurs and the social relationships existing between speakers. As

The various components of the ICE (www.ice-corpora.uzh.ch/en/ design.html) contain spontaneous conversations taking place in many different social contexts, ranging from face-to-face conversations to classroom discussions. The Michigan Corpus of Academic Spoken English (MICASE) contains samples of academic speech occurring in many different academic contexts, such as lectures given by professors to students as well as conversations between students in study groups. This ensured that the ultimate corpus created would represent the broad range of speech contexts in which academic speech occurs

Mega Corpora

The previous sections have discussed several methodological issues that need to be considered as one plans and creates a corpus. But as corpora have grown larger, it has become a much more complicated undertaking to ensure that a corpus is both balanced and representative. This is especially the case with web-based corpora, which are quite large in size and whose content is sometimes difficult to determine. For instance, at 1.9 billion words in length, the Corpus of Global Web-Based English is so lengthy that it would be impossible to determine not just the content of the corpus but the distribution of such variables as the gender of contributors, their ages, and so forth.

At the other end of the spectrum are those who would question the necessity of highly planned corpora such as the BNC.

Conclusions

To create a valid and representative corpus, it is important, as this chapter has shown, to carefully plan the construction of a corpus before the collection of data even begins. This process is guided by the ultimate use of the corpus. If one is planning to create a multi-purpose corpus, for instance, it will be important to consider the types of genres to be included in the corpus; the length not just of the corpus but of the samples to be included in it; the proportion of speech versus writing that will be included; the educational level, gender, and dialect backgrounds of speakers and writers included in the corpus; and the types of contexts from which samples will be taken. However, because it is virtually impossible for the creators of corpora to anticipate what their corpora will ultimately be used for, it is also the responsibility of the corpus user to make sure that the corpus he or she plans to conduct a linguistic analysis of is a valid corpus for the particular analysis being conducted. This shared responsibility will ensure that corpora become the most effective tools possible for linguistic research.

in italics or boldface, and to mark other features that are particular to written texts.

Linguistic annotation: The process of annotating a corpus involves running software that can (1) tag a corpus (add part-of-speech tags to all of the words in the corpus, such as nouns, prepositions, and verbs), or (2) parse a corpus (add markup that identifies larger structures, such as verb phrases, prepositional phrases, and adverbials). Grammatical markup is inserted when a corpus is tagged or parsed.

Metadata is a key component of any corpus: users need to know precisely what is in a corpus. Textual markup is important too, though corpora will vary in terms of how much of such markup they contain. For instance, marking segments of overlapping speech can be a timeconsuming process. Consequently, some spoken corpora may not mark where speakers overlap. Linguistic annotation varies from corpus to corpus as well. While it is quite common for corpora to be lexically tagged, parsing a corpus is a much more complicated process. Therefore, relatively few corpora have been parsed.

Even though the processes of collecting and encoding data for inclusion in a corpus are described above as separate processes, in many senses they are closely connected: After a conversation is recorded, for instance, it may prove more efficient to transcribe it immediately, since whoever made the recording will be available either to transcribe the conversation or to answer questions about the recording to aid in its transcription. If a text is collected, and saved for later computerization and annotation, the individual who collected the text may not be around to answer questions, and information about the text may consequently be lost. Of course, logistical constraints may necessitate collecting texts at one stage and computerizing and annotating them at a later stage, in which case it is crucial that as much information about the text be obtained initially so that, at a later stage, those working with the text will be able to easily recover this information. The kinds of information to collect will be discussed in greater detail in Section 3.2.

General Considerations

As discussed in Chapter 2, before the actual data for a corpus is collected, it is important to carefully plan exactly what will be included in the corpus: the kinds and amounts of speech and/or writing, for instance, as well the range of individuals whose speech and writing will become part of the corpus. Once these determinations are made, the corpus compiler can begin to collect the actual speech and writing to be included in the corpus. However, it is important not to become too rigidly invested in the initial corpus design, since obstacles and complications may be encountered while collecting data that may require changes in the initial corpus design: It might not be possible, for instance, to obtain recordings for all the genres originally planned for inclusion in the corpus, or copyright restrictions might make it difficult to obtain certain kinds of writing. In these cases, changes are natural and inevitable and, if they are carefully made, the integrity of the corpus will not be compromised.

The International Corpus of English (ICE) Project provides a number of examples of logistical realities that forced changes in the initial corpus design for some of the components. After the project began, it was discovered that not all the regional groups involved in the project would be able to collect all of the text categories originally planned for inclusion in the corpus. For instance, in ICE-East Africa (which includes texts collected in Kenya and Tanzania), it was not possible to collect examples of scripted monologues, such as legal presentations and scripted commentaries. However, to make up for this deficiency, additional texts were collected in the other categories: 120 scripted monologues (e.g. news broadcasts and speeches) instead of the 50 samples required in the other components of the ICE (

Collecting Samples of Speech

Speech is the primary mode of human communication. As a result, there are various types of speech: not just spontaneous multiparty dialogues but scripted and unscripted monologues, radio and television interviews, telephone conversations, class lectures, and so forth. Given the wealth of speech that exists, as well as the logistical difficulties involved in recording and transcribing it, collecting data for the spoken part of a corpus is much more labor-intensive than collecting written samples. As

In collecting any kind of speech, the central concern is obtaining speech that is "natural." This is a particularly important issue when gathering spontaneous multi-party dialogues, such as informal talks between two or more individuals. If multi-party dialogues are not carefully collected, the result can be a series of recordings containing very stilted and unnatural speech. Collecting "natural" multi-party dialogues involves more than simply recording people as they converse. As anyone who has collected language data knows, if speakers know that their speech is being recorded, they will change the way that they speak, a phenomenon called the "observer's paradox"

To avoid this problem, those creating earlier corpora, such as the London-Lund Corpus, recorded people surreptitiously, and only after recordings were secretly made were individuals informed that they had been recorded. While it may have been acceptable and legal back in the1950s and 1960s to record individuals without their knowledge, now such recordings are not only considered unethical within the scientific community but may in fact be illegal in many locales. It is therefore imperative both to inform individuals that their speech is being recorded and to obtain written permission from them to use their speech in a corpus. This can be accomplished by having individuals being recorded sign a release form prior to being recorded. In addition, many universities and other organizations have Institutional Review Boards that review any research conducted on "human subjects" by faculty members and will grant approval for use of such subjects only if accepted practices are followed.

Since it is not possible to include surreptitious speech in a corpus, does this mean that non-surreptitiously gathered speech is not natural? This is an open question, since it is not possible to answer it with any definite certainty. However, there are ways to increase the probability that that the speech included in a corpus will be natural and realistic.

First, before individuals are recorded, they should be given in written form a brief description of the project in which they are participating. In this description, the purpose of the project should be described, and it should be stressed that speech is being collected for descriptive linguistic research, not to determine whether those being recorded are speaking "correct" or "incorrect" English. In a sense, these individuals need to be given a brief introduction to a central tenet of modern linguistics: that no instance of speech is linguistically better or worse than any other instance of speech, and that all types of speech are legitimate, whether they are perceived as standard or nonstandard.

A second way to enhance naturalness is to record as lengthy a conversation as possible so that when the conversation is transcribed, the transcriber can select the most natural segment of speech from a much lengthier speech sample, for instance, 30 minutes or longer. This length of conversation increases the probability that a natural and coherent sample of speech can be extracted from this longer sample. The initial part of the conversation can then be discarded, since people are sometimes nervous and hesitant upon first being recorded but after a while become less self-conscious and start speaking more naturally. Moreover, a lengthier segment allows the corpus compiler to select a more coherent and unified segment of speech to ultimately include in the corpus.

How much speech needs to be recorded is also determined by the type of speech being recorded. Spontaneous dialogues, for instance, may require lengthier segments of recorded speech because of features such as hesitations, pauses, and interruptionsall of which slow down the pace of speech. On the other hand, monologues (especially scripted monologues) contain far fewer pauses and hesitations. Thus, less speech is needed to reach these necessary numbers of words for the particular corpus being created.

When it comes time to actually make recordings, whoever is making the recordings needs to follow a few basic principles of recording to ensure the most natural recordings as possible. Probably the least desirable way to make a recording is to have the research assistant sitting silently nearby with microphone in hand while the people being recorded converse. This all but ensures that the individuals being recorded will constantly be reminded that they are part of a "linguistic experiment." As much as possible, those making recordings should try to record individuals in the actual environments in which they typically speak, such as the family dinner table, the workplace, restaurants, the car, informal get-togethers and so forth.

Because the goal is to record people in natural speaking environments, it is best for the research assistant not to be present during recordings. He or she can simply set up the recording equipment, turn on the recorder, and leave. Alternatively, the people being recorded can be loaned the recording equipment and taught to set it up and turn on the recorder themselves. This latter option was used to gather speech in the demographic component of the British National Corpus (BNC). Individuals participating in the project were given portable tape recorders and an adequate supply of cassettes, and were instructed to record all of the conversations they had for periods ranging from two to seven days (Crowdy 1993: 260; www.natcorp.ox.ac.uk/docs/URG/BNCdes .html#body.1_div.1_div.5_div.1). To keep track of the conversations they had, participants filled out a logbook, indicating when and where the recordings occurred as well as who was recorded. This method of collection habituates participants to the process of being recorded and, additionally, ensures that a substantial amount of speech is collected.

In certain natural speaking environments, such as restaurants or automobiles, there will often be a considerable amount of ambient noise, resulting in recordings containing variable amounts of inaudible speech that cannot be accurately transcribed. This problem can be handled with annotation during the actual transcription of the recording that marks certain sections as inaudible. It can also sometimes be minimized using commonly available audio editing software. If high-quality recordings are desired, individuals can be recorded in an actual recording studio, as was done in collecting speech samples for the Map Task Corpus, a corpus of conversations between individuals giving directions to various locations (see

While collecting natural speech is a key issue when recording multi-party dialogues, it is less of an issue with other types of speech. For instance, those participating in radio and television broadcasts will undoubtedly be conscious of the way they are speaking, and therefore may heavily monitor what they say. However, heavily monitored speech is "natural" speech in this context. Therefore, this is precisely the kind of speech one wants to gather. Other types of spoken language, such as public speeches (especially if they are scripted), are also heavily edited.

When recording any kind of spoken English, it is important to consider the quality of the audio recorder and microphone to be used. In earlier corpora, most corpus compilers used analog recorders and cassette tapes, since such recorders were small and unobtrusive and cassette tapes were inexpensive. However, with the rise of digital technology, there are now a variety of recorders that are widely available, such as digital audiotape (DAT) recorders as well as Minidisc recorders. These recorders make high-quality digital recordings, which can then be exported and used in special software programs that aid in the transcription of speech.

It is equally important to consider the quality and type of microphone to be used to make recordings. A low-quality microphone will produce recordings that are "tinny" even if a good tape recorder is used. It is therefore advisable to invest resources in good microphones, and to obtain microphones that are appropriate for the kinds of recordings being made. To record a single individual, it is quite acceptable to use a traditional uni-directional microphone, a microphone that records an individual speaking directly into the microphone. For larger groups, however, it is better to use omni-directional microphones: microphones that can record individuals sitting at various angles from the microphone. Lavaliere microphones, which are worn around the neck, are useful for recording individuals who might be moving around when they speak, as people lecturing to classes or giving speeches often do. Wireless microphones are appropriate in recording situations of this type too, and avoid the problem of the speaker being constrained by the length of the cord attaching the microphone and recorder. There are also extra-sensitive microphones for recording individuals who are not close to the microphone, as is the case in a class discussion, where those being recorded are spread out all over a room and might not be close enough to a traditional microphone for an audible recording to be made. For recording telephone conversations, special adapters can be purchased that record directly off landline telephones. For cell phones, there are apps that can be used to record conversations. High-quality microphones can be fairly expensive, but they are worth the investment.

To find information on specific recorders and microphones, it is useful to consult what researchers who specialize in the study of spoken language use to make voice recordings. For instance, The Division of Psychology and Language Sciences at University College London provides a listing of recorders and microphones used by individuals doing research in areas such as phonetics and speech pathology (www .phon.ucl.ac.uk/resource/audio/recording.html). Anthropologists and ethnographers who do fieldwork also make use of recording equipment for recording spoken language (

Even with the best recording equipment, however, assembling a large number of recordings suitable for inclusion in a corpus is a timeconsuming and sometimes frustrating process: for every 10 recordings made, it may turn out that some of them are unusable. For instance, conversants might be speaking naturally for certain periods of time, only to stop and say something irrelevant, such as "Is the recorder still working?" Remarks such as this illustrate that no matter how hard one tries, it is impossible to make many individuals forget that their speech is being monitored and recorded. Other problems include excessive background noise that makes all or part of a conversation inaudible, or people who are given a recorder to record their speech and then operate it improperly, in some cases not recording any of the conversation they set out to record. Those compiling spoken corpora should therefore expect to gather much more speech than they will actually use to compensate for all the recordings they make that contain imperfections preventing their use in the ultimate corpus being created.

While most of the recordings for a corpus will be obtained by recording individuals with microphones, many corpora will contain sections of broadcast speech. This type of speech is best recorded not with a microphone but directly from a radio or television by running a cord from the audio output plug on the radio or television to the audio input plug on the tape recorder. It is also possible to get a wide variety of written transcriptions of broadcast speech from such sources as talk shows, interviews, and news conferences. But using such secondhand sources raises questions about the accuracy of the transcriptions: the extent to which what is transcribed accurately matches what was actually said.

Inaccuracies in such transcriptions can potentially be a problem. Molin (2007), for instance, found that in the Hansard Corpus of Canadian parliamentary proceedings, transcriptions of the proceedings did not always capture the precise language that was used by conversants. In many cases, changes were made so that usages that parliamentarians actually uttered conformed more to prescriptive norms: contracted forms were changed to full forms (e.g. don't to do not) or be going to was replaced with will (p. 207). In contrast, in a corpus containing transcripts from the broadcast network

A perusal of several transcripts and recordings made by the broadcast network NPR (www.npr.org/templates/transcript/transcript.php? storyId=15166387) revealed occasional mistranscriptions, but overall, a close fidelity with the actual language transcribed. One of the few errors observed involved transcribing react in the example below instead of the form actually used in the recording: interact.

It's also changed the way they react with doctors, their families, and even with strangers.

But it is important to note to that there will always be errors in any transcription, whether it is based directly on a recording or taken from a transcription made available by a second party. The ultimate goal is to get as much accuracy as possible and be practical. Therefore, the time saved in using second-party transcripts is worth the tolerance of a certain level of error, provided that some checking is done to ensure overall accuracy in the transcripts included in a corpus. The process of transcribing speech will be discussed in greater detail in 3.x.

Collecting Samples of Writing

Although collecting samples of writing is considerably less complicated than collecting samples of speech, one significant obstacle is encountered when collecting writing: copyright restrictions. Under the "fair use" provisions of current U.S. copyright laws, it is possible in certain circumstances to use copyrighted material without receiving the explicit permission from the copyright holder. However, the "circumstances" are stated rather generally and are subject to interpretation (www.copyright.gov/fair-use/more-info.html). Thus, including something in a corpus without getting explicit permission from the copyright holder could involve copyright infringement.

In many first-generation corpora, such as the Brown and BNC, clearance was obtained for all copyrighted material. But because of the huge size of recent mega-corpora, obtaining such clearance is simply not possible. With the Corpus of Contemporary American English (COCA), though, workarounds were implemented that allowed users full access to the corpus without violating copyright law. For instance, searches of the corpus retrieve only "very short 'Keyword in Context' displays, where users see just a handful of words to the left and the right of the word(s) searched for." Additionally, the corpus can be used only for academic research (www.english-corpora.org/copyright.asp).

If attempts are made to secure permission to use copyrighted material in a corpus, it is best to over collect the number of texts to include in each part of the corpus: A written text may be gathered for possible inclusion in a corpus, and its author (or publisher) may not give permission for its use, or (as was a common experience gathering written texts for inclusion in the American component of ICE) contact is made requesting permission but no reply is ever received.

Because of the difficulties in obtaining permission to use copyrighted materials, most corpus compilers have found themselves collecting far more written material than they are able to obtain permission to use: for ICE-USA, permission had been obtained to use only about 25 percent of the written texts initially considered for inclusion in the corpus. Moreover, some authors and publishers will ask for money to use their material: one publisher requested half an American dollar per word for a 2,000-word sample of fiction considered for inclusion in ICE-USA! Therefore, if a corpus is to be used only for non-profit academic research, this should be clearly stated in any letter of inquiry or email requesting permission to use copyrighted material and many publishers will sometimes waive fees. However, if there will be any commercial use of the corpus, special arrangements will have to be made both with publishers supplying copyrighted texts and those making use of the corpus.

In gathering written texts for inclusion in a corpus, the corpus compiler will undoubtedly have a predetermined number of texts to collect within a range of given genres: twenty 2,000-word samples of fiction, for instance, or ten 2,000-word samples of learned humanistic writing. However, because there is so much writing available, it is sometimes difficult to determine precisely where to begin to locate texts. Since most corpora are restricted to a certain time frame, this frame will of course narrow the range of texts, but even within this time frame, there is an entire universe of writing.

In earlier corpora, written texts needed to be scanned or re-typed to be converted into digital formats. Currently, however, most written texts are also available in digital formats. For instance, newspapers and magazines are accessible in digital formats that can be easily used in corpora. Many commercial publications, such as novels, are also available digitally, but often in formats that cannot be copied.

Keeping Records of Texts Gathered

As written texts are collected and spoken texts are recorded, it is imperative that accurate records are kept about the texts and the writers and speakers that created them. For ICE-USA, research assistants filled out a written checklist supplying specific information for each spoken and written text that was collected.

First of all, each text was assigned a number that designated a specific genre in the corpus in which the sample would be potentially included. For instance, a text numbered S1A-001 would be the first sample considered for inclusion in the genre of "direct conversations"; a text numbered S1B-001, on the other hand, would be the first sample considered for inclusion in the genre of "classroom lectures." A numbering system of this type (described in detail in Greenbaum 1996b: 601-14) allows the corpus compiler to keep easy record of where a text belongs in a corpus and how many samples have been collected for that part of the corpus. After a text was numbered, it was given a short name providing descriptive information about the sample. In ICE-Great Britain, sample S1A-001 (a spontaneous dialogue) was named "Instructor and dance student, Middlesex Polytechnic" and sample S1B-001 (a class lecture) was entitled "Jewish and Hebrew Studies, 3rd year, UCL." The names supplied to a text sample are short and mnemonic and give the corpus compiler (and future users of the corpus) an idea of the type of text that the sample contains.

The remaining information recorded about texts depended very much on the type of text that was being collected. For each spoken text, a record was kept of when the text was recorded, where the recording took place, who was recorded, who did the recording, and how long the recording was. For each person recorded, a short excerpt of something they said near the start of the recording was written down so that whoever transcribed the conversation would be able to match the speech being transcribed with the speaker. It can be very difficult for a transcriber to do this if he or she has only the recording to work with and must figure out who is speaking. For speech samples recorded from television or radio, additional information was written down, such as what station the recording was made from, where the station is located, and who should be contacted to obtain written permission to use the sample. For written texts, a complete bibliographical citation for the text was recorded along with the address of the publisher or editorial office from which permission to use the written text could be obtained.

In addition to keeping records of the texts that are recorded, it is equally important to obtain ethnographic information from individuals contributing either a sample of speech or a written text. The particular information collected will very much depend on the kind of corpus being created and the variables that future users of the corpus will want to investigate. Because ICE-USA is a general-purpose corpus, only fairly general ethnographic information was obtained from contributors: their age, gender, occupation, and a listing of the places where they had lived over the course of their lives. Other corpora have kept different information on individuals, relevant to the particular corpus being created. The Michigan Corpus of Academic Spoken English (MICASE) collected samples of spoken language in an academic context. Therefore, not just the age and gender of speakers in the corpus were recorded but their academic discipline (e.g. humanities and arts, biological and health sciences), academic level (e.g. junior undergraduates, senior faculty), nativespeaker status, and first language

Ethnographic information is important because those using the ultimate corpus that is created might wish to investigate whether gender, for instance, affects conversational style, or whether younger individuals speak differently than older individuals. It is important for these researchers to be able to associate variables such as these with specific instances of speech. While it is relatively easy to obtain ethnographic information from individuals being recorded (they can simply fill out the form when they sign the permission form), tracking down writers and speakers on radio or television shows can be very difficult. Therefore, it is unrealistic to expect that ethnographic information will be available for every writer and speaker in a corpus. And indeed, many current corpora, such as the BNC and ICE, contain missing information on many speakers and writers. After all the above information is collected, it can be very useful to enter it into a database, which will allow the progress of the corpus to be tracked. This database can contain not just information taken from the forms described above but other information as well, such as whether the text has been computerized yet, whether it has been proofread, and so forth. Creating a corpus is a huge undertaking, and after texts are collected, it is very easy to file them away and forget about them. It is therefore crucial to the success of any corpus undertaking that accurate information be kept about each text to be considered for inclusion in the corpus.

Encoding Spoken and Written Data

Encoding spoken data is a much more complicated process than encoding written data.

To encode spoken data, recordings of speech first need to be transcribed, an extremely lengthy process requiring the transcriber to listen to the same segments of speech repeatedly until an accurate transcription is achieved. Although the process of transcription has been automated, current voice recognition technology has not reached the level of sophistication to be able to accurately transcribe the most common type of speech: spontaneous conversations.

In earlier corpora, written texts, which existed in printed form, had to be optically scanned into a computera process that often produced digitized texts with numerous scanning errors that had to be manually corrected. However, so many written texts are now available in digitized formats that scanning is mainly restricted to texts dating back to the pre-electronic era.

There are a number of general considerations to bear in mind when beginning the process of computerizing both spoken and written texts. First, because texts to be included in a corpus will be edited with some kind of text editing program, it may be tempting to save computerized texts in a file format used by a word processing program (such as files with the extension .doc in Microsoft Word). However, these files will be incompatible with any of the programs customarily used with corpora, such as concordancing programs.

In earlier corpora, the standard was the ASCII (or text) file format, a format that had both advantages and disadvantages. The main advantage at the time was that ASCII was a universally recognized text format that could be used with any word processing program and the numerous software programs designed to work on corpora, such as taggers and parsers and concordances. The disadvantage was that because ASCII has a fairly limited set of characters, many characters and symbols cannot be represented in it. The creators of the Helsinki Corpus had to develop a series of special symbols to represent characters in earlier periods of English that are not part of the ASCII character set: the Old English word "ðaet" ("that"), for instance, is encoded in the corpus as "+t+at" with the symbol "+t" corresponding to the Old English thorn "ð" and the symbol "+a" to the Old English ash "ae"

When creating a corpus, it is easiest to save individual texts in separate files stored in directories that reflect the hierarchical structure of the corpus. This does not commit one to distributing a corpus in this format: the ICAME CD-ROM (2nd ed.) allows users to work with an entire corpus saved in a single file. But organizing a corpus into a series of directories and sub-directories makes working with the corpus much easier, and allows the corpus compiler to keep track of the progress being made on corpus as it is being created. Figure

Each ICE component consists of texts in two main directoriesone containing all the spoken texts included in the corpus, the other all the written texts. These two directories, in turn, are divided into a series of sub-directories containing the main types of speech and writing that were collected: the spoken part into monologues and dialogues, the written part into printed and non-printed material.  Each text included in a given ICE component is assigned an identification letter and number indicating the type of speech or writing that it represented. For instance, for the 90 texts labeled S1A-001 to S1A-090, the letter S indicates that each 2,000-word sample represents spoken English; the numerals 001-090 that it was a private conversation (either a spontaneous conversation or a telephone call); and the uppercase A that it was a dialogue. While this numbering system is unique to ICE components, a similar system can be developed for any corpus project.

Other directories can be created to fit the needs of the research team building a particular corpus. For instance, a "draft" directory is useful for early stages of corpus development and can contain spoken texts that are in the process of being transcribed or written texts that have been computerized but not proofread. Once a draft version of a text contains "metadata" and "textual markup", it can then be placed into a "lexical (pending proofreading)" directory to indicate that the text will be ready for use as a lexical version of the corpus once it has been proofread. There are then two stages of proofreading. The first stage involves proofreading each individual sample in a corpus with samples being placed into a "proofread 1" directory. A second round of proofreading is done after completion of the entire corpus so that the corpus can be viewed as a whole.

While a text is being worked on at a particular stage of analysis, it receives an additional file extension to indicate that work on the text is in progress. For instance, while a draft version of a text in the category of business transactions is being created, the text is saved as "S1B-071di", the "i" indicating that work on the text is incomplete. As a particular text is being worked on, a log is maintained that notes what work was done on the text and what work needs to be done. At each stage of analysis, to avoid duplication of work, it is most efficient to have a single person work on a text; at the proofreading stage, it is best to have the text proofread by someone not involved with any prior version of the text, since he or she will bring a "fresh" perspective to the text.

Finally, although inserting "structural" markup into a text is separate from the process of actually computerizing the text, there are many instances where markup can be inserted while texts are being computerized. For instance, in transcribing spontaneous conversations, the transcriber will encounter numerous instances of overlapping speechindividuals speaking at the same time. The segments of speech that overlap need to be marked so that the eventual user of the corpus knows which parts overlap in the event that he or she wishes to study overlapping speech. If annotating overlapping segments of speech is done separately from the actual transcription of the text, the individual doing the annotation will have to go through the tape repeatedly to reconstruct the overlapsa process that could be done more efficiently by the person doing the transcription. Likewise, speaker identification tagstags indicating who is speakingare more efficiently inserted during the transcription of texts. With written texts, if two line breaks are inserted between paragraphs while a text is being computerized, then paragraph tags can be inserted automatically at a later stage. Of course, some markup is probably better inserted after a text sample is computerized. But because computerizing and annotating a text is such an integrated process, it is best to combine the processes whenever this is possible.

But there are caveats to the process of creating a corpus outlined in this section. As corpora have become larger and larger, oftentimes containing millions of words of text, the feasibility of, for instance, proofreading a corpus or placing individual samples into neatly delineated sections becomes less viable: Such corpora are simply too large for any kind of proofreading to be done, or for a single text type (such as a press editorial from a particular newspaper) to be placed into a single directory.

Transcribing Speech

Traditionally, speech had been transcribed using a special transcription machine that has a foot pedal that stops and starts a cassette tape and also automatically rewinds the tape to replay a previous segment. As anyone who has ever transcribed speech knows, the flow of speech is much faster than the ability of the transcriber to type. Therefore, it is extremely important to have the capability of automatically replaying segments.

Because of recent advances in computer technology, it is now possible to use software programs designed specifically to transcribe samples of speech that have been digitized. "VoiceWalker 3.0b" was developed to aid in the transcription of texts included within the Santa Barbara Corpus of Spoken American English. "SoundScriber" is a similar program used to transcribe texts that are part of the MICASE (www-personal.umich.edu/~ebreck/code/sscriber/). Both of these programs are available at the above URLs as freeware, and work very much like cassette transcription machines: the transcriber opens a word processing program in one window and the transcription program in another. After a sample of digitized speech is loaded into the program, short segments of the sample can be automatically replayed until an accurate transcription is achieved. These programs accept as input a variety of different audio formats, including WAV files as well as MP3 files. And for files in different audio formats, the open source program Audacity can be used to convert many different audio files into WAV or MP3 files (www .audacityteam.org/). This program can also be used to convert cassette tapes to digital formats.

While transcription machines and software can ease the process of transcribing speech, there is no getting around the fact that speech must be manually transcribed. And while there are no immediate prospects that this process will be automated, speech recognition programs have improved considerably in recent years, and, in the near future, offer the hope that they can at least partially automate the process of transcribing speech.

Early transcription programs, such as Dragon Dictate (

More recently, there have been advances in the development of programs that can automatically transcribe samples of digitized speech. However, the accuracy of such transcriptions depends upon the type of speech that is being transcribed. Monologues (particularly those that are scripted rather than spontaneous) are the easiest types of speech to automatically transcribe because the voice of a single person is all that needs to be recognized. In contrast, spontaneous unscripted dialogues are much more difficult to transcribe because such speech contains multiple speakers whose conversations contain, for instance, hesitations, overlaps, re-formulations, and incomplete sentences.

But despite these difficulties, there has been progress in developing programs that may in the future help automate the transcriptions of spontaneous dialogues.

To test their system,

Transcribing speech is in essence a highly artificial process, since an exclusively oral form of language is represented in written form. Consequently, before any transcription is undertaken, it is important to decide just how much that exists in a spoken text one wishes to include in a transcription of it. Compilers of corpora have varied considerably in how much detail they have included in their transcriptions of speech. The spoken sections of the COCA do not contain speech that was recorded and transcribed in-house. Instead, spoken samples in this corpus consist entirely of transcriptions of numerous radio and television programs that were created by a thirdparty. Given the cost and effort of creating a corpus of speech, it is understandable why corpora of this type exist, and while they do not contain spontaneous face-to-face conversations, they do provide a substantial amount of spoken language occurring in other speechbased registers.

At the other extreme are corpora of speech that attempt to replicate in a transcription as much information as possible about the particular text being transcribed. The Santa Barbara Corpus of Spoken American English, for instance, contains not only an exact transcription of the text of a spoken conversation (including hesitations, repetitions, partially uttered words, and so forth) but also annotation marking various features of intonation in the text, such as tone unit boundaries, pauses, and pitch contours. This kind of detail is included because creators of this corpus attached a high value to the importance of intonation in speech. The main drawback of this kind of detailed transcription is the lengthy amount of time it takes to annotate a text with information about intonation. The advantage is that very detailed information about a spoken text is provided to the user, thus ensuring that a broad range of studies can be conducted on the corpus without any doubt about the authenticity of the data.

Whether one does a minimal or detailed transcription of speech, it is important to realize that it is not possible to record all the subtleties of speech in a written transcription. As Cook (1995: 37) notes, a spoken text is made meaningful by more than the words one finds in a transcription: How a conversation is interpreted depends crucially upon such contextual features as paralanguage (e.g. gestures and facial expressions), the knowledge the participants have about the cultural context in which the conversation takes place, their attitudes towards one another, and so forth. All of this extra-linguistic information is very difficult to encode in a written transcription without the corpus compiler developing an elaborate system of markup identifying this information and the transcriber spending hours both interpreting what is going on in a conversation and inserting the relevant markup. It is therefore advisable when transcribing speech to find a middle ground: to provide an accurate transcription of what people actually said in a conversation, and then, if resources permit, to add extra information (e.g. marking for various features of intonation).

In reaching this middle ground, it is useful to follow Chafe's (1995) principles governing the transcription of speech. A transcription system,

A General Overview of Metadata, Textual Markup, and Linguistic Annotation

As noted earlier, in order to make a corpus "usable" for linguistic analysis, it is necessary for the corpus compiler to insert Metadata, Textual Markup, and Linguistic Annotation into the corpus. Metadata and Textual Markup are more descriptive, noting, respectively, what is in a corpus and where, for instance, paragraph boundaries in a written corpus occur or speakers pause in a spoken corpus. Linguistic Annotation is more grammatical in nature, providing linguistic information about the various structures occurring within a particular corpus. For instance, if a corpus is lexically tagged, each word in the corpus is assigned a part-of-speech designation, such as noun, verb, preposition, and so forth. In contrast, if a corpus is syntactically parsed, various types of grammatical information is provided, such as which structures are noun phrases, verb phrases, subordinate clauses, and imperative sentences.

The following sections describe the three different ways of describing the content of a corpus, beginning with a brief overview of Metadata and more detailed descriptions of Textual Markup and Linguistic Annotation.

Metadata and Textual Markup

In earlier corpora, there was no standardized system of indicating in a spoken corpus, for instance, sequences of overlapping speech: places in a transcription where the speech of two or more individuals overlaps. Likewise, because written corpora were encoded in text files, there was no way to indicate certain features of orthography, such as italicization or boldface fonts. Consequently, different corpora contained different kinds of markup to describe the same linguistic phenomena. More recently, however, the Text Encoding Initiative (TEI) has developed a standardized system of annotation not just for linguistic corpora (www.tei-c.org/release/doc/ tei-p5-doc/en/html/CC.html) but for electronic texts in general (www .tei-c.org/). Metadata and textual markup are important because they can narrow the range of possible choices when a corpus is searched.

One important feature of a TEI-conformant document is what is called the TEI header (www.tei-c.org/release/doc/tei-p5-doc/en/html/ HD.html), which supplies Metadata about a particular electronic document. Although such headers can get quite complicated, one important part of the header is the file description, which provides information about a document, as illustrated below in a header for The Written Component of ICE-USA: Each part of the header is nested. For instance, the first part of the header, <teiHeader>, is enclosed in angle brackets; the last part of the header, </teiHeader>, is also enclosed in angle brackets but with a / (slash) after the first bracket to indicate that all the information between the first header and second header are part of the header. Other such relationships can be found throughout the header: the title of the document, The Written Component of ICE-USA, is enclosed with an open marker, <title>, and a close marker, </title>. Various levels of indentation are also used to illustrate the hierarchy.

There is also TEI-conformant Textual Markup to describe features occurring within a particular corpus. For instance, an unscripted conversation will contain features of speech such as speaker turns, pauses, or partially articulated words. In contrast, a written text will contain features of orthography, such as paragraph boundaries or font changes. The next section will describe this annotation as it applies to spoken language; a later section will focus on the annotation used in written texts.

Textual Markup for Features of Spoken Texts

While the particular words of a conversation are easy to transcribe using standard orthography, there are other features of speech (e.g. overlapping speech) for which particular Textual Markup is necessary. Because of the complexity of the TEI system of annotation for speech, the discussion will be more illustrative than comprehensive. This is in line with

Utterances

Unlike written texts, spoken texts do not always contain grammatically well-formed sentences. For instance, in the conversational excerpt below (taken from the East African component of ICE), the first speaker (B) leaves out the subject, I, before think, and does not finish the sentence, instead pausing with uh before the next speaker begins talking. In the first part of the second turn, the speaker (C) utters only a partial sentence, the negative particle Not followed by a prepositional phrase: out of uh the question:

<B> <u>think Mr Juma wants to say something maybe uh</u> <C> <u> Not out of uh the question </u> <u>What you're trying to discuss now is about the current situation and the near future of about the position of the</u> <B> <u> the political situation</u> <C> <u>Yeah and the position of Zanzibar President then is that what you're discussing</u>

All the units are marked with the tag <u>, which indicates that each construction is an utterance: a sequence of words that has meaning, even though it is not a grammatically well-formed sentence. In written texts, which overwhelmingly contain grammatical sentences, a different set of tags would be used: <s> and </s>, which mark the beginning and end of a sentence. Note too that each of the speaker turns receives a speaker identification tag: a capital letter (B or C) within angle brackets < >. In spoken corpora, the only punctuation marks typically used are apostrophes for contractions and possessives, capitalization of proper nouns, and hyphens for hyphenated words.

Vocalized Pauses and Other Lexicalized Expressions

Speech contains a group of one-or two-syllable utterances that are communicative but not fully lexical in nature. In the TEI system, these utterances are characterized as non-lexical (e.g. snorts, giggles, laughs, coughs, sneezes, or burps) or semi lexical

The next example contains the semi-lexical expression uh, which is very common in speech and serves the function of allowing the speaker to think of something that he or she wishes to say next in a conversation:

It is also common in speech to find examples of expressions that are spelled as two separate words, but pronounced as one word. For instance, got to, have to, and going to are commonly pronounced as, respectively, gotta, hafta, and gonna. Additional examples include kinda, sorta, and lotsa, which are shortened forms of kind of, sort of, and lots of, respectively. Practice will vary, but typically in corpora where the distinction is made, the form that matches the pronunciation is the one that will be transcribed.

Partially Uttered Words and Repetitions

Speech (especially unscripted speech) contains several false starts and hesitations resulting in words that are sometimes not completely uttered. In the example below, the speaker begins uttering the preposition in but only pronounces the vowel beginning the word. <$D> There are more this year than <.> i </.> in in your year weren't there (ICE-GB)

In the ICE Project, such incomplete utterances are given an orthographic spelling that best reflects the pronunciation of the incompletely uttered word, and then the incomplete utterance is enclosed in markup, <.> i </.>, that labels the expression as an instance of an incomplete word.

Repetitions can be handled in a similar manner. When speaking, an individual will often repeat a word more than once as he or she is planning what to say next. In the example below, the speaker repeats the noun phrase the police boat twice before she completes the utterance: <$B> <}_><-_>the police boat<-/> <=_>the police boat<=/> <}/ _>we know but the warden's boat we don't you know he could just be a in a little rowboat fishing and (ICE-USA)

To accurately transcribe the above utterance, the transcriber will want to include both instances of the noun phrase. However, this will have the unfortunate consequence of skewing a lexical analysis of the corpus in which this utterance occurs, since all instances of the, police, and boat will be counted twice. To prevent this, the ICE Project has special markup that encloses the entire sequence of repetitions (<}_><}/_>) and then places special markup (<=_>the police boat<=/>) around the last instance of the repetition, the only instance counted in analyses done by ICECUP, the text analysis program used in the ICE Project.

Unintelligible Speech

Very often when people speak, their speech is unintelligible. If two people speak simultaneously, for instance, they may drown out each other's words and the speech of both speakers will become unintelligible. Anyone doing a transcription of spoken dialogues will therefore encounter instances where speech cannot be transcribed because it is not understandable. In the example below (taken from ICE-USA), the TEI tags <unclear> and </unclear > surround the word them because the transcriber was uncertain whether this was actually the word the speaker uttered.

<u> What was Moses doing going off in <unclear> them </unclear> jeans </u> 3.9.5

Changing the Names of Individuals Referred to in Spoken Texts

In any conversation, speakers will address themselves by name, and they will talk about third-party individuals, sometimes in unflattering waysone spoken sample from the American component of ICE contains two brothers talking quite disparagingly about their parents.

In transcribing a recording taken from a public broadcast, such as a radio talk show, it is of little concern whether the actual names of individuals are included in a transcription, since such a conversation was intended for public distribution. In transcribing private conversations between individuals, however, it is crucial that names be changed in transcriptions to protect the privacy of the individuals conversing and the people they are conversing about. Moreover, many universities and other organizations have strict rules about the use of human subjects in research and the extent to which their anonymity must be preserved. And if the recordings accompanying the transcriptions are to be made available as well, any references to people's names (except in publicly available recordings) will have to be edited out of the recordingssomething that can be done quite easily with software that can be used to edit digitized samples of speech. In changing names in transcriptions, one can simply arbitrarily substitute new names appropriate to the gender of the individual being referred to or, as was done in the London-Lund Corpus, substitute "fictitious" names that are "prosodically equivalent to the originals"

Iconicity and Speech Transcription

Because writing is linear, it is not difficult to preserve the "look" of a printed text that is converted into an electronic document and transferred from computer to computer in text format: although font changes are lost, markup can be inserted to mark these changes; double-spaces can be inserted to separate paragraphs; and standard punctuation (e.g. periods and commas) can be preserved. However, as one listens to the flow of a conversation, it becomes quite obvious that speech is not linear: Speakers very often talk simultaneously, and while someone is taking their turn in a conversation, another party may fill in brief gaps in the turn with backchannels, expressions such as yea and right that tell the speaker that his or her speech is being actively listened to and supported. Attempting to transcribe speech of this nature in a purely linear manner is not only difficult but potentially misleading to future users of the corpus, especially if they have access only to the transcription of the conversation, and not the recording.

To explore the many options that have evolved for making transcriptions more iconic, it is instructive, first of all, to view how conversations were transcribed by early conversational analysts, whose transcriptions occurred mainly in printed articles, not in computerized corpora, and how using this early convention of transcription in computerized corpora has certain disadvantages. The conversational excerpt below contains a system of transcription typical of early systems. oh yeah I remember we did it before

In the excerpt above, the brackets placed above and below segments of speech in successive speaker turns indicate overlapping segments of speech. For instance, speaker A's uttering of "to edit now" overlaps with speaker B's uttering of "no it's figure." However, instead of representing these overlaps with markup (as a TEI-conformant system would do (cf. www.tei-c.org/release/doc/tei-p5-doc/en/html/TS.html#TSSAPA), this system attempts to indicate them iconically by vertically aligning the parts of the conversation that overlap to give the reader of the conversation a sense of how the flow of the conversation took place. While such visual representations of speech are appealing, the implementation of such a system, as

To overcome problems like this

In the excerpt above, segments of speech in adjoining cells of the table overlap: edit now in speaker A's turn, for instance, overlaps with no it's fig in speaker B's turn.

An alternative way to represent iconically not just overlapping speech but the flow of conversation in general is to lay it out as though it were a musical score. In the HIAT system (Ehlich 1993), a speaker's contribution to a conversation is represented on a single horizontal line. When the speaker is not conversing, his or her line contains blank space. If speakers overlap, their overlaps occur when they occupy the same horizontal space. In the example below, T begins speaking and, midway through his utterance of the word then, H overlaps her speech with T's. For Speakers S1, S2, and Sy, lines are blank because they are not contributing to the conversation at this stage.

Other attempts at iconicity in the above excerpt include the use of the slash in T's turn to indicate that H's overlap is an interruption. we already did figure three we did it before

(instantly) H:

Shall I (wipe it out)? S1: S2: Sy:

While iconicity is a worthy goal to strive for in transcriptions of speech, it is not essential. As long as transcriptions contain clearly identified speakers and speaker turns, and appropriate annotation to mark the various features of speech, a transcription will be perfectly usable. Moreover, many users of corpora containing speech will be interested not in how the speech is laid out but in automatically extracting information from it.

Computerizing Written Texts

Because written texts are primarily linear in structure, they can easily be encoded in text format: Most features of standard orthography, such as punctuation, can be maintained, and those features requiring some kind of description can be annotated with a TEI-conformant tag. For instance, in the example below, the tag "hi" indicates that the word very is highlighted in this context because it is italicized (www.tei-c.org/release/doc/tei-p5-doc/en/html/examplesemph.html).

The child was <hi rend="italics">very</hi> tired.

But while many features of writing can be annotated in a written corpus with TEI-conformant markup, annotating all of these features may prove to be unnecessary. While many users of a spoken corpus will potentially be interested in analyzing overlapping speech, there will likely be very little interest in studying italicized words, for instance, in a written corpus. Consequently, much of what could potentially be annotated in a written text is likely to be of marginal interest to future users of the corpus.

Earlier computer corpora, such as the Brown Corpus, contained texts taken from printed sources, such as newspapers, magazines, and books. To computerize texts from these sources, the texts had to be either keyed in by hand or optically scanned. Nowadays, however, so many written texts exist in digital formats that they can be easily adapted for inclusion in a corpus. For instance, the 14 billion-word iWeb Corpus consists entirely of texts taken from websites.

However, if one is creating a historical corpus and thus working with texts from earlier periods of English, converting a text into an electronic format can be a formidable task and, in addition, raise methodological concerns that the corpus linguist working with modern texts does not need to consider.

Because written texts from earlier periods may exist only in manuscript form, they cannot be optically scanned but must be typed in manually. Moreover, manuscripts can be illegible in sections, requiring the corpus creator to reconstruct what the writer might have written. Describing a manuscript extract of the Middle English religious work Hali Meidenhad,

Although only two versions of Hali Meidenhad have survived, thus reducing the level of difference between various versions of this text that the corpus compiler would have to consider, other texts, such as Ancrene Riwle, can be found in numerous manuscript editions:11 versions in English, 4 in Latin, and 2 in French

In theory, the corpus compiler could create a corpus containing every manuscript version of a text that exists, and then either let the user decide which version(s) to analyze, or provide some kind of interface allowing the user to compare the various versions of a given manuscript. The Canterbury Project gives users access to all 80 versions of Chaucer's Canterbury Tales and allows various kinds of comparisons between the differing versions

One of the earlier and more well-established historical corpora of English is the Helsinki Corpus. This corpus contains texts representing three periods in the development of English: Old English (850-1150), Middle English (1150-1500), and Early Modern English (1500-1710). The texts included from these periods represent various dialect regions of England, such as West-Saxon in the Old English period and East Midlands in the Middle English period. For texts taken from later periods, some sociolinguistic information is provided about some of the writers, such as their age and social status. Various registers are also included in the corpus, such as law, philosophy, history, and fiction.

In 2011, a TEI-XML version of the corpus was released. This represented the first attempt to create a historical corpus conforming to the standards of TEI. One reason for creating a TEI-XML version of the corpus was to avoid the fate of many corpora using their own annotation systems because "as new systems emerge, older ones in limited use are gradually forgotten and the data is rendered effectively inaccessible" (

Linguistic Annotation

Of the two types of Linguistic Annotation, word-class annotation is much more common in the field of corpus linguistics than annotation used to mark larger grammatical structures, such as noun phrases or verb phrases. The primary reason for this difference is that it is much easier to automatically assign word class "tags" to individual words in a corpus

While the focus in this section will be primarily on the most common types of annotationword class and grammatical annotationother types of annotation are possible too.

Word Class Annotation

Over the years, a number of different tagging programs have been developed to insert a variety of different tagsets. The first tagging program was designed in the early 1970s by

All of these programs are designed to assign various lexical tags to every word in a corpus. For instance, listed below is a lexically tagged sentence from the BNC2014, the second version of the BNC: Although the tags may seem rather idiosyncratic, there were several guiding principles that influenced the various abbreviations that were used.

While the CLAWS tagsets were developed to facilitate the study of the linguistic structure of various kinds of spoken and written texts, other tagsets were created to enable research in the area of natural language processing (NLP), an area of language inquiry that is more focused on the computational aspects of designing taggers (and also parsers) to annotate and study corpora. One of the more well-known tagsets is the Penn Treebank Tagset, which contains 36 tags (

Programs designed to insert word class tags into corpora are of three types: they can be rule-based, stochastic/probabilistic, or a hybrid of the two previous types. In a rule-based tagger, tags are inserted on the basis of rules of grammar written into the tagger. One of the earlier rule-based taggers was the "TAGGIT" program, designed by

Of the words reaching this stage of analysis, 61 percent will have one tag, and 51 percent of the remaining words will have suffixes associated with one tag. The remaining words will have more than one tag and are thus candidates for disambiguation. Initially, this is done automatically by a series of "context frame rules" that look to the context in which the word occurs. For instance, in the sentence The ships are sailing, the word ships will have two tags: plural noun and third person singular verb. The context frame rules will note that ships occurs following an article, and will therefore remove the verb tag and assign the tag plural noun tag to this word. Although the context frame rules can disambiguate a number of tags, the process is quite complex, as

While the TAGGIT program had a relatively low accuracy rate, subsequent rule-based taggers have increased overall accuracy rates considerably. For instance, the rule-based tagger EngCG-2 (cf. Samuelsson and Voutilainen 1997) was designed to overcome some of the problems in early rule-based taggers like TAGGIT. In particular, rules in EngCG-2 have wider application than in TAGGITand are able to "refer up to sentence boundaries (rather than the local context alone)"

While rule-based taggers rely on rules written into the tagger, other taggers are probabilistic/stochastic; that is, they assign tags based on the statistical likelihood that a given tag will occur in a given context.

The hybrid tagger CLAWS4 was used to tag the BNC2014, the most current version of the written component of the BNC. This tagger employs "a mixture of probabilistic and non-probabilistic techniques" (

The earlier stages of processing with the CLAWS4 tagger yielded an accuracy rate of 97 percent. While this seems like a very high accuracy rate, in a large corpus, the remaining 3 percent of words with multiple tags can constitute a very sizable number of words. To accurately assign a single tag to these words, two rule-based processes were developed, in the form of what are termed "template rules." For instance, words such as after, before, and since, which can be either prepositions or subordinating conjunctions, were consistently mis-tagged:

1. I will arrive after dinner. [preposition] 2. After the movie, we will go out for drinks. [preposition] 3. I will call you after I finish the report. [subordinating conjunction] 4. I will contact the editor after reviewing the book contract [subordinating conjunction]

To correctly identify each of these instances of after as either a preposition or a subordinating conjunction, the following rule was developed (

Basically, what this rule states is that after is more likely to be a preposition than a subordinating conjunction if after 16 words none of the following constructions appear:

a. a finite verb b. a verb having the form of a past participle c. a comma

In example (1) below, after is clearly a preposition because no verb follows it in the remainder of the sentence. In example (2), after is also a preposition because a comma follows movie. In contrast, in example (3), after is clearly a subordinating conjunction because a verb, finish, follows two words after it. Likewise, after is a subordinating conjunction in example (4) because a past participle, reviewing, occurs directly after it.

(1). I will arrive after dinner.

As the discussion in this section has shown, tagsets vary considerably in the number of part-of-speech tags that they contain. These variations reflect not just differing conceptions of English grammar but the varying uses that the tags are intended to serve.

The Penn Treebank tagset (www.ling.upenn.edu/courses/Fall_ 2003/ling001/penn_treebank_pos.html) contains 36 tags that provide information about the basic form classes in English: nouns, adjectives, adverbs, verbs, and so forth. For instance, there are four basic tags for the class of adverbs in English: RB Adverb (however, usually, naturally, here, good) RBR adverb, comparative (better) RBS adverb, superlative (best) WRB wh-adverb (when, where)

Although the Penn Treebank can be used purely to conduct linguistic analyses, its main purpose is to advance research in the area of Natural Language Processing (NLP), an area of research that does not always require a more finely graded system of lexical tags.

For those using corpora to conduct more detailed grammatical analyses, larger tagsets are more desirable because they allow the retrieval of a wide range of grammatical constructions. For instance, the ICE tagset is based on the view of grammar articulated in

The second type of adverb is in the class of Wh-Adverbs:

when ADV(rel)

The remaining six semantic classes contain tags for classifying various other types of adverbs expressing such notions as additive (add) both/ neither; exclusive (excl) only/merely; intensifier (inten) and very/too; particularizer (partic) mainly, in particular.

In contrast, the Penn Treebank tagset is much smaller (36 tags), mainly because this tagset was developed not necessarily to enable detailed linguistic analyses but to advance research in the area of natural language processing (a point described in detail earlier in this chapter). Thus, this tagset contains, as noted earlier, only four basic tags for adverbs.

While word-class annotation is very well established in corpus linguistics, there are other types of annotation as well. For instance, semantic tagging involves annotating a corpus with markup that specifies various features of meaning.

Parsing a Corpus

Tagging has become a very common practice in corpus linguistics, largely because taggers have evolved to the point where they are highly accurate: many taggers can automatically tag a corpus (with no human intervention) at accuracy rates exceeding 95 percent. Parsing programs, on the other hand, have variable accuracy rates, largely because it is computationally much more difficult to analyze larger structures, such as phrases and clauses, than individual lexical items. Copestake (2016: 500, note 1) also mentions the difficulty of evaluating the accuracy of parsers. She notes that accuracy rates can reach 90 percent "when trained and tested on newspaper text." But other text types can prove more difficult to parse, resulting in lower accuracy rates.

In corpus linguistics, parsed corpora serve two purposes: to enable the analysis of larger syntactic structures, such as phrases and clauses, by individuals conducting linguistic analyses, and to provide testbeds for those in the area of natural language processing interested in the development of parsers. This is not necessarily to suggest that these are mutually exclusive categories. For instance, while the Penn Treebank was created by linguists interested in the design of parsers, it can also be used to study the particular parsed linguistic constructions in the 2,499 articles that it contains from the Wall Street Journal (

One of the most extensively parsed corpora used for linguistic research is ICE-GB, the British component of ICE. Each component of ICE contains a variety of different genres of speech and writing, ranging from spontaneous conversations to scientific writing. Because these genres are so linguistically heterogeneous, a rather complicated methodology was developed to parse ICE-GB.

As was noted in an earlier section, ICE-GB contains a comprehensive set of lexical tags. As Figure

Figure

Following the release of ICE-GB, a second parsed corpus using the same architecture was created: the Diachronic Corpus of Present-Day Spoken English (DCPSE) (www.ucl.ac.uk/english-usage/projects/ dcpse/index.htm). This corpus contains 400,000 words of spoken English from ICE-GB and an additional 400,000 spoken words from the London-Lund Corpus, a corpus that was based on texts recorded between 1960 and 1980. Because of the complexity of parsing a corpus, there are relatively few corpora parsed in as much detail as ICE-GB and the DCPSE.

Much more common than fully parsed balanced corpora are Treebanks: corpora that are syntactically or semantically parsed but that lack the genre variation typically found in corpora used to conduct linguistic analyses. For instance, one of the more widely known Treebanks is the Penn Treebank. The latest version of the Penn Treebank, Treebank-3, contains a heterogeneous collection of texts, including fully parsed articles from the Wall Street Journal as well as parsed versions of the Brown Corpus and the Switchboard Corpus (

Treebanks contain sentences that have been either wholly or partially parsed, and a parser can make use of the already parsed structures in a Treebank to parse newly encountered structures and improve the accuracy of the parser. The example below contains a parsed sentence from the Lancaster Parsed Corpus:

The first line of the example indicates that this is the second sentence from sample "A01" (the press reportage genre) of the LOB Corpus, sections of which (mainly shorter sentences) are included in the Treebank. Open and closed brackets mark the boundaries of constituents: "[S" marks the opening of the sentence, "S]" the closing; the "[N" preceding a move marks the beginning of a noun phrase, "N]" following to stop its ending. Other constituent boundaries marked in the sentence include "Ti" (to-infinitive clause to stop Gaitskell from. . .), "Vi" (non-finite infinitive clause to stop), and "Vg" (nonfinite -ing participle clause nominating). Within each of these constituents, every word is assigned a part of speech tag: a, for instance, is tagged "AT", indicating it is an article; move is tagged "NN", indicating it is a singular common noun; and so forth. Although many Treebanks have been released and are available for linguistic analysis, their primary purpose is to train parsers to increase their accuracy. The Survey of English usage at University College London has as a page on its website (www.ucl.ac.uk/english-usage/projects/ice-gb/ compare.htm) that provides a detailed description and comparison of the various Treebanks and parsed corpora that are available.

Conclusions

The process of collecting and computerizing texts is, as this chapter has demonstrated, a labor-intensive effort. For instance, recording and transcribing spontaneous conversations requires considerable time because individuals need to be recorded and their recordings transcribed. While voice recognition software has made tremendous progress recently, it still works best on monologic speech: an individual speaking slowly into a microphone that is then transferred into written text in a document. Such programs do not work well with dialogic speech.

Written texts, in contrast, are now widely available in digital formats and can easily be incorporated in a corpus after permission has been received to use a given text. While lexically tagging corpora with part-of-speech information can now be done quickly and quite accurately, parsing a corpus is a much more difficult undertaking, since the level of accuracy decreases when identifying structures such as noun phrases or imperative sentences.

The Web has also increased the availability of texts, even certain types of spoken texts. The BYU corpora, for instance, contain different kinds of public speech made available in transcripts that are easily obtainable. While their accuracy cannot be guaranteed, the level of error does not appear to be high. It is also the case that voice recognition software will continue to improve so that in the future it could very well be possible to automate the process of converting speech to text, and thus expedite the inclusion of, for instance, spontaneous dialogues in corpora.

Analyzing a Corpus

The process of analyzing a completed corpus is in many respects similar to the process of creating a corpus. Like the corpus compiler, the corpus analyst needs to consider such factors as whether the corpus to be analyzed is lengthy enough for the particular linguistic study being undertaken and whether the samples in the corpus are balanced and representative. The major difference between creating and analyzing a corpus, however, is that while the creator of a corpus has the option of adjusting what is included in the corpus to compensate for any complications that arise during the creation of the corpus, the corpus analyst is confronted with a fixed corpus, and has to decide whether to continue with an analysis if the corpus is not entirely suitable for analysis, or find a new corpus altogether.

This chapter describes the process of analyzing a completed corpus. To illustrate how such analyses are conducted, the chapter opens with a discussion of Former President Donald Trump's usage of language, termed "Trump speak," and how corpora of his Twitter posts, transcribed speeches, and other spontaneous commentary can be used to study his unique uses of language. The discussion in this section illustrates how to (1) frame a research question, (2) select relevant corpora to carry out the analysis, (3) use a concordancing program to locate appropriate examples for analysis, and then (4) explain the results drawing upon relevant research on language usage, particularly theories of politeness.

The remainder of the chapter provides an overview of qualitative and quantitative research methodologies, discussing the differences between the two methodologies and providing examples of each. For instance, many of the corpus-based reference grammars of English, such as Quirk et al.'s A Comprehensive Grammar of the English Language, are more qualitative, as they draw upon linguistic corpora for authentic examples to illustrate the many points of English grammar discussed throughout the grammar.

In contrast, other corpus studies are more quantitative, subjecting the results of a corpus analysis to, for instance, statistical analyses to determine whether the particular linguistic differences in corpora under study are significant or not. For instance,

Trump Speak: Framing a Research Question

To determine exactly what research question one wishes to pursue, it is first of all necessary to review relevant articles or books written on the topic so that the ultimate question selected does more than merely repeat what others have written on the topic.

Although Donald Trump (hereafter DT) was new to the political scene -President of the United States was his first elected officehe was quite well-known prior to becoming President as a businessperson and television celebrity. However, in his relatively short time as president, he has established a very distinct persona. In particular, he flouts on a regular basis the norms of speech that one would expect from someone in his position. Consequently, his style of speaking has drawn considerable interest from corpus linguists.

Clarke and Grieve (2019) conducted a stylistic analysis of Trump's tweets in the Trump Twitter Archive between the years 2009 and 2021. This archive is very comprehensive and contains every Tweet that Trump posted from 2009 to January 8, 2021, when Trump was banned from Twitter and his account was closed. In their analysis, they note, for instance, changes in the length and frequency of tweets over time and instances when Trump was particularly active in criticizing a particular individual. For instance, the tweets increased in frequency when Trump engaged in a lengthy campaign questioning Obama's citizenship (p. 3).

To study Trump's style of communication, Clark and Grieve adapted Biber's notion of multi-dimensional analysis (see Section 4.6) to isolate certain features of Trump's tweeting style. In his work on register variation, Biber develops a series of what he calls dimensions: general parameters that describe a particular style of communication. For instance, Clark and Grieve's (2019: 18) Dimension 5: Advisory Style characterizes tweets in which Trump is giving advice: Sorry losers and haters, but my I.Q. is one of the highest -and you all know it! Please don't feel so stupid or insecure, it's not your fault

Other corpus linguists have also written about Trump's style of speaking. For instance, Xueliang Chen, Yuanle Yan, and Jie Hu (2019) conducted a corpus analysis of the use of language by Hillary Clinton and Donald Trump when they were running against each other for president. They considered two research questions in their analysis: To answer these questions, they analyzed two comparable corpora that they created, both containing, respectively, speeches presented by Trump and Clinton as they were running against each other. After analyzing the keywords (words with unexpected high frequencies) in the two corpora they created, they concluded that while Clinton's speeches were more positive in nature with top keywords such as women's rights, social justice, and kind, Trump's keywords were much more negative: bad, illegal, disaster. In fact, two of the top keywords in Trump's speeches were Hillary and Clinton, reflecting his frequent reference to her in his speeches and the emphasis he put on critiquing her and her policies.

Selecting Suitable Corpora to Address a Particular Research Question

Because a study of Trump's very often negative use of language is very focused and is a rather narrow topic of research, it will be necessary to draw upon data from very specific corpora to carry out this study. As a consequence, three corpora were analyzed: the Trump Twitter Archive (as described above), the Web, and a 500,000-word archive of, for instance, Trump's speeches and interviews (www.rev.com/blog/transcript-category/donald-trump-tran scripts). Because these are not traditional sources of data, it is worth discussing why they can be considered corpora.

First, corpora typically contain excerpts of texts taken from larger sources. For instance, the Brown Corpus contains 2,000-word samples taken from complete texts (e.g. newspaper articles). But because tweets are relatively short and are not part of any larger text, can they be considered texts themselves? For

Biden was asked questions at his so-called Press Conference yesterday where he read the answers from a teleprompter. That means he was given the questions, just like Crooked Hillary. Never have seen this before! (Trump Twitter Archive, July 1st, 2020)

The tweet above has a topic: Biden's reading his answers to questions from the press on a Teleprompter, and enough sentences to develop this topic so that the entire tweet can stand alone as a coherent unit. Consequently, it has unity of structure. In addition, the tweet has unity of texture: links that tie together the various parts of the tweet. For instance, the tweet opens with an initial reference to Biden. As the tweet develops there are two instances of the pronoun he that co-refer to Biden, tying sections of the tweet together and thus creating cohesion and ultimately coherence.

While the Web has increasingly become a corpus used for linguistic analysis, the other two sources of dataan archive of tweets and a collection of Trump's speeches and interviewsare specific to this particular analysis. But as was discussed in Chapter 1 (Section 1.5), there is a range of other types of corpora that can be used for analysis, including multi-purpose corpora, learner corpora, historical corpora, and parallel corpora.

Extracting Information from a Corpus

There are various ways that grammatical and lexical information can be retrieved from corpora. In the pre-electronic era, such information had to be manually retrieved. For instance, when Jespersen was looking for authentic examples to illustrate the various grammatical constructions he included in his seven-volume A Modern English Grammar on Historical Principles, he, along with a number of student helpers, had to read numerous books and periodicals to obtain authentic examples to illustrate the grammatical points that he was making. Obviously, this involved considerable time and effort.

As sources of data became computerized, concordancing programs were created that allow for various constructions (e.g. words or phrases) to be automatically retrieved from a corpus. For instance, in the analysis of Trump Speak, various examples of usages by Trump were retrieved using either AntConc (described in greater detail later in this section) or a concordancing program specifically designed to retrieve constructions in the Trump Twitter Archive.

Concordancing programs retrieve structures and present them in KWIC (key word in context) concordances. For instance, below is a sample list of examples of Trump's use of the word loser(s). This archive contains a built-in concordancing program that highlights the construction being searched, and also contains a short context in which, in this case, loser (158 examples) or losers (156 examples) occur. Below are some sample tweets retrieved by this program:

(3) Steve Scully of @cspan had a very bad week. When his name was announced, I said he would not be appropriate because of conflicts. I was right! Then he said he was hacked, he wasn't. I was right again! But his biggest mistake was "confiding" in a lowlife loser like the Mooch. Sad! (Oct 16th, 2020 -8:15:38 AM EST) (4) How dare failed Presidential Candidate (1% and falling!) @CoryBooker make false charges and statements about me in addressing Judge Barrett. Illegally, never even lived in Newark when he was Mayor. Guy is a total loser! I want better Healthcare for far less money, always. . . (Oct 13th, 2020 -5:56:41 PM EST)

(5) Joe Biden is a PUPPET of CASTRO-CHAVISTAS like Crazy Bernie, AOC and Castro-lover Karen Bass. Biden is supported by socialist Gustavo Petro, a major LOSER and former M-19 guerrilla leader.

Biden is weak on socialism and will betray Colombia. I stand with you! (Oct 10th, 2020 -2:38:59 PM EST) (6) Because I've beaten him and his very few remaining clients so much, and so badly, that he has become a blathering idiot. He failed with John McCain and will fail again with all others. He is a total loser. @MarshaBlackburn is a Tennessee Star, a highly respected (WINNER! Oct 7th, 2020 -8:18:13 AM EST)

This search yielded 314 examples of Trump's use of the expression loser(s). KWIK concordances are useful because they can easily and quickly be created and also provide the context in which search terms occur. Consequently, when it comes time to include examples in an article or presentation, one can easily integrate the examples into the discussion, and obtain frequency information that indicates whether the usage is common or uncommon.

The results of searches can also help in establishing trends in a corpus. For instance, when Trump was running for the Republican nomination for president, he had nicknames for each of the individuals against whom he was running. For instance, Marco Rubio was commonly referred to by Trump as Little Marco (109 mentions in the Trump Twitter Archive); Ted Cruz was Lyin' Ted Cruz (23 mentions); and Jeb Bush was Low Energy Jeb (or variations, e.g. Jeb low energy) (4 mentions). During his run for the presidency, Trump frequently referred to Hillary Clinton as Crooked Hillary (366 mentions).

A search of these names on the Web yields huge returns. For instance, a search for Little Marco yielded 345 million hits (May 16th, 2021). Similarly, large figures could be found for the other candidates as well. However, frequency information from the Web has to be cautiously interpreted because the returns may, for instance, come from webpages with identical content. Still, web examples and frequencies can provide at least a preliminary sense of how frequent the constructions occurfrequencies that can then be double-checked in other sources.

The concordancing program included with the Trump Twitter Archive is only able to retrieve individual strings of words from a corpus of Trump's tweets. However, other concordancing programs, such as AntConc, allow searches of corpora that are directly linked with AntConc. For instance, the concordancing window in Figure

Notice in the table that all of the search terms that were retrieved are highlighted and vertically aligned for easy reading. Also included is a short context containing a span of text that precedes and comes after the search terms. The size of the search window can be widened or narrowed depending upon how much text is desired both before and after the search term.

AntConc, as the top row reveals, can do other kinds of searches. Figure

N-grams or clusters are groups of words that co-occur. For instance, the expression talk about occurred 6 times in the corpus. Retrieving N-grams in corpora can, for instance, be useful for studying collocations: words that commonly occur together.

The BYU corpora are a set of many different corpora taken from various online resources. The corpora available range from a corpus of American English, the Corpus of Contemporary American English (COCA), to the Coronavirus Corpus, a corpus containing texts retrieved from the Web containing discussions of the Covid-19 virus. All told, 19 corpora are available for online analysis, and all are quite large. For instance, COCA is currently one billion words in length. The size of each corpus constantly changes because all corpora featured on the site are monitor corpora: corpora to which new texts are added on a regular basis.

All BYU corpora can be searched online with a custom-made concordancing program. The search terms specify that walk as a verb be retrieved rather than, for instance, walk as a noun. In addition, the symbol * ensures that all forms of the verb walk will be retrieved, ignoring forms of the  Because of the size of COCA, each of these forms are displayed in separate KWIK concordances. Therefore, selecting walk (which occurred 92,178 times) generates the following KWIC concordance (Figure

Another concordancing program, BNCweb (

The concordancing programs described thus far provide representative examples of the types of searches that such programs allow. However, there are additional programs available on the "Tools for Corpus Linguistics" website (

While concordancing programs are one way to extract constructions from corpora, there are other ways to do so as well.

Concordancing programs are able to locate and identify various sequences of words. However, if a corpus is parsed (i.e. contains larger structures such as phrases, clauses, and sentences that are annotated), it is possible to retrieve through searches much larger structures, such as phrases and clauses.

One lexically tagged and grammatically parsed corpus is ICE-GB, the British component of the International Corpus of English. Each individual word in the corpus is assigned a lexical tag (e.g. noun, verb, preposition, etc.). In addition, phrases and clauses are annotated as well as sentence functions, such as subject, direct object, and adverbial.

Because ICE-GB is both tagged and parsed, it is possible to retrieve many different types of structures, not just individual words but phrases and clauses as well as other types of constructions, such as appositives. Figure

All parse trees in ICE-GB contain features, functions, and categories.

In the parse tree in Figure

For instance, A son, Alistair is functioning as "subject" (SU) in the main clause in which it occurs. Categories are represented at both the phrase and word level: A son is a "noun phrase" (NP), as is Alistair.

To find all instances of proper nouns annotated with the feature "appo," ICECUP requires that a "fuzzy tree fragment" (FTF) be constructed (see Figure

As searches are conducted, it is necessary to collect other types of information that may be relevant for a future paper or article. For instance, any statistical information, such as frequencies, can be included in a spreadsheet. Relevant examples can be saved in a word processing program and include detailed information where the examples originated. Including this information as data is being

Integrating Relevant Linguistic Theories into Corpus Results

A common complaint about corpus analyses is that they often rely heavily on frequency information and statistical analyses, ignoring how this information is connected with relevant linguistic theories. While corpus linguistics has very little to say to those with interests in, for instance, generative grammar, it is of great value to those who are interested in studying pragmatics: how language is used.

One area of pragmatics that has been extensively studied is politeness: how people adjust their manner of speaking to conform to the norms of politeness for the language that they speak. These norms are contextually based: what would be considered impolite in one context may be perfectly acceptable in another context. An important characteristic of Trump Speak is that it often violates the norms of polite speech for both a candidate running for the presidency as well as an elected president. Although theories of politeness in human language are generally conceptualized to explain face-to-face conversations, these theories can be extended to a newer medium, such as Twitter, which is conversational in structure but, unlike interactive human communication, is monologic. For instance, one way that Trump amplifies the impoliteness of his tweets is by assigning particularly negative adjectives to rival candidates' first names. Consider below a series of tweets all containing the expression Crooked Hillary:  Stupid! RNC Further similarly negative expressions include "weak" (156); "dope" or "dopey" (117); "moron(s)" (54); and "dishonest" (115). Most of the examples documented above are insults, a type of speech act that violates many norms of politeness, particularly because Trump's insults of individuals occur in very public forums, such as Twitter, debates, and campaign rallies.

In their discussion of politeness in human language,

Even a cursory overview of Trump's tweets reveals numerous instances of FTAs -Michael Bloomberg is a loser, Hillary Clinton is crooked, Mitt Romney is the dumbest and worst candidate. Other individuals are stupid, weak, dopey, and dishonest. And while these insults, as noted earlier, would have had greater impact if they were uttered in face-to-face interactions with Trump, the fact that Twitter and other media forums are very public, these insults resonate perhaps more than if they were done privately in a face-toface conversation.

The frequent co-occurrence of negative descriptors with particular individuals can also result in what is referred to as a negative semantic prosody. For instance, if the expressions "Crooked Hillary" or "Little Marco" occur so frequently, over time, Hillary Clinton will be viewed as crooked and Marco Rubio as diminutive in stature.

One of

Although this maxim is not universal, it is a given in Western culture that it is better to praise rather than dispraise someone, and if dispraise is given, such speech acts are mitigated. Thus, a teacher might say to a student that his/her paper "has good ideas, but needs some work" rather than "this paper is a disaster, and needs to be completely rewritten." However, in Trump Speak, individuals are dumb, stupid, dopey, and dishonest. Nothing is mitigated. While much of Trump Speak is highly negative, there are two adjectives that Trump regularly uses that are much more positive: tremendous (297) and incredible (356). As the frequency counts demonstrate, these adjectives are two of the most frequently occurring usages in Trump Speak. In addition, they are generally highly positive in contrast to the highly negative connotations of so many of his other usages. While the first example below contains a usage of tremendous that is rather neutral in tone, the other usages are quite positive. For instance, he receives "tremendous support" from unions, the win by the US golf team was a "tremendous win." Likewise, in the last example, the Minneapolis police are characterized as "incredible." These usages are noteworthy because all the other previously discussed usages have been predominantly negative in tone.

• As bad as the I.G. Report is for the FBI and others, and it is really bad, remember that I.G. Horowitz was appointed by Obama. The term Fake News, which occurs quite frequently in his tweets, is typically used to disparage negative news reports against him in the media. For instance, in the second tweet below, he disparages a former government employee who made negative comments about him. In the third tweet, which is a response to media claims that he has not dealt very well with the coronavirus pandemic, he shifts the topic to the Obama administration's handling of the H1N1 Swine Flu: response by the ObamaBiden team to the H1N1 Swine Flu was considered a weak and pathetic one. Check out the polling, it's really bad. The big difference is that they got a free pass from the Corrupt Fake News Media! • Aug 16, 2020 12:31:20 PM. @FoxNews is not watchable during weekend afternoons. It is worse than Fake News @CNN. I strongly suggest turning your dial to @OANN. They do a really "Fair & Balanced" job! • Aug 15, 2020 03:06:50 PM More Fake News ! One final characteristic of Trump Speak has less to do with his repetition of particular vocabulary items and more with his general use of language, particularly his propensity to frequently lie. For instance, as of January 20, 2021 (the last day of his presidency), the Fact Checker Database had documented 30,573 "fake or misleading claims" that Trump has made since becoming President (www .washingtonpost.com/graphics/politics/trump-claims-database/?utm_ term=.27babcd5e58c&itid=lk_inline_manual_2&itid=lk_inline_ manual_2).

Although Trump's tendency to lie may seem outside the realm of linguistic theory, one of the maxims of

(7) Do not say what you believe to be false (8) Do not say that for which you lack adequate evidence

"We built the greatest economy in history, not only for our country, but for the world. We were number one, by far."

This claim, made 360 times by Trump, is proven to be false because there is ample economic documentation that many other countries over a span of many years have far exceeded the current state of the economy during Trump's tenure as president. The conversational implicature of Trump's frequent violation of the quality maxim is that, in general, many people do not trust the veracity of just about any claim that he makes.

Although the analysis of Trump speak in this section has included some statistical informationmainly frequency counts of the various usages that were discussedthe analysis has been primarily qualitative rather than quantitative: it has relied on an extensive analysis of the data rather than a statistical analysis of the frequency counts that are described. In fact, the frequency counts are included mainly to illustrate that the usages discussed occurred frequently enough to demonstrate the rather negative and, in some cases, derogatory nature of Trump Speak. The next section explores in greater detail the differences between qualitative and quantitative corpus analyses.

4.5

Quantitative and Qualitative Analyses

Although quantitative and qualitative approaches to corpus analysis are often viewed as differing ways of analyzing corpora, the difference between the two approaches is not necessarily discrete. For instance,

4.5.1

Qualitative Corpus Analysis

She notes, for instance, that such analyses are "exploratory" and "inductive" and permit "in-depth investigations of authentic language use" (p. 951). To explain particular patterns of usage, qualitative analyses, she continues, can incorporate non-linguistic elements, including various speaker variables, such as age, gender, and economic background. These variables are important for qualitative analyses because they help reveal how non-linguistic elements (e.g. an individual's age or gender) can influence language usage.

In contrast, quantitative analyses rely more heavily on statistical informationinformation that is very often based on corpora that have been annotated with lexical tags (marking, for instance, nouns or verbs) and/or syntactically parsed constructions (such as subjects, objects, and adverbials). These constructions can be automatically retrieved and subjected to statistical analyses that determine whether, for instance, the distributions of the constructions in one genre or another (e.g. fiction or press reportage) is statistically significant. This is not to suggest that there is always an absolute difference between quantitative and qualitative analyses. The two types of analyses can work together to produce an analysis involving what is sometimes termed mixed methods. Thus, one can view the difference between the two types of analyses being on a gradient: a spectrum on which there are varying degrees of quantitative and qualitative analyses.

Qualitative corpus analyses, as Hasko (2020: 952) comments, have a long tradition, dating back to the pre-electronic era. And there is ample evidence to support this claim. For instance, the lexicographer James Murray developed a methodology for collecting authentic usages of language to both determine and illustrate the meanings of words in the first edition of the Oxford English Dictionary. Early grammarians such as Otto Jespersen relied extensively on authentic examples from written texts to write their grammars.

The linguist Randolph Quirk created what is now known as the Quirk Corpus, a corpus of spoken and written British English collected between 1955 and 1985. Before they were digitized, citations of examples illustrating various grammatical categories were only available on slips in file drawers housed at the Survey of English Usage (University College London).

But the first computer corpus for the computational analysis of language, as Hasko (2020: 952) comments, was the Brown Corpus, a corpus that paved the way for not only quantitative corpus research but qualitative studies of language as well, as demonstrated by the particular registers included in the Brown Corpus.

The Brown Corpus marked the beginning of the era of computerized corpora that could be used as the basis for conducting empirically based linguistic investigations of English. Although by modern standards it was fairly short (one million words), it set a standard for the design of many subsequent linguistic corpora, not just of English but of other languages as well (cf. Chapter 1 for a more detailed description of the Brown Corpus).

The Brown Corpus contains 2,000-word samples of edited written American English grouped into two general categories: informative prose (374 samples) and imaginative prose (126 samples) (see Table

There were several methodological considerations that guided the structure of the corpus. First, to adequately represent the various kinds of written American English, a wide range of different types of written English were selected for inclusion in the corpus. For instance, the bulk of the corpus contains various kinds of informative prose, including press reportage, editorials, and reviews; government documents; differing types of learned writing; learned writing from, for instance, the humanities and social sciences. In addition to informative writing, the corpus contains differing types of imaginative prose, such as general fiction and science fiction. More examples of informative than imaginative writing were included because informative writing is much more common than imaginative writing.

Since neither informative nor imaginative writing is homogeneous, the corpus contained 2,000-word samples from a range of different articles, periodicals, books, and so forth to provide as broad a range as possible of different authors, books, periodicals, and so forth. This sampling procedure was also used to ensure that an extensive range of different authors, books, and periodicals were represented. Had lengthier samples been used, the style of a particular author or periodical, for instance, would have been over-represented in the corpus and thus have potentially skewed the results of studies done on the corpus. The Brown Corpus set the standard for how corpora were organized, and as a consequence, was the catalyst for the creation of several additional corpora that replicated its composition. For instance, the London/Oslo/Bergen (LOB) Corpus contains one million words of edited written British English as well as the same genres and length of samples (2,000 words) as the Brown Corpus with only very minor differences: In several cases, the number of samples within a given genre vary: popular lore contains 44 samples, while in the Brown corpus the same genre contains 48 samples (cf.

The inclusion in the Brown Corpus of many different types of written English made it quite suitable for the qualitative analysis of language usage. A select overview of research conducted on this corpus makes this point very clear. For instance,

(9) I know that he is here.

(10) I know he is here.

Because the inclusion of that is optional in such constructions, Elsness was interested in determining the various factors that influenced the retention or deletion of that. For instance, he considered such factors as whether the formality of the particular type of writing influenced the choice of either retaining or deleting that; whether that deletion was less common in formal writing than in informal writing; whether the particular verb influenced use or non-use of that; and whether including or not including that resulted in a difference in meaning.

To address these research questions, Elsness restricted his analysis to four different registers of the Brown Corpus: Press Reportage; Belles Lettres, Biography, etc.; Learned and scientific Writings; and Fiction: Adventure and Westerns. Because these are very different registers, Elsness would be able to determine, for instance, whether that-deletion has a restricted usage: occurring, for instance, less frequently in more formal registers, such as learned and scientific writing, than in less formal registers, such as fiction. Essentially, what Elsness is doing in this article is using frequency counts, which are quantitative in nature, to support qualitative claims about the usage of that-deletion in various genres of English in the Brown Corpus.

The Brown Corpus was also the first corpus to be lexically tagged. Consequently, it can be viewed as ushering in a new era of quantitative analyses of corpora. For instance, one of its earliest applications involved supplying word frequency lists for the American Heritage dictionary

As corpus linguistics developed as a discipline, and the technology used both to create and analyze corpora increased in sophistication, it became easier to create computerized corpora, conduct more automated analyses of them, and also to subject the results of analyses to statistical analyses. As a result, more quantitative analyses of corpora were able to be conducted.

Quantitative Corpus Analysis

This section explores various ways that quantitative analyses of corpora can be conducted. Such analyses involve the application of various statistical tests to determine whether the hypotheses being tested are statistically significant. Because of the complexity of statistical analyses, the discussion in this section will be restricted to two areas:

(11) The first section describes how descriptive statistics, such as Chi square or log likelihood, can be used to determine whether pseudo-titles in various stigmatized constructions such as Microsoft president Bill Gates occur with differing frequencies in numerous samples of press reportage taken from newspapers appearing in the various components of ICE.

Pseudo-titles are related to appositives. Thus, in the example above, a full appositive equivalent would be the president of Microsoft, Bill Gates. Appositives contain two co-referential noun phrases separated by a comma pause. In pseudo-titles, the first unit acts more like a modifier: no co-referential relationship exists between the two noun phrases, and there is no comma pause separating them. Pseudo-titles also have a very restricted usage, occurring predominantly in press reportage. However, their usage varies, with some newspapers banning them entirely, always requiring full appositives, while other newspapers, often tabloids but broadsheets as well, use them quite frequently.

(12) The second section describes how multi-dimensional analyses in the work of Douglas Biber can be used to study register variation: how various linguistic constructions occur more or less commonly in differing registers, ranging from press reportage to fiction.

The Statistical Analysis of Pseudo-Titles

Previous research on pseudo-titles has documented their existence in American, British, and New Zealand press reportage, and demonstrated that because their usage is stigmatized, certain newspapers (particularly in the British press) prohibit their usage. To examine their usage empirically, the press reportage in seven regional varieties of ICE were examined to determine their usage globally. Table

As Table

Although the results displayed in Table

(13) Lawyer Paul Muite and his co-defendants in the LSK contempt suit wound up their case yesterday and accused the Government of manipulating courts through proxies to silence its critics. . .Later in the afternoon, there was a brief drama in court when a lawyer, Ms Martha Njoka, was ordered out after she defied the judge's directive to stop talking while another lawyer was addressing the court. (ICE-East Africa)

Exploring a corpus qualitatively allows the analyst to provide descriptive information about the results that cannot be presented strictly quantitatively. But because this kind of discussion is subjective and impressionistic, it is better to devote the bulk of a corpus study to supporting qualitative judgements about a corpus with quantitative information.

Using Quantitative Information to Support Qualitative Statements

In conducting a microscopic analysis of data, it is important not to become overwhelmed by the vast amount of statistical information that such a study will be able to generate, but to focus instead on using statistical analysis to confirm or disconfirm the particular hypotheses one has set out to test. In the process of doing this, it is very likely that new and unanticipated findings will be discovered: A preliminary study of pseudo-titles, for instance, led to the discovery that the length of pseudo-titles varied by national variety, a discovery that will be described in detail below.

One of the most common ways to begin testing hypotheses is to use the "cross tabulation" capability found in any statistical package. This capability allows the analyst to arrange the data in particular ways to discover associations between two or more of the variables being focused on in a particular study. In the study of pseudo-titles, each construction was assigned a series of tags associated with six variables, such as the regional variety the construction was found in, and whether the construction was a pseudo-title or a corresponding apposition. To begin investigating how pseudo-titles and corresponding appositives were used in the regional varieties of ICE being studied, a cross tabulation of the variables "country" and "type" was generated. This cross tabulation yields the results displayed in Table

The results of the cross tabulation in Table

When comparing results from different corpora, in this case differing components of ICE, it is very important to compare corpora of similar length. If different corpora of varying length are compared and the results are not "normalized," then the comparisons will be distorted and misleading. For instance, if one were to count and then compare the number of pseudo-titles in one corpus of 40,000 words and another of 50,000 words, the results would be invalid, since a 50,000-word corpus is likely to contain more pseudo-titles than a 40,000-word corpus, simply because it is longer. This may seem like a fairly obvious point, but in conducting comparisons of the many different corpora that now exist, the analyst is likely to encounter corpora of varying length: corpora such as Brown or LOB are one million words in length and contain 2,000-word samples; the London-Lund Corpus is approximately 500,000 words in length and contains 5,000-word samples; and the British National Corpus is 100 million words long and contains samples of varying length. Moreover, often the analyst will wish to compare his or her results with the results of someone else's study, a comparison that is likely to be based on corpora of differing lengths.

To enable comparisons of corpora that differ in length,

The choice of norming to 1,000 words is arbitrary, but as larger numbers and corpora are analyzed, it becomes more advisable to norm to a higher figure (e.g. occurrences per 10,000 words).

Although the percentages in Table

Data that are normally distributed will yield a "bell curve": most cases will be close to the "mean", and the remaining cases will fall off quickly in frequency on either side of the curve. To understand why linguistic data are not normally distributed, it is instructive to examine the occurrence of pseudo-titles in the 40 texts that were examined (cf. Table

As the figures in Table

Because most linguistic data behave the way that the data in Table

In essence, the Chi square test calculates the extent to which the distribution in a given dataset either confirms or disconfirms the "null hypothesis": in this case, whether or not there are differences in the distribution of pseudo-titles and equivalent appositives in the four regional varieties of ICE being compared. To perform this comparison, the Chi square test compares "observed" frequencies in a given dataset with "expected" frequencies (i.e. the frequencies one would expect to find if there were no differences in the distribution of the data). The higher the Chi square value, the more significant the differences are. The application of the Chi square test to the frequencies in Table

With three degrees of freedom, the Chi square value of 65.686 is significant at less than the .000 level.

While it is generally accepted that any level below .05 indicates statistical significance, it is quite common for more stringent significance levels to be employed (e.g. p≦:001). Because the significance level for the data in Table

The Chi square test applied to the data in Table

Significance Level # of Tests Performed Corrected Value Table

The results in Table

While the Chi square statistic is a very useful statistic for evaluating corpus data, it does have its limitations. If the analyst is dealing with fairly small numbers resulting in either empty cells or cells with low frequencies, then the reliability of Chi square is reduced. Table

Three of the cells in the category of "Total Equivalence" contain fewer than five occurrences, making the Chi square statistic invalid for the data in Table 4.7. One way around this problem is to combine variables in a principled manner to increase the frequency for a given cell and thus make the results of the Chi square statistic more valid. One reason for recording the particular correspondence relationship for an appositive was to study the stylistic relationship between pseudo-titles and various types of equivalent appositives: to determine, for instance, whether a newspaper prohibiting pseudo-titles relied more heavily than those newspapers allowing pseudo-titles on appositives related to pseudo-titles by either determiner deletion (e.g. the acting director, Georgette Smith ! acting director Georgette Smith) or total equivalence (Georgette Smith, acting director ! acting director Georgette Smith). Because these two correspondence relationships indicate similar stylistic choices, it is justifiable to combine the results for both choices to increase the frequencies and make the Chi square test for the data more valid. Table

It was expected that ICE-GB would contain more instances of appositives exhibiting either total equivalence or determiner deletion, since in general British newspapers do not favor pseudo-titles and would therefore favor alternative appositive constructions. And indeed, the newspapers in ICE-GB did contain more instances. But the increased frequencies are merely a consequence of the fact that, in general, the newspapers in ICE-GB contained more appositives than the other varieties. Each variety followed a similar trend and contained fewer appositives related by total equivalence or determiner deletion and more related by partial equivalence. These findings call into question

While combining values for variables can increase cell values, often such a strategy does not succeed simply because so few constructions occur in a particular category. In such cases, it is necessary to select a different statistical test to evaluate the results. To record the length of a pseudo-title or appositive, the original coding system had six values: one word in length, two words, three words, four words, five words, and six or more words. It turned out that this coding scheme was far too delicate and made distinctions that simply did not exist in the data: many cells simply had too low a frequency to apply the Chi square test. And combining categories, as is done in Table

In cases like this, it is necessary to apply a different statistical test: the log-likelihood (or G 2 ) test.

The results of the log-likelihood test point to a clear trend in Table

Table

One reason for the general difference in length of appositives and pseudo-titles is that there is a complex interaction between the form of a given pseudo-title or appositive and its length. In other words, three variables are interacting: "type" (pseudo-title or appositive), "form" (simple noun phrase, genitive noun phrase, noun phrase with post-modification), and "length" (1-4 words or 5 words or more). Table

A Chi square analysis of the trends in Table

Because only three variables were being compared, it was decided to use a saturated model to investigate associations. This model generated the following potential associations:  Likelihood ratio and Chi square tests were conducted to determine whether there was a significant association between all three variables (a), and between all possible combinations of two-way interactions (b-d). In addition, the variables were analyzed individually to determine the extent to which they affected the three-and two-way associations in 16a-d. The results are presented in Table

The first line in Table

To determine which of these associations were strongest, a procedure called "backward elimination" was applied to the results. This procedure works in a step-by-step manner, at each stage removing from the analysis an association that is least strong and then testing the remaining associations to see which is strongest. This procedure produced the two associations in Table

Interpreted in conjunction with the frequency distributions in Table

The loglinear analysis applied to the data in Table

Although the GoldVarb program was specifically designed for linguists working with variable rules, most corpus linguists can usefully apply the many differing statistical tests provided by any of the commonly available statistical programs. While the Chi square test is one of the more common tests used with linguistic data, as Oakes' (1998: 1-51) survey of statistical tests for corpus data demonstrates, there are a range of other tests as well. There is also a successor to GoldVarb, Rbrul (www.danielezrajohnson.com/rbrul .html), that has similar capabilities but a more user-friendly interface and that runs more quickly on a computer.

4.6

Multi-dimensional Analysis

In his 1988 book, Variation across Speech and Writing, Douglas Biber introduced the notion of multi-dimensional analysis and how it could be used to identify significant linguistic differences between spoken and written language. He critiques traditional linguistic beliefs concerning the differences between speech and writing, such as the claim "that speech is more elaborated and complex than writing" (p. 5). Instead, he argues that it is more informative to view the differences in terms of a series of dimensions that he proposes. The purpose of these dimensions is to demonstrate empirically that "neither speech nor writing is primary; that they are rather different systems, both deserving careful analysis" (p. 7).

Below is a list of the six dimensions that Biber proposed, dimensions that are based on a "methodology to empirically analyze the ways in which linguistic features co-occur in texts and the ways in which registers vary with respect to those co-occurrence patterns"

To understand how each of the Dimensions work, consider how the registers that are associated with Dimension 1 illustrate the spectrum between those registers that are more highly involved (i.e. interactional) versus those that are more informational (i.e. focused primarily on content):

Registers that are more involved: telephone conversations, face-to-face conversations, personal letters, spontaneous speeches and interviews

Registers that are more informational: Biographies, press reviews, academic prose, press reportage, official documents summation of risks at the individual level as it does with chronic diseases. <#>To relate risk assessments at the individual and population levels, knowledge of contact patterns is essential. <#>The purposes of this paper are 1) to demonstrate the lack of stability of chronic disease risk measures with contagious diseases, 2) to demonstrate how risk assessment for contagious diseases depends upon assessment of contact patterns even when contact patterns do not cause appreciable differences in the overall epidemic pattern, and 3) to present a new formulation for the action of one important determinant of contact patterns in sexually transmitted diseases, namely biased selection of partners from the potential partners encountered. <#>This new formulation supersedes our previous selective mixing formulation (1).</p> (www.ice-corpora.uzh.ch/en/joinice/Teams/iceusa.html)

Before examining the two samples of speech and writing, it is worth noting that while spontaneous conversations share many similarities, there are considerable differences between different types of academic writing. For instance, there are, as

Table

Thus, the use of chronic here is more of a report back to a scientific study on radiation rather than part of, for instance, a casual conversation.

The words in the table from the spoken texts also predominate in speech, but their usages, though less frequent, do occur in the written texts as well. Because the pronoun you is clearly interactional, involving more than one speaker, it overwhelmingly predominates in speech.

Well, first of all, I am a member of the Democratic leadership. I've been in the Democratic caucus. But this is what I will also say, if you look at polling in this country, what you find is that a whole lot of people are dissatisfied with both the Democratic and Republican parties.

However, you can also occur in written texts. In the example below, you is used because the two sentences occur in an instructional context. Thus, you is used in two imperative sentences.

Write your sentences as quickly and as clearly as possible. Make sure you complete four sentences.

Before a multi-dimensional analysis can be performed, a corpus needs to be both lexically tagged and then analyzed by a particular statistical method termed factor analysis. As was noted in Chapter 3, a corpus that is lexically tagged contains part-of-speech information for each word in the corpus. For instance, in the very simple sentence I like pizza, the pronoun I would be tagged as a first person pronoun; the verb like would be as a present tense lexical verb; and the noun pizza as a singular common noun. Of course, different tagging programs will have different terminology and provide varying levels of detail about the items being tagged. Lexical tagging is crucial because it will enable the factor analysis program to determine where the various parts of speech occur: e.g. first person pronouns in more interactive texts; passive verbs in more informational texts.

A tagging program used to conduct multi-dimensional analyses is the Biber tagger

Lexically tagged texts serve as input for a multi-dimensional analysis, an analysis that Biber describes as: a methodology to empirically analyze the ways in which linguistic features co-occur in texts in the ways in which registers vary with respect to those cooccurrence patterns

To discover these patterns, it is necessary to subject the data to a factor analysis, a statistical program that is able to isolate both positive and negative correlations between variables

Note, for instance, how first person pronouns correlate positively with questions, but negatively with passives and have a very weak positive correlation with nominalizations. Questions, in turn, correlate negatively with passives and nominalizations. These correlations match the distributions of these constructions in the registers of conversation and academic writing discussed earlier in this section.

Since the publication of

Conclusions

As

In the pre-electronic era, textual analysis was largely a matter of analyzing 'static' texts: written texts existing only in printed form that had to be analyzed by hand.

He comments that the major earlier grammarians from this era, such as Otto Jespersen and Hendrik Poutsma, based their grammars on "primarily canonical written texts (e.g. novels)," from which they manually extracted relevant examples to illustrate the points of grammar that they discussed. Since this era, advances in corpus linguistics have resulted in the creation of many different types of corpora containing authentic texts ranging from spontaneous conversations to technical writing in the social and natural sciences. Because these texts are in an electronic format, they can be searched with software such as concordancing programs, and relevant linguistic constructions can be instantly retrieved. While it is certainly not the case that any particular linguistic item can be automatically retrieved instantlymany linguistic constructions are simply too complex for this type of "instant" retrievalnevertheless, the process of corpus analysis has been greatly automated.

Concluding Remarks

The final chapter of the 1st edition of this book (hereafter ECL1), "Future Prospects in Corpus Linguistics," began with the following paragraph:

In describing the complexity of creating a corpus,

In the context of current work done in corpus linguistics, the points made in the previous paragraph raise some interesting issues:

(1) Planning a Corpus: In ECL1, it is noted that "as more and more corpora have been created, we have gained considerable knowledge of how to construct a corpus that is balanced and representative and that will yield reliable grammatical information" (138). It is certainly the case that many recently created corpora are balanced and representative. Even some mega corpora, such as the one billion-word Corpus of Contemporary American English (COCA), contain various registers, such as fiction, speech, press reportage, and academic writing. But there are many challenges involved in creating "small and beautiful corpora," such as the British National Corpus (BNC) and the International Corpus of English (ICE). The situation is certainly understandable. It requires considerable resources to create balanced corpora, such as the BNC, whereas COCA contains texts downloaded from websites, requiring much less effort than building a corpus such as the BNC.

(2) Collecting and Computerizing Data: In ECL1, it is noted that "because so many written texts are now available in computerized formats in easily accessible media. . . The collection and computerization of written texts has become much easier than in the past" (139). A similar situation exists in the present, and because the Web has grown even larger in recent years, plenty of texts can be downloaded. The situation with spoken texts was quite different: they had to be transcribed manually, a very timeconsuming endeavor. In the present, there are transcriptions of different types of public speech, such as press conferences or talk shows, that can be downloaded and that are reasonably accurate. In fact, COCA contains quite a bit of public speech.

(3) Annotating a Corpus: Corpus annotation has changed considerably since ECL1. For instance, the opening of the section in ECL1 on "Structural Markup" makes reference to ASCII (American Standard Code for Information Interchange) text formats and standard generalized markup language (SGML). ASCII has since been replaced by Unicode, which has far more characters.

(4) Tagging and Parsing a Corpus: Lexically tagging a corpus has become quite routine, particularly because the accuracy of current tagging programs is quite high. Parsing a corpus is a more complicated undertaking, since larger structures, such as phrases and clauses, must be identified. Consequently, more post-editing has to be done after a corpus has been parsed.

(5) Analyzing a Corpus: Great strides have been made in developing software that can enhance the analysis of corpora. For instance, there are concordancing programs that can be used to identify and retrieve various grammatical constructions in a corpus, such as particular words or phrases. And if a corpus has been lexically tagged, concordancing programs can retrieve various grammatical structures: lexical items, such as nouns or verbs; phrases, such as noun phrases or verb phrases; and clauses, such as relative clauses or adverbial clauses.

In the future, we can expect further advances to enhance the creation and analysis of linguistic corpora. In particular, we can expect an increase in accuracy for certain corpus tools, such as improvement in the accuracy of software used to parse and lexically tag corpora, and the increased accuracy of tools, such as concordancing programs, used to retrieve constructions from corpora.

Discussion Topics

Below are a series of questions that explore the topics discussed in the first four chapters of the book. The questions go beyond fill in the blank-or short essay-types of responses and require respondents to think more deeply and critically in their answers. Consequently, there are no correct or incorrect answers.

Chapter 1

1. In the opening section of the chapter, a distinction is made between the "armchair linguist," whose sources of data are essentially sentences that he/she makes up, and the "corpus linguist," who believes that it is better to draw upon authentic data as the basis of linguistic analyses. In the book, it is noted that these two approaches are not necessarily mutually exclusive: the generative linguist, for instance, could very well work with examples that are corpus-based rather than examples that are made up. But is this true? Couldn't the generative linguist very well work with only made-up sentences, especially since his/her goal is to theorize about language, and spending additional time finding relevant data in a corpus is simply unnecessary? 2. What is the difference between "corpus" and "experimental" data?

In the chapter, it is stated that these two different kinds of data "complement" each other. However, couldn't it also be argued that the two types of data are so different that they are incompatible? Drawing upon evidence in Section 1.3 of the chapter, argue that the two types of data are either compatible or incompatible. 3. Section 1.4 contains a discussion of how the corpus methodology has many different applications. For instance, it can be used to study language variation or to help in the creation of dictionaries. Select one of the areas described in this section and briefly discuss how corpus methodology has changed the way that research is done in the area. You may want to focus your response on areas that are discussed in greater detail in Section 1.4 than some of the other areas. 4. Section 1.6 contains a discussion of how the theory of construction grammar can shed light on the "problematic" nature of apposition in English. Do you agree with this claim? What evidence exists to support the claim?

Chapter 2

1. The chapter opens with a discussion of three different types of corpora: the British National Corpus (BNC), the Corpus of Contemporary American English (COCA), and the Corpus of Early English Correspondence (CEEC). Below are some questions to consider: a. The BNC was released in 1994. Is it too out of date to be of use anymore? If not, what value might it have? For instance, how could it be compared with the BNC2014, a more modern corpus that is directly modeled after the BNC? Think of additional corpora it could be compared to. b. The COCA is a web-based corpus. It can be analyzed online.

What advantages does it have being online rather than locally on a personal computer? c. The CEEC is actually a group of corpora containing letters written by individuals during various time periods, ranging from 1410 to 1663. The corpora were designed so that a range of sociolinguistic variables, such as age, social class, and gender, could be studied. Because these are historical corpora, what kinds of sociolinguistic variables would be especially worth studying? One point that is made in the chapters is that fewer letters written by women are in the corpus than letters written by men. 2. The idea of the Web as a corpus was initially a controversial notion, largely because at the time, more traditional corpora were the norm, and analyzing the Web, with its seemingly endless size and unknown content, contradicted the whole idea of what a corpus was thought to be. In the chapter,

How would you construct the corpus? For instance, would you want the corpus to contain a specific section of a newspaper (e.g. editorials). Or would you want more broad representation? Would you want complete texts, or only text excerpts? how would you ensure that the corpus is balanced? Consider other variables discussed in the chapter in your response.

Chapter 3

1. As is noted in the chapter, collecting and transcribing spoken language is one of the more labor-intensive parts of creating a corpus. But including spoken language in a corpus is very important because spoken language is the essence of human language, particularly spontaneous conversation. There are ways to get transcriptions of spontaneous speech, but mainly from conversations broadcast on radio or television, a highly restricted type of language. If you were considering creating a corpus of spontaneous conversations, how would you go about recording and transcribing them? Draw upon information in the chapter to write your response. 2. Let's say you want to create a corpus of newspaper editorials. How would you create a balanced corpus of them? For instance, how would you achieve gender balance? Would it be necessary to control for the types of newspapers from which you collect texts (e.g. broadsheets vs. tabloids)? What about other variables such as age and ethnicity? What are some variables whereby it would be too difficult to obtain information? 3. Go to the following website, which contains a much fuller description of the International Corpus of English (ICE) than the chapter does: www.ice-corpora.uzh.ch/en/design.html. Would you consider the ICE corpora balanced corpora? Why or why not? 4. What is the difference between metadata and textual markup?

Why is it important that a standard system, such as the Text Encoding Initiative, is developed to ensure that everyone follows a standardized system of annotation for representing metadata and textual annotation?

Chapter 4

1. Section 4.5 discusses how corpus analyses can be quantitative, qualitative, or a combination of the two (sometimes termed mixed methods). What's the difference between the three types of analysis? Is any one type of analysis better than another? 2. Conducting a corpus analysis is a multi-stage process, involving framing a research question, finding a suitable corpus or corpora for the analysis, extracting relevant information from the corpus or corpora chosen for the analysis, and integrating information from relevant articles or books into the analysis. Why are all these stages necessary? What would happen if, for instance, you did your analyses but did not integrate relevant research into the writeup (book, article, presentation) of your results? 3. Go to www.english-corpora.org/ and select a corpus from the list of corpora on the page. After you have selected a corpus, you will need to create an account to use the corpus. Once you have created an account, you should replicate some of the searches in Section 4.3 of the chapter but try your own searches too. You could also try BNCweb (

BYU Corpora: a set of many different corpora taken from various online resources (www.english-corpora.org/): see also entry under C for the Corpus of Contemporary American English.

C

CallHome Corpus: 120 spontaneous 30-minute phone conversations between intimates, either friends or members of the same family. (

Child Language Data Exchange System, or CHILDES Corpus: includes transcriptions of children engaging in spontaneous conversations in English and other languages. (

Collins Corpus: a 4.5 billion-word monitor corpus used as the basis for creating the COBUILD dictionaries. (

Corpus of Age and Gender (see Murphy 2010): an internal corpus not publicly available. (

Corpus of Contemporary American English (COCA): a one billion-word corpus containing various registers of American English that can be searched online. (

Corpus of Early English Correspondence: consists of a collection of corpora containing various types of correspondence written in the fifteenth to seventeenth centuries. (www2.helsinki.fi/en/researchgroups/ varieng/corpus-of-early-english-correspondence)

Corpus of Global Web-Based English (GloWbE): a 1.9 billionword corpus containing samples of English from 20 different countries in which English is used. (

Corpus of Middle English Prose and Verse: contains the works of Chaucer and other Middle English writers. (www.hti.umich.edu/eng lish/mideng/) D Diachronic Corpus of Present-Day Spoken English (DCPSE): 400,000 words of spoken English from ICE-GB and an additional 400,000 spoken words from the London-Lund Corpus. (www.ucl.ac.uk/english-usage/projects/dcpse/index.htm) Dictionary of Old English Corpus: a three million-word corpus containing all surviving Old English texts. (www.sheffield.ac.uk/library/ cdfiles/doec#:~:text=The%20Dictionary%20of%20Old%20English,the %20collected%20works%20of%20Shakespeare) ICLE Corpus (The International Corpus of Learner English): contains samples of written English produced by advanced learners of English as a foreign language from 25 different language backgrounds. (

E

Y

York English Corpus: a 1.5 million-word corpus that has been subjected to extensive analysis and that has yielded valuable information on dialect patterns (both social and regional) particular to this region of England. (www.researchgate.net/figure/Sample-designof-the-York-English-Corpus_tbl1_227609144)
List of Figures

Introduction

Linguistics is a science, and should therefore use scientific methodology. The currently dominant methodology is the hypothetico-deductive one associated in the philosophy of science with Karl

1. Some aspect of the natural world, that is, a domain of interest, is selected for study, and a research question that will substantially further scientific knowledge of the domain is posed. 2. A hypothesis that answers the research question is stated. 3. The hypothesis is tested by observation of the domain. If it is incompatible with observation the hypothesis must either be emended to make it compatible or, if this is not possible, must be abandoned. If it is compatible then the hypothesis is said to be supported but not proven; no scientific hypothesis is ever proven because it is always open to falsification by new evidence from observation. On this model, the science of the selected aspect of the domain of interest at any given time is a collection of hypotheses that are valid with respect to observations of the domain made up to that time, or, in other words, a collection of best guesses about what that aspect of the natural world is like.

Because falsifiable hypotheses are central in science, it is natural to ask how they are generated. The consensus in philosophy of science is that hypothesis generation is non-algorithmic, that is, not reducible to a formula, but is rather driven by human intellectual creativity in response to a research question

In linguistics the research domain is text, where 'text' is understood generically as output from the human language faculty. In branches of linguistics concerned with chronological, geographical, and social language variation, text takes the form of collections of spoken and / or written language, or corpora. In those concerned with the structure of the language faculty, and in generative linguistics more specifically, the role of text is less obvious but no less fundamental: native speaker grammaticality judgment has historically been used as the basis for hypothesis generation and confirmation instead of evidence derived from corpora, but such judgment is based on example sentences generated by the researcher's language faculty, not on direct access to the language faculty within the human cognitive system itself, and therefore on text in the above sense. The use of corpora in Western linguistics began in the late eighteenth century with the postulation of an Indo-European protolanguage and its reconstruction based on examination of numerous living languages and of historical written documents; after two centuries of development the importance of corpora in linguistics research has increased to the extent that a subdiscipline has come into being, corpus linguistics, whose remit is to develop methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing that data with the aim of generating or testing hypotheses about the structure of language and its use in the world.

Throughout the nineteenth and much of the twentieth centuries corpus linguistics was exclusively paper-based. The collections at the root of the discipline were in the form of hand-written or printed documents, and research using such collections involved reading through the documents, often repeatedly, creating data by noting features of interest on some paper medium such as index cards, inspecting the data directly, and on the basis of that inspection drawing conclusions that were published in printed books or journals. The advent of digital electronic technology in the second half of the twentieth century and its evolution since then have increasingly rendered this traditional methodology obsolete. On the one hand, the possibility of representing language electronically rather than as visual marks on paper, together with the development of electronic media, infrastructure, and computational tools for creation, emendation, storage, and transmission of electronic text have led to a rapid increase in the number and size of corpora available to the linguist, and these are now at or beyond the limit of what an individual researcher can efficiently use in the traditional way. On the other, data abstracted from very large corpora can themselves be so extensive and complex as to be impenetrable to understanding by direct inspection. Digital electronic technology has been a boon the corpus linguistics community, but in linguistics, as in life, it's possible to have too much of a good thing.

One response to digital electronic text and data overload is to use only corpora of tractable size or, equivalently, subsets of large corpora, but simply ignoring available information is not scientifically respectable. The alternative is to look to related research disciplines for help. The overload in corpus linguistics is symptomatic of a more general trend. Daily use of digital electronic information technology by many millions of people worldwide in their professional and personal lives has generated and continues to generate truly vast amounts of electronic speech and text, and abstraction of information from all but a tiny part of it by direct inspection is an intractable task not only for individuals but also in government and commerce -what, for example, are the prospects for finding a specific item of information by reading sequentially through the huge number of documents currently available on the World Wide Web? In response, research disciplines devoted to information abstraction from very large collections of electronic text have come into being. These go under a variety of names such as informatics, information science, information retrieval, text summarization, data mining, and natural language processing. They overlap to greater or lesser degrees but share an essentially identical remit: to make interpretation of large collections of digital text tractable. To achieve this they draw on concepts and methods from a range of other disciplines including mathematics, statistics, computer science, and artificial intelligence.

An increasingly important class of these concepts and methods is cluster analysis, which is used across a broad range of sciences for hypothesis generation based on identification of structure in data which are too large or complex, or both, to be interpretable by direct inspection. The aim of the present discussion is to show how cluster analysis can be used for corpusbased hypothesis generation in linguistics by applying it to a case study of a dialect corpus, the Diachronic Electronic Corpus of Tyneside English , and thereby to contribute to the development of an empirically-based quantitative methodology for hypothesis generation in the linguistics community.

This aim is realized by presenting the relevant material so as to make it accessible to that community. The implication is that accessibility is a problem, and in the author's view it is, for several reasons.

-The number of available clustering methods is so large that even specifically dedicated surveys of them are selective, more are continually being proposed, and the associated technical literature is correspondingly extensive and growing. Selection of the method or methods appropriate to any given research application requires engagement with this literature, and for the non-specialist the magnitude of the task can be a substantial obstacle to informed use. The present discussion addresses this obstacle by selectivity. There is no prospect of being able to cover all or even very many of the available clustering methods at a level of description sufficient to convey understanding, and as such no attempt is made at comprehensiveness; surveys exist already, and no obvious purpose would be served by a précis of them here. What is offered instead are detailed descriptions of a relatively small selection of methods which are likely to be of most use to corpus linguists, where usefulness is judged on criteria of intuitive accessibility, theoretically and empirically demonstrated effectiveness, and availability of software implementations for practical application. These detailed descriptions are supplemented by references to variations on and alternatives to the methods in question, thereby providing the reader with pointers to the broader range of methods in which the selected ones are embedded. Needless to say, there is a subjective element in this, and a different writer might well have made different choices.

-Understanding of the nature, creation, representation, and properties of data is fundamental to successful cluster analysis. The clustering literature typically assumes familiarity with these topics, however, and consequently tends to deal with them in relatively cursory fashion; anyone lacking the requisite familiarity must acquire it by engaging with the relevant and also very extensive mathematical, statistical, and data processing literatures. To forestall the need for this, the account of the selected cluster analytical methods that follows is preceded by a detailed discussion of data issues. -Cluster analysis and data processing are based on concepts from mathematics, statistics, and computer science, and discussions of them in the above-mentioned literatures are, in general, quite technical. This can be a serious obstacle in that, though it has become less pronounced, the arts / science divide is still with us, and many professional linguists have little or no background in and sometimes even an antipathy to mathematics and statistics; for further discussion of this assertion see Chapter 4 below. Understanding of these concepts and associated formalisms is, however, a prerequisite for informed application of cluster analysis, and so introductory-level explanations of them are provided.

No a priori knowledge of them is assumed, and all are explained before any use is made of them. They are, moreover, introduced in the course of discussion as they are needed, so that the requisite knowledge is built up gradually. This approach itself presents a problem. Some linguists are as mathematically and statistically sophisticated as any in the traditional 'hard' sciences, and to such readers intuitive explanations can seem tedious. The choice, therefore, is between puzzling some readers and probably putting them off the discussion entirely, and boring others. There is no obvious solution. In my experience more linguists need the explanations than not; every effort is made to avoid longwindedness, but where there is a choice between brevity and intuitive clarity, the latter wins every time. The discussion is in six main chapters. The first chapter is the present Introduction. The second chapter motivates the use of of cluster analysis for hypothesis generation in linguistics. The third deals with data creation: the nature of data, its abstraction from text corpora, its representation in a mathematical format suitable for cluster analysis, and transformation of that representation so as to optimize its interpretability. The fourth chapter describes a range of cluster analysis methods and exemplifies their application to data created in Chapter 3. The fifth shows how these methods can serve as the basis for generation of linguistic hypotheses, and the sixth reviews existing applications of cluster analysis in corpus linguistics. In addition, there is an Appendix that identifies software implementations which make the clustering methods described in Chapter 4 available for practical application. Exemplification throughout the discussion is based on data abstracted from the Diachronic Electronic Corpus of Tyneside English .

This book is one of a series entitled Quantitative Linguistics, whose remit is to cover 'all aspects of quantitative methods and models in linguistics, text analysis, and related research fields', and more specifically 'the whole spectrum of theoretical and empirical research, ultimately striving for an exact mathematical formulation and empirical testing of hypotheses: observation and description of linguistic data, application of methods and models, discussion of methodological and epistemological issues, modelling of language and text phenomena'. In addressing methodological issues in linguistic hypothesis generation using mathematically-based data creation and cluster analysis methods, the author's belief is that it satisfies this remit.

As noted in the Introduction, cluster analysis is a tool for hypothesis generation. It identifies structure latent in data, and awareness of such structure can be used to draw the inferences on the basis of which a hypothesis is formulated. To see how this works, let us assume that the domain of interest is a speech community and that one wants to understand the relationship between phonetic usage and social structure within it; for concreteness, that community will be assumed to be Tyneside in north-east England, shown in Figure

Tyneside

Is there systematic phonetic variation in the Tyneside speech community, and, if so, does that variation correlate systematically with social factors?

To answer the question, a representative sample of Tyneside speech is collected and relevant data is abstracted from it. The sample used here is the Diachronic Electronic Corpus of Tyneside English (henceforth DECTE), a collection of interviews with Tyneside English speakers that will be fully described in due course.

A group of 24 speakers is selected at random and a set of phonetic variables descriptive of Tyneside pronunciation is defined. The number of times each speaker uses the phonetic variable or variables of interest is recorded, thereby building up a body of data. To start, each speaker's speech is described by a single variable, the phonetic segment @ 1 ; the labels in the Speaker column of Table

As noted in the Introduction, cluster analysis is a family of computational methods for identification and graphical display of structure in data when the data are too large either in terms of the number of variables or of the number of objects described, or both, to be readily interpretable by direct inspection. All the members of the family work by partitioning a set of objects in the domain of interest into disjoint subsets in accordance with how relatively similar those objects are in terms of the variables that describe them. The objects of interest in Tables 2.1, 2.2, and 2.3 are speakers, and each speaker's phonetic usage is described by a set of variables. Any two speakers' usage will be more or less similar depending on how similar their respective variable values are: if the values are identical then so are the speakers in terms of their usage, and the greater the divergence in values the greater the differences. Cluster analysis of the data in Table

This and other varieties of cluster analysis are described in detail later in the discussion; the aim at this stage is to give an initial impression of how they can be used in linguistic analysis. Figure

Once the structure of the data has been identified by the above procedure it can be used for generation of a hypothesis in response to the research question.

-Is there systematic phonetic variation in the Tyneside speech community?

Since the relative lengths of the branches joining subclusters represents their relative similarity, the speakers included in the analysis can be seen to fall into two main clusters, labeled A and B in the tree, such that the speakers in cluster A are relatively much more similar to one another than any of them are to speakers in cluster B, and vice versa.

A reasonable hypothesis based on this finding would be that there is systematic phonetic variation in the Tyneside speech community, and more specifically that the speakers who constitute that community fall into two main groups. -Does that variation correlate systematically with social factors?

DECTE includes a range of social information for each speaker, such as age, gender, educational level, occupation, and so on. Also included is an indication of whether the speaker comes from Newcastle on the north shore of the river Tyne or Gateshead on the south side; correlating place of residence with the cluster tree in Figure

This hypothesis can be refined on the one hand by correlating the internal structures of clusters A and B with a larger number of social factors, and on the other by identifying the phonetic segments which are most important as determinants of the cluster structure. The former is analogous to what has already been described and does not need to be made explicit at this stage, though subsequent discussion will do so. One approach to the latter is to create summary descriptions of the phonetic characteristics of the two main clusters A and B and then to compare them. This is done by taking the mean of variable values for the speakers in each cluster, as in Table

All the speakers whom the cluster tree assigns to A are collected in the cluster A sub-table of Table

Cluster analysis can be applied to hypothesis generation in any research where the data consists of objects described by variables; since most research uses data of this kind, it is very widely applicable. It can usefully be applied where the number of objects and variables is so large that the data cannot easily be interpreted by direct inspection, as above. The foregoing discussion has sketched one sort of application to linguistic analysis; a few other random possibilities are, briefly:

-A historical linguist might want to infer phonetic or phonological structure in a legacy corpus on the basis of spelling by cluster analyzing alphabetic n-grams for different magnitudes 2, 3, 4 . . . of n. -A generative linguist might want to infer syntactic structures in a littleknown or endangered language by clustering lexical n-grams for different magnitudes of n. -A philologist might want to use cluster analysis of alphabetic n-grams to see if a collection of historical literary texts can be classified chronologically of geographically on the basis of their spelling. Further applications to linguistic analysis are given in Chapter 6, the literature review.

'Data' is the plural of 'datum', the past participle of Latin 'dare', to give, and means things that are given. A datum is therefore something to be accepted at face value, a true statement about the world. What is a true statement about the world? That question has been debated in philosophical metaphysics since Antiquity and probably before, and, in our own time, has been intensively studied by the disciplines that comprise cognitive science

Data are ontologically different from the world. The world is as it is; data are an interpretation of it for the purpose of scientific study. The weather is not the meteorologist's data -measurements of such things as air temperature are. A text corpus is not the linguist's data -measurements of such things as lexical frequency are. Data are constructed from observation of things in the world, and the process of construction raises a range of issues that determine the amenability of the data to analysis and the interpretability of the analytical results. The importance to cluster analysis of understanding such data issues can hardly be overstated

The discussion is in three main sections. The first section deals with data creation, the second presents a geometrical interpretation of data on which all subsequent discussion is based, and the third describes several ways of transforming data prior to cluster analysis in terms of that geometrical interpretation.

3.1

Data creation

Research domain

To state the obvious, data creation presupposes a research domain from which the data are to be abstracted. In corpus-based linguistics the research domain is some collection of natural language utterances. In the present case the domain is the English spoken in the Tyneside region of north-east England

The DECTE corpus

DECTE contains samples of the Tyneside English speech variety dating from the later 20th and the early 21st centuries collected from residents of Tyneside and surrounding areas of North-East England. The main locations in this area represented in the corpus are the cities of Newcastle upon Tyne on the north side of the river Tyne and Gatesheadon the south side, but its geographical reach is currently being extended to include speakers from other regions in the North-East such as County Durham, Northumberland, and Sunderland. It updates the existing Newcastle Electronic Corpus of Tyneside English (NECTE), which was created between 2000 and 2005 and consists of two pre-existing corpora of audio-recorded Tyneside speech

The earlier of the two components of NECTE , the Tyneside Linguistic Survey (TLS) , was created in the late 1960s

The other component of NECTE is the Phonological Variation and Change in Contemporary Spoken English (PVC) project

NECTE amalgamated the TLS and PVC materials into a single Text Encoding Initiative (TEI)-conformant XML-encoded corpus and made them available online in a variety of aligned formats: digitized audio, standard orthographic transcription, phonetic transcription, and part-of-speech tagged.

In 2011-12 the DECTE project combined NECTE with the NECTE2 corpus, which was begun in 2007 and is ongoing. NECTE2 consists of digitized audio recordings and orthographic transcriptions of dyadic interviews together with records of informant social details and other supplementary material, collected by undergraduate and postgraduate students and researchers at Newcastle University. The interviews record the language use of a variety of local informants from a range of social groups and extend the geographical domain covered in the earlier collections to include other parts of the North East of England. Successive cohorts of students add their own interviews to NECTE2. The components of DECTE and their interrelationship are shown schematically in Figure

The DECTE phonetic transcriptions

The main motivation for the TLS project was to see whether systematic phonetic variation among Tyneside speakers of the period could be interestingly correlated with variation in their social characteristics. To this end the project developed a methodology that was radical at the time and remains so today: in contrast to the then-universal and still-dominant theory driven approach, where social and linguistic factors are selected by the analyst on the basis of some combination of an independently-specified theoretical framework, existing case studies, and personal experience of the domain of inquiry, the TLS proposed a fundamentally empirical approach in which salient factors are extracted from the data itself and then serve as the basis for model construction.

To realize this research aim using its empirical methodology, the TLS had to compare the audio interviews it had collected at the phonetic level of representation. This required the analog speech signal to be discretized into phonetic segment sequences, or, in other words, to be phonetically transcribed. The TLS project wanted a very detailed phonetic transcription of its audio files, and the standard International Phonetic Alphabet scheme was not detailed enough. It therefore developed an extended version of the IPA scheme; the sample from the TLS encoding manual in Figure

There are four columns in the encoding table. The first three give symbols for increasingly fine-grained transcription, and the fourth examples of what speech sounds the symbols represent: the OU column lists phonological segments, the PDV ('Putative Diasystemic Variable') column lists IPA-level phonetic segments, and the State column lists the TLS's detailed elaboration of the IPA scheme.

The graphical symbols had to be numerically encoded for computational processing. For this encoding, each PDV symbol was assigned a unique fourdigit code. A fifth digit was then added to any given PDV code to give a five-digit State code. This fifth digit was the State symbol's position in the left-to-right sequence in the State column. For example, the PDV code for [i:] is 0002; the State code for, say, [ i -] is 00024, because [ i -] is fourth in the left-to-right State symbol sequence. Using this encoding scheme, a PDV-level transcription of any given speaker interview audio file is a sequence of fourdigit codes, and a State-level transcription is a sequence of five-digit codes. Reference to the DECTE transcriptions will henceforth be to the PDV level.

Associated with each of the 63 DECTE speakers is a file representing a transcription of about the first 10 minutes of the audio interview and containing a sequence of numerical State-level codes of which only the first four

PDV-level digits are used in what follows. A fragment of one of these is shown in Table

The 63 phonetic transcription files and the social data associated with them constitute the corpus from which the data used in the remainder of this discussion were abstracted. For ease of cross-reference, abbreviated versions of the DECTE naming conventions for speakers together with the phonetic symbols and corresponding numerical codes are used throughout.

A final note: earlier work

Research question

Any aspect of the world can be described in an arbitrary number of ways and to arbitrary degrees of precision. A desktop computer can, for example, be described in terms of its role in an the administrative structure of an organization, its physical appearance, its hardware components, the functionality of the software installed on it, the programs which implement that functionality, the design of the chips on the circuit board, or the atomic and subatomic characteristics of the transistors on the chips, not to speak of its connectivity to the internet or its social and economic impact on the world at large. Which description is best? That depends on why one wants the description. A software developer wants a clear definition of the required functionality but doesn't care about the details of chip design; the chip designer doesn't care about the physical appearance of the machines in which her devices are installed but a marketing manager does; an academic interested in the sociology of computers doesn't care about chip design either, or about circuit boards, or about programs. In general, how one describes a thing depends on what one wants to know about it, or, in other words, on the question one has asked.

The implications of this go straight to the heart of the debate on the nature of science and scientific theories in philosophy of science

In a scientific context, the question one has asked is the research question component of the hypothetico-deductive model outlined earlier. Given a domain of interest, how is a good research question formulated? That, of course, is the central question in science. Asking the right questions is what leads to scientific breakthroughs and makes reputations, and, beyond a thorough knowledge of the research area and possession of a creative intelligence, there is no known guaranteed route to the right questions. What is clear from the preceding paragraph, though, is that a well-defined question is the key precondition to the conduct of research, and more particularly to the creation of the data that will support hypothesis formulation. The research question provides an interpretative orientation; without such an orientation, how does one know what to observe in the domain, what is important, and what is not? A linguist's domain is natural language, but syntacticians want to know different things about it than semanticists, and they ask commensurately different questions. In the present case we will be interested in sociophonetics, and the research question is the one stated earlier:

Is there systematic phonetic variation in the Tyneside speech community, and, if so, does that variation correlate systematically with social variables?

Variable selection

Given that data are an interpretation of some domain of interest, what does such an interpretation look like? It is a description of objects in the domain in terms of variables. A variable is a symbol, that is, a physical entity to which a meaning is assigned by human interpreters; the physical shape A in the English spelling system means the phoneme /a/, for example, because all users of the system agree that it does. The variables chosen to describe a domain constitute the conceptual template in terms of which the domain is interpreted and on which the proposed analysis is based. If the analysis is to be valid with respect to the domain, therefore, it is crucial that the set of selected variables be adequate in relation to the research question, where adequacy is understood as follows:

-The variables should represent all and only those aspects of the domain which are relevant to the research question, that is, relevant aspects of the domain should not be unrepresented in the set of variables, and irrelevant aspects should not be represented. Failure to include relevant aspects in the data renders the description of the domain incomplete and thereby self-evidently compromises the validity of analysis based on it; inclusion of irrelevant aspects is less serious but introduces potentially confounding factors into an analysis. -Each variable should be independent of all the others in terms of what it represents in the domain, that is, the variables should not overlap with one another in what they describe in the domain because such overlap describes the same thing multiple times and can thereby skew the analysis by overemphasizing the importance of some aspects of the domain over others. In general, adequacy so defined cannot be guaranteed in any given research application because neither relevance nor independence is always obvious. Any domain can be described by an essentially arbitrary number of finite sets of variables, as the foregoing example of computer description makes clear. Selection of one particular set can only be done on the basis of personal knowledge of the domain and of the body of scientific theory associated with it, tempered by personal discretion. In other words, there is no algorithm for choosing an adequate set of variables.

The research question defined on DECTE involves phonetic analysis, and that implies phonetic transcription of the audio speaker interviews: a set of variables is defined each of which represents a characteristic of the speech signal taken to be phonetically significant, and these are then used to interpret the continuous signal as a sequence of discrete symbols. The standard way of doing this is to use the symbols defined by the International Phonetic Alphabet (IPA), but the TLS researchers felt that the IPA was too restrictive in the sense that it did not capture phonetic features which they considered to be of interest, and so they invented their own transcription scheme, described earlier. The remainder of this discussion refers to data abstracted from these TLS transcriptions, but it has to be understood that the 156 variables in that scheme are not necessarily optimal or even adequate relative to our research question. They only constitute one view of what is important in the phonetics of Tyneside speech. In fact, as we shall see, many of them have no particular relevance to the research question.

Variable value assignment

Once variables have been selected, a value is assigned to each of them for each of the objects of interest in the domain. This value assignment is what makes the link between the researcher's conceptualization of the domain in terms of the variables s/he has chosen and the actual state of the world, and allows the resulting data to be taken as a valid representation of the domain. The type of value assigned to any given variable depends on its meaning. The fundamental distinction of types is between quantitative, that is, numerical values, and qualitative ones such as binary 'yes / no' or categorial 'poor / adequate / good / excellent'

The objects of interest in DECTE are the 63 speakers, each of whom is described by the values for each of the 156 phonetic variables. What kind of value should be assigned? One possibility is to use qualitative binary ones: the value of any given variable is 'yes' if the speaker in question uses the corresponding phonetic segment in his or her interview, and 'no' if not. Another, and the one adopted here, is to use quantitative values which represent the number of times the speaker uses each of the phonetic segments.

Data representation

If they are to be analyzed using mathematically-based computational methods like cluster analysis, the descriptions of the entities in the domain of interest in terms of the selected variables must be mathematically represented. A widely used way of doing this in, for example, information retrieval

The most basic characteristic of data is that they be complete and accurate, where 'complete' means that all variables for all cases in the data have values associated with them, and 'accurate' that all values assigned to variables faithfully reflect the reality they represent. These are stringent requirements: most datasets large enough to have cluster analysis usefully applied to them probably contain error, known as 'noise', to greater or lesser degrees. Measurement error arises in numerous ways -tolerances in measuring instruments, human inaccuracy in the use of the instruments, corruption at one or more points in data transmission, and so on.

Because error in data distorts analytical results, it has to be eliminated as much as possible. This is a two-step process, the first step of which is to determine the amount and nature of the error, and the second to mitigate or remove it. Methods for error identification and correction are standardly discussed in statistics and multivariate analysis textbooks, for example by

The DECTE data are generated by counting the frequency of phonetic segments in interviews, so completeness and accuracy should not be issues if the survey is carefully done using a reliable procedure; manual counting of features in physical text by direct observation is in general far less accurate than the software equivalent for electronic text.

Data geometry

Data matrices have a geometrical interpretation, and the remainder of the discussion is based on it. This section first presents a few relevant mathematical and geometrical concepts and then shows how data can be represented and interpreted in terms of them.

Space

In colloquial usage, the word 'space' denotes a fundamental aspect of how humans understand their world: that we live our lives in a three-dimensional space, that there are directions in that space, that distances along those directions can be measured, that relative distances between and among objects in the space can be compared, that objects in the space themselves have size and shape which can be measured and described. The earliest geometries were attempts to define these intuitive notions of space, direction, distance, size, and shape in terms of abstract principles which could, on the one hand, be applied to scientific understanding of physical reality, and on the other to practical problems like construction and navigation. Basing their ideas on the first attempts in ancient Mesopotamia and Egypt, Greek philosophers from the sixth century BCE onwards developed such abstract principles systematically, and their work culminated in the geometrical system attributed to Euclid (floruit ca. 300 BCE), which remained the standard for more than two millennia thereafter

In the nineteenth century CE the validity of Euclidean geometry was questioned for the first time both intrinsically and as a description of physical reality. It was realized that the Euclidean was not the only possible geometry, and alternative ones were proposed in which, for example, there are no parallel lines and the angles inside a triangle always sum to less than 180 degrees. Since the nineteenth century these alternative geometries have continued to be developed without reference to their utility as descriptions of physical reality, and as part of this development 'space' has come to have an entirely abstract meaning which has nothing obvious to do with the one rooted in our intuitions about physical reality. A space under this construal is a mathematical set on which one or more mathematical structures are defined, and is thus a mathematical object rather than a humanly-perceived physical phenomenon

Cartesian product

Given two sets A and B, the Cartesian product

Vector space

If the mathematical structures of addition and scalar multiplication, that is, multiplication by a single number, are defined on the n-tuples of a Cartesian product X , then X together with these two structures is a vector space V subject to a range of conditions which are for present purposes assumed to apply

Given an n-dimensional vector space V , n vectors can be selected from the space to constitute a basis for it, and the set of these n vectors is so called because all other vectors in V can be generated from it using the operations of addition and scalar multiplication, as described below. Selection of basis vectors is constrained by certain conditions explained in any and every linear algebra textbook, but understanding of these constraints is unnecessary for present purposes because we shall be using orthogonal bases, and such bases automatically satisfy the conditions. Orthogonal basis vectors have the property that their inner product is zero; the inner product of n-dimensional vectors v and w, also called the dot product and written v.w, is defined in Equation (3.1).

v

that is, corresponding components of v and w are multiplied and all the products summed. For n = 2, the inner product of, say, v = [2.2, 3.5] and w = [1.9, 6.0] is (2.2 × 1.9) + (3.5 × 6.0) = 25.18, and so v and w are not orthogonal, but the inner product of v = [12.89, 0] and w = [0, 3.8] is 0, and in this case they are.

For orthogonal basis vectors v 1 , v 2 , . . . v n and scalars s 1 , s 2 , . . . s n , a linear combination of the v 1 . . . v n generates a new vector x of the same dimensionality, as given in Equation (3.2):

where, in multiplication of a vector by a scalar, each component of the vector is multiplied by the scalar, and in vector addition corresponding components are added. For example, take V to be based on a twofold Cartesian product A = R × R of the set of real numbers R. Select any two orthogonal vectors from V , say v 1 = [12.89, 0] and v 2 = [0, 3.8], adopting the convention that vectors are shown between square brackets and components are commaseparated. Table

Vector spaces have a geometrical interpretation. Under this interpretation, orthogonal vectors are perpendicular to one another, as in Figure

Vectors generated by linear combination of the basis vectors of an ndimensional space are conceptualized as coordinates of points in the space. Thus, for the linear combination of the basis vectors v =

Finally, note in the above examples that the non-zero components of basis vectors can be any real number. It is, however, convenient to have a standard basis for every dimensionality n = 1, 2, 3 . . . . These standard bases, called orthonormal bases, consist of vectors whose values are restricted to 0 and 1, so that the basis for n = 2 is (1, 0), (0, 1), for n = 3 (1, 0, 0), (0, 1, 0), (0, 0, 1) and so on. This restriction does not affect the capacity of the bases to generate all other n-dimensional vectors in the space.

Manifolds in vector space

Given a set A and an n-fold Cartesian product X of A, a relation is a subset of X . The subset may be random or made on the basis of some explicit criterion. In the latter case, for example, if A is the set of all people in some city, then X = A × A is the set of all possible pairings of people in the city. If one now defines a selection criterion, say 'loves', then the subset of pairs which satisfy the criterion constitute the relation: it is the set of all pairs of people one of whom loves the other.

In a vector space, a relation defined on an n-fold Cartesian product is a subset of vectors in the space. Geometrically, such a subset is a manifold

What has been said about manifolds in two-dimensional space applies straightforwardly to arbitrary dimensionality n; for n > 3 lines are referred to as hypercurves and planes and nonlinear surfaces as hyperplanes and hypersurfaces. Hyper-objects cannot be directly visualized or even conceptualized except by analogy with two and three dimensionalshapes, but as mathematical objects they are unproblematical.

Proximity in vector space

The geometrical proximity of two vectors v and w in a vector space V is determined by a combination of the size of the angle between the lines joining them to the origin of the space's basis, and by the lengths of those lines. Assume that v and w have identical lengths and are separated by an angle θ , as in Figure

The angle between two vectors v and w can be found by first finding its cosine and then translating that into the corresponding angle using standard trigonometric tables. The cosine is found using the formula given in Equation (3.3):

where:

1. θ is the unknown angle between v and w.

2. |v| and |w| are the lengths or 'norms' of v and w, that is, the lengths of the lines connecting them to the origin in the basis, as in Figure

3. The division of a vector v by its length |v| is always a unit vector, that is, a vector of length 1. 4. The dot between the two division terms designates the inner product, as described earlier. The formula for finding the cosine of the angle between two vectors is based on the observation that, if the lengths of the vectors are the same, then the sole determinant of the distance between them is the angle, as noted. The formula rescales both vectors to the same length, that is, 1, and the inner product of the rescaled vectors is the cosine of the angle between them.

To see why the inner product of length-normalized vectors should be the cosine of the angle between them, recall that the cosine of either one of the non-right angles in a right-angled triangle is defined as the ratio of the length of the side adjacent to the angle of interest to the hypotenuse, as in Figure

The vector v is rotated 30 degrees about the origin keeping its length constant; the new coordinates are [0.87, 0.50] as in Figure

Moving on to proximity measurement by distance, the distance between two vectors v and w in a vector space V can be measured in terms of a metric. Given a set X , a metric (cf.

1. d(x, y) ≥ 0, that is, the distance between any two vectors in the space is non-negative.

2. d(x, y) = 0 if and only if x = y, that is, the distance from a vector to itself is 0, and for vectors which are not identical is greater than 0. 3. d(x, y) = d(y, x), that is, distances are symmetrical. 4. d(x, y) ≤ d(x, y) + d(y, z), that is, the distance between any two vectors is always less than or equal to the distance between them and a third vector. This is the triangle inequality, shown diagrammatically in Figure

. . . A metric space M(V, d) is a vector space V on which a metric d is defined in terms of which the distance between any two points in the space can be measured. Numerous distance metrics exist

1. Linear metrics, where the distance between two points in a manifold is taken to be the length of the straight line joining the points, or some approximation to it, without reference to the shape of the manifold. 2. Nonlinear metrics, where the distance between the two points is the length of the shortest line joining them along the surface of the manifold and where this line can but need not be straight. This categorization is motivated by the earlier observation that manifolds can have shapes which range from perfectly flat to various degrees of curvature. Where the manifold is flat, as in Figure

. . . . Distance in vector space will figure prominently in the discussion from this point onwards, and as such it is discussed in some detail; see further

The most commonly used linear metric is the Minkowski, given in Equation (3.5); for others see

where 1. M is a matrix each of whose n-dimensional row vectors specifies a point in n-dimensional metric space. 2. i and j index any two row vectors in M. 3. p is a real-valued user-defined parameter in the range 1 . . . ∞. 4. |M i, j -M j,k | is the absolute difference between the coordinates of i and j in the space. 5. ∑ k=1..n |M i, j -M j,k | generalizes to n dimensions the Pythagorean theorem for the length of the hypotenuse of a right angled triangle in two dimensions, which is the shortest distance between any two 2 dimensional vectors. Three parameterizations of Minkowski distance are normally used in clustering contexts. Where p = 1 the result is the Manhattan or city-block distance in

which simplifies to Equation (3.7)

where the vertical bars | . . . | indicate absolute value, so that, for example, |2 -4| is 2 rather than -2; this captures the intuition that distances cannot be negative. If, for example, n = 2, M i =

By far the most often used parameterization of the Minkowski metric is p = 2, the Euclidean distance, shown in Equation (3.8).

This is just the Pythagorean rule known, one hopes, to all schoolchildren, that the length of the hypotenuse of a right-angled triangle is the square root of the sum of the squares of the lengths of the other two sides. Again for n = 2, M i =

which simplifies to Equation (3.10)

The parameter p can be any positive real-number value: where 1 < p < 2, the Manhattan approximation approaches the Euclidean distance, and where p > 2 the approximation moves away from the Euclidean and approaches the Chebyshev. Like the Euclidean distance, the Manhattan distance is based on the differences between any pair of row vectors M i and M j in their n dimensions, but it merely sums the absolute values of the differences without using them to calculate the shortest distance, and is thereby an approximation to the shortest distance; Chebyshev takes the distance between any pair of row vectors M i and M j to be the maximum absolute difference across all n dimensions and, like Manhattan, is therefore an approximation to linear distance.

There are many nonlinear metrics

. . Mathematically, geodesic distance is a generalization of linear to nonlinear distance measurement in a space: the geodesic distance g x,y is the shortest distance between two points x and y on a manifold measured along its possibly-curved surface

A spanning tree for G is an acyclic subgraph of G which contains all the nodes in G and some subset of the arcs of G

and so on. The graph distance table and the Euclidean one from which it was derived in this way are shown in Tables 3.6 and 3.7. As expected, the sum of distances and mean distance for the graph matrix are both substantially greater than for the Euclidean one, and the graph distance between M 1 and M 7 is three times larger than for the Euclidean, which Figure

In general, the graph approximation of geodesic distance is constrained to follow the shape of the manifold by the need to visit its nodes in the course of minimum spanning tree traversal. Intuitively, this corresponds to approximating the geodesic distance between any two cities on the surface of the Earth, say from New York to Beijing in Figure

Geometrical interpretation of data

Data are a description of objects from a domain of interest in terms of a set of variables such that each variable is assigned a value for each of the objects. We have seen that, given m objects described by n variables, a standard representation of data for computational analysis is a matrix M in which each of the m rows represents a different object, each of the n columns represents a different variable, and the value at M i j describes object i in terms of variable j, for i = 1 . . . m, j = 1 . . . n. The matrix thereby makes the link between the researcher's conceptualization of the domain in terms of the semantics of the variables s/he has chosen and the actual state of the world, and allows the resulting data to be taken as a representation of the domain based on empirical observation.

Once data are represented as a matrix M, the foregoing geometrical concepts apply directly to it. Specifically:

-The dimensionality of M, that is, the number n of columns representing the n data variables, defines an n-dimensional data space. -The sequence of n numbers comprising each row vector of M specifies the coordinates of the vector in the space, and the vector itself is a point at the specified coordinates; because the row vectors represent the objects in the research domain, each object has a specified location in the data space. -The set of all data vectors in the space constitutes a manifold; the shape of the manifold is the shape of the data. -Distance between the data vectors comprising the manifold can be measured linearly or nonlinearly. The issue of whether the data manifold is linear or nonlinear will be prominent in the discussion to follow because it reflects a corresponding distinction in the characteristics of the natural process that the data describe. Linear processes have a constant proportionality between cause and effect. If kicking a ball is a linear system and kicking it x hard makes it go y distance, then a 2x kick will make it go 2y distance, a 3x kick 3y distance and so on for nx and ny. Experience tells us that reality is not like this, however: air and rolling resistance become significant factors as the ball is kicked harder and harder, so that for a 5x kick it only goes, say, 4.9y, for 6x 5.7y, and again so on until it bursts and goes hardly any distance at all. This is nonlinear behaviour: the breakdown of strict proportionality between cause and effect. Such nonlinear effects pervade the natural world, giving rise to a wide variety of complex and often unexpected, including chaotic, behaviours

Conceptualizing data as a manifold in n-dimensional space is fundamental to the discussion of clustering that follows for two main reasons. On the one hand, it becomes possible to visualize the degrees of similarity of data vectors, that is, the rows of a data matrix, as clusters in a geometrical space, thereby greatly enhancing intuitive understanding of structure in data. And, on the other, the degrees of similarity among data vectors can be quantified in terms of relative distance between them, and this quantification is the basis for most of the clustering methods presented later on.

Data transformation

Once a data matrix has been constructed, it can be transformed in a variety of ways prior to cluster analysis. In some cases such transformation is desirable in that it enhances the quality of the data and thereby of the analysis. In others the transformation is not only desirable but necessary to mitigate or eliminate characteristics in the matrix that would compromise the quality of the analysis or even render it valueless. The present section describes various types of transformation and the motivations for using them.

Variable scaling

The variables selected for a research project involving cluster analysis may require measurement on different scales. This is not an issue with respect to MDECTEbecause all its variables measure phonetic segment frequency and are thus on the same scale, but it is not difficult to think of cases in corpusbased linguistics where it can be. In sociolinguistics, for example, speakers might be described by a set of variables one of which represents the frequency of occurrence of some phonetic segment in interviews, another one speaker age, and a third income. Because these variables represent different kinds of thing in the world, they are measured in numerical units and ranges appropriate to them: phonetic frequency in the integer range, say, 1..1000, age in the integer range 20..100, and income in some currency in the real-valued range 0..50000. Humans understand that one can't compare apples and oranges and, faced with different scales, use the variable semantics to interpret their values sensibly. But cluster analysis methods don't have common sense. Given an m × n data matrix M in which the m rows represent the m objects to be clustered, the n columns represent the n variables, and the entry at M i j (for i = 1 . . . m, j = 1 . . . n) represents a numerical measure of object i in terms of variable j, a clustering method has no idea what the values in the matrix mean and calculates the degrees of similarity between the row vectors purely on the basis of the relative numerical magnitudes of the variable values, as we shall see. As a consequence, variables whose scales permit relatively larger magnitudes can have a greater influence on the cluster analysis than those whose scales restrict them to relatively small values, and this can compromise the reliability of the analysis, as has often been noted -cf., for example,

Table

In Table

The trees in Table

That the result of cluster analysis should be contingent on the vagaries of scale selection is self-evidently unsatisfactory both in the present case and also more generally in research applications where variables are measured on different scales. Some way of eliminating scale as a factor in such applications is required; a way of doing this follows.

Relative to a data matrix M, a solution to the above problem is to standardize the variables by transforming the values in the column vectors of M in such a way that variation in scale among them is removed: if all the variables are measured on the same scale, none can dominate. The textbook method for doing this is via standard score, also known as z-score and autoscaling -cf., for example,

For the ith value in any given vector x, the z-standardization is defined as in Equation (3.11)

where -µ(x) is the mean of the values in the vector. Given a variable x whose values are represented as a vector of n numerical values distributed across some range, the mean or average of those values is the value at the centre of the distribution. The values in Table

where µ is the conventional symbol for 'mean', ∑ denotes summation, and n is the number of values in x: the mean of a set of n values is their sum divided by n. In the case of Table

Given a variable x whose values are represented as a vector of n values [x 1 , x 2 . . . x n ], variance is calculated as follows.

• The mean of the values µ is (

• The amount by which any given value x i differs from µ is then x i -µ. • The average difference from µ across all values is therefore ∑ n i=1 x i -µ/n. • This average difference of variable values from their mean almost corresponds to the definition of variance. One more step is necessary, and it is technical rather than conceptual. Because µ is an average, some of the variable values will be greater than µ and some will be less. Consequently, some of the differences (x i -µ) will be positive and some negative. When all the (x i -µ) are added up, as above, they will cancel each other out. To prevent this, the (x i -µ) are squared. • The definition of variance for n values

where δ is the standard deviation. The standard deviation of A is the square root of 594.44 = 24.38, which tells one that, on average, the marks for A vary by that amount, and by 2.83 for B, both of which are readily interpretable in terms of their respective runs of marks. The z-standardization of an arbitrary vector x is shown in Table

Application of z-standardization transforms any vector into one having a mean of 0 and a standard deviation of 1, and, because division by a constant is a linear operation, the shape of the distribution of the original values is preserved, as is shown by the pre-and post-standardization plots in Table

When z-standardization is applied to each of the column vectors of a matrix, any variation in scale across those variables disappears because all the variables are now expressed in terms of the number of standard deviations from their respective means. Tables 3.12a-c show, for example, zstandardization of the matrices in Table

Application of z-standardization appears to be a good general solution to the problem of variation in scaling among data variables, and it is in fact widely used for that purpose. It is, however, arguable that, for cluster analysis, z-standardization should be used with caution or not at all, again as others have observed

The argument against z-standardization for cluster analysis depends on making a distinction between three properties of a variable:

-The absolute magnitude of values of a variable is the numerical size of its values, and can for present purposes be taken as the absolute maximum of those values. For Frequency in Table

Coefficient of Variation

The intuition gained from direct inspection of the matrices in Table

How does this relate to the use of z-standardization of data for cluster analysis? It is a general property of every z-standardized vector, noted above, that its standard deviation is 1. Application of z-standardization to multiple columns of a matrix therefore imposes a uniform absolute magnitude of variability on them. This is shown in Table

Because the absolute magnitude of variability determines the degree of a variable's effect on clustering, the implication is that all the column vectors in a z-standardized matrix have an equal influence; we have already seen an example of this above. This obviously eliminates any possibility of dominance by variables with relatively high absolute magnitudes of variability, but there is a price, and that price might be felt to be too high in any given research application. Intuitively, real-world objects can be distinguished from one another in proportion to the degree to which they differ: identical objects cannot be distinguished, objects that differ moderately from one another are moderately easy to distinguish, and so on. Data variables used to describe real-world objects to be clustered are therefore useful in proportion to the variability in their values: a variable with no variability says that the objects are identical with respect to the characteristic it describes and can therefore contribute nothing as a clustering criterion , a variable with moderate variability says that the corresponding objects are moderately distinguishable with respect to the associated characteristic and is therefore moderately useful as a clustering criterion , and again so on. Variables v 1 and v 2 in Table

For multivariate data whose variables are measured on different scales, what is required is a standardization method that, like z-standardization, eliminates the distorting effect of disparity of variable scale on clustering but, unlike z-standardization, also preserves the relativities of size of the pre-standardization intrinsic variabilities in the post-standardization absolute magni- tudes of variability. In other words, what is required is a method that generates standardized variable vectors such that the ratios of their absolute magnitudes of variability are identical to those of the intrinsic variabilities of the unstandardized ones. In this way the standardized variables can influence the clustering in proportion to the real-world distinguishability of the objects they describe. Such a method follows.

The literature

The right-hand side of Table

Since, therefore, (i) the coefficient of variation is a scale-independent measure of variability, and (ii) the standard deviation of a mean-standardized variable is always identical to the coefficient of variation of the unstandardized variable, and (iii) the standard deviation of a variable is what measures its absolute magnitude of variability, mean-standardization fulfils the above-stated requirements for a general standardization method: that it eliminate the distorting effect of disparity of variable scale on clustering while preserving the ratios of the intrinsic variabilities of the unstandardized variables in the ratios of the absolute magnitudes of variation of the standardized ones. The absolute magnitudes of variation of mean-standardized variables are identical to the intrinsic variabilities of the unstandardized ones, and hence so are the ratios.

Figures 3.23a-3.23c compare the cluster trees for the unstandardized, zstandardized, and mean-standardized versions of the matrix in Table

Direct inspection of the unstandardized matrix in Figures 3.23a-3.23c reveals three value-groups for v 1 , four groups for v 2 , and small random variations on a constant for v 3 . The primary clustering in Figure3.23a is by v 1 because it has the highest absolute magnitude of variability and subclustering within the three primary clusters is by v 2 , with the effect of v 3 invisible, all as expected. The cluster tree for the z-standardized matrix is much more complex, and any sense of the groups observable in v 1 and v 2 is lost as the clustering algorithm takes account of the numerically much-enhanced random variation in v 3 generated by z-standardization; the tree in Figure

Most statistics, data processing, and cluster analysis textbooks say something about standardization. The z-standardization procedure is always mentioned and, when different methods are cited or proposed, there is typically little discussion of the relative merits of the alternatives, though, as noted earlier, quite a few express reservations about z-standardization. The relatively few studies that are devoted specifically to the issue are empirical, that is, they assess various methods' effectiveness in allowing clustering algorithms to recover clusters known a priori to exist in specific data sets, and their conclusions are inconsistent with one another and with the results of the present discussion.

Normalization

This section deals with a problem that arises when clustering is based on frequency data abstracted from multi-document corpora and there is substantial variation in the lengths of the documents. The discussion is in three main parts. It first shows why variation in document length can be a problem for frequency-based clustering, then goes on to describe a matrix transformation or 'normalization' designed to deal with the problem, and finally shows that such normalization is ineffective where documents are too short to provide reliable probability estimates for data variables. The 63 interviews that comprise the DECTE corpus differ substantially in length and so, consequently, do the phonetic transcriptions of them. Figure

The labels (A)-(C) in Figure

The reason for this effect is easy to see. Whatever set of linguistic features one is counting, be they phonetic, phonological, morphological, lexical, syntactic, or semantic, it is in general probable that a longer document will contain more instances of those features than a shorter one: a newspaper will, for example, contain many more instances of, say, the word 'the' than an average-length email. If frequency profiles for varying-length documents are constructed, as here for the phonetic usage of the DECTE speakers, then the profiles for the longer documents will, in general, have relatively high values and those for the shorter documents relatively low ones. The preceding discussion of scaling has already observed that clustering is strongly affected by the relative magnitudes of variable values. When, therefore, the rows of a frequency matrix are clustered, the profiles are grouped according to relative frequency magnitude, and the grouping will thus be strongly influenced by document length.

The solution to the problem of clustering in accordance with document length is to transform or 'normalize' the values in the data matrix in such a way as to mitigate or eliminate the effect of the variation. Normalization is accomplished by dividing the values in the matrix by some constant factor which reflects the terms in which the analyst wants to understand the data; a statistician, for example, might want to understand data variation in terms of standard deviation, and so divides by that. In the present case the normalization factor is document length, so that the frequency values representing any given document are divided by its length or by the mean length of all the documents in the collection to which it belongs. Such normalization is an important issue in Information Retrieval because, without it, longer documents in general have a higher probability of retrieval than shorter ones relative to any given query.

The associated literature consequently contains various proposals for how such normalization should be done -for example

Normalization by mean document length (Spärck Jones, Walker, and Robertson 2000) is used as the basis for discussion in what follows because of its intuitive simplicity. Mean document length normalization involves transformation of the row vectors of the data matrix in relation to the average length of documents in the corpus being used, and, in the present case, transformation of the row vectors of MDECTE in relation to the average length of the m = 63 DECTE phonetic transcriptions, as in Equation (3.17).

where M i is the matrix row representing the frequency profile of i'th DECTE transcription T i , length(T i ) is the total number of phonetic segments in T i , and µ is the mean number of phonetic segments across all transcriptions T in DECTE , as in Equation (3.18).

The values in each row vector M i are multiplied by the ratio of the mean number of segments per transcription across the set of transcriptions T to the number of segments in transcription T i . The longer the document the numerically smaller the ratio, and vice versa; the effect is to decrease the values in the vectors that represent long documents, and increase them in vectors that represent short ones, relative to average document length.

MDECTEwas normalized by mean document length and then cluster analyzed using the same clustering method as for Figure

The tree in Figure

Caveat emptor, however. Mean document length normalization has eliminated variation in transcription length as a factor in clustering of the DECTE speakers. There is a limit to the effectiveness of normalization, however, and it has to do with the probabilities with which the linguistic features of interest occur in the corpus. Given a population E of n events, the empirical interpretation of probability

Applying these observations to the present case, each of the constituent transcriptions of T is taken to be a sample of the population of all Tyneside speakers. The longer the transcription the more likely it is that its estimate of the population probabilities of the 156 phonetic segment types in T will be accurate, and, conversely, the shorter the transcription the less likely this will be. It is consequently possible that a very short transcription will give very inaccurate probability estimates for the segment types. The normalization procedure will then accentuate this inaccuracy, and this will in turn affect the validity of the clustering. The obvious solution to the problem of poor population probability estimation by short documents or transcriptions is to determine which documents in the collection of interest are too short to provide reasonably good estimates and to eliminate the corresponding rows from the data matrix. But how short is too short? The answer lies in statistical sampling theory; for further details see

Dimensionality reduction

The dimensionality of data is the number of variables used to describe the data objects: data describing humans in terms of height and weight are twodimensional, the weather in terms of temperature, atmospheric pressure, and wind speed are three-dimensional, and so on to any number of dimensions n.

Reducing the dimensionality of data as much as possible with as little loss of information as possible is a major issue in data analysis across a wide range of research disciplines

As will be seen, cluster analysis is based on measurement of proximity between and among data objects in n-dimensional space, and the discussion of data geometry has presented some proximity measures. For low-dimensional spaces, that is, for spaces where n = 2 and n = 3 which can be graphically represented, these measures are intuitively reasonable. In Figure

The manifold for MDECTEconsists of 63 vectors in a 156-dimensional vector space; it cannot be shown as in Figure

-For a fixed number of data vectors m and a uniform and fixed variable value scale, the manifold becomes increasingly sparse as their dimensionality n grows. To see this, assume some bivariate data in which both variables take values in the range 0..9: the number of possible vectors like (0, 9), (3, 4), and so on is 10 × 10 = 100. For trivariate data using the same range the number of possible vectors like (0, 9, 2) and (3, 4, 7) is 10× 10× 10 = 1000. In general, the number of possible vectors is r d , where r is the measurement range (here 0..9) and d the dimensionality. The r d function generates an extremely rapid increase in data space size with dimensionality: even a modest d = 8 for a 0..9 range allows for 100, 000, 000 vectors. This very rapid increase in data space size with dimensionality is widely known as the 'curse of dimensionality', discussed in, for example,

What about using more data? Let's say that 24 percent occupancy of the data space is judged to be adequate for manifold resolution. To achieve that for the 3-dimensional case one would need 240 vectors, 2400 for the 4-dimensional case, and 24,000,000 for the 8-dimensional one. This may or may not be possible. And what are the prospects for dimensionalities higher than 8?

-As dimensionality grows, the distances between pairs of vectors in the space become increasingly similar. In the relevant information retrieval and data mining literature, proximity between vectors in a space is articulated as the 'nearest neighbour' problem: given a set V of ndimensional vectors and an n-dimensional vector w not in V , find the vector v in V that w is closest to in the vector space. This is an apparently straightforward problem easily soluble by, for example, calculating the Euclidean distance between w and each of the v in V , and selecting the shortest one.

As dimensionality increases, however, this straightforward approach becomes increasingly unreliable because "under certain broad conditions . . . as dimensionality increases, the distance to the nearest neighbour approaches the distance to the farthest neighbour. In other words, the contrast in differences to different data points becomes nonexistent"

To demonstrate this, a sequence of 1000 matrices, each with 100 rows containing random values in the range 0 . . . 1, was generated such that the first matrix had dimensionality k = 1, the second had dimensionality k = 2, and so on to dimensionality k = 1000, as shown in Table

The implication for clustering is straightforward: because the most popular cluster analysis methods group vectors on the basis of their relative distances from one another in a vector space, as the distances between vectors in the space approach uniformity it becomes less and less possible to cluster them reliably.

One response to these characteristics of high-dimensional data is to use it as is and live with the consequent unreliability. The other is to attempt to mitigate their effects by reducing the data dimensionality. The remainder of this section addresses the latter alternative by presenting a range of dimensionality reduction methods.

The foregoing discussion of data creation noted that variables selected for a research project are essentially a first guess about how best to describe the domain of interest, and that the guess is not necessarily optimal. It may, therefore, be the case that the initial selection can be refined and, more specifically, that the number of variables can be reduced without losing too much relevant information. Given the importance of dimensionality reduction in data processing generally, there is an extensive literature on it and that literature proposes numerous reduction methods. The following account of these methods cannot be exhaustive. Instead, the aim is to provide an overview of the main approaches to the problem and a discussion of the methods that are most often used and / or seem to the author to be most intuitively accessible and effective for corpus linguists.

The methods for dimensionality reduction are of two broad types. One type selects a subset of the more important data variables and eliminates the remainder from the analysis, using some definition of importance. The other type abstracts a new and usually much smaller set of variables on the basis of the existing ones. In the relevant machine learning, artificial intelligence, and cognitive science literatures these approaches to dimensionality reduction are called 'feature selection' and 'feature extraction', but the present discussion has so far used the more generic term 'variable' for what these disciplines call features, and will continue to do so.

The dimensionality of data can be reduced by retaining variables which are important and eliminating those which are not, relative to some criterion of importance; for data in vector space format, this corresponds to eliminating the columns representing unimportant variables from the data matrix. The literature on variable selection is extensive -for summary accounts see

The point of cluster analysis is to group objects in a domain of interest in terms of their relative degrees of similarity based on the variables used to describe them. Intuitively, a variable is useful for this purpose if it has the following characteristics: frequency, variability, and nonrandomness. We will first briefly discuss these three characteristics in general before presenting ways of selecting variables for each.

-Frequency: In general, a variable should represent something which occurs often enough for it to make a significant contribution to understanding of the domain. For example, in the DECTE interviews the two most frequent phonetic segments occur 12454 and 8255 times respectively out of a total of 157116 segment tokens across all 63 speakers, but 13 segments occur only once. The frequent segments are prominent features which any attempt to understand the phonetics of Tyneside speech must take into account, whereas the infrequent ones tell one little about Tyneside speech and may well be just noise resulting from speaker mispronunciation or transcription error. -Variability: The values which the variable takes should vary substantially. As the discussion of variable scaling has already noted, realworld objects can be distinguished from one another in proportion to the degree to which they differ: identical objects cannot be distinguished, objects that differ moderately from one another are moderately easy to distinguish, and so on. Data variables used to describe real-world objects are therefore useful for distinguishing the objects they describe in proportion to the variability in their values: a variable with no variability says that the objects are identical with respect to the characteristic it describes and can therefore contribute nothing to distinction of objects, a variable with moderate variability days that the corresponding objects are moderately distinguishable with respect to the associated characteristic and therefore moderately useful for the purpose, and again so on. -Nonrandomness: The variation in the values which the variable takes should be nonrandom. Random variation of the aspect of the domain which the variable describes means that there is no systematic variation among objects, and all one can say on this basis is that, in this respect, the objects differ, which is obvious from the outset. A variable is, therefore, useful for clustering to the extent that the values which it takes have a nonrandom distribution of variability among objects.

The remainder of this section presents ways of selecting variables for each of these criteria, then identifies associated problems, and finally proposes a way of resolving the problems.

Frequency

An m × n frequency matrix F is constructed in which the value at F i j is the number of times variable j (for j = 1 . . . n) occurs in document i (for i = 1 . . . m). The frequency of occurrence of variable j across the whole corpus is given in

Frequencies for all the columns of F are calculated, sorted, and the less frequent variables are removed from F, thereby reducing the dimensionality of F. MDECTE is a frequency matrix, so the summation and sorting process can be applied directly; the result of doing so is shown in Figure

Variability

The degree of variability in the values of a variable is described by its variance or, expressed in the original units of measurement, its standard deviation. Given a data matrix in which the rows are the objects of interest and the Note that, where variables are measured on different scales, conclusions about their relative variabilities based on the magnitudes of their variances can be misleading. The foregoing discussion of variable scaling made a distinction between absolute and intrinsic variability, where the first is the amount of variation in values expressed in terms of the scale on which those values are measured and the second is the amount of variation expressed independently of scale. Absolute magnitude of variability is measured by standard deviation, and comparison of the standard deviations of a set of variables therefore offers a scale-dependent assessment of their variabilities. The discussion of variable scaling also showed why scale dependence can be misleading -essentially, because the magnitude of a variable's standard deviation is strongly influenced by the magnitude of its values, so that, judged by its standard deviation, a variable with a relatively lower intrinsic variability but relatively larger values can appear to have greater variability than one with relatively higher intrinsic variability but relatively smaller values. For this reason, intrinsic variability as measured by the coefficient of variation, introduced earlier, should used as the criterion for variable selection where variables are measured on different scales. All the variables in MDECTE are measured on the same scale, segment frequency, and so this problem does not arise.

Nonrandomness

Two approaches to assessing nonrandomness in the distribution of variability are considered here: Poisson distribution and term frequency -inverse document frequency.

A widely used measure of nonrandomness is the ratio of the variance of a set of values to their mean. To understand this measure it is first necessary to understand the Poisson distribution, a statistical model of randomness. This part of the discussion briefly introduces the Poisson distribution, then shows how the variance-to-mean ratio relates to it, and finally describes the application of the ratio to dimensionality reduction.

The Poisson distribution models the number of times that a random and rare event occurs in some specified spatial or temporal interval; see for example

where:

p is a probability.

x is the variable in question.

r is the number of events that occur over an interval i, and r! is r factorial.

e is the base of the natural logarithm, that is, 2.71828.

-λ is the mean value of x over many intervals i. For a Poisson process whose mean rate of occurrence of events λ over the designated interval i is known, therefore, this function gives the probability that some independently specified number r of events occurs over i. For example, assume that 7 cars pass through a rural intersection on Thursday, 3 on Friday, and 5 on Saturday; the mean number λ of cars passing through the intersection on a given day is 5. What is the probability that 4 cars will pass through on Sunday? The calculation is given in Equation (3.21)

The Poisson distribution can be used to test whether the values in a given data variable are random or not: if there is a close fit between the data and the theoretical distribution, it is probable that the data was generated by a random process.

How can degrees of adherence to the Poisson distribution be determined with respect to a set of variable values? A characteristic of the theoretical Poisson distribution is that its mean and variance are identical. Given a frequency matrix whose columns represent the variables of interest, therefore, the degree to which any column j diverges from Poisson can be determined by calculating the degree to which j's mean and variance differ. This ratio is known as the 'variance-to-mean ratio' (vmr) and is defined on a vector x by

V mr is also known as the 'index of dispersion', which indicates its use as a measure of dispersion of some set of values relative to a statistical distribu-tion. Relative to a Poisson distribution it measures degree of divergence from randomness.

The vmr can be used for dimensionality reduction as follows; a document collection D containing m documents is assumed. The production of a natural language document, and more specifically the successive occurrence of tokens of variables which constitutes the document, is taken to be a Poisson process. For each of the n variables x j describing the documents of

-The mean rate of occurrence λ j of x j in the m documents is the total number of occurrences of x j in D divided by m. -The actual number of occurrences of x j in document

-The question being asked with respect to x j is: since the documents are taken to be generated by a Poisson process, and therefore that each document d i is expected, on average, to contain λ j tokens of x j , how probable is the actual number of occurrences r i j in each of the d i ? -If the probability of x j is high across all the d i , then it fits the Poisson distribution, that is, the occurrence pattern of x j is random and it can therefore be eliminated as a variable. If, however, the probability of x j is low for one or more of the documents, then x j diverges from the distribution -in other words, x j occurs nonrandomly to a greater or lesser degree and should therefore be retained. In the ideal case the probability of x j is low for a proper subset of the documents in D and high elsewhere, indicating that its occurrence pattern is nonrandom in some documents and random in the remainder and that it is therefore a good criterion for document classification. The assumption that any natural language document or document collection is generated by a stochastic process is, of course, unjustified

The vmr values for the column vectors of MDECTEwere calculated, sorted in descending order of magnitude, and plotted as shown in Figure

Spärck

In short, word frequency on its own is not a reliable clustering criterion . The most useful words are those whose occurrences are, on the one hand, relatively frequent, and on the other are not, like 'computer', more or less randomly spread across all collection documents but rather occur in clumps such that a relatively few documents contain most or all the occurrences and the rest of the collection few or none; the word 'debug', for example, can be expected to occur frequently in documents that are primarily about computer programming and compiler design, but only infrequently if at all in those about, say, word processing. On this criterion, the usefulness of lexical types is assessed in accordance with their 'clumpiness' of occurrence across documents in a collection.

When she proposed clumpiness of distribution as a criterion, Spärck Jones also provided a method for calculating it. That method, together with some emendments to it made by

where d f j is the document frequency, that is, the number of documents belonging to D in which t j occurs. The inverse document frequency of a term, therefore, is the ratio of the total number of documents in a collection to the number of documents in which the term occurs; log 2 is not conceptually part of id f , but merely scales the m/d f j ratio to a convenient interval.

There is a problem with id f : it says that terms which occur only once in a corpus are the most important for document classification. Assuming a 1000-document collection, the id f of a term that occurs in one document is log 2 (1000/1) = 9.97, for a term that occurs in two documents log 2 (1000/2) = 8.97 and so on in a decreasing sequence. This is counterintuitive -does one really want to agree with id f that a lexical type which occurs once in a single document is a better criterion for document classification than one which occurs numerous times in a small subset of documents in the collection? It also contradicts the empirically-based and widely used principle

where t f (t j ) is the frequency of term t j across all documents in D. Using this formulation, the t fid f of some lexical type A that occurs once in a single document is 1× log 2 (1000/1) = 9.97, and the t fid f of a type B that occurs 400 times across 3 documents is 400 × log 2 (1000/3) = 3352.3, that is, B is far more useful for document differentiation than A, which is more intuitively satisfying than the alternative

The notion of clumpiness in the distribution of lexical items across document collections extends naturally to other types of variables such as the MDECTE phonetic segments. Its application to dimensionality reduction is analogous to that of the methods already presented: the columns of the data matrix are sorted in descending order of t fid f magnitude, the t fid f values are plotted, the plot is used to select a suitable threshold k, and all the columns below that threshold are eliminated. The plot for MDECTE is given in Figure

Though t fid f has been and is extensively and successfully used in Information Retrieval, it has a characteristic which compromises its utility for dimensionality reduction. Because clumpiness in t fid f relative to some Where the id f is near 0 the t fid f of v is very small, and when it is at 0that is, where v occurs in every document -the t fid f remains 0 irrespective of v's frequency. It is, however, possible that v is nonrandomly distributed across the documents even where it occurs in every document -some documents might, for example, contain only one or two tokens of v, while others might contain scores or hundreds -and t fid f cannot identify such a distribution. Use of t fid f for dimensionality reduction therefore runs the risk of eliminating distributionally-important variables on account of the definition of clumpiness on which it is based. A small change to the formulation of t fid f prevents the id f term evaluating to zero and this allows relative frequency to remain a factor, as shown in Equation (3.25).

Since (m + 1) must always be greater than d f the id f and consequently the t fid f are always greater than 0.

All the methods for dimensionality reduction presented so far, from frequency through to t fid f , suffer two general problems. The first is that selection of a threshold k below which variables are discarded is problematic. Visual intuition based on plotting indicates that, the further to the left of the plot one goes, the more important the variable. But where, exactly, should the threshold be drawn? In Figure

The second problem is incommensurateness. The selection criteria focus on different aspects of data, and as such there is no guarantee that, relative to a given data matrix, they will select identical subsets of variables. Indeed, the expectation is that they will not: a variable can have high frequency but little or no variability, and even if it does have significant variability, that variability might or might not be distributed nonrandomly across the matrix rows. This expectation is fulfilled by MDECTE, as shown in Figure

Each of the rows (a)-(d) of Figure

Given that the four variable selection criteria in the foregoing discussion can be expected to, and for MDECTE do, select different subsets of variables, which of them is to be preferred or, alternatively, how can their selections be reconciled? With respect to threshold selection, the literature appears to contain no principled resolution, and none is offered here; selection of a suitable threshold remains at the discretion of the researcher. The remainder of this discussion deals with the problem of incommensurateness.

In any given research application there might be some project-specific reason to prefer one or another of the four variable selection criteria. Failing this, there is no obvious way of choosing among them. The alternative is to attempt to reconcile them. The reconciliation proposed here attempts to identify and eliminate the variables which fail to satisfy the principles of fre-quency, variability, and nonrandomness set out at the start of the discussion. This can be done by calculating frequency, variance, vmr, and t fid f values for each column of the data matrix, sorting each set of values in descending order of magnitude, z-standardizing for comparability, and then co-plotting. For MDECTE this amounts to co-plotting the frequency, variance, vmr, and t fid f curves from The variables with high-frequency, high-variance, and high-nonrandomness values are on the left of the plot, and these values diminish smoothly as one moves to the right. If a threshold is now selected, the variables to the right of it can be discarded and those to the left retained, yielding the required dimensionality reduction; as before, threshold selection is subjective, and in Figure

It is, moreover, possible to further reduce dimensionality by refining the selection to the left of the threshold. This refinement is based on tabulation of the retained variables, as shown for MDECTE in Table

-Column 3 contains high-frequency, high-variance variables, but the variance is near-randomly distributed. -Column 4 contains variables whose values are nonrandomly distributed, but they are low-frequency and low-variance. -Column 5 contains a high-frequency variable with little variance, and such variance as it has is near-random. -Column 6 contains a low-frequency, low-variance variable whose values, on one measure, are nonrandomly distributed. -Column 7 contains quite a large number of low-frequency, low-variance variables whose values on the t fid f measure are nonrandomly distributed. The initial selection based on the threshold in Figure

Finally, the various dimensionality reduction methods described thus far are general in the sense that they are applicable to any data matrix in which the rows represent objects to be clustered and the columns the variables describing those objects. Where the variables are lexical, however, there is additional scope for dimensionality reduction via stemming and elimination of so-called stop-words. This is a substantial topic in its own right, and is not discussed here; for further information see for example

As noted, the methods for dimensionality reduction are of two broad types. One type selects a subset of the more important data variables and eliminates the remainder from the analysis, using some definition of importance. The other type abstracts a new and usually much smaller set of variables on the basis of the existing ones. The preceding discussion has dealt with the first of these; the remainder of this section moves to the second, variable extraction.

The discussion of data creation noted that, because selection of variables is at the discretion of the researcher, it is possible that the selection in any given application will be suboptimal in the sense that there is redundancy among them, that is, that they overlap with one another to greater or lesser degrees in terms of what they represent in the research domain. Where there is such redundancy, dimensionality reduction can be achieved by eliminating the repetition of information which redundancy implies, and more specifically by replacing the researcher-selected variables with a smaller number of non-redundant variables that describe the domain as well as, or almost as well as, the originals. Slightly more formally, given an n-dimensional data matrix, dimensionality reduction by variable extraction assumes that the data can be described, with tolerable loss of information, by a manifold in a vector space whose dimensionality is lower than that of the data, and proposes ways of identifying that manifold.

For example, data for a study of student performance at university might include variables like personality type, degree of motivation, score on intelligence tests, scholastic record, family background, class, ethnicity, age, and health. For some of these there is self-evident redundancy: between personality type and motivation, say, or between scholastic record and family background, where support for learning at home is reflected in performance in school. For others the redundancy is less obvious or controversial, as between class, ethnicity, and score on intelligence tests. Variable extraction methods look for evidence of such redundancy between and among variables and use it to derive new variables which give a non-redundant, reduceddimensionality representation of the domain. In the foregoing example, the researcher-defined variables personality type, motivation, scholastic record, and score on intelligence tests might be replaced by a 'general intelligence' variable based on similarity of variability among these variables in the data, and family background, class, ethnicity, age, and health with a 'social profile' one, thereby reducing data dimensionality from nine to two.

The following discussion of variable extraction first gives a precise definition of redundancy, then introduces the concept of intrinsic dimension, and finally presents some variable extraction methods.

If there is little or no redundancy in variables then there is little or no point to variable extraction. The first step must, therefore, be to determine the level of redundancy in the data of interest to see whether variable extraction is worth undertaking. The methods for doing this described below are all based on assessing the degrees of overlap between data variables in terms of the information about the domain that they represent, and they do this by measuring the similarity between and among the column vectors of the data matrix which represent the variables.

We have seen that the values in an n-dimensional vector are the coordinates of its location in n-dimensional space. The similarity of values in any two vectors in the space will consequently be reflected in the distance between them: vectors with very similar values will be close together, and to the extent that the differences in values increase they will be further apart. By calculating the distances between all unique pairings of column vectors in a data matrix, it is possible to identify degrees of similarity and therefore of redundancy between them. The Euclidean distances between all unique pairings of the 156 column vectors of MDECTE in 63-dimensional space were calculated, sorted in descending order of magnitude, and plotted in

Angle has an advantage over distance as an indicator of degree of redundancy. The magnitude of distances measured between vectors is determined by the scale on which the variables are measured, and as such it is difficult to know how to interpret a given distance in terms of degree of redundancy: does a distance of, say, 50 represent a lot of redundancy or only a little? A given distance is diagnostically useful only in relation to other distances, as in Figure

A third and frequently used way of measuring redundancy is correlation. In probability theory two events A and B are said to be independent if the occurrence of A has no effect on the probability of B occurring, or vice versa, and dependent otherwise. Given two variables x and y and an ordered sequence of n observations at times t 1 ,t 2 . . .t n for each, if the measured value for x at time t i (for i = 1 . . . n) has no predictive effect on what the measured value for y at t i will be, then those variables are independent, or, failing this condition, dependent. In statistics, variables that are dependent are said to be associated, and the degree of association is the degree to which they depart from independence. Statistics provides various measures of association, the most often used of which, Pearson's product-moment correlation coefficient, or 'Pearson's correlation coefficient' for short, is described below.

To understand Pearson's correlation coefficient, one first has to understand the concept of covariance between any two variables x and y, which is a measure of the degree to which there is a linear relationship between the values taken at successive observations in the time sequence t 1 ,t 2 . . .t n : as the observed values of x change in the sequence, do the values of y at each corresponding observation change in a constant proportion? Figure

In Figure

where where µ x and µ y are the means of x and y respectively, n is the number of observations in x and y, and the (x i -µ x )(y i -µ y ) expression is the inner product of vectors x and y adjusted by subtracting their respective means. Using this formula, the covariances of the variables in Figure

Pcorr divides x and y by their respective standard deviations, thereby transforming their values to a common scale and so eliminating the effect of scale. This is shown in Table

The Pearson coefficient captures the intuition gained from examination of the plots in Figure

Figure

Though they derive from different conceptual frameworks, that is, from vector space geometry and probability, cosine similarity and Pearson correlation are very closely related in that, with respect to determination of vector redundancy, both are based on the inner product of the vectors, both recognize that disparity of scale between the vectors is a problem, and both address this problem by normalization. The only substantive difference between them is that Pearson correlation subtracts their respective means from the vectors, with the result that the vectors are now mean-centred, that is, the sums of their values are 0; if vectors are mean-centred prior to cosine calculation, cosine similarity and Pearson correlation are equivalent. The theoretical basis for variable extraction is the concept of intrinsic dimension. We have seen that an m × n matrix defines a manifold in n-dimensional space. In such a space, it is possible to have manifolds whose shape can be described in k dimensions, where k < n. Figure

The data in Figure

Another example is a plane in three-dimensional space, shown in Figure

This plane can be redescribed in two-dimensional space, as in Figure

This account of variable extraction methods first presents the standard method, linear principal component analysis (PCA), and then goes on to survey a range of other methods.

Because real-world objects can be distinguished from one another by the degree to which they differ, the data variables used to describe those objects are useful for clustering in proportion to how well they describe that variability, as already noted. In reducing dimensionality, therefore, a reasonable strategy is to attempt to preserve variability, and that means retaining as much of the variance of the original data in the reduced-dimensionality representation as possible. Redundancy, on the other hand, is just repeated variance, and it can be eliminated from data without loss of information. PCA reduces dimensionality by eliminating the covariance while preserving most of the variance in data. Because it is the standard dimensionality reduction method, PCA is described in greater or lesser degrees of detail and clarity by most publications in the field. The standard reference works are those of

Given an n-dimensional data matrix containing some degree of redundancy, linear PCA replaces the n variables with a smaller set of k uncorrelated variables called principal components which retain most of the variance in the original variables, thereby reducing the dimensionality of the data with only a relatively small loss of information. It does this by projecting the ndimensional data reduction into a k-dimensional vector space, where k < n and closer than n to the data's intrinsic dimensionality. This is a two-step process: the first step identifies the reduced-dimensionality space, and the second projects the original data into it.

Figure

Vectors v 1 and v 2 both have a substantial degree of variance, as shown both by the standard deviation and the scatter plot, and the coefficient of determination shows that they are highly redundant in that they share 90 percent of their variance. The aim is to reduce the dimensionality of this data from 2 to 1 by eliminating the redundancy and retaining the total data variance, that is, the combined variance of v 1 and v 2 .

The first step is to centre the data on 0 by subtracting their respective means from v 1 and v 2 . This restates the data in terms of a different orthogonal basis but does not alter either the variable variances or their covariance. The mean-centred variables and corresponding plot are shown in Figure

The basis vectors are now rotated about the origin, preserving their orthogonality, so that one or the other of them -in this case the horizontal onebecomes the line of best fit to the data distribution, as shown in Figure

The line of best fit is the one that minimizes the sum of squared distances between itself and each of the data points. Two of the distances are shown by tered. Almost all the variance after rotation is in v 1 and very little in v 2 , as shown in the data table in Figure

This idea extends to any dimensionality n and always proceeds in the same three steps:

-The data manifold is mean-centred. -A new orthogonal basis for the mean-centred data is found in which the basis vectors are aligned as well as possible along the main directions of variance in the manifold. -Dimensionality is reduced by identifying and discarding the variables corresponding to the basis vectors with negligible variance. The second step in this procedure is the key, and that is what PCA offers: it finds an orthogonal basis for any given n-dimensional data matrix such that the basis vectors lie along the main directions of variance in the data manifold. These basis vectors are the principal components of the data. Given a data matrix D whose m rows represent the m objects of interest and whose n columns represent the n variables describing those objects, PCA creates two matrices which we shall call EV ECT and EVAL:

-EV ECT is an n × n matrix whose column vectors are the principal components of D and constitute an orthonormal basis for D. -EVAL is an n×n diagonal matrix, that is, one in which the only nonzero values are in the diagonal from its upper left to its lower right corner. These diagonal values are the lengths of the basis vectors in EV ECT , and represent the magnitudes of the directions of variance in the data manifold. The diagonal values in EVAL are sorted in descending order of magnitude and are synchronized with EV ECT such that, for j = 1 . . . n, EVAL j is the length of basis vector EV ECT j . Using EVAL, therefore, less important directions of variance can be identified and the corresponding basis vectors eliminated from EV ECT , leaving a k < n dimensional space into which the original data matrix D can be projected. Where such elimination is possible, the result is a dimensionality-reduced data matrix.

In the following example D is taken to be a fragment of length-normalized MDECTE small enough for convenient exposition, and is shown in Table

The covariance of each unique pair of columns DMC i and DMC j (for i, j = 1 . . . n) is calculated as described in earlier in the discussion of redundancy and stored in C, where C is an n × n matrix in which both the rows i and columns j represent the variables of DMC, and the value at C i, j is the covariance of variable column i and variable column j in DMC. The values on the main diagonal are the 'covariances' of the variables with themselves, that is, their variances. Table

This orthonormal basis is found by calculating the eigenvectors of C. Calculation of eigenvectors is a fairly complex matter and is not described here because the details are not particularly germane to the discussion. Most linear algebra textbooks provide accessible accounts; see for example

Table

The orthonormal basis is n-dimensional, just like the original data matrix. To achieve dimensionality reduction, a way has to be found of eliminating any basis vectors that lie along relatively insignificant directions of variance. The criterion used for this is the relative magnitudes of the eigenvalues associated with the eigenvectors in EVAL.

The calculation of eigenvectors associates an eigenvalue with each eigenvector, as already noted, and the magnitude of the eigenvalue is an indication of the degree of variance represented by the corresponding eigenvector. Since the eigenvalues are sorted by magnitude, all the eigenvectors whose eigenvalues are below some specified threshold can be eliminated, yielding a k dimensional orthogonal basis for C, where k < n. This is shown in Table

Once the reduced-dimensionality space has been found, the mean-centred version DMC of the original n-dimensional data matrix D is projected into the reduced k-dimensional space, yielding a new n × k data matrix D reduced that still contains most of the variability in D. This is done by multiplying DMC T on the left by the reduced-dimensionality eigenvector matrix EV ECT T reduced , where the superscript T denotes matrix transposition, that is, re-shaping of the matrix whereby the rows of the original matrix become columns and the columns rows. This multiplication is shown in Equation (3.28).

D T reduced can now be transposed again to show the result in the original format, with rows representing the data objects and columns the variables, as in Table

Most of the variance in MDECTE is captured by the first few basis vectors, and virtually all of it is captured in the first 20 or so. MDECTE can, therefore, be reduced from 156-dimensional to 20-dimensional with a small loss of information about the phonetic usage of the speakers that it represents. Or, seen another way, 20 or so variables are sufficient to describe the phonetic usage that the original 156 variables described redundantly.

Several issues arise with respect to PCA:

-selection: Probably the most important issue is selection of a dimensionality threshold below which components are eliminated. There is no known general and optimal way of determining such a threshold, and selection of one is therefore subjective. There are, however, criteria to guide the subjectivity.

• A priori criterion: The number k of dimensions to be selected is known in advance, so that the eigenvectors with the k largest eigenvalues are chosen. If, for example, one wants to represent the data graphically, only the first two or three dimensions are usable. The obvious danger here is that too few dimensions will be selected to retain sufficient informational content from the original matrix, with potentially misleading results, but this is a matter of judgement in particular applications. • Eigenvalue criterion: Only eigenvectors having an eigenvalue ≥ 1 are considered significant and retained on the grounds that significant dimensions should represent the variance of at least a single variable, and an eigenvalue < 1 drops below that threshold. • Scree test criterion: The scree test is so called by analogy with the erosion debris or scree that collects at the foot of a mountain. The eigenvalues are sorted in descending order of magnitude and plotted; the 'scree' descends from the 'mountain' at the left of the plot to the 'flat' on the right, and the further to the right one goes the less important the eigenvalues become.  The first 20 eigenvectors capture 90 percent of the variance in the original data; the first 30 capture 95 percent, and the first 61 100 percent. Even the safest option of keeping the first 61 eigenvectors would result in a very substantial reduction in dimensionality, but one might take the view that the small gain in terms of data loss over, say, 20 or 30 is not worth the added dimensionality.

-Variable interpretation: In any data matrix the variables typically have labels that are semantically significant to the researcher in the sense that they denote aspects of the research domain considered to be relevant. Because PCA defines a new set of variables, these labels are no longer applicable to the columns of the dimensionality-reduced matrix. This is why the variables in the PCA-reduced matrices in the foregoing discussion were given the semantically-neutral labels v 1 . . . v 4 ; the values for these variables are self-evidently not interpretable as the frequencies of the original data since some of them are negative. In applications where the aim is simply dimensionality reduction and semantic interpretation of the new variables is not an issue, this doesn't matter. There are, however, applications where understanding the meaning of the new variables relative to the original ones might be useful or essential, and there is some scope for this.

Singular value decomposition (SVD)

SVD

where -U , S, and V are the matrices whose product gives D.

-The column vectors of U are an orthonormal basis for the column vectors of D. -The column vectors of V are an orthonormal basis for the row vectors of D; the T superscript denotes transposition, that is, V is rearranged so that its rows become columns and its columns rows. -S is a diagonal matrix, that is, a matrix having nonzero values only on the diagonal from S 1 , 1 to S m , n, and those values in the present case are the singular values of D in descending order of magnitude. These singular values are the square roots of the eigenvectors of U and V . Because the column vectors of V are an orthonormal basis for D and the values in S are ranked by magnitude, SVD can be used for dimensionality reduction in exactly the same way as PCA. Indeed, when D is a covariance or correlation matrix, SVD and PCA are identical. SVD is more general than PCA because it can be applied to matrices of arbitrary dimensions with unre-stricted numerical values whereas PCA is restricted to square matrices containing covariances or correlations, but in practice it is a straightforward matter to calculate a covariance or correlation matrix for whatever data matrix one wants to analyze, so the choice between SVD and PCA is a matter of preference.

Factor Analysis (FA)

FA is very similar to PCA, and the two are often conflated in the literature. Relative to a redundant n-dimensional matrix D, both use eigenvector decomposition to derive a set of basis vectors from the variance / covariance matrix for D, both use the relative magnitudes of the eigenvalues associated with the eigenvectors to select a reduced number of basis vectors k < n, and the k vectors are taken to constitute a reduced-dimensionality basis into which D can be projected in order to reduce its dimensionality from n to k. They differ, however, both conceptually and, as a consequence, in how variability in data is analyzed.

PCA is a formal mathematical exercize that uses patterns of covariance in redundant data to find the main directions and magnitudes of variance, and these directions are expressed as a set of non-redundant synthetic variables in terms of which the original variables can be re-stated. These synthetic variables may or may not have a meaningful interpretation relative to the research domain that the original variables describe, but there is no explicit or implicit claim that they necessarily do; PCA is simply a means to a dimensionality reduction end. FA differs in that it does make a substantive claim about the meaningfulness of the variables it derives. Specifically, the claim is that observed data represent significant aspects of the natural process which generated them in a way that is obscured by various kinds of noise and by suboptimal selection of redundant variables, that these significant aspects are latent in the observed data, and that the factors which FA derives identifies these aspects. As such, FA offers a scientific hypothesis about the natural process to which it relates.

The basis for this claim is what distinguishes FA mathematically from PCA: in deriving components, PCA uses all the variance in a data matrix, but FA uses only a portion of it. To understand the significance of this, it is first necessary to be aware of the distinction between different kinds of variance that FA makes. Relative to a given observed data variable v i in an n-dimensional matrix D, where i is in the range 1 . . . n:

-The common variance of v i is the the variance that v i shares will all the other variables in D; this is referred to as its communality. -Specific variance of v i is the variance unique to v i . -Error variance of v i is the variance due to the noise factors associated with data. -Total variance of v i is the sum of its common, specific, and error variances. PCA analyzes total variance, but FA analyzes common variance only, on the grounds that common variance reflects the essentials of the natural process which the data describes, and that analysis of the patterns of covariance in the data calculated only on communality allows scientifically meaningful factors to be extracted.

FA has two main disadvantages relative to PCA: (i) the common variance on which FA is based is complicated to isolate whereas the total variance on which PCA is based is straightforward, and (ii) the factors which FA generates are not unique, whereas PCA generates a unique and optimal summary of the variance in a matrix. On the other hand, FA has the advantage when meaningful interpretation of the derived variables is required by the researcher. Where, therefore, the aim is simply dimensionality reduction and interpretation of the extracted variables is not crucial, PCA is the choice for its simplicity and optimality, but where meaningful interpretation of the extracted variables is important, FA is preferred.

For discussions of FA see

Multidimensional Scaling (MDS)

PCA uses variance preservation as its criterion for retaining as much of the informational content of data as possible in dimensionality reduction. MDS uses a different criterion, preservation of proximities among data objects, on the grounds that proximity is an indicator of the relative similarities of the real-world objects which the data represents, and therefore of informational content; if a low-dimensional representation of the proximities can be constructed, then the representation preserves the informational content of the original data. Given an m × m proximity matrix P derived from an m × n data matrix M, MDS finds an m × k reduced-dimensionality representation of M, where k is a user-specified parameter. MDS is not a single method but a family of variants. The present section describes the original method on which the variants are ultimately based, classical metric MDS , and a variant, metric least squares MDS .

Classical MDS requires that the proximity measure on which it is to operate be Euclidean distance. Given an m × n data matrix M, therefore, the first step is to calculate the m × m Euclidean distance matrix E for M. Thereafter, the algorithm is:

-Mean-centre E by calculating the mean value for each row E i (for i = 1 . . . m) and subtracting the mean from each value in E i . -Calculate an m × m matrix S each of whose values S i, j is the inner product of rows E i and E j , where the inner product is the sum of the product of the corresponding elements as described earlier and the T superscript denotes transposition:

-Calculate the eigenvectors and eigenvalues EV ECT and EVAL of S, as already described. -Use the eigenvalues, as in PCA, to find the number of eigenvectors k worth retaining. -Project the original data matrix M into the k-dimensional space, again as in PCA: M T reduced = EV ECT T reduced xM T . This algorithm is very reminiscent of PCA, and it can in fact be shown that classical MDS and PCA are equivalent and give identical results -cf.

Classical MDS and PCA both give exact algebraic mappings of data into a lower-dimensional representation. The implicit assumption is that the original data is -free. This is, however, not always and perhaps not even usually the case with data derived from real-world observation, and where noise is present classical MDS and PCA both include it in calculating their lowerdimensional projections. Metric least squares MDS recognizes this as a problem, and to compensate for it relaxes the definition of the mapping from higher-dimensional to lower-dimensional data as algebraically exact to approximate: it generates an m×k representation matrix M ′ of an m×n numericalvalued matrix M by finding an M ′ for which the distances between all distinct pairings of row vectors i, j in M ′ are as close as possible to the proximities p i j between corresponding row vectors of M, for i, j = 1 . . . m. The reasoning is that when the distance relations in M and M ′ are sufficiently similar, M ′ is a good reduced-dimensionality representation of M. Metric least squares MDS operates on distance measurement of proximity. This can be any variety of distance measure, but for simplicity of exposition it will here be assumed to be Euclidean.

The mapping f from M to M ′ could in principle be explicitly defined but is in practice approximated by an iterative procedure using the following algorithm:

1. Calculate the Euclidean distance matrix D(M) for all distinct pairs (i, j) of the m rows of M, so that δ i, j ∈ D(M) is the distance from row i to row j of M, for i, j = 1 . . . m. so that the distances between their new locations in the k-space more closely approximate the corresponding ones in D(M), and return to step (3). Finding M ′ is, in short, a matter of moving its row vectors around in the kspace until the distance relations between them are acceptably close to those of the corresponding vectors in M.

The stress function is based on the statistical concept of squared error. The difference or squared error e 2 between a proximity δ i, j in M and a distance d i, j in M ′ is given in Equation (3.30).

The total difference between all δ i j and d i j is therefore as in Equation (3.31).

This measure is not as useful as it could be, for two reasons. The first is that the total error is a squared quantity and not easily interpretable in terms of the original numerical scale of the proximities, and the solution is to unsquare it; the reasoning here is the same as that for taking the square root of variance to obtain a more comprehensible measure, the standard deviation. The second is that the magnitude of the error is scale-dependent, so that a small difference between proximities and distances measured on a large scale can appear greater than a large difference measured on a small scale, and the solution in this case is to make the error scale-independent; the reasoning in this case is that of the discussion of variable scaling earlier on. The reformulation of the squared error expression incorporating these changes is called stress, which is given in

This is the stress function used to measure the similarity between D(M) and D(M ′ ). By iterating steps (3) and (4) in the above MDS algorithm, the value of this stress function is gradually minimized until it reaches the defined threshold and the iteration stops. Minimization of the stress function in MDS is a particular case of what has become an important and extensive topic across a range of science and engineering disciplines, function optimization. Various optimization methods such as gradient descent are available but all are complex and presentation would serve little purpose for present concerns, so nothing further is said about them here; for details of their application in MDS see

As with other dimensionality reduction methods, a threshold dimensionality k must be determined for MDS. The indicator that k is too small is nonzero stress. If k = n, that is, the selected dimensionality is the same as the original data dimensionality, the stress will be zero. Any k less than the (unknown) intrinsic dimension will involve some increase in stress; the question is what the dimensionality should be to give an acceptable level. The only obvious answer is empirical. Starting with k = 1, MDS is applied for monotonically-increasing values of k, and the behaviour of the stress is observed: when it stops decreasing significantly with increasing k, an approximation to the intrinsic dimension of the data, and thus of optimal k, has been reached (ibid.: 4f.). Figure

The indication is that an appropriate dimensionality for MDECTE is in the range 20 . . . 30. Having generated a reduced-dimensionality representation, it is natural to ask how good a representation of the original data it is. The degree of stress is an obvious indicator, though an ambiguous one because there is no principled criterion for what stress value constitutes an acceptable threshold of closeness

For MDS see

Sammon's Mapping

Sammon's mapping is a nonlinear variant of metric least squares MDS. It differs from MDS in a single modification to the stress function shown in Table

The manifold is nonlinear, but linear measurement does not capture the geodesic distances between the points on it equally well. The distance from f to e, for example, is relatively short and the linear measure is a good approximation to the geodesic distance; from f to c and f to a it is longer and the linear measure is a less good approximation; from a to k it is less good still. The best way to capture the shape of the manifold is to add the distances a → b, b → c, and so on, and simply to disregard the remaining distances. Sammon's mapping is based on this idea. When the normalization term, that is, the distance δ i, j , has a relatively large value, the value of the stress function is relatively small, but if δ i, j is relatively small the stress function value is relatively large; because the iterative procedure underlying metric least squares MDS and Sammon minimizes the stress function, this means that the minimization is based much more on the smaller than on the larger linear distances in the data: the larger the value of the stress function the greater the adjustment to the output matrix in the MDS algorithm. In other words, in generating the reduced-dimensionality matrix Sammon's mapping concentrates on the smaller linear distances in the input matrix because these are better approximations to the shape of whatever nonlinearity there is in the data, and incrementally ignores the distances as they grow larger.

As with metric least squares MDS, the reduced dimensionality k is user specified and can be estimated by applying Sammon's mapping to the data for incrementally increasing values of k, recording the stress for each k, and then plotting the stress values to see where they stop decreasing significantly. Figure

Also as with metric least squares MDS, the goodness of the reduceddimensionality representation can be assessed by calculating the degree of correlation between the distances of the original data and the distances of the reduced-dimensionality representation, which can be stated as a correlation coefficient or visualized as a scatter plot, or both. Though still close, the correlation between distances in the original-dimensionality and reduced-dimensionality spaces is here slightly less good than for MDS relative to the same data. Formally, therefore, the MDS result is better than the Sammon one, even if only marginally. This does not, however, warrant the conclusion that the MDS dimensionality reduction is better than the Sammon. Such a conclusion assumes that the original linear Euclidean distances accurately represent the shape of the MDECTE data manifold. But, as we have seen, MDECTE contains substantial nonlinearity. The Sammon dimensionality-reduced matrix represents that nonlinearity and is for that rea-son to be preferred; the slightly less good formal indicators arise because, in taking account of the nonlinearity, Sammon loses some linear distance information.

The original paper for Sammon's mapping is

Isomap

Isomap is a variant of MDS which reduces dimensionality by operating on a nonlinear rather than on a linear distance matrix. Given a linear distance matrix D L derived from a data matrix M, Isomap derives a graph-distance approximation to a geodesic distance matrix D G from D L , and D G is then the basis for dimensionality reduction using either the classical or the metric least squares MDS procedure; graph distance approximation to geodesic distance has already been described in the foregoing discussion of data geometry. The Isomap approximation differs somewhat from the procedure already described, however, in that it uses the topological concept of neighbourhood. The present discussion gives a brief account of this concept before going on to describe Isomap and applying it to the MDECTE data.

Topology

Topology replaces the concept of metric and associated coordinate system with relative nearness of points to one another in the manifold as the mathematical structure defined on the underlying set; relative nearness of points is determined by a function which, for any given point p in the manifold, returns the set of all points within some specified proximity to p. But how, in the absence of a metric and a coordinate system, is the proximity characterized? The answer is that topological spaces are derived from metric ones and inherit from the latter the concept of neighbourhoods. In a metric space, a subset of points which from a topological point of view constitutes a manifold can itself be partitioned into subsets of a fixed size called neighbourhoods, where the neighbourhood of a point p in the manifold can be defined either as the set of all points within some fixed radius ε from p or as the k nearest neighbours of p using the existing metric and coordinates; in Figure

Topological spaces are supersets of metric spaces, so that every metric space is also a topological one. This observation is made for convenience of reference to geometrical objects in subsequent discussion: these are referred to as manifolds irrespective of whether they are embedded in a metric space or constitute a topological space without reference to a coordinate system.

Assume now the existence of an m × n data manifold M embedded in a metric space and a specification of neighbourhood size as a radius ε or as k nearest neighbours; in what follows, only the k nearest neighbour specification is used to avoid repetition. Isomap first transforms M into a topological manifold by constructing a set of k-neighbourhoods. This is done in two steps:

1. A matrix of linear distances between the data objects, that is, the rows of M, is calculated; assume that the measure is Euclidean and call the matrix D. 2. A neighbourhood matrix N based on D is calculated which shows the distance of each of the data objects M i (i = 1 . . . m) to its k nearest neighbours. This is exemplified with reference to the small randomly generated twodimensional matrix M whose scatterplot is shown with row labels in   M and D are self-explanatory in the light of the discussion so far. N is less so. Note that, apart from 0 in the main diagonal, each row of N has exactly 4 numerical values, corresponding to k = 4. The numerical value at N i, j indicates both that j is in the k-neighbourhood of i and the distance between i and j; the k-neighbourhood of N 1 , for example, includes N 2 , N 4 , N 5 , and N 8 , which can be visually confirmed by Figure

Isomap now interprets N as a graph in which data objects are nodes, the numerical values are arcs labelled with distances between pairs of nodes, and the in f values indicate no arc. In graph representation, the N of Table

Using the graph, the shortest node-to-node distance between any two points in the data manifold can be calculated using one of the standard graph traversal algorithms

It remains, finally, to consider an important problem with Isomap. The size of the neighbourhood is prespecified by the user, and this can be problematical for Isomap in two ways. If k or ε is too small the neighbourhoods do not intersect and the graph becomes disconnected; Figure

The way to deal with this problem is incrementally to increase k until the matrix of all distances G no longer contains in f entries. This potentially creates the second of the above-mentioned problems: too large a neighbourhood leads to so-called short-circuiting, where the connectivity of the neighbourhoods fails correctly to represent the manifold shape. To show what this involves, a simplified version of the Swiss roll manifold of Figure

The geodesic distance from P to R in Figure

The short-circuiting problem was identified by

Isomap was proposed by

Identification of nonlinearity

The foregoing discussion has made a distinction between dimensionality reduction methods appropriate to linear data manifolds and methods appropriate to nonlinear ones. How does one know if a manifold is linear or nonlinear, however, and therefore which class of reduction methods to apply? Where the data are low-dimensional the question can be resolved by plotting, but this is impossible for higher-dimensional data; this section describes various ways of identifying nonlinearity in the latter case.

Data abstracted from a natural process known to be linear are themselves guaranteed to be linear, but data abstracted from a known nonlinear process are not necessarily nonlinear. To see why, consider the sigmoid function used to model a range of processes such as population growth in the natural world, shown in Figure

The linearity or otherwise of data based on empirical observation of a natural process is determined by what part of the process is observed. If observation comes from the linear part (A) of the theoretical distribution in Figure

In practice, data abstracted from observation are likely to contain at least some noise, and it is consequently unlikely that strictly linear relationships between variables will be found. Instead, one is looking for degrees of deviation from linearity. Three ways of doing this are presented.

The graphical method is based on pairwise scatter-plotting of variables and subsequent visual identification of deviation from linearity. In Figure

where n is the number of variables; for n = 100, there would be 4950 different variable pairs to consider. This can be reduced by examining only a tractable subset of the more important variables in any given application, and so is not typically an insuperable problem; for what is meant by important variables and how to select them, see

-Because the aim is simply to decide whether given data are linear or nonlinear rather than to find the optimal mathematical fit, the discussion confines itself to parametric regression, where a specific mathematical model for the relationship between independent and dependent variables is proposed a priori, and does not address nonparametric regression, where the model is inferred from the data. -The discussion is based on the simplest case of one independent variable; the principles of regression extend straightforwardly to multiple independent variables. The first step in parametric regression is to select a mathematical model that relates the values of the dependent variable y to those of the independent variable x. A linear model proposes a linear relationship between x and y, that is, a straight line of the general form given in Equation (3.34).

where a and b are scalar constants representing the slope of the line and the intercept of the line with the y-axis respectively. In regression a and b are unknown and are to be determined. This is done by finding values for a and b such that the sum of squared residuals, that is, distances from the line of best fit to the dependent-variable values on the y-axis, shown as the vertical lines from the data points to the line of best fit in Figure

where the a n . . . a 0 are constants and n is the order of the polynomial; where n = 1 the polynomial is first-order, where n = 2 it as second-order and so on, though traditionally orders 1, 2, and 3 are called 'linear', 'quadratic', and 'cubic' respectively. As with linear regression, nonlinear regression finds the line best fit by calculating the coefficients a n . . . a 0 which minimize the sum of squared residuals between the line and the y values; Figure

The upper plot in Figure

Various goodness-of-fit statistics can be used to corroborate the above graphical methods. Some often-used ones are briefly outlined below; others are discussed in

-Runs test: The runs test is a quantification of the intuitions underlying residual plots. A run is a series of consecutive data points that are either all above or all below the regression line, that is, whose residuals are all positive or all negative. If the residuals are randomly distributed above and below the regression line, then one can calculate the expected number of runs: where N a of the number of points above the line and N b the number of points below, one expects to see the number of runs given by Equation (3.36).

where n is the number of data points, the y i are the actual values of the dependent variable, and the ŷi the corresponding values on the regression line. The smaller the SSE the less deviation there is in dependent variable values from the corresponding ones on the regression line, and consequently the better the fit. In Figure

where rd f is the residual degrees of freedom, defined as the number n of data points minus the number of fitted coefficients c in the regression: rd f = nc. For the n = 120 data points in Figure

SSE is defined as above, and SST is the sum of squares of y-value deviations from their mean, as in

where ȳ is the mean of the dependent variable values and y i is the ith of those values. The SSE/SST term is therefore the ratio of the variability of the dependent variable relative to the regression model and its total variability relative to its mean. The numerically smaller the ratio, and hence the larger the R 2 , the better the fit: if the model fits perfectly then there is no residual variability, SSE = 0, and R 2 = 1, but if not SSE approaches SST as the fit becomes less and less good, and R 2 approaches 0. R 2 is a widely used measure of goodness of fit, but discussions of it in the literature generally advise that it be so used only in combination with visual inspection of data plots. This caution derives from

Knowledge of the likelihood and scale of noise in the domain from which the data were abstracted can help in deciding, but this is supplemented by literature offering an extensive range of model selection methods (ibid.: Ch. 5). Two of the more frequently used methods are outlined and exemplified here, one based on statistical hypothesis testing and the other on information theory.

-Extra sum-of-squares F-test -cf.

For the data in Figure

-Akaike's Information Criterion (AIC) -cf.

p = e -0.5∆ 1 + e -0.5∆

(3.41)

where e is the entropy of the model and ∆ is the difference in AIC scores between the two models. When the two AIC scores are identical, both models have a 0.5 probability of being correct. Otherwise the difference in probabilities can serve as the basis for model selection. In the case of the data for Figure

An alternative to regression is to make the ratio of mean nonlinear to mean linear distances among points on the data manifold the basis for nonlinearity identification. This is motivated by the observation that the shape of a manifold represents the real-world interrelationship of objects described by variables, and curvature in the manifold represents the nonlinear aspect of that interrelationship. Linear metrics ignore the nonlinearity and will therefore always be smaller than nonlinear ones; a disparity between nonlinear and linear measures consequently indicates nonlinearity, and their ratio indicates the degree of disparity.

The ratio of mean geodesic to mean Euclidean distance between all pairs of nodes in a graph gives a measure of the amount of nonlinearity in a data manifold. If the manifold is linear then the two means are identical and the ratio is 1; any nonlinearity makes the mean of geodesic distances greater than the Euclidean mean, and the ratio is greater than 1 in proportion of the degree of nonlinearity. Figure

The linear manifold in Figure

An apparent problem with the graph-based approach is computational tractability. Central in theoretical computer science is the study of the intrinsic complexity of computing mathematical functions, and in particular the classification of mathematical functions according to the time and memory space resources which computation of them requires

Calculation of a minimum spanning tree scales in time as O(elog(v)), where e is the number of arcs in the graph and v is the number of nodes

The horizontal axis represents the dimensionality of the Euclidean distance matrix and the vertical axis the number of seconds required calculate the minimum spanning tree and pairwise graph distances for each 10-increment graph. Using a conventional desktop computer running Matlab at 2.6 GHz, for the 2000-row / column matrix 10.11 seconds were required and, extrapolating, 26.43 seconds are required for 3000, 63.37 seconds for 5000, 246.22 for 10000, and 1038.81 for 20000. These requirements do not seem excessive, and they could easily be reduced by using a faster computer. Eventually, of course, the shape of the curve guarantees that execution of the algorithm will become prohibitively time consuming for very large matrices. But 'large' is a relative concept: in research applications involving matrices up to, say, dimensionality 20000 computational!complexity is not a significant factor in using the proposed approach to nonlinearity identification and graph distance calculation.

A potential genuine disadvantage of the graph distance-based approach is that it does not make the distinction between model and noise that the regression-based approach makes, and treats the data matrix as a faithful representation of the domain from which the data was abstracted. Unless the data is noiseless, therefore, the graph distance-based approach includes noise, whether random or systematic, in its calculations, which may or may not be a problem in relation to the application in question.

Using the graphical and regression-based methods outlined above, no strictly or even approximately linear relationships between pairs of variables were found in MDECTE. In a relatively few cases the relationships looked random or near-random, but most showed a discernible pattern; the pair O: Using O: as the independent variable and A: as the dependent, a selection of polynomials was used to model the nonlinear relationship. These are shown in Figure

The Euclidean 63 × 63 distance matrix E was calculated for MDECTE, the minimum spanning tree for E was found, and the graph distance matrix G was derived by tree traversal, all as described in the foregoing discussion. The distances were then linearized into vectors, sorted, and co-plotted to get a graphical representation of the relationship between linear and graph distances in the two matrices. This is shown in Figure

The Introduction described cluster analysis as a family of mathematicallybased computational methods for identification and graphical display of structure in data when the data are too large either in terms of the number of variables or of the number of objects described, or both, for them to be readily interpretable by direct inspection. Chapter 2 sketched how it could be used as a tool for generation of hypotheses about the natural process from which the data were abstracted, and Chapter 3 used it to exemplify various data issues, in both cases without going into detail on how it actually works. The present chapter now provides that detail.

The discussion is in three main parts: the first part attempts to define what a cluster is, the second presents a range of clustering methods, and the third discusses cluster validation, that is, the assessment of how well a clustering result has captured the intrinsic structure of the data. As in Chapter 3, the MDECTE matrix provides the basis for exemplification, but in a dimensionality-reduced form. Specifically, dimensionality is reduced to 51 using the variable selection method which combines the frequency, variance, vmr and t fid f selection criteria; variable selection rather than extraction was used for dimensionality reduction because the original variables will be required for hypothesis generation later in the discussion.

Cluster definition

In cluster analytical terms, identification of structure in data is identification of clusters. To undertake such identification it is necessary to have a clear idea of what a cluster is, and this is provided by an innate human cognitive capability. Human perception is optimized to detect patterning in the environment

Direct perception of pattern is the intuitive basis for understanding what a cluster is and is fundamental in identifying the cluster structure of data, but it has two main limitations. One limitation is subjectivity and consequent unreliability. Apart from the obvious effect of perceptual malfunction in the observer, this subjectivity stems from the cognitive context in which a given data distribution is interpreted: the casual observer brings nothing to the observation but innate capability, whereas the researcher who compiled the data and knows what the distribution represents brings prior knowledge which potentially and perhaps inevitably affects interpretation. In Figure

The obvious way to address these limitations is by formal and unambiguous definition of what a cluster is, relative to which criteria for cluster membership can be stated and used to test perceptually-based intuition on the one hand and to identify non-visualizable clusters in higher-dimensional data spaces on the other. Textbook and tutorial discussions of cluster analysis uniformly agree, however, that it is difficult and perhaps impossible to give such a definition, and, if it is possible, that no one has thus far succeeded in formulating it. In principle, this lack deprives cluster analysis of a secure theoretical foundation. In practice, the consensus is that there are intuitions which, when implemented in clustering methods, give conceptually useful results, and it is on these intuitions and implementations that contemporary cluster analysis is built.

The fundamental intuition underlying cluster analysis is that data distributions contain clusters when the data objects can be partitioned into groups on the basis of their relative similarity such that the objects in any group are more similar to one another than they are to objects in other groups, given some definition of similarity. In terms of the geometric view of data on which the present discussion is based, the literature conceptualizes similarity in two ways: as distance among objects in the data space, and as variation in the density of objects in the space. Two quotations from a standard textbook

-"A cluster is an aggregation of points in the test space such that the distance between any two points in the cluster is less than the distance between any point in the cluster and any point not in it". -"Clusters may be described as connected regions of multi-dimensional space containing a relatively high density of points, separated from other such regions by a region containing a relatively low density of points". These distance and density views of similarity may at first sight appear to be a distinction without a difference: data points spatially close to one another are dense, and dense regions of a space contain points spatially close to one another. There is, however, a substantive difference, and it corresponds to that between the metric space and topological geometries introduced in the discussion of data in the preceding chapter. The distance conceptualization of similarity uses a metric to measure the proximities of data points relative to a set of basis vectors which define the embedding space of the data manifold, whereas the density one uses relative proximity of points on the data manifold without reference to an embedding space. As we shall see, the difference is important because clustering methods based on density are able to identify a greater range of cluster shapes than at least some of the ones based on distance in metric space.

Clustering methods

The Introduction noted that a large number clustering methods is available and that only a selection of them can be included in the present discussion. It also gave some general selection criteria: intuitive accessibility, theoretically and empirically demonstrated effectiveness, and availability of software implementations for practical application. The discussion has now reached the stage where these criteria need to be applied. Clustering methods assign the set of given data objects to disjoint groups, and the literature standardly divides them into two categories in accordance with the kind of output they generate: hierarchical and nonhierarchical. Given an m × n dimensional data matrix D, hierarchical methods regard the m row vectors of D as a single cluster C and recursively divide each cluster into two subclusters each of whose members are more similar to one another than they are to members of the other on the basis of some definition of similarity, until no further subdivision is possible: at the first step C is divided into subclusters c 1 and c 2 , at the second step c 1 is divided into two subclusters c 1.1 , c 1.2 , and c 2 into c 2.1 , c 2.2 , at the third step each of c 1.1 , c 1.2 , c 2.1 , c 2.2 is again subdivided, and so on. The succession of subdivisions can be and typically is represented as a binary tree, and this gives the hierarchical methods their name. Nonhierarchical methods partition the m row vectors of D into a set of clusters C = c 1 , c 2 ..c k such that the members of cluster c i (i = 1 . . . k) are more similar to one another than they are to any member of any other cluster, again on the basis of some definition of similarity. Both hierarchical and nonhierarchical methods partition the data; the difference is that the nonhierarchical ones give only a single partition into k clusters, where k is either pre-specified by the user or inferred from the data by the method, whereas the hierarchical ones offer a succession of possible partitions and leave it to the user to select one of them. Because these two categories offer complementary information about the cluster structure of data, examples of both are included in the discussion to follow, starting with non-hierarchical ones.

As might be expected from the foregoing comments, the literature on clustering is vast

1. The literature subcategorizes hierarchical and non-hierarchical methods in accordance with the data representation relative to which they are defined: graph-based methods treat data as a graph structure and use concepts from graph theory to define and identify clusters, distributional methods treat data as a mixture of different probability distributions and use concepts from probability theory to identify clusters by decomposing the mixture, and vector space methods treat data as manifolds in a geometric space and use concepts from linear algebra and topology. The discussion of data creation and transformation in the preceding chapter was based on vector space representation, and to provide continuity with that discussion only vector space clustering methods are included in this chapter. For information about graphbased methods see

Unfortunately there is some inconsistency of terminological usage, particularly with respect to 'classification' and 'clustering / cluster analysis'. The focus of this book is on hypothesis generation based on discovery of structure in data. As such it is interested in the second of the above types of categorization and uses the terms 'clustering' and 'cluster analysis' with respect to it throughout the discussion, avoiding 'classification' altogether to forestall confusion.

Nonhierarchical clustering methods

As noted, given m n-dimensional row vectors of a data matrix D, nonhierarchical methods partition the vectors into a clustering C consisting of a set of k clusters C = c 1 . . . c k such that the members of cluster c i (i = 1 . . . k) are more similar to one another than they are to any member of any other cluster.

The theoretical solution to finding such a partition is to define an objective function f , also called an error function or a criterion function, which measures the goodness of a partition relative to some criterion in order to evaluate each possible partition of the m vectors into k clusters relative to f , and, having done this, to select the partition for which the value of f is optimal. In practice, such exhaustive search of all possible clusterings for the optimal one rapidly becomes intractable. The rather complex combinatorial mathe-matics of this intractability are discussed in

Projection clustering

The dimensionality reduction methods described in the preceding chapter can be used for clustering by specifying a projection dimensionality of 2 or 3 and then scatter plotting the result. Figure

The following account of the SOM is in five parts: the first part describes its architecture, the second exemplifies its use for cluster analysis by applying it to the MDECTE data, the third discusses interpretation of the lowdimensional projection which it generates, the fourth surveys advantages and disadvantages of the SOM for clustering, and the fifth gives pointers to developments of the basic SOM architecture. The standard work on the SOM is

A good intuition for how the SOM works can be gained by looking at the biological brain structure it was originally intended to model: sensory input systems (Van Hulle 2000),

Figure

The mathematical model corresponding to the above physical one has three components together with operations defined on them:

-An n dimensional input vector R, for some arbitrary n, which represents the retina. -A p × q output matrix M which represents the sensory cortex, henceforth referred to as the lattice. -A p × q × n matrix C which represents the connections, where C i, j,k is the connection between the neuron at M i, j (for i = 1 . . . p, j = 1 . . . q) and the one at R k (for k = 1 . . . n). Three-dimensional matrices like C have not previously been introduced. They are simply a generalization of the familiar two-dimensional ones, and can be conceptualized as in Figure

. . .  For data clustering a SOM works as follows, assuming an m × n data matrix D is given. For each row vector D i (for i = 1 . . . m) repeat the following two steps:

1. Present D i as input to R.

2. Propagate the input along the connections C to selectively activate the cells of the lattice M; in mathematical terms this corresponds to the inner product of R with each of the connection vectors at C i, j . As described earlier, the inner product of two vectors involves multiplication of corresponding elements and summation of the products, yielding a scalar result. For two vectors

). Once all the data vectors have been processed there is a pattern of activations on the lattice M, and this pattern is the cluster analysis of the data matrix D.

Thus far, an important issue has been left latent but must now be addressed. In biological systems, evolution combined with individual experience of a structured environment determines the pattern of connectivity and cortical response characteristics which implement the mapping from highdimensional sensory inputs to a two-dimensional representation. An artificially constructed model of such a system must specify these things, but the SOM does not do this explicitly. Instead, like other artificial neural network architectures

In terms of the physical model, SOM learning tries to find a pattern of connection strength variation such that similar high-dimensional 'sensory' input signals are mapped to spatially close regions in the 'cortex', that is, so that the similarity relations among the input signals are preserved in their low-dimensional representation. In terms of the corresponding mathematical model, this can be restated as the attempt to find a set of connection vectors C such that the inner products of the c ∈ C and the set of input vectors d ∈ D generates in the output lattice M a pattern of activation that represents the neighbourhood relations of D with minimum distortion.

Given a set of input vectors D, SOM learning is a dynamic process that unfolds in discrete time steps t 1 ,t 2 . . .t p , where p is the number of time steps required to learn the desired mapping. At each time step t i , a vector d j ∈ D is selected, usually randomly, as input to the SOM, and the connection strength matrix C is modified in a way that is sensitive to the pattern of numerical values in d j . At the start of the learning process the magnitude of modifica-tions to C is typically quite large, but as the SOM learns via the modification process the magnitude decreases and ultimately approaches 0, at which point the learning process is stopped -the p'th time step, as above. The question, of course, is how exactly the input-sensitive connection strength modification works, and the answer involves looking at the learning algorithm in detail. At the start of learning, the SOM is parameterized with user-specified values:

-Dimensionality of the input vectors R. This is the same as the dimensionality of the data to be analyzed. -Dimensionality and shape of the output lattice M. In theory the output lattice can have any dimensionality and any shape, though in practice its dimensionality is usually 2 and its shape is usually rectangular or hexagonal. There is evidence that lattice shape can affect the quality of results (Ultsch and Herrmann 2005); 2-dimensional rectangular or hexagonal lattices are assumed in what follows. -Size of the output lattice M. This is the number of cells in M.

-The following further parameters are explained below: neighbourhood shape, initial neighbourhood size, neighbourhood decrement interval, initial learning rate, and learning rate decrement size and interval. In addition, the values in the connection matrix C are initialized in such a way that they are non-uniform; uniform connections would preclude the possiblity of learning. This initialization can be random or can use prior information which allows the SOM to learn more quickly

1. An input vector d k ∈ D is selected.

2. The propagation of the input signal through the connections to the lattice in the physical SOM is represented as the inner product of d k and the connection vector C i, j for each unit of the lattice. The result of each inner product is stored in the corresponding cell of the lattice matrix M i, j , as above. Once all the inner products have been calculated, because the connections strengths in C were initialized to be non-uniform, the result is a non-uniform pattern of activation across the matrix. 3. The lattice matrix M is now searched to identify the cell with the largest numerical activation value. We shall call this cell u i j , where i and j are its coordinates in the lattice. 4. The connection strengths in C are now updated. This is the crucial step, since it is how the SOM learns the connections required to carry out the desired mapping. This update proceeds in two steps:

(a) Update of the connections linking the most highly activated cell u i j in M to the input vector d k . This is done by changing the connection vector associated with u i j according to Equation (4.2):

where t denotes the current time step.

-C i, j,k (t + 1) is the new value of the kth element of the connection vector. -C i, j,k (t) is the current value of the kth element of the connection vector. l(t) is the learning rate, a parameter which controls the size of the modification to the connection vector. More is said about this below. -(d k -C i j ) is the difference between the current input vector and the connection vector. This difference is the sum of element-wise subtraction:

where k is the length of the input and connection vectors. In essence, therefore, the update to the connection vector C i j associated with the most active cell u i j is the current value of C i j plus some proportion of the difference between C i j and the input vector, as determined by the learning rate parameter. The effect of this is to make the connection vector increasingly similar to the input vector. Figure 4.5 gives an example for some d k and associated C i j : Before update the input and connection vectors differ significantly; to make this clearer the difference between vectors is shown as differences in grey-scale components in Figure

Summarizing, the SOM's representation of high dimensional data in a low-dimensional space is a two-step process. The SOM is first trained using the vectors comprising the given data. Once training is complete all the data vectors are input once again in succession, this time without training. The aim now is not to learn but to generate the two-dimensional representation of the data on the lattice. Each successive input vector activates the unit in the lattice with which training has associated it together with neighbouring units, though to an incrementally diminishing degree; when all the vectors have been input, there is a pattern of activations on the lattice, and the lattice is the representation of the input manifold in two-dimensional space.

A SOM with a 11 × 11 output lattice and random initialization of the connections was used to cluster MDECTE, and the results are shows in

Which of these two, if either, is the preferred interpretation of the layout of vector labels on the lattice? Without further information there is no way to decide: the lattice gives no clear indication where the cluster boundaries might be. The eye picks out likely concentrations, but when asked to decide whether a given data item is or is not part of a visually-identified cluster, one is often at a loss. An apparent solution is to fall back on knowledge of the subject domain as a guide. But using a priori knowledge of the data to disambiguate the lattice is not a genuine solution, for two reasons. Firstly, it misses the point of the exercize: the lattice is supposed to reveal the structure of the data, not the other way around. And, secondly, the structure of large, complex, real-world data sets to which the SOM is applied as an analytical tool is not usually recoverable from mere inspection -if it were, there would be little point to SOM analysis in the first place. To be useful as an analytical tool, the SOM's representation of data structure has to be unambiguously interpretable on its own merits, and the problem is that an activation lattice like that in the above figures does not contain enough information to permit this in the general case. The problem lies in the subjectivity of visual interpretation with which this chapter began: humans want to see pattern, but the pattern any individual sees is determined by a range of personal factors and, among researchers, by degrees of knowledge of the domain which the lattice represents. Some objective interpretative criterion is required, and this is what the following section provides.

Actually, the problem is even worse than it looks. When a SOM is used for cluster analysis, inspection of the pattern of activation on the lattice can not only be subjective but can also be based on a misleading assumption. There is a strong temptation to interpret the pattern spatially, that is, to interpret any groups of adjacent, highly activated units as clusters, and the distance between and among clusters on the lattice as proportional to the relative distances among data items in the high-dimensional input space, as with, for example, MDS . That temptation needs to be resisted. The SOM differs from these other methods in that the latter try to preserve relative distance relations among objects on the data manifold, whereas the SOM tries to preserve the manifold topology, that is, the neighbourhood relations of points on the manifold -cf.

We have seen that each lattice cell has an associated vector which represents its connections to the input vector. Since the dimensionality of the connection vectors is the same as that of the inputs, and the dimensionality of the inputs is that of whatever n-dimensional input space is currently of interest, the connection vectors are, in fact, coordinates of points in the ndimensional space. Assume that there is a data manifold in the input space and that the connection vectors have been randomly initialized. In this initial state, there is no systematic relationship between the points specified by the set of connection vectors and the surface of the manifold. By incrementally bringing the connection vectors closer to training vectors taken from the data manifold, a systematic relationship is established in the sense that the connection vectors come to specify points on the manifold; at the end of training, the connection vectors map each of the points on the manifold specified by the training vectors to a particular lattice cell. Moreover, it can and usually does happen that data vectors which are close together on the manifold activate the same unit u i j , as described earlier. In this case, the connection vector for u i j has to be brought closer not only to one but to some number k of input vectors. Since these k vectors are close but not identical, the SOM algorithm adjusts the connection vector of u i j so that it becomes a kind of average vector that specifies a point on the manifold which is intermediate between the k input vectors. In this way, u i j becomes associated not only with a single point on the data manifold, but with an area of the manifold surface containing the k vectors that map to it. This is shown in Figure

1. The process of finding an 'average' vector, or 'centroid', which is intermediate between k vectors was described in Chapter 2 is known as vector quantization: for a set V of k vectors, find a 'reference' vector v r of the same dimension as those in V such that the absolute difference d = |v rv i | between each v i ∈ V and v r is minimized. The SOM training algorithm quantizes the input vectors, and the connection vectors are the result -cf.

2. The partition of a manifold surface into areas surrounding reference vectors is a tesselation. TheSOM algorithm implements a particular type, the Voronoi tesselation , in which a reference vector is the centroid, and the area of k associated vectors surrounding the centroid is a neighbourhood; the neighbourhood of a given reference vector v r in a Voronoi tesselation is defined as the set of vectors closer to v r than to any other reference vector (ibid.: 59f.). 3. The set of neighbourhoods defined by the Voronoi tesselation is the manifold's topology, as discussed in the preceding chapter. And because, finally, the SOM algorithm adjusts the connection vector not only of the most-activated unit u i j but also of units in a neighbourhood of gradually diminishing radius, it ensures that adjacent manifold neighbourhoods map to adjacent lattice units.

How does all this relate to cluster interpretation of a SOM lattice? As noted, a Voronoi tesselation is an instance of a topology, that is, a manifold and a discrete collection of subsets of points on the manifold called neighbourhoods. When it is said that a SOM preserves the topology of the input space, what is meant is that it represents the neighbourhood structure of a manifold: when data is input to a trained SOM, the vectors in a given Voronoi neighbourhood are mapped to the same lattice cell, and the vectors in adjoining Voronoi neighbourhoods are mapped to adjacent lattice cells. The result of this topology preservation is that all vectors close to one another in the input space in the sense that they are in the same or adjoining neighbourhoods will be close on the SOM output lattice.

The problem, though, is this: just because active cells are close together on the SOM lattice does not necessarily mean that the vectors which map to them are topologically close in the input space. This apparently-paradoxical situation arises for two reasons -see discussion in, for example, Ritter, Martinetz, and Schulten (1992: Ch. 4).

1. The topology of the output manifold to which the SOM maps the input one must be fixed in advance. In the vast majority of applications the SOM output topology is a two-dimensional plane, that is, a linear manifold, with rectangular or hexagonal neighbourhoods which are uniform across the lattice except for at the edges, where they are necessarily truncated. There is no guarantee that the intrinsic dimensionality of the input manifold is as low as two, and therefore no guarantee that the output topology will be able to represent the input manifold well. In theory, the SOM is not limited to two-dimensional linear topology, and various developments of it, cited later, propose other ones, but where the standard one is used some degree of distortion in the lattice's representation must be expected -cf.

How can a SOM lattice be interpreted so as to differentiate cells which are spatially close because they are topologically adjacent in the input space and therefore form a cluster, and cells which are spatially close but topologically more or less distant in the input space? The answer is that it cannot be done reliably by visual inspection alone; interpretation of a SOM lattice by visual inspection is doubly unreliable -a subjective interpretation of an ambiguous data representation. This is a well known problem with SOMs

The U -matrix representation of SOM output uses relative distance between reference vectors to find cluster boundaries. Specifically, given an m × n output lattice M, the Euclidean distances between the reference vector associated with each lattice cell M i j (for i = 1..m, j = 1 . . . n) and the reference vectors of the immediately adjacent cells M i-1, j , M i+1, j , M i, j-1 , and M i, j+1 are calculated and summed, and the result for each is stored in a new matrix U i j having the same dimensions as M. If the set of cells immediately adjacent to M i j is designated as M ad jacent(i, j) , and d represents Euclidean distance, then, according to Equation (4.3)

U is now plotted using a colour coding scheme to represent the relative magnitudes of the values in U i, j . Any significant cluster boundaries will be visible. Why? The reference vectors are the coordinates of the centroids of the Voronoi tesselation of the data manifold and thus represent the manifold's topology, as we have seen. Where the sum of distances between the reference vector associated with M i j and those associated with M ad jacent(i j) is small, the distance between those centroids on the manifold is small; conversely, a large sum indicates a large distance between centroids on the manifold. Low-magnitude regions in U thus represent topologically close regions on the manifold, and high-magnitude ones topologically distant regions on the manifold. Assuming a grayscale colour coding scheme, therefore, clusters appear on the lattice as regions containing dark gray cells, and boundaries between clusters as regions containing light gray or white ones, or vice versa. Consider, for example, the U -matrix representation of the SOM lattice for the trivial data in Table

A SOM with an 11 × 11 lattice was trained on these data, with the result that the four row vectors in Table

The SOM has three major advantages for cluster analysis. The first and most important is that it takes account of data nonlinearity in its projection: because the Voronoi tesselation follows the possibly-curved surface of the data manifold and the neighbourhood relations of the tesselation are projected onto the output lattice, the SOM captures and represents any nonlinearities present in the structure of the manifold. The second is that, unlike most other clustering methods, the SOM requires no prior assumptions about the number and shapes of clusters: clusters on the SOM lattice represent relative densities of points, that is, of clusters in the input manifold, whatever their number and shape -see:

-The default lattice structure, the linear two-dimensional rectangular grid, restricts its ability to represent data manifolds having nonlinear shapes and intrinsic dimensionalities higher than 2 without topological distortion; the foregoing discussion of lattice interpretation shows the consequence of this for SOM-based cluster analysis. -There is no theoretical framework within which initial values for the fairly numerous SOM parameters can be chosen. Empirical results across a large number of applications have shown that, when parameter values which are 'sensible' in relation to existing rules of thumb are selected, the choice is not crucial in the sense that the SOM usually converges on identical or similar results for different initializations. This is not invariably the case, however, and different combinations, in particular different initializations of the Voronoi centroids and differences in the order of presentation of the training vectors can and do generate different clusterings -cf.

-Lattice structure: The foregoing discussion has noted that the default SOM lattice structure, the linear two-dimensional rectangular grid, restricts its ability to represent data manifolds of higher intrinsic dimensionality and nonlinear shape directly without topological distortion. One type of development addresses this restriction by proposing more flexible lattice structures which can better represent the input topology -cf.

Clustering results from the GTM and the SOM are typically very similar, which is unsurprising given their close similarity. What the GTM offers, however, is on the one hand an understanding of results in terms of a well developed probability theory, and on the other an objective measure for assessing the goodness of those results. Finally, a fairly recent development of projection clustering must be mentioned: subspace clustering. The foregoing discussion of dimensionality reduction has described linear and nonlinear ways of reducing data of observed dimensionality n to an approximation of its intrinsic dimensionality k, where k is less than n. This assumes that all the data objects are best described using the same number k of latent variables, which is not necessarily the case. Subspace clustering groups variables in accordance with the optimal number of latent variables required to describe them, or, put another way, of the i dimensional subspace (for i = 1 . . . n) of the original n-dimensional data space in which they are embedded. This approach to clustering has in recent years found extensive application in areas like computer vision, motion segmentation, and image processing, and there is now a substantial literature devoted to it. Recent surveys are available in

Proximity-based clustering

Nonhierarchical proximity-based approaches treat clustering as a mathematical optimization problem, where only a small subset of all possible partitions is examined in the hope of finding the optimal one. An initial k-cluster partition is defined and an iterative procedure is used in which, at each step, individual data points are moved from cluster to cluster to form a new partition and the result is evaluated in relation to the objective function f : if the value of f shows an improvement over the preceding one the new partition is retained, and if not it is discarded and another one is tried. Such iterative procedures are widely used and are known as gradient descent or gradient ascent procedures depending on whether optimality of the objective function is defined by a minimum or maximum value. Ideally, the procedure will gradually converge on a partition for which no change leads to an improvement in the value of f , at which point the partition is taken to be optimal. This assumption does not always hold, however, because gradient procedures can and often do converge on local maxima or minima, that is, where further iteration produces no improvement in the value of f but the true maximum or minimum has not been reached. Figure

Since it was first proposed in the mid-1960s

k-means is based on the idea that, for a given set of data objects O, each cluster is represented by a prototype object, and a cluster is defined as the subset of objects in O which are more similar to, or in distance terms closer to, the prototype than they are to the prototype of any other cluster. An objective function is used find a set of clusters each of which optimally meets this criterion. For a data set O comprising m n-dimensional data points, O is partitioned into k prototype-centred clusters by the following iterative procedure:

1. Initialize the procedure by selecting k n-dimensional prototype locations in the data space; these can in principle be anywhere in the space, so that they might correspond to data points but need not. The prototypes are the initial estimate of where the clusters are centred in the space, and their locations are refined in subsequent steps. Placement of initial prototypes and selection of a value for k, that is, of the number of required clusters, is problematical, and is further discussed below. 2. Assign each of the m data points to whichever of the k prototypes it is closest to in the space using a suitable proximity measure. This yields k clusters. 3. Calculate the centroid of each of the k clusters resulting from

where x is a data point, C i is the i'th of k clusters, and p i is the prototype of the i'th cluster. This expression says that the SSE is the sum, for all k clusters, of the Euclidean distances between the cluster prototypes and the data points associated with each prototype. For k-means to have optimized this function, the prototypes have to be placed in the data space so that the Euclidean distances between them and their associated data points is globally minimized across all clusters. It is easy to see that this is what k-means does: the procedure converges on stable cluster centroids, and a centroid is by definition the minimum distance from the all the points on which it is based. Use of k-means is not restricted to Euclidean distance, though this is the most frequently used measure. A variety of different measures an associated objective functions can be used. For example, cosine similarity might be more appropriate for some kinds of data, and in that case a different objective function shown in Equation (4.5), Total Cohesion, can be used instead of SSE:

Cosine similarity is an attractive alternative to Euclidean distance when the data has not been normalized, as described earlier, because by basing proximity measurement solely on the angles between pairs of vectors the magnitudes of vector values (or, equivalently, vector lengths) are eliminated as a factor in clustering. The implication of using cosine similarity, however, is that vector length doesn't matter. There are undoubtedly applications where it does not, in which case cosine proximity is the obvious alternative to data normalization, but there are also applications where it does. With respect to MDECTE, for example, use of cosine proximity implies that all phonetic segments, from very frequent to very infrequent, are equally important in distinguishing speakers from one another, but as the foregoing discussion of data has argued, a variable should in principle represent something which occurs often enough for it to make a significant contribution to understanding of the research domain. The frequent segments in the DECTE interviews are prominent features which any attempt to understand the phonetics of Tyneside speech must consider, whereas the very infrequent ones tell one little about Tyneside speech and may well be just noise resulting from speaker mispronunciation or transcription error. Cosine proximity measurement eliminates the distinction, and is therefore unsuitable in this application. This observation applies equally to the use of cosine proximity measurement with other clustering methods as an alternative to measures which take vector magnitude into account.

Relative to the selection criteria for inclusion in this discussion, k-means is a prime candidate: it is intuitively accessible in that the algorithm is easy to understand and its results are easy to interpret, it is theoretically well founded in linear algebra, its effectiveness has repeatedly been empirically demonstrated, and computational implementations of it are widely available. In addition, -Its computational time complexity grows with data space size as O(nkdt), where n is the number of data vectors, k is the number of clusters, d is the data dimensionality, and t is the number of iterations. This means that k-means essentially grows linearly with data size, unlike other clustering methods to be considered in what follows, and is therefore suitable for clustering very large data sets in reasonable time -cf.

The procedure of k-means also has several well known problems, however.

-Initialization. k-means requires two user-supplied parameter values: the number of clusters k and the locations of the k initial centroids c 1 . . . c k in the data space. These values crucially affect the clustering result. On the one hand, if the value chosen for k is incompatible with the number of clusters in the data, then the result is guaranteed to mislead because k-means will deliver k clusters whatever the actual number of clusters intrinsic to the data, including none. For example, Figure

Various ways of selecting an initialization compatible with the intrinsic cluster structure of the data exist. The obvious one is to base the initialization on reliable a priori knowledge of the domain from which the data comes, where available. Failing this, a projection clustering method can be used to visualize the data and thereby to gain some insight into its cluster structure, or one of the range of initialization heuristics proposed in the literature can be applied

The PCA and MDS two-dimensional visualizations of MDECTE derived earlier in Figure

-Initialization. No general and reliable method for selecting initial parameter values for the number of clusters and placement of prototypes in known, and given the crucial role that these play in determining the k-means result, it is unsurprising that initialization remains a research focus. Several approaches to the problem have already been mentioned.

For initial prototype placement one of these approaches was to use of various parameter value selection heuristics. The simplest of these heuristics is random selection; others use a variety of criteria derived from the data.

To work well in practice, however, Isodata requires optimization of no fewer than six threshold parameters, and the extra complication might not be thought worthwhile relative to simply finding an optimal value of k empirically, as described above. -Convergence. As noted, the standard k-means procedure does not guarantee convergence to a global optimum. Stochastic optimization techniques like simulated annealing, genetic algorithms, and neural networks

Density-based clustering

Standard k-means fails to identify non-linearly-separable clusters because it partitions the data into clusters without reference to its density structure. The obvious solution is to take account of density, and this is what density-based clustering methods do; for general discussions see:

Dbscan is based on a topological view of data manifolds, which was introduced in the discussion of Isomap in the preceding chapter. On this view, a data manifold is defined not in terms of the positions of its constituent points in an n-dimensional space relative to the n basis vectors, but rather as a set of adjacent, locally-Euclidean neighbourhoods. The essential idea is that clusters are collections of sufficiently dense adjacent neighbourhoods, and that neighbourhoods which are insufficiently dense are noise, given some definition of 'sufficiently dense'. This is shown in Figure

To implement this idea Dbscan requires predefinition of two parameters: the radius r, called E ps, which defines the size of the neighbourhood, and the threshold number of points for sufficient density, called MinPts. Relative to these parameters, three types of point are distinguished:

-Core points, whose neighbourhood contains MinPts or more points.

-Border points, whose neighbourhood contains fewer than MinPts but which are themselves in the neighbourhood of one or more core points. -Noise points, which are all points that are not either core or border points. Assuming m data points, the Dbscan algorithm is as follows:

1. Visit each data point m i , i = 1 . . . m, labelling each as a core, border, or noise point in accordance with the above definitions 2. Eliminate all the noise points.

3. Link all pairs of core points within a radius E ps of one another. 4. Abstract the clusters, where a cluster is the set of all linked core points. 5. Assign the border points to the clusters. If a border point is in the neighbourhood of only one core point, assign it to the cluster to which the core point belongs. If it is in more than one neighbourhood, assign it arbitrarily to one of them. Like k-means, Dbscan was selected for inclusion because it is a easy to understand and interpret, is mathematically well founded, has an established user base, and is readily available in software implementations. It has important advantages over k-means, however. One is that Dbscan does not require and in fact does not permit prespecification of the number of clusters, but rather infers it from the data; selection of k is one of the main drawbacks of k-means, as we have seen. Another is that Dbscan can find non-linearlyseparable clusters, which extends its range of applicability beyond that of kmeans. The k-means procedure was, for example, able to identify the linearly separable clusters in Figures 4.18a  Dbscan's computational time complexity is somewhat more demanding than that of k-means, though still reasonably moderate in ranging from O(mlogm) to a worst-case O(m 2 ), where m is the number of data points

-Selection of parameter values. As with k-means, selection of suitable parameter values strongly affects the ability of Dbscan to identify the intrinsic data cluster structure. Given some value for MinPts, increasing the size of E ps will allow an increasing number of points to become core points and thereby include noise points in the clusters, as in Fig-

Core

The problem is that if E ps is small enough to keep the lower cluster separate from the upper one then it is too small to allow the upper one to be identified as a single cluster, and if E ps is large enough for the upper one to be identified as a single cluster then it is too large to keep the upper cluster separate from the lower one. These problems are related. Various combinations of E ps with different MinPts additional to those shown in Figure

Because Dbscan can identify a superset of data shapes identifiable by kmeans, it is tempting simply to dispense with k-means and to use Dbscan as the default analytical method. The foregoing discussion of problems with Dbscan and its application to MDECTE show, however, that this would be illadvised. Where it is known or strongly suspected that the data density structure is linearly separable, the more reliable k-means method should be used, and if the data is non-linearly separable then results from Dbscan should, in view of its initialization and sparsity problems, be corroborated using some other clustering method or methods.

That variation in data density is a problem for Dbscan was quickly recognized, and proposals for addressing it have appeared, including G Gdbscan

A fairly obvious approach to identification of density in data is to cover the data space with a grid and count the number of data objects in each cell, as shown in Figure

For general discussions of grid-based clustering see: Jain and Dubes (ibid.: Ch. 3.3.5),

Kernel-based clustering is based on concepts from statistical density function estimation, the aim of which is to find a mathematical function that generates some given data distribution

Hierarchical clustering

Given an m × n data matrix D which represents m objects in n-dimensional space, hierarchical analysis does not partition the m objects into k discrete subsets like the clustering methods described so far. Instead, it constructs a constituency tree which represents the distance relations among the m objects in the space and leaves it to the user to infer a partition from the tree. Hierarchical clustering is very widely used, and so is covered in most accounts of cluster analysis, multivariate analysis, and related disciplines like data mining. A selection of discussions is

The method

Construction of a hierarchical cluster tree is a two-step process: the first step abstracts a proximity table from the data matrix to be analyzed, and the second constructs the tree by successive transformations of the table. An intuition for how tree construction proceeds is best gained by working through an example; the example presented in what follows is based on MDECTE, and more specifically on a subset of MDECTE small enough to render illustrative tables and figures tractable for graphical representation. The concept of proximity among data objects has already been described in the foregoing discussion of data geometry. Euclidean distance is used to exemplify its application to construction of a proximity table for the first 6 of the full 63 rows of MDECTE. The Euclidean distances between all possible pairings of these 6 rows were calculated and stored in a 6 × 6 matrix D, shown in Table

Initially, each row vector of the data matrix is taken to be a cluster on its own; clusters here and henceforth are shown in brackets. Table

(1,3) 0 (

Table

The matrix in Table

1. Rows and columns ((((1,3),6),4) and (

Table

Variants

For a matrix with m rows there will at any step in the above tree-building sequence be a set of p clusters, for p in the range 2 . . . m, available for joining, and two of these must be selected. At the first step in the clustering sequence, where all the clusters contain a single object, this is unproblematical: simply choose the two clusters with the smallest distance between them. At subsequent steps in the sequence, however, some criterion for judging relative proximity between composite and singleton cluster pairs or between composite pairs is required, and it is not obvious what the criterion should be. The one exemplified in the foregoing sequence is such a criterion , known as Single Linkage, but there are various others

For simplicity of exposition, it is assumed that a stage in the tree building sequence has been reached where there are p = 3 clusters remaining to be joined. This is shown in Figure

-The Single Linkage criterion defines the degree of closeness between any pair of clusters (X ,Y ) as the smallest distance between any of the data points in X and any of the data points in Y : if there are x vectors in X and y vectors in Y, then, for i = 1 . . . x, j = 1 . . . y, the Single Linkage distance between X and Y is defined as in Equation (4.6).

SingleLinkageDistance(X

where dist(X i ,Y j ) is the distance between the i'th vector in X and the j'th vector in Y stated in terms of whatever metric is being used, such as Euclidean distance. The Single Linkage distances between all unique pairs of the p vectors remaining to be clustered are calculated, and the pair with the smallest distance is joined. This is exemplified for the three clusters of The arrowed lines in Figure

where dist(X i ,Y j ) is the distance between the ith vector in X and the jth vector in Y stated in terms of whatever metric is being used, such as Euclidean distance. The Complete Linkage distances between all unique pairs of the p vectors remaining to be clustered are calculated, and the pair for which the Complete Linkage distance is smallest is joined. This is exemplified for the three clusters of The arrowed lines in Figure

where dist is defined as above. The centroid distances between all unique pairs of the p vectors remaining to be clustered are calculated,and the pair for which the distance is smallest is joined.

The centroid distances for all unique pairings of the p clusters are calculated using the proximity matrix, and the pair for which centroiddistance(A, B) is smallest is joined. This is exemplified for the three clusters of The arrowed lines in 4.32a represent distances between centroids in cluster pairs (A, B), (A,C), and (B,C), which are shown as crosses; the one between A and B is shortest, so these two clusters are joined, as in Figure

where dist is defined as previously; note that distances of objects to themselves are not counted in this calculation, and neither are symmetric ones on the grounds that the distance from, say X i to Y j is the same as the distance from Y j to X i . -Increase in Sum-of-Squares Linkage (Ward's Method) defines the degree of closeness between any pair of clusters (X ,Y ) in terms of minimization of variability using an objective function. To describe it, two measures need to be defined. The error sum of squares (ESS) is the sum of squared deviations of the vectors in A from their centroid. If A contains m vectors, then ESS is defined by Equation (4.10):

The total error sum of squares (TESS) of a set of p clusters is the sum of the error sum of squares of the p clusters. At each step in the treebuilding sequence, the ESS for each of the p clusters available for joining at that step is calculated. For each unique combination of cluster pairs the increase in TESS is observed, and the pair which results in the smallest increase in TESS is joined. Finally, hierarchical variants are standardly divided into agglomerative vs. divisive methods. Agglomerative tree construction was exemplified above: it begins by partitioning the set of data objects so that each member of the set is a cluster on its own, and then builds the tree incrementally by joining pairs of clusters at each step until no more pairs remain and the tree is complete. Because it incrementally builds trees of increasing complexity from simpler components, agglomerative clustering is also called bottom-up clustering. Divisive tree construction begins with a single cluster consisting of all the data objects, and builds the tree incrementally by partitioning that cluster into subtrees at each step until each cluster contains a single data object and no more division is possible, at which point the tree is complete; because it incrementally subdivides a set into subsets, divisive clustering is also known as topdown clustering. Divisive clustering, described in detail by

Though the above tree-building sequence makes it obvious, it nevertheless seems worth making explicit the distinction between the tree generated by a hierarchical analysis and its graphical representation: the tree is the table of joining distances (c) in Table

Because a cluster tree represents constituency only, the sequential ordering of constituents has no interpretative significance. Given the tree in Table 4.9d, for example, any pair of constituents can be rotated about its axis, thereby reversing the sequencing, without affecting its constituency structure, as in Figure

Note also that dendrograms are more often shown sideways, than in the 'icicle' or downward-facing format more familiar to linguists from phrase structure trees. This is a purely practical matter: an icicle format rapidly broadens out as the number of data objects grows, making it impossible to display on a page.

Issues

The main and considerable advantage of hierarchical clustering is that it provides an exhaustive and intuitively accessible description of the proximity relations among data objects, and thereby provides more information that a simple partitioning of the data generated by the non-hierarchical methods covered thus far. It has also been extensively and successfully used in numerous applications, and is widely available in software implementations. There are, however, several associated problems.

-How many clusters? Given that a hierarchical cluster tree provides an exhaustive description of the proximity relations among data objects, how many clusters do the data 'really' contain? As already noted, it is up to the user to decide. Looking at a dendrogram like the one in Figure

In Figure

One way of constraining tree selection is to observe that there is a fundamental difference between the Single Linkage criterion and the oth-ers listed above: the latter are distance-based clustering methods, and the former is a density-based one. Complete Linkage, Average Linkage, Centroid Linkage, and Ward's Method all build clusters on the basis of linear distance between data points and cluster centres relative to the coordinates of the metric space in which the data is embedded.

The result is a tree each level of which is a Voronoi partition of the data space: starting at the root, the first level divides the space into two partitions each of which contains the data points closer to their centre than they are to the centre of the other partition, the second level divides the space into four such partitions, and so on. This partitioning is, moreover, done without reference to the density structure of the data. Single Linkage, on the other hand, builds clusters solely on the basis of local neighbourhood proximity and without reference to cluster centres; it is to the other kinds of hierarchical clustering, therefore, as Dbscanis to k-means. As such, the expectation is that the non-Single Linkage group will, like k-means, correctly identify the cluster structure of data when its dense regions are linearly separable but not otherwise, whereas Single Linkage will be able to identify non-linearly separable clusters.  This difference between Single Linkage clustering and the others underlies the commonly-made observation in the literature that the non-Single Linkage criteria have a strong predisposition to find roughly spherical clusters in data even where clusters of that shape are known not to be present or indeed where the data are known not to have any meaningful cluster structure at all; see for example

Where this is not known, selection of the best cluster tree, that is, the one which best captures the intrinsic cluster structure of the data, must be guided by the cluster validation methods discussed later in this chapter. -Outliers and noise. All the joining criteria are affected by outliers and noise to different degrees and for different reasons; see for example

Developments

Hierarchical methods are not parameterized, and so initialization is not a problem, but cluster shape, data size, noise, and outliers are.

-Cluster shape. As noted, all the hierarchical methods except Single Linkage are predisposed to find linearly separable clusters in data even where the data do not actually contain that structure or indeed any cluster structure at all. The obvious solution is to use Single Linkage instead, but as we have seen this has a predisposition to generate uninformative chained structures when the data contains noise. Ideally, one would want a hierarchical method which is not limited to linearly separable structures on the one hand and not compromised by chaining on the other. CURE

-Linear separability. The non-single linkage hierarchical methods are limited to data with a linearly-separable density structure because the Minkowski distance measures that the literature associates with them are linear. These methods neither know nor care how the values in the distance matrices on the basis of which they construct cluster trees were derived, and there is no obstacle in principle to using values generated by a nonlinear metric like the geodesic one described in the discussion of data geometry. This allows non-linearly separable regions to be separated nonlinearly and removes the linear separability limitation on non-single linkage hierarchical methods. Compare, for example, the Average Linkage trees for Euclidean and geodesic distance measures in Figure

As usual, however, there is a caveat. The foregoing discussion of nonlinearity detection pointed out a potential disadvantage of the graph approximation to geodesic distance measurement: that it does not make the distinction between model and noise which the regression-based approach makes, and treats the data matrix as a faithful representation of the domain from which the data was abstracted. Because the graph distance-based approach includes noise, whether random or systematic, in its calculations, this may or may not be a problem in relation to the application in question.

Cluster validation

The fundamental purpose of a cluster analysis method is to identify structure that might be present in data, that is, any non-regular or non-random distribution of points in the n-dimensional space of the data. It is, however, a commonplace of the cluster analysis literature that no currently available method is guaranteed to provide this with respect to data in general, and the foregoing discussion of a selection of methods confirms this: projection methods based on dimensionality reduction can lose too much information to be reliable, and the linear ones together with k-means and linear hierarchical methods fail to take account of any nonlinearity in the data; the reliability of the SOM, k-means, and Dbscan depends on correct parameterization; different hierarchical joining criteria can assign data points to different clusters and typically impose different constituency structures on the clusters. In addition, some methods impose characteristic cluster distributions on data even when the data are known not to contain such distributions or indeed to have any cluster structure at all.  Except for the limiting case where each point is regarded as a cluster on its own, the distribution in Figure

Given these sources of unreliability, validation of cluster analytical results is required. One obvious and often-used approach to validation is to generate a series of results using methods based on different clustering criteria in the hope that they will mutually support one another and converge on a consistent solution: if a range of methods based on dimensionality reduction, topology preservation, proximity, and density give identical or at least compatible results, the intuition is that the reliability of the solution is supported by consensus. It would, however, be useful to supplement such a consensus with one or more alternative validating criteria. And, of course, there might not be a consensus, in which case a selection must be made, which implies selection criteria. This section presents a range of quantitative ones.

The discussion is in two parts. The first part considers ways of determining the degree to which any given data has a cluster structure prior to application of clustering methods, known in the literature as 'clustering tendency'. The motivation here is the observation that, if data is known to contain little or no cluster structure, then there is no point to attempting to analyze it, and, if an analysis is carried out despite this, then the result must be an artefact of the method. The second part then presents a range of validation criteria for results from application of different analytical methods to data known to contain cluster structure.

Clustering tendency

Figure

Graphical tests for clustering tendency

Where data are two or three-dimensional they can be scatter-plotted directly, and visual interpretation of the plot will reveal the presence or absence of structure. It is usual in the literature to express distrust in the subjectivity of graphical interpretation, but this subjectivity is a matter of degree. It would be unreasonable to deny that Figure

Where the data is high-dimensional it can be reduced to dimensionality 2 or 3 for scatter-plotting, but, depending on the intrinsic dimensionality of the data, this might lose too much information to provide a reliable result. Visual Assessment of cluster Tendency (VAT)

The very distinct clusters of Figure

Statistical tests for clustering tendency

Statistical identification of clustering tendency in data is based on testing the null hypothesis that the data are random and therefore have no cluster structure. This testing is done by comparing the data to what they would look like if they were random relative to some criterion. If the criterion indicates that the data deviate sufficiently from randomness, then the null hypothesis is falsified and the alternative hypothesis, that the data have cluster structure, is adopted with some specified degree of confidence. Otherwise, the null hypothesis stands and the conclusion is that the data very probably contain no cluster structure.

There are several ways of defining randomness in data

Relative to some m x n matrix M, where m is the number of data objects and n the dimensionality, the Hopkins statistic determines whether the distribution of the m objects in the n-dimensional space deviates significantly from a random distribution of objects in that space. It does this on the basis of two measurements. The first measurement randomly selects k < m row vectors of M, and for each of these k vectors the distance to one or more of its nearest neighbours in M is calculated using some distance metric; the k nearest-neighbour distances are then summed and designated p. The second measurement generates k vectors randomly distributed in the data space, and the nearest-neighbour distances between each of these k vectors to the row vectors of M are calculated and summed, and designated q. The Hopkins statistic H is then defined by Equation (4.11):

H is calculated multiple times and summed, each time using a new random selection of k data vectors and a new set of k random vectors, and the mean of the sum is taken to be an indication of how far the distribution is from randomness. The reasoning here is as follows. If the data distribution is random then p and q will, on average, be the same, and the above formula gives 0.5; this is the baseline value for random data. If the data distribution is not random then p will on average be smaller than q because the nearestneighbour distances between clustered points in a data space are on average smaller than between randomly-distributed ones. In such a case the value of the (p + q) term is smaller than when p = q, which gives increasingly larger H-values as p approaches 0 up to a maximum H = 1.

The mean value of H was calculated for both the distributions of Figure

For general discussions of clustering tendency see

Validation

We have seen that clusters can have a variety of shapes, and that clustering methods do not cope equally well with identifying different varieties. Because each method has different criteria for identifying clusters, each has a predisposition to find certain kinds of structure, and this predisposition can prevent it from identifying other kinds of structure or cause it to impose the structure it is designed to find on data

One possible response to this is that it doesn't matter when, as here, cluster analysis is used as a tool for hypothesis generation, because the aim is to stimulate ideas which lead to hypotheses about the research domain from which the clustered data was abstracted, and a poor hypothesis based on a poor analysis can be falsified and eliminated in subsequent hypothesis testing. There is no obvious objection to this in principle, but in practice it is inefficient. A more efficient alternative is to base hypothesizing on clustering results in whose accuracy one can have a reasonable degree of confidence. Cluster analysts have increasingly taken the latter view and have developed a general methodology for providing that confidence, whereby a given data set is multiply analyzed using different clustering methods and different parameterizations of each method, and the result which best represents its intrinsic cluster structure is selected. The problem, of course, is knowing which is best; this section outlines the current state of cluster validation strategies and methods designed for that purpose.

A traditional and commonly-used criterion for cluster validation is domain knowledge, whereby a subject expert selects the analysis which seems most reasonable in terms of what s/he knows about the research area. This has some validity, but it is also highly subjective and runs the risk of reinforcing preconceptions and discounting the unexpected and potentially productive insights which are the prime motivation for use of cluster analysis in hypothesis generation, as already noted. Domain knowledge needs to be supported by objective criteria for what counts as 'best'; the remainder of this section outlines such criteria.

The foregoing account of clustering tendency has already dealt with part of the problem of cluster validation: that the given data are random, and that any clustering result is therefore an artefact of the analytical method used. The discussion to follow assumes that the result to be validated is based on data known to be sufficiently non-random to be meaningfully clustered, and that the aim of validation is to determine how close it has come to identifying the non-random structure, that is, to finding the correct number of clusters on the one hand, and to assigning the correct data objects to each of the clusters on the other. The literature contains numerous methods for doing this. Decades ago

Note that the methods described in what follows apply only to so-called 'crisp' clustering, where each data object is assigned to one and only one cluster, and not to fuzzy clustering, where a data object may belong to one or more cluster; fuzzy clustering

Cluster validation with respect to cluster compactness and separation

The validation methods in this category assess a given clustering in terms of how well it adheres to the proximity-based cluster definition with which this chapter began: a cluster is an aggregation of points in the test space such that the distance between any two points in the cluster is less than the distance between any point in the cluster and any point not in it. They are therefore applicable only when the underlying data density structure is linearly separable. The two criteria most often used are compactness and separation -cf., for example,

-Compactness. Compactness is the degree to which the members of a proposed cluster are similar to one another. The Root Mean Square Standard Deviation (RMSSTD) validity index -cf.

The RMSSTD is then the square root of the mean SS, as in Equation (4.13):

RMSST D = SS kn (4.13) Figure

The essence of the measure remains the same across these specifics, however.

The smaller the distance between clusters and the larger the size of the largest cluster, the less compact and well-separated the clusters are and the smaller the value of Dunn(C). Conversely, the larger the distance between clusters and the smaller the size of the largest cluster the more compact and well separated they are, and the larger the value of Dunn(C). Using average Euclidean distance between and within clusters to measure both cluster separation and compactness, the Dunn index value for the compact and well-separated clusters of Figure

Another traditional index that combines cluster compactness and separation is that proposed by

where δ i and δ j are the average distances of all vectors in clusters c i and c j from their respective cluster centroids, and distance is the distance between the centroids of c i and c j . The δ terms measure the compactness of two clusters in terms of the degree of dispersion of their members, and the Davies-Bouldin (DB) index is therefore the average cluster dispersion to cluster distance ratio. The greater the dispersion and the smaller the distance the less compact and well-separated the clusters, and the larger the value of DB(C).

And, conversely, the smaller the dispersion and the larger the distance the more compact and better-separated the clusters, and the smaller the index value. Applied to the data underlying the plots in Figure

Methods similar to Dunn and Davies-Bouldin are reviewed in

The cophenetic correlation coefficient (

Table

Once the cophenetic distance matrix is constructed it can be compared to the Euclidean distance matrix to see how similar they are and thereby the degree to which the clustering from which the cophenetic matrix was derived preserves the distance relations among the data objects as represented by the Euclidean distance matrix. This can be done in various ways, the simplest of which is to row-wise linearize the two matrices and then to calculate their Pearson correlation coefficient, which gives the cophenetic correlation coefficient; the linearization is shown in Table

The cophenetic correlation coefficient can be used to select the best from a set of hierarchical analyses of the same data.  As expected, the tree structures in Figure

The cophenetic correlation coefficient is a measure of the strength of the linear relationship between distance and cophenetic matrices, but though it is widely used its validity for assessing the relative goodness of hierarchical clusteringresults has been disputed.

Cluster validation of topology preservation

Preservation of manifold topology differs from preservation of linear distances among the data points which constitute it, as explained earlier, and as such the SOM or any other topology-preserving clustering method cannot be assessed by validation indices designed for distance-preserving methods. This section outlines two alternative indices for the SOM.

The foregoing discussion of the SOM noted that, where the intrinsic dimensionality of data is greater than the dimensionality of the output lattice, some distortion of the topology of the input manifold is to be expected. The discussion also noted that different selections of initial parameter values such as the locations of the Voronoi centroids, lattice size and shape, and different sequencings of training data items can generate different cluster results, and this calls the status of the result as a reliable representation of the intrinsic data structure into question. One approach to SOM assessment is to attempt to identify an objective function which the SOM optimizes and, using that function, to select the optimal result in any specific application, as with MDS, Sammon's Mapping, and Isomap. It has been shown that a general objective function applicable across all SOM parameterizations does not exist

Two alternative criteria that have been developed are quantization error and topology preservation error; for these and others see De Bodt,

-Quantization error. As noted in the discussion of the SOM, the process of finding a centroid vector which is intermediate between the k vectors of a Voronoi partition in the course of SOM training is known as vector quantization: for a set V of k vectors, find a 'reference' vector v r of the same dimension as those in V such that the absolute difference d = |v rv i | between each v i ∈ V and v r is minimized. The SOM training algorithm quantizes the input vectors, and the reference vectors are the result

The one proposed in

where u is a function that returns 0 if the best matching unit and second best matching unit are adjacent in the lattice and 1 otherwise. Selecting the best from a set of SOM analyses generated using different parameter values is a matter of choosing the one that minimizes both the quantization and topology preservation errors. This is exemplified in Fig-

Stability assessment

Stability assessment successively applies a given clustering scheme, that is, some combination of clustering method and parameterization , to a given data matrix and to multiple versions of that matrix which have been perturbed in some way, and observes the behaviour of the cluster result with respect to the original matrix and to each of the perturbed ones. If the cluster result does not change it is stable, and if it does then the result is unstable to proportion to the degree of change. The relative stability of the result is on statistical grounds taken to indicate the extent to which the cluster result captures the intrinsic cluster structure of the data (Ben-Hur, Elisseeff, and Guyon 2002; There are various ways of perturbing data, but

In most practical applications all one has is a single sample S 0 and repeated sampling of the population from which it came is not feasible, but the k-sample sequence can be simulated using bootstrapping, which works as follows. Assume that the data matrix D 0 abstracted from S 0 contains m rows and n columns. Then D 1 is an m x n matrix each of whose rows is randomly selected from D 0 ; repeated selection of the same row is allowed, so D 1 may be but probably is not identical to D 0 . Each successive D 2 . . . D k in constructed in the same way. This looks at first glance like what

Once the D i have been generated, the original data matrix D 0 is clustered to give C 0 , and each of the bootstrap data matrices D 1 , D 2 . . . D k is clustered to give C 1 ,C 2 . . .C k . The similarity of each of the C i to C 0 is then calculated to give an index value I(C i ,C 0 ), the k index values are summed, and the mean of the sum is taken to be a measure of the stability of C: the larger the mean, the greater the similarity of the bootstrapped clusterings to C 0 , and therefore the greater the stability of C 0 across multiple bootstrapped samples. There are many ways of measuring the similarity between clusters

where ∩ is the set-theoretic intersection and ∪ the union of A and B. In the present application, the Jaccard coefficient is used to compare two clusters, A from C 0 and B from C i , and is the ratio of the number of objects which are both in A and B to the number of objects in either A or B or both. For two clusterings C 0 and C i , each cluster in C 0 is paired with the corresponding cluster in C i and the Jaccard index is calculated for each pair; the index values are summed, and the mean of the sum is the Jaccard index for C 0 and C i . The stability index can be used just like the other types of validation index already discussed to look for an optimal clustering scheme by applying the above procedure to each such scheme and then selecting the scheme that generates the most stable clustering result.

Validation was until fairly recently the Cinderella of cluster analysis. Some early work on the topic has been cited; surveys of it are available in

What emerges from these discussions and others like them is that, at present, cluster validation methods are far from foolproof. Much like the clustering methods they are intended to validate, they have biases which arise on account of the assumptions they make about the nature of clusters and about the shapes of the manifolds which clustering purports to describe

The preceding chapters have -identified a research domain: the speech community of Tyneside in north-east England; -asked a research question about the domain: is there systematic phonetic variation in the Tyneside speech community, and, if so, does that variation correlate systematically with social variables? -abstracted phonetic frequency data from the DECTE sample of speakers and represented them as a matrix MDECTE; -normalized MDECTE to compensate for variation in interview length and reduced its dimensionality; -described and exemplified application of a selection of cluster analytical methods to the transformed MDECTE data. The present chapter develops a hypothesis in answer to the research question based on cluster analysis of MDECTE. The discussion is in two main parts: the first part reviews and extends the clustering results presented so far, and the second uses these extended results to formulate the hypothesis.

Cluster analysis of MDECTE

Apart from Dbscan, where variation in data density was a problem, all the cluster analyses in the preceding chapter agreed that there are two highly distinctive clusters in MDECTE: a larger one containing speakers g01 to g56, and a smaller one containing speakers n57 to n63. These are not exhaustively repeated here; the MDS result in Figure

-   The SOM topology preservation indices given in the validation section of the previous chapter indicate that the 25 × 25 lattice given in Figure

For PCA the first two dimensions represented in Figure

Nonhierarchical partitional methods

Application of Dbscan to MDECTE56 required experimentation with numerous combinations of MinPts and E ps parameters to determine the optimal ranges of both, where optimality was judged as lying between combinations which designated all data points as noise points on the one hand, and those which included all data points in a single cluster. The number of clusters is not prespecified for Dbscan but is inferred from the density structure of the data, and for no combination of parameters was anything other than a twocluster structure found. The optimal range for MinPts was found to be 2 . . . 5, and for E ps 59 . . . 61; the partition given for MinPts = 4 and E ps = 60 in Table

The relationship between the k-means and Dbscan results is straightforward:

-For k = 2 Dbscan cluster 1 is a subset of k-means cluster 1, and Dbscan cluster 2 is a subset of k-means cluster 2, with the exception of g04 which is in different clusters in Dbscan and k-means. -For k = 3 the result is the same as for k = 2 in that none of the k-means cluster 2 data points are in either Dbscan cluster.  The initial visual impression is that these trees are all very different except for the Euclidean and geodesic Single Linkage ones, which are identical, but closer examination of them reveals regularities common to them all: among others, for example, the sets of data points (4 14 19 24 27 31 46 48 50 51) and

Hierarchical partitional methods

Comparison of results

Having applied the different clustering method categories to MDECTE56 individually, the next step is to correlate the results in order to determine the degree to which they are mutually supporting with respect to the cluster structure of the data.

Comparison of projection and nonhierarchical partition results

Comparison of the k-means k = 2 and k = 3 results with the projection ones are dealt with separately, beginning with the former; because the Dbscan clustering is a subset of the k-means k = 2 one, the k-means k = 2 comparison subsumes it and there is consequently no need to deal with it separately.  show the k-means partition for k = 2 on each of the projections, using the '*' symbol to represent data points belonging to k-means cluster 1, and '+' for k-means cluster 2.       In each case k-means for k = 2 partitions the projections into two regions, which are demarcated by dashed lines for convenience of reference. This is unsurprising because, as we have seen, k-means clusters linearly separable regions of the data space, and the regions of the projections in Figures 5.11-5.15 can be regarded as linearly separable when the distortions introduced by the projection methods are taken into account.

As can be seen, k-means k = 3 differs from k = 2 only in dividing the k = 2 cluster 2 into two subclusters. Shown in relation to the PCA projection in Figure

Comparison of projection and nonhierarchical partition with hierarchical clustering results

In this section the hierarchical analyses are correlated with the Dbscan and k-means partitions of MDECTE56. Figures 5.17    Using the additional information that the correlation with the Dbscan and k-means provides, the relationships among the various trees becomes clearer.

-The Euclidean and geodesic distance based Single Linkage trees are identical, and differ from all the other trees in their characteristic chained structure. That chaining keeps all the data points corresponding to those in Dbscan cluster 1 and all the points corresponding to those in Dbscan cluster 2 together, and the rest of the structure is an apparently random mix of remaining points from the k-means clusters.

Single Linkage has, in other words, identified the same clusters as Dbscan, which is unsurprising because, as noted earlier, Single Linkage is a density-based clustering method unlike the other varieties of hierarchical analysis, which are distance-based. -Complete, Average, and Ward Linkage trees based on Euclidean distance are very similar though not identical, and correspond quite closely to the k-means partition for k = 3. In each case there are two main clusters corresponding to k-means clusters 1 and 3 which are, respectively, supersets of Dbscan clusters 1 and 2. There is also a small number of data points clustered separately from the main ones. These latter points are observable as outliers or at the periphery of the main data cloud in the projection plots, and though the selection of points differs somewhat from tree to tree, they correspond essentially to k-means cluster 2; the distributions are shown in Table

Hypothesis formulation

The first part of the research question, Is there systematic phonetic variation in the Tyneside speech community?, can now be answered affirmatively on the basis of the foregoing results. The DECTE speakers fall into two main clusters, a larger one containing speakers g01 . . . g56 and a smaller one containing speakers n57 . . . n63. The g01 . . . g56 cluster itself has a subcluster structure, for which the foregoing analyses have revealed two alternatives. All the projection and nonhierarchical partition results as well as all the hierarchical results apart from Average Linkage based on geodesic distance agree that there are two main subclusters, but that these do not include all the speakers:

1. The Single Linkage tree and Dbscan partition a minority of the data points into two clusters and regard the rest of the data as noise. 2. The Euclidean distance based Complete, Average, and Ward Linkage trees group a small number of speakers separately from the two main clusters in slightly different ways; these speakers are observable at the periphery of the main data cloud in the projection plots or as outliers to it, and correspond substantially to the smallest of the clusters in the k-means result for k = 3. 3. The geodesic distance based Complete and Ward Linkage trees are partially compatible with (2) in partitioning most of the speakers into two main clusters and the remaining speakers into a small one, but differ from (

It remains to consider the second part of the question: Does that variation correlate systematically with associated social variables?. To answer it, the cluster results are supplemented with social data associated with the speakers in the DECTE corpus.

The unanimous partition of the 63 speakers into two subclusters g01 . . . g56 and n57 . . . n63 corresponds to speakers from Gateshead on the south bank of the river Tyne, betokened by the 'g' prefix, and those from Newcastle on the north bank betokened by the 'n' prefix. DECTE does not include any social data for the Newcastle speakers, though surviving members of the original Tyneside Linguistic Survey team have confirmed that the n57 . . . n63 speakers were in fact the academics who comprised the team. The Gateshead speakers, on the other hand, were with a few exceptions working class with the minimum legal level of education and in manual skilled and unskilled employment. The primary clustering of the DECTE speakers therefore has a clear sociolinguistic interpretation based on educational level and employment type.

For the Gateshead subcluster the two alternatives identified above are available for interpretation. Since the aim here is methodological, that is, to exemplify the application of cluster analysis to hypothesis generation, only the first of them is addressed, though clearly the second would also have to be considered if the aim were an exhaustive investigation of the Geordie dialect as represented by DECTE . For clarity of presentation, the hierarchical result with the best cophenetic index, the Average Linkage tree based on Euclidean distance, is used as the basis for discussion.

Initially the full range of social variables provided by DECTE was correlated with the Average Linkage tree, and variables with no discernible systematic correlation with the tree structure were eliminated, The surviving variables are shown in Figure

The two main clusters, labelled C and D in Figure

The second part of the research question can now also be answered affirmatively on the above evidence. The primary distinction in phonetic usage in DECTE separates a small group of highly-educated, middle-class Newcastle professionals from a much larger Gateshead group whose members were predominantly speakers in manual employment and with a minimal or sometimes slightly higher level of education. The indication is that the primary differentiating factor among the Gateshead speakers is gender, though the existence of cluster B suggests that educational level and type of employment are also factors.

The hypothesis developed thus far can be augmented by identification of the phonetic segments which are chiefly responsible for differentiating the DECTE speakers into clusters. This is done using cluster centroids, as introduced in Chapter 2. The centroid of any given cluster represents the average usage of its constituent speakers; variable-by-variable comparison of any two centroids reveals the degree to which the clusters differ, on average, for every variable, and allows those with the largest differences to be identified. Centroids for successive pairs of clusters were calculated and compared using bar plots to represent the comparisons graphically. Figure

There is systematic phonetic variation in the Tyneside speech community, and this variation correlates in a sociolinguistically significant way with educational level, employment type, and gender. The phonetic segments shown in As already noted, a full rather than the present expository study of DECTE would have to go into greater detail, investigating such matters as the variation in the cluster structuring and allocation of speakers to clusters found in the foregoing discussion, the possible relevance of additional social factors, and centroid analysis of additional subcluster differences. The next step is to test this hypothesis with respect to data drawn from speakers not included in DECTE .

In principle, this discussion of hypothesis generation based on cluster analysis of DECTE should stop here, but it happens that existing work on the phonetics of Tyneside English provides results based on samples apart from the TLS component of DECTE

Variation in the GOAT vowel is a prominent feature of Tyneside English. Basing their results on the somewhat more recent PVC component of DECTE ,

-PVC [o:] / DECTE [O:] is the unmarked variant preferred by all speakers apart from working-class males in the PVC corpus. -PVC [8:] / DECTE [A:] is the almost exclusive preserve of working class males, and is described by

All of these are consistent with the relevant column pairs in Figures 5.22 These confirmations indicate that cluster analysis has generated an empirically supportable hypothesis in the present case, and suggest that it can do so both for other phonetic segments in DECTE and in corpus linguistic applications more generally. This chapter reviews the use of cluster analysis in corpus linguistics to date. As such, the following review sets a tractable limit on the literature it surveys, and that limit is the literature specific to corpus linguistics. The Introduction defined corpus linguistics for the purposes of the present discussion as a methodology in the service of the science of language. This implic-itly excludes a range of language technologies such as information retrieval, document classification , data mining, and speech processing, as well as areas of artificial intelligence like natural language generation / understanding and machine learning. These technologies work with natural language text and speech and thereby have much in common methodologically with corpus linguistics, including application to cluster analysis to text corpora; indeed, many of the concepts and techniques presented in the foregoing chapters come from their literatures. Their aims are, however, not language science but language engineering, and they therefore fall outside the self-imposed remit of this review. Also excluded on these grounds are quantitative stylometry and author attribution, whose aims are literary rather than linguistic.

The review is in two main parts: the first part outlines work on quantitative methods in corpus linguistics to serve as a context for the second, which deals with the cluster analytic work specifically.

Context

The hypothesis generation methodology described in the foregoing chapters is intended as a contribution to corpus linguistics, whose remit the Introduction described as development of methodologies for creating collections of natural language speech and text, abstracting data from them, and analysing those data with the aim of generating or testing hypotheses about the structure of language and its use in the world. This is a not-uncontroversial position. The literature draws a distinction between corpus-driven and corpusbased linguistics, where the former is taken to be a scientific paradigm in the sense that behaviourist and generative linguistics are paradigms, that is, ontologies in terms of which linguistic theories can be stated, and the latter to be a methodology, that is, a collection of techniques for the formulation and testing of hypotheses within some already-existing paradigm. The present discussion does not engage with this debate, and simply adopts the corpus-based view of corpus linguistics; for recent discussions see

Despite its relatively well-developed state, corpus linguistic methodology has historically been unevenly distributed across the linguistics research community as a whole. This is especially the case for application of quantitative methods: simple quantifications such as frequencies, means, and percentages together with uni-and bivariate graphs are fairly extensively represented in the literature, but application of anything more complex than that is much rarer.

In grammatical linguistics, here understood as the study of the architecture of human language, statistical analysis of data derived from empirical observation was part of the standard methodology in the heyday of behaviourism in the first half of the twentieth century. Linguists at this time studied ways of inferring grammatical structure from linguistic performance using distributional information extracted from natural language corpora. A major figure in this approach to linguistics was Zellig Harris, who in a series of publications -for example

The eclipse of corpus-based methodology is characteristic of linguistics as practised in the United States and Britain. In continental Europe the application of mathematical and statistical concepts and methods to analysis of corpus data for derivation of linguistic laws has continued to be developed. An exhaustive listing of the relevant earlier European literature, most of which is not in English and therefore little known in the English-speaking linguistics world, is given in

Quantitative methods have, moreover, always been central in some linguistics subfields concerned with the architecture of language: quantitative analysis of empirical and, increasingly, corpus-derived data is fundamental in psycholinguistics -cf.

Variationist linguistics, here understood as the study of how language use varies in chronological, social, and geographical domains, is fundamentally empirical in that it uses data abstracted from observation of language use either to infer hypotheses about patterns of linguistic variation in or to test hypotheses about a language community. These three domains have historically been the preserves of historical linguistics, sociolinguistics, and dialectology respectively, though there is substantial overlap among them. Because they are based on analysis of data abstracted from language use, all three are naturally suited to quantitative and more specifically statistical methods, and, as corpus linguistics has developed in recent years, the corresponding research communities have increasingly adopted it, albeit unevenly. This unevenness is particularly evident with respect to quantitative methods, as sampling of the recent literatures in a way analogous to that for grammatical linguistics above testifies. are:

-The use of quantitative methods in chronological variation research has been concentrated in linguistic phylogeny, the study of relatedness among languages. This study has historically been associated primarily with the Indo-European group of languages, and was throughout the nineteenth and most of the twentieth centuries dominated by the Neogrammarian paradigm. This paradigm assumed that the Indo-European language interrelationships and, latterly, language interrelationships in general could be modelled by acyclic directed graphs, or trees, and used the Comparative Method to construct such trees. The Comparative Method has been and remains largely non-quantitative; quantitative comparative methods were first introduced in the midtwentieth century via lexicostatistics, which involves calculation of proportions of cognates among related languages, and glottochronology, which posits a constant rate of linguistic change analogous to that of biological genetic mutation -cf.

2),

-Textbooks, monographs, and tutorial papers:

Because many modern-day corpus linguists have been trained as linguists, not statisticians, it is not surprising that they have been reluctant to use statistics in their studies. Many corpus linguists come from a tradition that has provided them with ample background in linguistic theory and the techniques of linguistic description but little experience of statistics. As they begin doing analyses of corpora they find themselves practising their linguistic tradition in the realm of numbers, the discipline of statistics, which many corpus linguists find foreign and intimidating. As a consequence, many corpus linguists have chosen not to do any statistical analysis, and work instead with frequency counts. . .

Cluster analysis in corpus linguistics

Several of the above-cited textbooks and monographs on quantitative methods in linguistics include accounts of cluster analysis and its application. The following discussion of specific application areas adopts the foregoing categorization of subdisciplines into grammatical and variationist linguistics, and the latter into social, geographical, and chronological variation .

Cluster analysis in grammatical research

In the aftermath of Chomsky's critique of empirical approaches to the study of language in his 1959 review of Skinner's Verbal Behaviour

Empirical work continued to be done, however, and some of it used cluster analysis.

It was not until the later 1980s, however, that there was a resurgence of interest in empirical and more specifically statistical approaches among linguists as a consequence partly of the advent of 'connectionist' cognitive science with its emphasis on the empirical learning of cognitive functions by artificial neural networks, partly of the success of stochastic methods like Hidden Markov Models in the natural language and speech processing communities, and partly of the increasing availability of large-scale digital electronic natural language corpora from which to extract distributional information reliably, including corpora augmented with grammatical information such as the Penn Treebank and WordNet. Since ca. 1990 the volume of empirical corpus-based linguistic research has increased quite rapidly, as sketched above, and with it the use of cluster analysis. Most of the work has concentrated on lexis, but there is also some on phonology, syntax, and language acquisition

Over the past two decades or so a large amount of work has been done on inference of grammatical and semantic lexical categories from text (Korhonen 2010), driven mainly by the requirements of natural language processing tasks like computational lexicography, parsing, word sense disambiguation, and semantic role labelling, as well as by language technologies like information extraction, question answering, and machine translation systems. This work is based on the intuition that there are restrictions on which words can co-occur within some degree of contiguity in natural language strings, and that the distribution of words in text can therefore be used to infer grammatical and semantic categories and category relatedness for them. This intuition underlies the "distributional hypothesis" central to Harris' work on empirically-based inference of grammatical objects

Representative examples of more recent work in this area are:

-

Work in other areas of grammar involving cluster analysis is less extensive, as noted.

Cluster analysis in chronological variation research

As noted, the use of quantitative methods in chronological variation research has been concentrated in linguistic phylogeny, and application of cluster analysis in this subfield shares that orientation.

Examples of earlier work are as follows.

Since about 2000, language classification has increasingly been associated with cladistic language phylogeny, and cluster analysis is one of a variety of quantitative methods used in cladistic research. The general focus in this work is not so much achievement of a definitive result for some language group, but methodological discussion of data creation and clustering issues.

-

As diachronic corpora have begun to appear, so have applications of cluster analysis in chronological variation research apart from language phylogeny

Cluster analysis in geographical variation research

Applications of cluster analysis in geographical variation research are associated primarily with the universities of Salzburg and Groningen. These are reviewed first, and work by other individuals and groups is covered subsequently.

In a series of articles from 1971 onwards, Goebl developed a dialectometric methodology which culminated in three books: Dialektometrie. Prinzipien und Methoden des Einsatzes der Numerischen Taxonomie im Bereich der Dialektgeographie

Since the late 1990s Nerbonne and his collaborators have developed a methodology based on the dialectometry pioneered by Séguy and Goebl, whose defining characteristics are creation of data based on quantification of observed language use and multivariate analysis of that data, and which incorporates the most sophisticated application of cluster analysis in current linguistic research. To gain an understanding of that methodology a good place to begin is with

Quantification of observed language use

The discussion in the foregoing chapters of this book has been based on quantification of observed language use in terms of the vector space model: a set of n variables was defined to describe a set of m objects -in the case of DECTE , speakers -in the research domain, and the frequency of occurrence of each of the variables in the domain for each of the objects or speakers was recorded. This approach to quantification has been used in dialectometry by, for example,

Given its centrality in the Groningen group's methodology, the Levenshtein distance is described in greater or lesser degrees of detail in many of its publications; see for example

Multivariate analysis

Multivariate analysis in dialectometric terms is the simultaneous use of multiple linguistic features to infer dialect distributions from data. The Groningen group describes this simultaneous use of linguistic features as 'aggregation', and contrasts it with the traditional dialectological procedure of analyzing the distribution of a single or at most a very few features. The reasons for the group's preference for aggregation-based analysis are made explicit in

Cluster analysis is an important class of multivariate method, and has been fundamental to the Groningen methodology from the outset; see

For the period before 2000, the earliest example of the use of cluster analysis for geographical variation research appears to be that of

Examples of work since 2000 are:

Cluster analysis in social variation research

Several researchers have used cluster analysis for sociolinguistic interpretation of data relating to variation in phonetic usage among speakers. In chronological order: -

-

Recent research has seen the development of sociolectometry as a parallel to dialectometry, which studies lexical variation across language varieties in relation to social factors using vector space models based on the distributional hypothesis referred to earlier, and a variety of clustering methods such as multidimensional scaling, principal component analysis, hierarchical clustering, and clustering by committee

The foregoing discussion has proposed cluster analysis as a tool for generating linguistic hypotheses from natural language corpora. The motivation for doing so was practical: as the size and complexity of corpora and of data abstracted from them have grown, so the traditional paper-based approach to discerning structure in them has become increasingly intractable, and cluster analysis offers a solution. Hypothesis generation based on cluster analysis has two further advantages in terms of scientific methodology, however. These are objectivity and replicability

-Objectivity. The question of whether humans can have objective knowledge of reality has been central in philosophical metaphysics and epistemology since Antiquity and, in recent centuries, in the philosophy of science. The issues are complex, controversy abounds, and the associated academic literatures are vast -saying what an objective statement about the world might be is anything but straightforward, as Chapter 2 has already noted. The position adopted here is that objective knowledge is ultimately impossible simply because no observation of the natural world and no interpretation of such observation can be independent of the constraints which the human cognitive system and the physiological structures which implement it impose on these things. On this assumption, objectivity in science becomes a matter of attempting to identify sources of subjectivity and to eliminate them as factors in formulating our species-specific understanding of nature. An important way of doing this in science is to use generic methods grounded in mathematics and statistics, since such methods minimize the chance of incorporating subjective assumptions into the analysis, whether by accident or design. -Replicability is a foundational principle of scientific method. Given results based on a scientific experiment, replicability requires that the data creation and analytical methods used to generate those results are sufficiently well specified to allow the experiment to be reproduced and for that reproduction to yield results identical to or at least compatible with the originals. The obvious benefit is elimination of fraud.

There have been and presumably always will be occasional cases of scientific fraud, but this is not the main motivation for the replicability requirement. The motivation is, rather, avoidance of error: everyone makes mistakes, and by precise and comprehensive specification of procedures the researcher enables subject colleagues suspicious of the validity of results to check them. Cluster analysis and the associated data representation and transformation concepts are objective in the above sense in that they are mathematically grounded, and analyses based on them are replicable as a consequence in that experimental procedures can be precisely and comprehensively specified. In the foregoing search for structure in the DECTE corpus the initial selection of phonetic variables was subjective, but the data representation and the transformation methods used to refine the selection were generic, as were the clustering methods used to analyze the result. The data and clustering methodology was, moreover, specified precisely enough for anyone with access to the DECTE corpus to check the results of the analysis.

Given the methodological advantages of cluster analysis for hypothesis generation, the hope is that this book will foster its adoption for that purpose in the corpus linguistics community. This Appendix lists software implementations of the clustering methods presented earlier. The coverage is not exhaustive: only software known to the author to be useful either via direct experience or online reviews is included.

8.1

Cluster analysis facilities in general-purpose statistical packages

Most general-purpose statistics / data analysis packages provide some subset of the standard dimensionality reduction and cluster analysis methods: principal component analysis, factor analysis, multidimensional scaling, k-means clustering, hierarchical clustering, and sometimes others not covered in this book. In addition, they typically provide an extensive range of extremely useful data creation and transformation facilities. A selection of them is listed in alphabetical order below; URLs are given for each and are valid at the time of writing. A similar case for programming is made by

There are numerous programming languages, and in principle any of them can be used for corpus linguistic applications. In practice, however, two have emerged as the languages of choice for quantitative natural language processing generally: Matlab and R. Both are high-level programming languages in the sense that they provide many of the functions relevant to statistical and mathematical computation as language-native primitives and offer a wide range of excellent graphics facilities for display of results. For any given algorithm, this allows programs to be shorter and less complex than they would be for lower-level, less domain-specific languages like, say, Java or ••, and makes the languages themselves easier to learn.

Matlab ( ØØÔ »»ÛÛÛºÑ Ø ÛÓÖ ×ºÓºÙ ») is described by its website as "a high-level language and interactive environment for numerical computation, visualization, and programming". It provides numerous and extensive libraries of functions specific to different types of quantitative computation such as signal and image processing, control system design and analysis, and computational finance. One of these libraries is called "Math, Statistics, and Optimization", and it contains a larger range of dimensionality reduction and cluster analysis functions than any of the above software packages: principal component analysis, canonical correlation, factor analysis, singular value decomposition, multidimensional scaling, Sammon's mapping, hierarchical clustering, k-means, self-organizing map, and Gaussian mixture models. This is a useful gain in coverage, but the real advantage of Matlab over the packages is twofold. On the one hand, Matlab makes it possible for users to contribute application-specific libraries to the collection of language-native ones. Several such contributed libraries exist for cluster analysis, and these substantially expand the range of available methods. Some examples are: ØØÔ »»ÛÛÛº ×º ÙØº »ÔÖÓ Ø×»×ÓÑØÓÓÐ ÓÜ» On the other hand, because the user has access to the program code both for the native Matlab and the contributed libraries, functions can be modified according to need in line with current research requirements, or, as a last resort, the required functions can be written ab initio using the rich collection of already-existing mathematical and statistical ones. Finally, there is a plethora of online tutorials and Matlab textbooks ranging from introductory to advanced, so accessibility is not a problem. R ( ØØÔ »»ÛÛÛºÖ¹ÔÖÓ ØºÓÖ ») is described by its website as "a free software environment for statistical computing and graphics", and it has the same advantages over the clustering software packages as Matlab. R provides an extensive range of dimensionality reduction and cluster analysis functions, which are listed at the following websites:

- Corpus linguistic-specific applications using clustering 311

There are numerous online tutorials for R generally and for cluster analysis using R specifically, as well as a range of textbooks. Of particular interest to corpus linguists are:

In recent years R has been emerging as the preferred language for quantitative natural language processing and corpus linguistics, not least because, unlike Matlab, it is available online free of charge.

8.4

Corpus linguistic-specific applications using clustering

The dialectometric methodology of the Groningen group, described in the foregoing literature review, has been implemented in Gabmap, described as "a Web application aimed especially to facilitate explorations in quantitative dialectology -or dialectometry -by enabling researchers in dialectology to conduct computer-supported explorations and calculations even if they have relatively little computational expertise"

composite 11, 204-208 consistency 178 cophenetic distance 242 criterion 61, 85, 226 cluster merge, 211 Dbscan see also

S
INTRODUCTION

Corpus linguistics requires an understanding of various linguistic and technological issues relating to the availability of technological facilities for the generation, storage, management, annotation, processing, analysis, and dissemination of language data and information as well as a conscious realization of the importance of language corpus in the process of advancing technologies, languages, and societies. In a context where modern computer technology continues to penetrate life and living of common people, we need more clarity to visualize the importance of digital corpora in developing 'knowledge-based societies' where language data, linguistic information, and technology developed with language data and information play a beneficial role for the betterment of societies. In recent years, we have noted a positive change in approach to the use of language corpora as a reliable resource in many domains of language technology and linguistics. This phenomenon of trust in corpora is triggered through several factors such as easy and customized accessibility of corpus data, objective analysis and description of a language based on actual empirical evidence, utilization of language data and information in various domains of language application and utilization of language-based technology in development of new societies empowered with linguistic support systems. Also, it reflects on ideological and technological changes that have taken place during the last seven decades or so.

The use of computer technology in linguistics opens many new methods of collecting, storing, and processing language data, interpreting and analyzing them, and fruitfully utilizing them in different domains of humanities, social sciences, natural sciences, and technology. This was hardly possible before the use of computers in the generation and processing of linguistic data. In the earlier years, we had to be happy with a limited amount of language data for linguistic works because we had no automated system under our disposal by which we could assemble a large amount of language data from various domains of language use, analyze them, interpret them, and utilize them for our purposes. Now we use a computer to collect language data of any size, type, and variety; classify them, process them, analyze them, and utilize them in various works. The importance of a corpus is acknowledged because it contributes to making new findings, helps in utilizing data in applications, provides scopes for modifying old observations, generates opportunities for formulating new theories, and prepares new fields for applying language data in direct service to society. Moreover, it brings in new insights to look into the cognitive operation of the human mind to understand the complex cognitive processes like receiving, processing, comprehending, and sharing linguistic signals

After seven decades of corpus use in various domains of linguistics and language technology, we have now understood that the utility of a corpus is not limited to linguistics and technology alone. It has proved its usefulness in many other domains of natural and social sciences where language data is an integral part of study and application

(e)

Emphasis on scientific inquiry rather than on hypothetical speculation. (f) Emphasis on language application rather than on language description.

A corpus is a resource that serves a large number of domains of linguistics, language technology, cognitive linguistics, and sister disciplines. Due to the multidisciplinary application of real-life language data, corpus linguistics emerges as a new approach towards linguistics-a new method of studying and applying language data using techniques and tools of computer science

WHAT IS LANGUAGE TECHNOLOGY?

What is language technology? This is an age-old intriguing question. Is it a technology for language or a language for technology? The compound expression (i.e. language technology) is quite deceptive, ambiguous, and confusing. In our understanding, it is a bidirectional domain that combines both approaches. It is an applied field that includes linguistics, computer science, statistics, and other areas. It is primarily concerned with interactions between language data and computers. It is now treated as a sub-branch of Artificial Intelligence (AI) because language processing is a highly complex method of human-computer interaction. Understanding a natural language by computer is an AI problem because it requires exhaustive information and extensive knowledge about the linguistic and extralinguistic world and an ability to manipulate this information and knowledge in a computer system. The question of whether natural language processing is different from or identical to language technology is a matter of perspective. On one side, we may define language technology by way of focusing on the theoretical aspects of language processing; on the other side, we may look at it as a way of analyzing and devising systems for language application on digital platforms. All language technology systems are, therefore, primarily grounded on machine learning and a major load of information that is needed in machine learning comes from language data. As researchers of language technology, we have to depend on theories, information, and insights gathered from different disciplines of human knowledge (e.g., Linguistics, Computer Science,

The journey of language technology, in a simple count, started in the early 1950s, though researchers may trace it back to much earlier years

Recent research in language technology increasingly focuses on unsupervised and semi-supervised algorithms, which learn from multimodal language corpora-both annotated and non-annotated. This is a more difficult task than supervised learning and it typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data, which often makes up for the inferior results. Essentially, modern approaches to language technology are grounded on various machine learning strategies although the paradigms of machine learning are significantly different from those that were applied at the early stages of language processing. The modern machinelearning paradigms, instead of using general learning algorithms, often depend on statistical results derived from large language corpora (i.e., big data) to automatically learn linguistic rules through analysis of several typical real-life examples. For instance, consider the task of part-of-speech (POS) annotation (i.e., determining the correct POS of each word in a given sentence of a text). A typical machine-learningbased tool for POS annotation works in three basic sequential stages:

Over the years, many different types and classes of machine learning algorithms have been applied to language technology tasks. The most common trait of these algorithms is that they take a large set of 'features' that are generated from the analysis of input language data as inputs. For example, for a question-answering system, some of the typical features (or functions) involve the recognition of a linguistic expression as a question, analysis of the type of the question, understanding the syntactic structure of a question, capturing meaning embedded into it, recognizing the sentiment hidden in the question, searching out an appropriate document that may carry an answer, extraction of the answer from a document, evaluation of the answer before responding, and finally generating the answer either in text or speech mode. Such a complex task of language technology depends on inputs taken from two ends: linguistic data and information from corpora, and technological help from computer science. In essence, language technology is happy in engaging technological devices like computers and mobiles to do a large number of purposeful activities with natural languages, both spoken and written. It brings in new advances in computing and develops tools and systems to serve language-related human requirements. It is recognized as one of the highly productive domains of IT. We sum up the major domains of language technology in the following manner (Fig.

Fig.

[3] Digital language resource development: lexical databases, dictionaries, thesaurus, references, encyclopedias, machine-readable dictionaries, wordnets, sense nets, course books, grammars, study materials, etc.

[4] Human-machine interface development: systems for word sense disambiguation, information retrieval, optical character recognition, text summarisation, text conversion, machine translation, web-based learning, question-answering, computer-aided instruction, computer-assisted language education, text preparation, e-governance, online language education, etc.

[5] Machine translation support system: multilingual resource access, multilingual information access, transliteration, cross-lingual language information retrieval, text alignment, etc.

[6] Speech technology development: speech recognition, synthesis, and processing, pronunciation verification and recognition, voice recognition, speaker identification, text-to-speech, speech-totext, speech disorder recognition and repair, etc.

Most of these have direct real-life applications and many of them serve as sub-tasks to solve larger language technology challenges. These tasks not only refer to the volume of research that is to be carried out in future but also reiterate the fact that each task, in itself, is a well-defined problem, the solution of which depends on how it deals with information derived from corpora and the technology generated through computation. In the following sections, I shall discuss digital font generation, corpus generation, corpus processing, corpus annotation, and application of corpus in different areas of human knowledge.

DIGITAL FONT GENERATION AND CONVERSION

In typography, a font is a collection of sorts that constitutes the complete character set of a single size and style of a particular typeface (e.g., Times New

With regard to non-advanced and non-digitized languages (which are indeed very large in number), this is a crucial issue and a daunting task for scientists engaged in font technology, one of the primary areas of natural language processing. We have to keep in mind that, even at this advanced stage of language processing, there are many non-advanced and minority languages, which have not yet been successful in producing digital language texts or linguistic resources due to the non-availability of digital fonts that could be used to produced digital texts. Against this background, some of the major issues in font technology for less advanced languages are the followings:

(1) Designing fonts for all minor languages which have separate scripts and writing systems to be used for digital text generation. (2) Conversion of existing printed text documents into machine-readable texts through the application of the Optical Character Recognition (OCR) system. (3) Developing fonts of different types and styles to address various academic, administrative, and commercial requirements. (4) Conversion of non-Unicode-based fronts into Unicode-based fonts for global access to texts.

(5) Designing Unicode-based fonts for old and obsolete characters (that were once used in scripts) for proper conversion and rendering of old printed and handwritten texts into digital forms. (6) Making Unicode-compatible fonts available to people of those languages so that they can use these to produce digital texts. (7) Development of transcription modules for bidirectional-transcription of texts (e.g., English to Mundari and Mundari to English). (8) Localization of tools and systems of font technology for non-advanced languages for creating lasting impacts in the development of knowledge-based societies. (9) Sharing knowledge and font technology that is already developed and applied in advanced languages with less advanced and non-advanced language scripts. (10) Making font technology a part of digital ethnography in the preservation, protection, and promotion of scripts and writing systems of less-advanced languages.

In the present global context, these works are still unattended for many non-advanced languages. We have to think seriously about how all these problems are addressed and solved so that the benefits of language technology reach every language community-advanced or non-advanced, resource-rich or resource-poor, and technology-savvy or technology-hungry.

LANGUAGE CORPUS BUILDING

The introduction of a corpus in language study adds a new dimension to linguistics. Corpus linguistics is not a 'new area' of language study; it is a 'new approach' (or a new method) to language study. It argues for studying a language through empirical analysis of language data produced in machine-readable form with a large collection of texts. This approach, over the years, has been successful in bringing in new perspectives towards language study in several domains, namely, language description, language education, language experiments, and language computation. The underlying theoretical idea of corpus linguistics is quite broad. It refers to a process of an exhaustive analysis of a substantial body of authentic spoken and/or written texts and processing the same for various academic, social, and commercial needs. Digital corpus is now a known thing to us and we have come to a common agreement to understand what counts as a 'corpus', what are its characteristics, how it can be built and classified, how it can be annotated and processed, and how it can be analyzed and utilized. We have also understood that we can refer to corpora to understand some of the complex cognitive and linguistic questions about how people use language for communication, information generation, and knowledge sharing. Also, there are technical motivations for compiling language corpora for building intelligent devices and systems that will efficiently interact with human beings and perform many language-related tasks. All such goals have inspired computer scientists and linguists to work together to develop language corpora to be processed and utilized in designing intelligent systems like machine translation, speech recognition, information extraction, question answering, sense disambiguation, sentiment recognition, language education, machine-aided instruction, etc. What we have understood from our involvement with activities like corpus generation, processing, annotation, analysis, and applications over the years, is that both language and technology receive huge benefits from insights and information gathered from corpora. This confirms the importance of the description and analysis of data and application of corpus in diverse spheres.

The term corpus is derived from the Latin word corpus which means "body". In the present context, we use this term to refer to a large digital collection of natural language texts that are assumed to be representative of a given language, dialect, or a subset of a language, to be used for linguistic annotation, processing, and analysis. It typically contains a collection of representative samples that are obtained from texts of different varieties of language use in various domains. Theoretically, it is capable of gathering unlimited selections of texts, compatible with computers, operational in research and application, representative of a source language, processed by a machine, unlimited in the amount of data, and systematic in formation and representation. From a theoretical perspective, the salient features of a corpus are the following

[1] Quantity: A corpus should be big. It should contain a large amount of data from spoken and written sources. It is the total of components, which constitute its body.

[2] Quality: It refers to the authenticity of data. All text samples should be collected from genuine use of speech and writing. Samples should be collected from original communication (and not from experimental conditions or artificial circumstances).

[3] Representation: It should include samples from a wide range of disciplines. Samples should be balanced to all disciplines to represent wide domain varieties. Later studies devised on it will require authentication of information from texts representing a language.

[4] Simplicity: A corpus should primarily contain simple plain texts. The text should be free from any annotation that carries linguistic and extralinguistic information. Corpus users are not always willing to have additional information tagged to texts.

[5] Equality: Text varieties should be of even size concerning the number of words. It is, however, a controversial issue, and therefore, cannot be applied blindly. The sampling process varies based on a specific requirement.

[6] Retreavability: Data and information should be retrieved from a corpus. It draws attention to storage techniques of data in electronic form in an archive. Modern technology allows one to store a corpus in such a manner that one can easily retrieve data from it.

[7] Verifiability: Corpus should be open for empirical verification. Users are free to use data from it to examine earlier observations. This puts a corpus ahead of generative linguistic study.

[8] Augmentability: A corpus, unless it is a fixed one, should grow regularly. This allows a corpus to reflect on the linguistic changes that take place in a language over time. By regular addition of synchronic data, a corpus attains a diachronic dimension.

[9] Documentation: Information about corpus building should be preserved for documentation. It is to be stored as metadata in a header file. This becomes useful at subsequent stages of corpus management and reference.

[10] Management: It involves the storage of texts for faster and easier retrieval. The utility of a corpus is increased by an elegant arrangement of texts in an archive. It makes the application of data more effective and fruitful. It may involve schemes for maintenance, standardization, augmentation, up-gradation, and dissemination of data.

Classification of Corpus

Digital language corpora are new things. We are yet to come to a consensus about their classification.

The scheme that I propose here offers a reasonably workable way of classifying language corpora with delimited categories wherever possible. I argue for using the external and internal criteria for classifying corpora. The external criteria refer to a text type that is linked with participants, occasion, social setting, and function of a text. The internal criteria, on the other hand, refer to the use of language properties within a piece of text. It is known to us that corpora can be of many types with regard to texts, languages, modes of data sampling, methods of corpus creation, manners of text annotation, and goals of text utilization, etc. For instance, one corpus may contain samples of written texts, the other one may contain samples of spoken texts, and the third one may contain transcripted spoken texts.

Similarly, a corpus may contain samples of present-day use of a language, while the other may contain samples from old texts; a corpus may be monolingual with a collection of data from a single language, while another may be bilingual or multilingual by including texts from two or more languages; texts included in a corpus may be collected from one source, two sources or many sources of a particular field or across fields. This implies that numerous factors control the content, type, and use of a corpus as well as numerous ways that one can employ to classify corpora. It also implies that the kind of text included as well as a combination of different text types is crucial in classifying corpora. Each corpus developer is, however, free to adopt unique methods in language representation, text collection, and text application. These factors can make a corpus unique in form, content, feature, and function

Corpus Generation

There are various issues relating to the design, generation, and management of a corpus. These issues vary based on the type of text and the purpose of its use. For instance, issues relating to speech corpus generation differ from issues relating to text corpus development. Development of a speech corpus involves issues and factors like purpose of use, selection of speakers, choice of settings, size of a corpus, use of recording instruments, manner of data sampling, manner of data elicitation, nature of transcription, types of data encoding, management of audio files, editing of input data, processing of spoken texts, annotation of speech, analysis of speech corpus, etc. On the other hand, the development of a text corpus addresses issues like the size of a corpus, representation of text types, question of nativity of language users, determination of target users, selection of time-span of production of texts, coverage of disciplines, selection of text documents, collection of source text materials, methods of data sampling, manners of data collection, manners of text normalization, management of corpus files, types of text annotation, and issues of copyright, etc. That means, based on the type of text, one has to address various issues of corpus generation. Keeping this view, I address below some of the common issues of corpus generation. How big a corpus should be? This is related to the size of a corpus as size is an important issue in corpus generation. In the case of a speech corpus, it is related to the amount of audio data (in MB or GB) or duration of a speech (in hours and minutes). In the case of a speech corpus, on the other hand, it is linked with the total number of sentences, unique words (types), and total words (tokens) in a corpus. It also depends on the decision of how many text categories are kept in a corpus, how many text samples are put in each category, and how many words are kept in each sample. Although size does matter on the scale of the 'reliability' of a corpus, in reality, a corpus, however big it may be, is a small representation of the vast amount of speech and text varieties that are produced in a language. In the early 1960s, when computer technology was not conducive to collecting a large amount of language data, a corpus of one million words was considered good enough (e.g., Brown Corpus, Lancaster-Oslo-Bergen (LOB) Corpus, KCIE (Kolhapur Corpus of Indian English)). In the last seventy years, computer technology has undergone a sea change in storage, accessing, and processing capabilities; and due to this, size is no longer an issue. This is an age of big data. Therefore, the bigger the size of a corpus, the more is its utility, faithfulness, and reliability. In the new millennium, a corpus that contains hundreds or more millions of words is a preferred choice (e.g., British National Corpus

The issue of size becomes less relevant in the context of text representation. A large corpus does not guarantee a balanced representation of all varieties of texts of a language, if not desired so. A small corpus with wider text variations is a much better representative than a large corpus with fewer text variations.

A large yet less varied corpus cannot be used for the generalization of a language. A corpus is truly 'representative' when findings from it are generalized to a language or a part of it. Therefore, rather than focussing on the quantity of data in a corpus, it is always sensible to emphasize varieties of data. That means language data should come from the texts of all possible domains of language use. The size of a corpus should be set against the diversity of texts to achieve proper representation. In any corpus, the greater the number of unique text samples, the higher the reliability of the analysis of linguistic variables. The Brown, LOB corpus, and Survey of English Usage (SEU), for instance, contain a wide variety of text types, and therefore, they are considered much better representatives of English. However, the BNC and the ANC, which are not only very large but also far more diversified in structure and text types, empirically settle the issues relating to size and representation.

The question of the nativity of text producers is another crucial issue in corpus generation. The general argument is that texts produced by native language users should get priority over texts produced by nonnative users in a general and monitor corpus. A general corpus tends to represent a language, which is considered 'standard' in works in linguistics and language technology. The non-native use of texts in a general corpus can have an adverse effect on linguistic analysis of data and information. One of the reasons behind building a general corpus is to enable scholars to analyze 'standards' to see what does occur and what does not than looking into non-standard interpolations. The primary idea is that we should have a general corpus that includes 'benchmarked' and 'standard' data so that we can collect information about how accepted standards are commonly used in mainstream linguistic activities. Based on a general corpus, we like to produce texts and reference materials that will guide in spelling, pronunciation, grammar (i.e., syntax), meaning and usage.

In the case of a special corpus, we like to include text samples produced by non-native users as the aim of this corpus is to highlight linguistic peculiarities that are typical to non-native users. Here the question of representation is not linked with the language as a whole but with the language used by a particular type of people who are primarily non-native speakers. The question of nativity, however, becomes quite complicated and sensitive when we notice that the same language (e.g., English) is used by people separated by some geographical or political barriers (e.g., British English, American English, Australian English, Indian English, African English, Jamaican English). In this case, for instance, we may find forms that are correct by the 'rules of grammar and usage of Indian English' (and perfectly understandable); but are not the 'right' forms in the 'rules of grammar and usage of the British and American English'.

There is no fixed target user for a general corpus, as such. Anybody can use it for any purpose. In the case of a special corpus, the identification of target users is important. Since each research has a specific goal, a special corpus has to be designed accordingly. For instance, a person working on developing a tool for machine translation (MT) requires a parallel corpus than a general one. Similarly, a person working on comparative studies between two or more languages requires a multilingual comparable corpus than a monitor one. To summarise this feature, I generalize a model to relate the types of a corpus and its users (Table

Target Users

The

An important issue in corpus building is the selection of text type. It is necessary to decide if a corpus should contain all types of written text samples or specific sets of text samples. A general corpus usually includes standard text samples as it aims to identify what are central and common features of a language. It hardly cares for any typical, special, and unique features that are represented in the database. Therefore, we can furnish a general corpus with standard text samples of contemporary writings and a calculated and proportional representation of texts can suffice our requirement. On the other hand, in case of a monitor corpus, which tries to represent all possible text varieties, we may include all non-standard and ordinary text samples along with standard and established text samples. It also includes texts produced by little-known writers along with texts created by well-known and reputed writers. A heterogeneous monitor corpus includes text samples from all subject areas and the individuality of the source of a text and its writer are well attested. Thus, a monitor corpus becomes a worthy representation of texts that are ever produced in a language. Diversity is a useful safeguard for a monitor corpus against skewed representation.

The task of sampling of texts has to be done with collected text materials based on the character of a corpus. The sorting of texts can be random, regular, or selective. Since there are several methods for data sampling to ensure the optimum representation of texts in a corpus, we may pre-define the kind of data we want to study before we define a sampling procedure. The application of the random sampling method usually saves a corpus from being skewed and less representative. This method is widely used in many corpus generation projects across languages. Alternatively, we may apply selective sampling methods that are used in the Brown Corpus and the LOB corpus or consider more suitable methods of text representation keeping in mind the goal and purpose of a corpus.

The management of a corpus is a complex and tedious task. It involves diverse activities like storage, metadata generation, archiving, augmentation, upgradation, processing, retrieval, dissemination, and other works. Once a corpus is built and stored in an archive, we need robust schemes for regular maintenance, upgradation, and augmentation of the resource. There are always some errors to be corrected, some modifications to be made, and some improvements to be implemented. Adaptation to new hardware and software and addressing the requirements of users are two vital issues that have to be taken care of. Moreover, there are technical issues with retrieval, processing, and applying analytic tools and systems on databases. We have to be comfortable with the application of modern toolkits on corpus as such devices help us execute many useful tasks on a corpus.

The processes of corpus sanitation start when a corpus is made ready for use. There are many linguistic and technical issues relating to corpus sanitation (e.g., text normalization, orthographic error correction, spelling error correction, real word-error correction, grammatical error correction, punctuation error removal, and tokenization). Many of these works have to be done on a corpus before a corpus is made ready for application in technology and other works

It is necessary to dissolve issues of copyright of texts with copyright holders before a corpus is made open for application. Copyright laws are complicated and usually vary from country to country. If one uses texts for personal purposes, then perhaps, there is hardly any problem. This is fine not only for a single individual but also for a group of scholars who are working together on some areas of research. With regard to the use of a corpus for academic purposes, there are also problems with ethical 'right, wrong, legal or illegal'. As long as it is not directly used for commercial purposes, one can utilize a corpus. However, when one uses a corpus to produce tools, systems, and resources and commercialize these, one has to face copyright problems. In direct commercialization of corpus, one should seek permission from legal copyright holders.

CORPUS PROCESSING

The processing of a corpus starts after a corpus is generated and normalized. Processing is necessary for utilizing corpus data in linguistic research and technology development. There are various ways of processing (e.g., data analytics, concordance, collocation, keyword marking, local-word-grouping, lexical clustering, lemmatization, morphological processing, sentence processing, and named entity marking), which are applied to a corpus with appropriate technological supports. The outputs obtained from processing may agree with or contradict the results of earlier studies and findings. At present, there are many processing tools and software that are available mostly freely for corpora. I briefly discuss here a few well-known corpus processing techniques for understanding these technical issues. More details are available in some recent works

Studies in mathematical linguistics, computational linguistics, corpus linguistics, applied linguistics, forensic linguistics, stylometrics, and other domains require statistical and quantitative results from a corpus. Without knowledge of statistics about various properties of a language, we make mistakes in the analysis of language data and inference deduction. A corpus often becomes a subject of quantitative and qualitative statistical analysis for addressing empirical and theoretical queries. In quantitative statistical analysis, we classify linguistic properties or features in a corpus, count their frequency of occurrence, and construct statistical models to explain what we observe. We also try to discover which phenomenon is likely to be a genuine reflection of a language and which is a mere chance occurrence. In qualitative statistical analysis, on the other hand, we look at multidimensional analysis of observed phenomena in a corpus and try to find factors behind the occurrence of a feature or an element in a text. Both types of analysis have something to contribute to corpus-based language study. While descriptive statistical analysis enables summarising the most important properties of observed data, inferential statistical analysis helps to use information from descriptive statistical analysis to reply to questions, formulate hypotheses, and verify propositions. The evaluative statistical analysis, on the other hand, enables testing if hypotheses are supported by true evidence in data and how mathematical models and theoretical distributions of data are related to reality. To perform comparative studies, we apply multivariate statistical techniques (e.g., Factor Analysis, Multidimensional Scaling, Cluster Analysis, Log-linear Models, and Pearson Correlation) to extract hidden patterns of language use from frequency data obtained from a corpus.

Numerical sorting is one of the most straightforward approaches to working with quantitative data of a corpus. Here items are classified according to a particular scheme, and an arithmetical count is made on the number of items that belong to each class in the scheme within a corpus. Information collected from frequency counts is rendered in alphabetical or numerical order. Both lists are arranged in ascending and descending order based on the requirements of the study. Anyone studying a corpus may like to know the frequency and patterns of use of each item in it. A frequency list of unique words, for instance, is a useful clue in the identification of the type of text

Lexical concordance is a process of making an index to words used in a corpus

An exiting game

was played between the two teams. The winner was declared in the last game of the match. They won the Bridge competition in the last game.

The next Olympic games will be held in London.

The annual games and sports were held in December. The couples are not new to this game of love. The final game ended in six-all. They lost but played a good game.

I was just playing a game with you jokingly. It is none of your games. So that was your game, which I failed to understand.

The game of the authority annoyed the people. It was a wild game in last night's party.

The hunters went to the game reserve in a group at night The hound chased the game into the wild grass They knew the game was over for their leader.

Table

The method of lexical collocation helps to understand the role and position of word pairs in texts

Example from English: British National Corpus English Phrase: "glass of"

Fig.

The extraction of information of part-of-speech of a word and representation of information relating to inflection

Input Word : ghumāiteichhilen Root : √ghumā) Inflection Part : -āiteichhilen Number : Sing +Plural Aspect : -ite-Particle_Emphatic : -i-Auxiliary : -ch-Tense_Past : -il-Person_3rd : -en-Honorific : -en-Part-of-Speech : Finite Verb Meaning : "was indeed sleeping"

Table

CORPUS ANNOTATION

Corpus annotation is an innovative process of adding interpretative information to the text of a corpus. Since a corpus contains text samples either from a written or spoken discourse, annotation is done by adopting a value-encoding process, which attaches interpretative information at different levels of a text. The interpretative information may be related to prosody, pronunciation, orthography, grammar, meaning, sentence, anaphora, discourse, rhetorics, etymology, sociolinguistics, and other issues. Interpretative information is classified into two types: (i) intralinguistic information, which is linked with linguistic features and properties of a text, and (ii) extralinguistic information, which is related to information not linked with linguistic properties and features of a text. Analysis of corpus texts shows that apart from pure intralinguistic information, a corpus also carries several kinds of extralinguistic information. While intralinguistic information helps to understand internal issues and aspects of a text, extralinguistic information provides external information based on which we have a better insight and interpretation of a text.

Both types of information are annotated to a text-either attaching them to the elements of a text or interspersing them within a text. The primary purpose of the annotation is to add additional information to a text so that future analysis and interpretation of a text becomes easy, accurate, and useful. Compared to raw text, annotated text is complex in structure and difficult to access, but easier in analysis, better in interpretation, and more user-friendly in an application. In the last seventy years, we have come across many innovative methods of text annotation

Fig.

Intralinguistic Annotation

Based on the type of information to be added, a corpus can have two broad kinds of annotation: (a) intralinguistic annotation, and (b) extralinguistic annotation

(a) Orthographic annotation: We identify various types of characters, symbols, and signs used in a written text and mark their exact function in a text. Through interpretation and analysis of the function of orthographic symbols, we know how a piece of text is designed and developed, which script is used for the composition of a text, and in most cases, with reference to a script, we identify the language of a text

(b) Prosodic Annotation: we annotate crucial prosodic features in a spoken text. Our primary goals are to indicate and mark stress and accent in speech, patterns of intonation, spontaneous speech features, suprasegmental properties, and non-verbal cues (e.g., silence, pause, hesitation, repetition, non-ends) present in a spoken text

(c) Grammatical Annotation: we assign exact part-of-speech to words and terms after we understand their grammatical roles in a text. That is why it is also known as part-of-speech (POS) annotation. To assign a POS value to a word, we have to first analyze the role of a word in a sentence and identify in which part-of-speech it is actually used

(d) Named item Annotation: our primary goal is to identify all proper names used in a text and mark them in different types of named entities based on their form and function in a text

(e) Multiword Annotation: a multiword unit crosses a single-word boundary (or space). An expression that is composed of two or more words and is not predictable by any of the words which are used to construct it is considered a multiword unit. In multiword annotation, we identify and mark multiword units with a set of tags so that, at a later stage, a machine identifies them to analyze their forms and functions to understand their role in a text

(f)

Syntactic Annotation: It is also known as parsing. We identify and annotate all the phrases and phrasal expressions that are used in the meaningful construction of a sentence (Garside, et al. 1997;

Extralinguistic Annotation

In extralinguistic annotation, we annotate a text with that kind of information, which is not physically available inside a text. For instance, when we annotate a text at an anaphora level, there is no apparent information available in a text-based on which we can identify how words are linked with others in relation to co-reference. We go beyond the textual level to understand how words are co-indexed or bound with co-referential relations. Some common types of extralinguistic annotation include semantic annotation, anaphoric annotation, discourse annotation, etymological annotation, rhetoric annotation, and ethnographic annotation. Primary ideas about some extralinguistic annotations are presented below.

(a) Semantic Annotation: it is also known as word sense annotation. Here we assign semantic values to both open and closed classes of words. The primary goal of semantic annotation is to identify the sense a word denotes when it is used in a text. This sense, which is normally known as contextual sense, may differ from a lexicographic sense of words available in a dictionary. The contextual sense is utilized in language applications such as word sense disambiguation

(b) Anaphoric Annotation: we identify and co-index pronouns and nouns that are used in a text in a co-reference scheme following a broad framework of cohesion

(c) Discourse Annotation: we annotate a text at a level, which is beyond the level of sentence and meaning. We do this to understand discourse and pragmatic strategies deployed in a text. It is noted that the identification of specific discourse markers and pragmatic functions of units in a text gives scopes to understand the purposes and goals of a text generation

An etymologically annotated text addresses all the questions and challenges related to the etymology of words.

(e) Rhetoric Annotation: it is also known as a figure-of-speech annotation. We identify and mark various rhetorical devices used in a text. We annotate these devices based on standard methods of classification and categorization of rhetorics. The use of rhetorics is a common practice in text generation. Each piece of text (either spoken or written) contains a few rhetorics-either explicitly visible or implicitly embedded. Analysis of rhetorics sheds new insights into the theme and structure of a text. For instance, a piece of text made with rhetorical devices is different from a text without rhetorics. A rhetorically developed text differs in choice of words, order of words, sentence structure, grammatical form, sense implication, and reference. Such properties put up challenges in scientific argumentation in a text

(f) Ethnographic Annotation: it is an expected deviation from discourse annotation. It involves marking sociolinguistic cues, ethnographic elements, ecolinguistic properties and cultural information that are normally concealed in a text

Besides these major types of text annotation, we think of annotation of other types. In most cases, they posit new challenges. We have to think of new strategies to solve these challenges. For instance, we have not yet investigated how a poetic text should be annotated; and what kinds of textual, linguistic, stylistic, prosodic, and figurative annotation techniques we should design and apply to poetic texts. Problems and challenges are also there in the annotation of medical texts, texts of court proceedings, conversational texts, multimodal texts (where audio-visual elements are included), mythological texts, and others. Further challenges are involved in the annotation of heritage and classical texts, which require separate tagsets and methods for annotation of mythological characters, events, anecdotes, ecology, and sociocultural properties.

CORPUS UTILIZATION

Some questions are often raised with regard to the use of language corpus in the study of languages and designing tools of language technology: What is the use of a corpus? Who will be using it? Where will it be used? How will it be used? What purpose will it serve in general linguistics? How can it benefit applied linguistics? How does it contribute to language technology? Most of these questions are addressed in some earlier studies

In a general view, there are two major applications of a corpus: (a) as a diluted source of data to work as a yard-stick for linguistic and extralinguistic verification and authentication, and (b) as a test-bed for training and testing of devices, tools, techniques, and systems of applied linguistics and language technology. Keeping these issues in mind, we visualize the value of a corpus in the following ways:

(a)

It is an indispensable resource for developing systems, tools, and software for language technology, (b) It is a useful resource in general language description, language analysis, and language teaching and training, (c)

It is a reliable treasure-house for lexical databases, dictionaries, thesauruses, reference books, and course books, etc., (d) It is a ready-made handy resource for multi-purpose non-linguistic uses and references, (e)

It is a repository of linguistic features and properties, which are used to study the use of language across styles, genres, topics, etc. (f)

It is a customizable text for studying particular areas of interest relating to life, language, and society.

There are many benefits of a corpus in all areas and subareas of linguistics and language technology. Both from a theoretical and application point of view, corpus linguistics is a powerful method, which is empirical, scientific, realistic, authentic, open-ended, verifiable, and reliable. We must agree that quantitative data retrieved from a corpus is necessary not only in language technology but also in many areas of linguistics (e.g., speech analysis, lexicography, discourse analysis, language teaching, stylometrics, translation, and language planning). Within mainstream descriptive linguistics, a corpus is a useful resource based on which faithful language description can be made. Many successful works on language description are based on data obtained from corpora. In language teaching, information about the use of phonemes, morphemes, words, and sentences in corpora is used by teachers while they teach a language scientifically. Information about the frequency of use of language properties is not available from introspection; it is to be collected from language corpora only. In language acquisition, observation of actual evidence is a source for verification and validation, since no intuitive judgment can justify a phenomenon observed in language use by infants. Even generative linguists acknowledge the value of a speech corpus as a source of evidence in language acquisition studies. Due to many such advantages, criticisms against corpus linguistics, although partly succeeded at the initial stage, have failed to stop corpus linguistics. In the present context of linguistics and language technology, we have clear ideas about the application of corpus in various domains (Fig.

No
Introduction

This textbook aims to teach you how to analyse and interpret language data in written or orthographically transcribed form (i.e. represented as if it were written, if the original data is spoken). It will do so in a way that should not only provide you with the technical skills for such an analysis for your own research purposes, but also raise your awareness of how corpus evidence can be used in order to develop a better understanding of the forms and functions of language. It will also teach you how to use corpus data in more applied contexts, such as e.g. in identifying suitable materials/examples for language teaching, investigating sociolinguistic phenomena, or even trying to verify existing linguistic theories, as well as to develop your own hypotheses about the many different aspects of language that can be investigated through corpora. The focus will primarily be on Englishlanguage data, although we may occasionally, whenever appropriate, refer to issues that could be relevant to the analysis of other languages. In doing so, we'll try to stay as theory-neutral as possible, so that no matter which 'flavour(s)' of linguistics you may have been exposed to before, you should always be able to understand the background to all the exercises or questions presented here.

The book is aimed at a variety of readers, ranging mainly from linguistics students at senior undergraduate, Masters, or even PhD levels who are still unfamiliar with corpus linguistics, to language teachers or textbook developers who want to create or employ more real-life teaching materials. As many of the techniques we'll be dealing with here also allow us to investigate issues of style in both literary and non-literary text, and much of the data we'll initially use actually consists of fictional works because these are easier to obtain and often don't cause any copyright as that of morphology, or even phonology. Having reached the end of the book, you'll hopefully be aware of many of the different issues involved in collecting and analysing a variety of linguistic -as well as literary -data on the computer, which potential problems and pitfalls you may encounter along the way, and ideally also how to deal with them efficiently. Before we start discussing these issues, though, let's take a few minutes to define the notion of (linguistic) data analysis properly.

Linguistic Data Analysis

1.1.1 What's data?

In general, we can probably see all different types of language manifestation as language data that we may want/need to investigate, but unfortunately, it's not always possible to easily capture all such 'available' material for analysis. This is why, apart from the 'armchair' data available through introspection (cf. Fillmore 1992: 35), we usually either have to collect our materials ourselves or use data that someone else has previously collected and provided in a suitable form, or at least a form that we can adapt to our needs with relative ease. In both of these approaches, there are inherent difficulties and problems to overcome, and therefore it's highly important to be aware of these limitations in preparing one's own research, be it in order to write a simple assignment, a BA dissertation, MA/PhD thesis, research paper, etc.

Before we move on to a more detailed discussion of the different forms of data, it's perhaps also necessary to clarify the term data itself a little more, in order to avoid any misunderstandings. The word itself originally comes from the plural of the Latin word datum, which literally means '(something) given', but can usually be better translated as 'fact'. In our case, the data we'll be discussing throughout this book will therefore represent the 'facts of language' we can observe. And although the term itself, technically speaking, is originally a plural form referring to the individual facts or features of language (and can be used like this), more often than not we tend to use it as a singular mass noun that represents an unspecified amount or body of such facts.

Forms of data

Essentially, linguistic data comes in two general forms, written or spoken. However, there are also intermediate categories, such as texts that are written to be spoken (e.g. lectures, plays, etc.), and which may therefore exhibit features that are in between the two clear-cut variants. The two main media types often require rather radically different ways of 'recording' and analysis, although at least some of the techniques for analysing written language can also be used for analysing transliterated or (orthographically) transcribed speech, as we'll see later when looking at some dialogue data. Beyond this distinction based on medium, there are of course other classification systems that can be applied to data, such as according to genre , register , text type , etc., although these distinctions are not always very clearly formalised and distinguished from one another, so that different scholars may sometimes be using distinct, but frequently also overlapping, terminology to represent similar things. For a more in-depth discussion of this, see

To illustrate some of the differences between the various forms of language data we might encounter, let's take a look at some examples, taken from the Corpus of English Novels

Apart from being useful examples of register differences , the extracts provided below also exhibit some characteristics that make them more difficult to process using the computer. We'll discuss these further below, but I've here highlighted them with boxes.

Sample A -from The Glimpses Of The Moon by Edith Wharton, published 1922

IT rose for them--their honey-moon--over the waters of a lake so famed as the scene of romantic raptures that they were rather proud of not having been afraid to choose it as the setting of their own. "It required a total lack of humour, or as great a gift for it as ours, to risk the experiment," Susy Lansing opined, as they hung over the inevitable marble balustrade and watched their tutelary orb roll its magic carpet across the waters to their feet. " Yes--or the loan of Strefford's villa," her husband emended, glancing upward through the branches at a long low patch of paleness to which the moonlight was beginning to give the form of a white house-front.

INTRODUCTION 5 before it. It is not by the direct method of a scrupulous narration that the explorer of the past can hope to depict that singular epoch. If he is wise, he will adopt a subtler strategy. He will attack his subject in unexpected places; he will fall upon the flank, or the rear; he will shoot a sudden, revealing searchlight into obscure recesses, hitherto undivined. He will row out over that great ocean of material, and lower down into it, here and there, a little bucket, which will bring up to the light of day some characteristic specimen, from those far depths, to be examined with a careful curiosity.

Sample C -from The Big Drum by Arthur Wing Pinero, published 1915

Noyes.

[Announcing Philip.] Mr. Mackworth.

Roope.

[A simple-looking gentleman of fifty, scrupulously attired-jumping up and shaking hands warmly with Philip as the servant withdraws.] My dear Phil! Philip.

[A negligently-almost shabbily-dressed man in his late thirties, with a handsome but worn face.] My dear Robbie! Roope.

A triumph, to have dragged you out! [Looking at his watch.] Luncheon isn't till a quarter-to-two. I asked you for half-past-one because I want to have a quiet little jaw with you beforehand.

Philip.

Delightful.

Roope.

Er-I 'd better tell you at once, old chap, whom you'll meet here to-day .

Sample A is clearly a piece of narrative fiction, mixing narrative description and simulated reported speech, references to characters and situations that are depicted as life-like, as well as featuring a number of at least partly evaluative reporting verbs, such as opined and emended. Sample B, on the other hand, contains no reported speech and reporting verbs, although it's clearly also narrative -albeit non-fictional -, with a relatively complex sentence structure, including numerous relative and adverbial clauses, and an overall high degree of formality. Sample C, in contrast, exhibits clear characteristics of (simulated) spoken language, much shorter and less complex syntax, even single-word 'sentences', with names, titles and informal terms of address (old chap) used when the characters are addressing/introducing each other, exclamations, contractions, and at least one hesitation marker (Er). And even though the language in the latter sample seems fairly natural, we can still easily see that it comes from a scripted text, partly because of the indication of speakers (which I've highlighted in bold-face), and partly due to the stage instructions included in square brackets.

As we haven't discussed any of the issues in processing such text samples yet, it may not be immediately obvious to you that these different types of register may potentially require different analysis approaches, depending on what our exact aims in analysing them are. For instance, for Sample A, do we want to conceptually treat the reported speech as being of the same status as the descriptive parts, and do we thus want to analyse them together or separately? Or are we possibly just interested in how the author represents the direct speech of the characters in the novel, and would therefore want to extract only that? And if so, how would we best go about this?

Sample B is probably relatively straightforward to analyse in terms of perhaps a frequency analysis of the words, but what if we're also interested in particular aspects of syntax or lexis that may be responsible for its textual complexity or the perceived level of formality, respectively? And, last but not least, concerning Sample C, similarly to Sample A, which parts of the text would we be interested in here and how would we extract them? Are the stage instructions equally important to us as the direct speech exchanges between the characters? Or, if, for example, we're interested in the average number of words uttered by each character, how do we deal with hesitation markers? Do we treat them as words or 'non-words' simply to be deleted? As I've already tried to hint at in the beginning of this paragraph, the answers to these questions really depend on our research purpose(s), and can thus not be conclusively stated here.

Something else you may have noticed when looking at the samples I've provided above is that they're all from the early 20th century. As such, the language we encounter in them may sometimes appear overly formal (or even archaic) to some extent, compared to the perhaps more 'colloquial' language we're used to from the different registers these days. I've chosen extracts from these three particular texts and period for a number of reasons: a) their authors all died more than 70 years ago so the texts are in the public domain; in other words, there are no copyright issues, even when quoting longer passages; b) they are included in corpus compilations; and c) they not only illustrate register/genre differences but also how the conventions for these may change over time, as can be seen, for example, in the spelling of to-day in the final extract.

As pointed out above, another interesting aspect of these samples is that they exhibit particular formatting issues, which again may not be immediately apparent to you yet, but are due to somewhat bizarre typographical conventions. If you look closely at the samples, you can see that in Sample A there are double dashes marking the parenthetical counterpart (i.e. reference resolution) "their honey-moon" to the sentence-initial cataphoric pronoun "IT". What is in fact problematic to some extent for processing the text is that these dashes actually look like double hyphens, i.e. they're not surrounded by spaces on either side, as would be the normal convention. Now, many computer programs designed to count words will split the input text on spaces and punctuation. Unfortunately, though, this would leave us with some very strange 'words' (that superficially look like hyphenated compounds ), them-their and honey-moon-over, in any resulting word-frequency list. This is obviously something we do not want and which introduces errors into any automatic analysis of the data. Something similar, albeit not to signal a parenthetical but instead some kind of pseudo-punctuation, happens again for "Yes-or" a little further down in the text. We can already see, therefore, from this relatively short sample of text that a failure to deal with this feature could cause issues in a number of places throughout the text. The same problem occurs in the other two samples, only that there the dash doesn't actually consist of two separate characters, but one single m-dash.

A different problem occurs in the use of initial capitals in Samples A and B. As you can see, the words it and the appear in capital letters throughout, signalling the beginning of the chapter typographically. Again, as 'human consumers' of the text, this will not cause any processing problems, but for the computer, the, The, and THE are in fact three different 'words', or at least word forms. Thus, even single initial capitals at the beginning of sentences may become issues in identifying and counting words on the computer. We'll talk more about this type of issue in Section 4.4.1, where we'll explore ways of dealing with such features of the text in order to retain relatively 'clean' data.

Collecting and analysing data

When collecting our own data, we obviously need to consider methodologies that allow us to collect the right types and amount(s) of data to answer our particular research questions. This, however, isn't the only type of consideration necessary, but we also need to bear in mind ethical issues involved in the collection -such as asking people for permission to record them or to publish their recordings, etc.and which type of format that data should be stored in so as to be most useful to us, and potentially also other researchers.

When using other people's existing data, there are usually issues in accessing data stored in their specific format(s) or converting the data to a format that is more suitable to one's own needs, as we've just seen above, such as removing unwanted types of information or transforming overly specific information into simpler forms of representation. In this textbook, we'll also look at some of the important aspects of collecting or adapting data to one's needs, as well as how to go about analysing and presenting them in various ways, once a suitable format has been established.

In order to be able to work with electronic data, we also need to become familiar with a variety of different programs, some written specifically for linguistic analysis, some for more general purposes of working with texts. One of the key features of this book is that the programs I'll recommend to you are almost exclusively obtainable free of charge, i.e. so-called freeware. This doesn't mean that there aren't other excellent programs out there that may do some of the tasks we want to perform even better, or in simpler or more powerful ways, but simply reflects the fact that there are already many useful free programs available, and also my own philosophy that we shouldn't need to spend substantial amounts of money just to enable us to do research. This is at least part of the reason why I make most of my own programs available to the research community in this way, apart from the fact that this makes my own research (results) more easily reproducible by others, and therefore caters for the aims of satisfying accountability and academic honesty. For the sake of completeness, though, I'll generally try to at least refer to alternative commercial programs, but without discussing them in any detail.

Corpus linguistics, as a form of data analysis methodology, can of course be carried out on a number of different operating systems, so I'll also try to make recommendations as to which programs may be useful for the most commonly used ones, Windows, Mac OS X, and Linux. Because there are many different 'flavours' of Linux, though, with a variety of different windowing interfaces, I'll restrict my discussions to two of the most popular ones, KDE and Gnome. Unfortunately, I won't be able to provide detailed support on how to actually install the programs themselves, as this may sometimes involve relatively detailed information about your system that I cannot predict. Instead, however, I'll actually try to avoid/pre-empt such issues by recommending default programs that are probably already installed, provided that they do in fact fulfil all or at least most of our needs.

Outline of the Book

This book is organised into four sections. The first section (comprising Chapters 1 and 2) begins with a very brief introduction to the history and general design of corpora, simply to 'set the scene', rather than to provide an extensive coverage of the multitude of corpora that have been created for different purposes and possibly also made available for free or in the form of various types of interfaces. More extensive coverage on the subject, including more theoretical implications, is already provided in books like

The introductory section is followed by an overview of different methods to compile and prepare corpora from available online resources, such as text archives or the WWW. This section (spanning Chapters 3 and 4) should essentially provide the basis for you to start building your own corpora, but also introduces you to various issues related to handling language on the computer, including explanations of different file types you may encounter or want to use, as well as certain types of meta-information about texts.

Section 3 (Chapters 5 to 10) then deals with different approaches to corpusbased linguistic data analysis, ranging from basic searching (concordancing) via learning about more complex linguistic patterns, expressed in the form of regular expressions, to simple and extended word (frequency) list analyses. This part already contains information on how to tag your data morpho-syntactically, using freely available tagging resources, and how to make use of tagging in your analyses. The final section then takes the notion of adding linguistic information to your data further, and illustrates how to enrich corpus data using basic forms of XML in order to cyclically improve your analyses or publish/visualise analysis results effectively.

As corpus linguistics is a methodology that allows us to develop insights into how language works by 'consulting' real-life data, it should be fairly obvious that we cannot learn how to do corpus research on a purely theoretical basis. Therefore, as far as possible, all sections of this book will be accompanied by practical exercises. Some of these will appear to be relatively straightforward, almost mechanical, ones where you simply get to follow a sequence of steps in order to learn how to use a specific function inside a program or web interface, while others are more explicitly designed to enable you to develop your own strategies for solving problems and testing hypotheses in linguistics. Please bear in mind, though, that for the former type of exercise, simply following the steps blindly without trying to understand why you're doing them will not allow you to learn properly. So, as far as possible, at each point you should try to understand what we're trying to achieve and how the particular program we're using only gives us a handle on producing the relevant data, but does not actually answer our research questions for us. In the same vein, it's also important to understand that once we actually have extracted some relevant data from a corpus, this is rarely ever the 'final product'. Such data generally either still needs to be interpreted, filtered, or evaluated as to its usefulness, if necessary by (re-)adjusting the search strategy or initial hypotheses and/or conclusions, or, if it's to be used for more practical purposes, such as in the creation of teaching materials or exercises, to be brought into an appropriate form.

As we move on and you learn more and more techniques, the exercises will also get more complex, sometimes assuming the size of small research projects, if carried out in full detail. As a matter of fact, as these exercises require and consolidate a lot of the knowledge gained in prior sections, they might well be suitable for small research projects to be set by teachers, and possibly even form the basis of BA theses or MA dissertations.

Of course, you won't be left alone in figuring out the solutions to these exercises; both types will be solved at the end of each respective section, either in the form of detailed and precise explanations, or, whenever the results might be open to interpretation, by appropriate comments illustrating what you could/should be able to observe. For the more extensive exercises referred to in the previous paragraph, I'll often start you off with suitable explanations regarding the procedures to follow, and also hint at some potential issues that may arise, but will leave the completion up to you, to help you develop your awareness independently. Furthermore, as real corpus linguistics is not just about getting some 'impressive' numbers but should in fact allow you to gain real insights into different aspects of language, you should always try to relate your results to what you know from established theories and other methods used in linguistics, or even other related disciplines, such as for example sociology, psychology, etc., as far as they may be relevant to answering your research questions. This is also why the solutions to, and discussions of, the exercises may often represent those parts of the book that cover some of the more theoretical aspects of corpus linguistics, aspects that you'll hopefully be able to master once you've acquired the more practical tools of the trade. Thus, even if you may think you've already found a perfect answer to an exercise, you should probably still spend some time reading carefully through each solution.

As this textbook is more practical in nature than other textbooks on corpus linguistics, at the end of almost all chapters, I've also added a section entitled 'Sources and Further Reading'. These sections essentially provide lists of references I've consulted and/or have found most useful and representative in illustrating the particular topic(s) discussed in the chapter. You can consult these references if you want to know more about theoretical or practical issues that I am unable to cover here, due to reasons of space. These sections may not necessarily contain highly up-to-date references, for the simple reason that, unfortunately, later writings may not always represent improvements over the more fundamental works produced in some of the areas covered. Once you understand more about corpus linguistics, though, you may want to consult the individual chapters in two of the recent handbooks, O'

Conventions Used in this Book

In linguistics, there are many conventions that help us to distinguish between different levels of analysis and/or description, so as to better illustrate which different types of language phenomena we're dealing with at any given point in time. Throughout this book, I'm going to make use of many, if not most, of these conventions, so it's important to introduce them at this point. In addition to using these conventions as is done in linguistics, I may also use some of them to indicate special types of textual content relevant to the presentation of resources in this book, etc. Double quotes ("…") indicate direct speech or short passages quoted from books.

Single quotes ('…') signal that an expression is being used in an unusual or unconventional way, that we're referring to the meaning of a word or construction on the semantic level, or to refer to menu items or sometimes button text in programs used. The latter may also be represented by a stylised button text, e.g. . Curly brackets ({…}) are used to represent information pertaining to the level of morphology.

Angle brackets (<…>) indicate that we're dealing with issues related to orthography or spelling. Alternatively, they're also used in certain types of linguistic annotation.

Forward slashes/square brackets generally indicate that we're discussing issues on the levels of phonology or phonetics. Within quoted material, they may also signal amendments to the original material made in order to fit it into the general sentence structure.

Italics are used to represent words or expressions, sometimes whole sentences, that illustrate language materials under discussion. In some cases, they may also be used to indicate emphasis/highlighting, especially if co-occurring with boldface.

Small caps are used to indicate lemmas, i.e. forms that allow us to conveniently refer to all instances of a verb, noun, etc.

Monospaced font indicates instructions/text to be typed into the computer, such as a search string or regular expression.

A Note for Teachers

The relatively low number of chapters may make this book appear deceptively short, and you might be wondering whether it would be suitable for a course that runs for a whole semester of up to 18 weeks; there's no need to worry, though, that you may necessarily have to supplement it with further materials, although this is of course possible.,

The sections and chapters of the book have been arranged to be thematically coherent, but, if you're planning to use it as a textbook in class, you'll frequently find that one chapter corresponds to more than one classroom unit. I'd therefore suggest that, while preparing specific topics, even -or especially -if you may already be an expert in the field, you at least try out the exercises carefully yourself, and then attempt to gauge how long it may take your students to carry them out. If your audience is already highly technically literate and has a strong background in linguistics, then obviously the exercises can be done much more quickly. If, on the other hand, your students are somewhat 'technophobic' or do not yet have a strong background in linguistics, you may either wish to spread the content over multiple units, or set at least some of the exercises as homework. In order to save time, you can also ask your course participants to perform certain preparatory tasks, such as downloading and installing different pieces of software, or registering for online resources, prior to coming to class.

Online Resources

This book also has an accompanying web page, where you'll be able to find some online exercises, links to my own software, updated information about programs or features discussed in the book, etc. The web address for this page is

All my own software is provided under GPL 3 licence, so you can download and distribute it freely. The programs were originally designed to run on Windows, but can easily be used through Wine (

What's Out There?

A General Introduction to Corpora

What's a Corpus?

A corpus (pl. corpora) is a collection of spoken or written texts to be used for linguistic analysis and based on a specific set of design criteria influenced by its purpose and scope. There are various, and sometimes conflicting, definitions in the relevant literature (c.f. e.g.

Although, theoretically, corpora can simply consist of texts that are in nonelectronic form, and indeed some of the earliest corpora were just collections of index cards or slips of paper (McCarthy & O'Keeffe 2010: 4), these days, almost all corpora in use are computerised. When we talk about corpora from now on, we'll thus always be referring to computerised ones, unless otherwise stated.

Corpus Formats

Most corpora -unless they're only accessible through an online interface -are stored in plain-text format (to be explained in more detail in Section 3.2.3) and can therefore easily be viewed using any basic text editor, but if a corpus example contains phonetic transcriptions, then of course a specialised typeface (font ) may be required in order to view it. Complications may also arise if the character set is not directly supported by the computer the corpus is viewed on. This may for example happen when the corpus is in a language that uses a different alphabet from the standard Western European ones that are supported on all computers by default. Even displaying European languages, such as Greek, may represent a (minor) problem, but the real difficulties start when one wants to work with East Asian character sets (such as for Chinese, Japanese, Korean & Vietnamese), Indic languages (such as Hindi or Urdu), or right-to-left languages like Arabic and Hebrew. For a simple simulation of this, see the online resources at:

Another way to store a corpus is to save it into a database. This makes it more difficult to view and process without having the appropriate database management system (DBMS) installed or if a web-based online interface isn't working as expected, due to browser issues or download speed restrictions. On the other hand, though, this makes it possible for the basic text to be linked to various types of data-enriching annotations, as well as to perform more complex search operations, or to store intermediate or final results of such searches for different users and for quicker access or export later. We'll experience the advantages of this when we set up/work with accounts for access to some web-based corpus interfaces, such as BNCweb or COCA.

Synchronic vs. Diachronic Corpora

Corpora can be designed and used for synchronic (i.e. 'contemporary') and diachronic (i.e. 'historical'/comparative) studies. Different issues may apply to the design of these two types of corpora. For instance, historical corpora may contain old-fashioned or unfamiliar words and spellings or a large number of spelling variants (e.g. yeare, hee, generalitie, it selfe, etc.), as well as possible even characters (letters) that no longer exist in a modern alphabet, such as the Old English thorn (þ), which we've already encountered in the above Beowulf extract.

Historical corpora are also by nature restricted to written materials because there just are no recordings of native speakers of Old or Middle English in existence. Furthermore, the restriction does not only apply to the types of material available but also to the amount of data we can draw on because, in former times, there simply wasn't such a wealth of documents available, and from as many different sources as we have them today.

'Early' synchronic corpora

Another major distinction between different types of corpora is whether they comprise spoken or written data. This is an extremely important distinction because written language generally tends to be far easier to process than spoken language, as it does not contain fillers, hesitations, false starts or any ungrammatical constructs. When creating a spoken corpus, one also needs to think about whether an orthographic representation of the text will be sufficient, whether the corpus should be represented in phonetic transcription, or whether it should support annotation on various different levels (see Chapter 11).

Initially, computerised language corpora tended to contain only written language, which was easier to obtain, and presumably also deemed to be more important than spoken language, a notion that unfortunately still seems to be all-too-prevalent in our often 'literature-focussed' society and education.

Written corpora

Let's start our investigation into the nature of corpora with a look at some of the earliest written ones, accompanied by a short exercise to sensitise you towards certain issues. At the time these first corpora were created, one million words still seemed like a huge amount of data, partly because computers in those days had a hard time handling even this amount, and partly because no-one had ever had such easy access to so much language data in electronic form before.

Table

A complete set of all manuals of corpora distributed by the ICAME (International Computer Archive of Modern and Medieval English) can also be downloaded from

Exercise 1

Table

What kind of language would you expect inside the different categories, and can you identify anything particularly interesting regarding them?

If you're already planning a research project, do you think data from these will fit your particular needs and help you to answer your research questions?

Once you've done this, also open some of the links to other manuals given in Table

You may have noticed that some letters are missing from the categorisation scheme, notably I, O, and Q. This is probably because the uppercase letter I can easily be confused with the number 1 or lowercase l, and uppercase O with the number 0. I have no logical explanation why Q is not used, unless the assumption is that Q is similar to O, and hence the same options for confusion may arise.

Spoken corpora

Next, let's take a look at a selection of important spoken corpora to develop a better understanding of where the differences between written and spoken corpora are.

Exercise 3

Open the ICE website and navigate to the 'Corpus Design' page. In light of the information you've already come across for individual written and spoken corpora, try to evaluate how similar/different the composition of the ICE corpora is to/from these 'traditional' corpora.

Modern mega corpora and national corpora

With the use of corpora becoming more popular, and techniques for data analysis improving, researchers soon realised that corpora of one million words were not nearly large enough for observing all interesting linguistic phenomena, especially not those that involve idiomatic structures or collocations (see Chapter 10). Especially for investigating rarer features of the language, the basic notion thus seems to be 'the bigger, the better', and thus researchers, often supported by publishing houses who wanted to create better dictionaries or textbooks, began to compile corpora of 100 million words or more. Such corpora, due to their rather large size, are of course more difficult to process on our own computers, and may not even be easy or affordable enough to obtain for individual research on a smaller scale. However, the latter issue is often not such a big problem after all because most openly accessible mega corpora these days provide online interfaces that users can sign up for free of charge. These interfaces will be covered more extensively in later chapters.

As we already have a basic understanding concerning the composition of the earlier, much smaller, corpora, we can now try and develop a basic understanding of how large-scale mega corpora may differ in that respect, and what the advantages in this may be, apart from simply having more text from different genres. This will also help to prepare you for working with them later. In order to do so, let's take the BNC as an example and find out where some of the more significant differences may lie, and why using it may represent an advantage over simply working with the smaller earlier corpora that may be much easier to process on our own computer.

Exercise 4

Open the BNC website and read through the descriptions.

As before, try to understand in which way it may be similar to/different from other corpora, and where the advantages in this may lie.

As should be obvious from Table

The creation of the BNC also set an example for other countries to pursue the development of their own national corpora, some of which are directly modelled on the BNC. Such corpora exist for example for Polish (PELCRA), Czech (CNC), Chinese (Modern Chinese Language Corpus: MCLC), and Korean (Sejong/Korean National Corpus: KNC), to list but a few. For a more extensive overview of these, see

Examples of diachronic corpora

Since we have already covered some of the more important aspects of diachronic/ historical corpora in conjunction with representing texts, and are mainly concerned with applied rather than historical aspects of corpora here, we will not discuss this type of corpora further at this point. Thus, Table

General vs. Specific Corpora

Apart from the distinctions noted above, we can also draw another one, namely to distinguish between corpora that are compiled for general purpose research and such that are highly domain specific. The former are deemed representative of the whole language and generally to be used for a wide variety of different research objectives. The latter, in contrast, are often only of limited use to the general public, but may also sometimes be useful because they can highlight particular differences between standard language and specific registers, etc. An example of a domain-specific literary corpus would be the collected works of an author, which can be used to investigate the style of this particular author, or even to verify disputes about the authorship of a piece of literature where this may be contentious. Related to this are the types of specific corpora of witness and police officers' statements in forensic linguistics discussed in

Examples of specific corpora

To some extent, we've already seen examples of specific corpora when we looked at some of the spoken corpora currently available. In recent years, there's also been a growing interest in many other types of specific corpora. Among these, we'll only discuss two specific types here, academic and learner corpora.

Academic corpora

Academic corpora deal exclusively with language produced in academic contexts, i.e. English for Academic Purposes (EAP), a special type of English for Specific Purposes (ESP). They may consist of transcripts of academic lectures and seminars, various types of writing produced in a university context, but also sometimes include other academic activities, such as meetings or advisory/supervision sessions. They thus tend to reflect the speech of both experts and non-experts in the field of academia. Table

The basic idea behind creating and exploiting academic corpora is to be able to extract samples of expert and non-expert language use in academic settings in order to be able to investigate the nature of academic speech and writing, and make suggestions for teaching or best practice in academia.

Learner corpora

In contrast to the academic corpora introduced in Section 2.4.1.1, learner corpora, as their name implies, do not contain materials produced by experts in a field, but instead by students at different levels and stages of language acquisition, often restricted to non-native speakers, i.e. L2 learners, of a language. Occasionally, though, we can also find corpora of L1 learners, i.e. native speakers of a language. These are often created and used for comparison purposes to investigate differences between L1 and L2 learners, but may also be employed to explore different stages of development in the native language. Table

Pragmatically annotated corpora

Pragmatically annotated corpora, i.e. corpora that are annotated for speech acts or other pragmatically relevant information, are still relatively rare. Many of them are also task-oriented, i.e. deal with relatively limited topics and domains, so that they're often not very representative of general spoken interaction. However, despite this limitation, they can already provide us with valuable insights into some of the general mechanisms involved in natural spoken language exchanges. Table

Static Versus Dynamic Corpora

One further distinction we can make between different types of corpora is between those that are fixed in size and finalised in that they're never intend to change (i.e. static) and more dynamic types of corpora, which are explicitly designed to change over time and to keep on reflecting the ever-changing nature of language. We can refer to the former type as snapshot corpora, whereas the common term for the latter is monitor corpora. By this definition, in fact, almost all the corpora discussed above are snapshot corpora. Even the diachronic ones are, because they've not been designed to be added to later, even if, for example, at some point in time a further Old English epic may somehow be unearthed and could thus theoretically be included in a new edition of the Helsinki Corpus. To date, there are only two real monitor corpora in existence, the COCA and the Bank of English. The latter has not been covered earlier, as it's not directly accessibility to outsiders, although it forms part of the Collins WordBanks Online

A genuine monitor corpus, as pointed out above, makes it possible to continually compare a language in its different stages of development, including the most recent changes, in and through one single corpus. An alternative solution to tracking change diachronically is to use static corpora, such as the LOB and the FLOB, which were designed to reflect the same categories of language, but using samples whose production dates were 30 years apart.

Other Sources for Corpora

Apart from some corpora that are made freely available on the web, there are also a few institutions that distribute corpora at a certain cost. The cost usually depends on whether or not someone wants to use the corpora commercially. Obviously, licences for commercial use tend to be a lot more expensive than for private or educational use, but unfortunately some corpora may still seem prohibitively expensive for the individual. Some major sources for corpora are:

Solutions to/Comments on the Exercises Exercise 1

First of all, you'll probably note that there may be some unfamiliar or almost 'archaic-sounding' labels among the categories. For instance, without looking up the terms or taking a look at the detailed list of texts, would you have known immediately what, for example, such category labels as 'Popular Lore' or 'Belle Lettres' refer to? Religion, with 17 texts, also seems to play a fairly dominant role. Do you think this reflects modern-day practices/interests, and illustrates contemporary language in a suitable manner?

When comparing the information in the manuals, you'll hopefully spot that the composition of the different corpora is roughly modelled on that of the Brown Corpus, with only small variations in categories and numbers of sample texts.

The slight cultural difference here is that what is originally labelled "Adventure and Western Fiction" in Brown has been adapted to "bush fiction".

Exercise 2

If you, for instance, compare the categories of the SEC to those of the Brown Corpus, you should be able to see that there are certain parallels in that, for example, both corpora try to cover press materials, such as reportage and commentaries, religion, literature, and learned materials, to some extent, although of course the 'angle' is different because of the differences in the medium. For instance, the treatment of the topic of 'Religion' (D) in the Brown Corpus is generally of a more scholarly or esoteric nature, whereas the category 'Religious Broadcast' (E) in the SEC purely consists of religious services, rather than scholarly discussions of religious issues, and category K (General Fiction) in the Brown Corpus consists of fiction texts treated as texts to be read, whereas 'Fiction' (G) in the SEC is perhaps unusual in the sense that it covers written materials that are simply presented as read aloud, rather like modern-day audio books. There are, however, also some categories that are exclusively reserved to one type of medium, such as, for example, the category 'Dialogue' (J) or the two 'Lecture' categories

To faithfully represent spoken language in most of its facets, it's minimally important to include prosodic information, such as pauses, and pitch movements, along with information about who's speaking at which time, whether there's overlap, etc. Although, as in the case of the LLC, spoken corpora are generally represented in orthographic form, all aspects of verbal behaviour, including 'nonwords', such as fillers or even laughter, etc., need to be represented as accurately as possible, as they may be relevant to the communication. Representing the data in this way also makes it possible to investigate vocabulary, (morpho-)syntax, as well as pragmatic or semantic features of spoken language in combination with more phonological or interactional ones.

Exercise 3

First of all, it should be immediately clear that the ICE corpora, despite following the '1 million-word model', are very different in their composition from the corpora we've discussed before in that they contain both written and spoken language, with an unusual (positive) emphasis on the latter, as 300 out of the 500 texts are spoken. The other noticeable feature is that there's stronger balance in the materials in that the spoken parts distinguish between public vs. private or scripted vs. unscripted speech, and that the written parts are differentiated into different levels/abilities and types of writing.

Exercise 4

Essentially, the fact that the BNC is a mega corpus can easily be seen in the sheer number of words it contains. Although there are still 9 times as many words in the written materials, the fact that it already contains 10 million words of spoken language makes it far more representative of the spoken language actually produced in Britain at the end of last millennium, covering a wide variety of public and private contexts. Furthermore, the design principles try to distinguish clearly between which types of language are produced by a majority of speakers of the language, and which ones are predominantly received and are therefore highly influential, despite the fact that few speakers of the language would ever produce them, such as, for example, novels or other pieces of more elaborate writing. One additional highly important attribute of the corpus is the fact that it contains a very large amount of meta-information, i.e. information about who produced which type of document(s) and in which contexts.

Understanding Corpus Design

Food for Thought -General Issues in Corpus Design

In this chapter, we'll raise some issues that are often heavily debated by experts in corpus linguistics, but have as yet unfortunately not been solved to any degree of satisfaction. Most of these tend to be related to the construction of large general corpora, though, something a short textbook like this cannot really teach you, so that we'll focus mainly on creating your own, specialised, and generally much smaller corpora. The main reason why we nevertheless need to discuss these points here is that you ought to be aware of the possible shortcomings of corpora in order for you to be able to anticipate any potential problems for your analysis or classroom use. Apart from this, though, we also want to develop an understanding of what (electronic) texts really are and how they can best be stored on the computer to facilitate analysis and exchange of data.

As we've seen before, the very first electronic corpus, the Brown Corpus of written American English, contained 500 (written) text samples of 2,000+ words, and its size and composition set a standard for the compilation of new corpora for many years. For a long time after the publication of the corpus in 1964, many people thought that 1 million words of text represented a general-enough sample to provide sufficient information about the makeup of the English language. However, over the years, researchers have realised that even this type of size and stratification may not be sufficient for performing certain tasks -such as research in lexicography or collocations (see Chapter 10) -efficiently, and hence started devising ways of creating a variety of different types of corpora, representative of both spoken and written language, and of varying sizes, ranging from very small specialised corpora to some that comprise many millions of words.

Sampling

One very important issue in putting together (compiling) a corpus is to determine what exactly should go into it. This may depend on a variety of factors, such as availability, the potential for obtaining permission for copyrighted data, how many people are actually working on creating the corpus, etc. For building corpora of older forms of language -such as Old or Middle English -, natural limitations exist in that there are limited numbers of texts available, as well as the fact that these obviously can only exist in written form. For modern corpora, however, and especially with the advent of recording technology, the choice of materials has become much more difficult. Here, we need to consider how we want to obtain our data in the first place. Do we still want only written language, such as for the first generation corpora, or do we want to include spoken language in phonetically or orthographically transcribed form, too? Or do we want a fully-fledged spoken corpus that will never be published in any general written form?

In terms of what should be included in a corpus,

The BNC, as an example of a modern mega corpus, attempts to strike at least some kind of balance between spoken and written materials, although the percentage of spoken materials (10%) is still rather low, something which possibly exemplifies an unfortunate continuing dominance of written language in linguistics. On the other hand, maybe keeping the amount of spoken data in the BNC relatively low was actually not too bad an idea, since transcribing spoken language is an expensive and time-consuming business, and one where corpus compilers often take too many 'shortcuts'. In the case of the BNC, this can unfortunately be seen rather clearly in the quality of some of the transcriptions, where, for example, many apostrophes have ended up in the wrong places or gone missing, so that some plural markers are turned into a genitive {s}, or the contraction we're turned into were at least 6 times within a single dialogue (<bncDoc id=D96>).

Within the written section of the BNC, there's a 75:25 balance between 'informative' and 'imaginative' prose, where the latter also includes a certain amount of 'written-to-be-spoken' materials, i.e. speeches, plays, etc. The spoken part consists of a 'context-governed' section, sampled from public recordings, etc., and a 'demographic' one, comprising recordings made by private individuals who carried tape recorders with them for a period of two days, respectively.

The sampling distribution of the COCA

Size

The size a corpus ought to, or can in fact, have depends on a few different factors. First of all, as we've already said, there may be limitations in terms of the amount of material that is available, in which case it may be necessary to be content with whatever data one can obtain. Apart from 'natural limitations' -such as for corpora of older variants of language -some limitations may be imposed by funding. This often raises the issue of quantity vs. quality, for which there seems to be an unfortunate tendency towards the former at the cost of the latter, especially for the larger corpora of 100 million words, such as the BNC or the ANC. On the other hand, though, if a corpus is too small, it may not be very useful for general purpose research because the amount of data needed to conduct research into, for example, collocations (the habitual co-occurrence of words; see Section 10.5) apparently increases exponentially (c.f.

Balance and representativeness

Two further issues in the compilation of corpora are those of balance and representativeness. In principle, the former only applies to corpora for general use, though, as it's often assumed that these ought to provide an equal amount of materials from many different genres or areas of relevance that allow us to investigate a representative and realistic sample of the language and draw more or less universally applicable conclusions. Obviously, this is an aim that's very hard -if not impossible -to achieve. In order to do so, in the past corpus compilers have frequently resorted to using samples of approximately 2,000 words from different texts and genres to achieve balance, but both that number and 'cutting out' part of a text appear fairly arbitrary, and may in fact make such samples somewhat unrepresentative of the texts they're supposed to represent as a whole. For instance, while the beginning (Introduction) of a research article may be very similar to its end (Conclusion) in that both discuss the background and aims of the article, the 'middle parts' that describe the actual research are normally very different, and therefore arbitrarily picking either text from the beginning/end or the middle will potentially provide a very skewed picture of the language of research articles.

There is no virtue from a linguistic point of view in selecting samples all of the same size. True, this was the convention in some of the early corpora, and it has been perpetuated in later corpora with a view to simplifying aspects of contrastive research. Apart from this very specialised consideration, it is difficult to justify the continuation of the practice. The integrity and representativeness of complete artefacts is far more important than the difficulty of reconciling texts of different dimensions.

Representativeness, albeit also difficult to measure, may be more easily achievable, especially for domain-specific corpora or limited fields of investigation, because often there are relatively clearly definable criteria for what represents a certain genre of text or domain. As we've seen earlier, though, the composition of the first 1-million word corpora roughly consisted of samples of the 16 genres listed in Table

Legal issues

Deciding what exactly you'd like or need to include to make it useful isn't the only thing you need to bear in mind when constructing your own corpus. There are also a number of legal points you ought to consider when making decisions about which data to incorporate. The most important one for written or multimedia data (e.g. transcripts of televised materials or radio broadcasts, etc.) is that of copyright, which may well differ from country to country, so that it's difficult to provide definite rules. In the US and EU, the general rule for written published works is that the copyright expires 70 years after the death of an author, unless it has been transferred to someone else (e.g. the author's heirs). In the US, works published before 1923 also no longer enjoy copyright protection. In other countries around the world, the situation may either be handled in a more relaxed or, in contrast, even harsher way, so it's always advisable to enquire about the exact copyright situation of the country in question, especially if you later want to make your corpus available to other researchers around the world. Some countries also recognise the concept of fair use, which allows relatively insubstantial parts of copyrighted materials to be used for purposes such as research, education, review, etc. (Wikipedia: Fair Use), although, in practice, this will probably not allow you to include sufficient amounts of text or other materials in your corpus.

Although it's become common practice in recent years to use materials from the internet (see Section 4.2), according to

When collecting spoken materials that have not been pre-recorded by someone else, it's also important to note that, in many countries around the world, it's in fact illegal to record someone surreptitiously, i.e. without first obtaining their consent. It's therefore advisable to, whenever possible, let everyone you're planning to record sign a consent form in order to avoid any legal complications later, as well as to give them a chance to refuse in the first place.

Having covered the basic theoretical and legal aspects of creating corpora, we'll now turn to the structural nature of texts and other associated properties that may exist in the form of meta-data (e.g. additional information about the text, etc.).

What's in a Text? -Understanding Document Structure

When we read ordinary printed documents, such as books, we mainly concentrate on their text content, and often -though by no means always -tend to ignore that they may in fact contain a number of additional pieces of information apart from the main text. These occur in other parts of the document that precede or follow the main section, and are generally referred to as front and back matter, at least for longer documents. The content of these sections represents meta-data, i.e. additional data about the text, but does not form part of the original text. An example of the meta-data that you encounter in everyday life would be the imprint page inside the front matter of a book, where you find the title of the book, the author, the publisher, edition, year of publication, its ISBN, the typeface and size it has been set in, as well as many other types of information that are mainly independent of the content per se. Another type of meta-information is represented by a table of contents (in the front matter) or an index (in the back matter) of a scholarly book, where the meta-information serves as a kind of navigational aid in accessing individual parts of the book, and where the information is clearly linked to the content -or organisation thereof -itself.

Although all this is interesting information which is conventionally represented inside the document, as well as affecting its pagination/layout, it generally does not form part of the meaning potential of the text itself, which is what we're usually most interested in when we analyse texts from a linguistic point of view. Thus researching or making use of meta-information for instructional purposes normally doesn't make much sense because it represents language data (in the widest sense) from highly limited/restricted domains. What it may, however, allow us to do is to make a conscious choice on how to restrict the language samples we may wish to analyse to, for example, a specific period in the historical development of the language, or the style of a particular author, etc. We'll see examples of how this may be achieved later on when we'll for example create subcorpora from the BNC in BNCweb (see Section 9.3.2).

Headers, 'footers' and meta-data

In 'linguistic' data, such meta-information is often stored either externally in a different file or database, or inside the document itself. In the latter case, it's usually contained in something called a header (cf.

In some files, there may also be some additional information that appears after the main body of the text (which we could correspondingly refer to as a 'footer'), so that it's best to check the beginning and the end of a text for information that's not part of the main text of the book.

Let's get a first impression of this separation of texts into meta-data and content as an exercise by looking at the source of a web page.

Exercise 5

First open

Then, if you're using Firefox or Google Chrome, or Konqueror on Linux, press 'Ctrl + u' to display the HTML source of the document. In Safari for Windows, you need to press 'Ctrl + Alt + u', while on the Mac, you can press 'alt/option + + u'. In other browsers, such as Internet Explorer, you may need to find an item on the 'View' menu that allows you to see the page source, as there may not be a shortcut defined.

Another option for accessing the HTML source code in all browsers is generally to trigger the context menu by right-clicking (Windows/Linux) or tapping with two fingers in a blank area of the page (Mac), and then selecting 'Show Page Source', 'View Document Source', or any other relevant option.

For the moment, simply try to identify the 'head' and 'body' sections of the page and see whether you can possibly also understand which type of meta-information the different parts of the header may relate to. Don't be alarmed if you see a lot of coding inside angle brackets (<…>) that you don't understand yet. We'll learn more about this a little later.

The structure of the (text) body

Just like we frequently ignore front and back matter to some extent, we may also 'overlook' -i.e. not consciously pay attention to -the fact that texts have a specific organisational structure and are generally -or at least may be -made up of chapters, sections, sub-sections, and paragraphs. Among the paragraphs, we also encounter one special type, that of the heading, which acts as a kind of guide to the particular chapter or (sub-)section the individual paragraphs occur in.

Headings fulfil multiple functions in a text. The first of these is that they act as a means to reflect the hierarchical structure and logic of the text. In other words, they illustrate how the author has chosen to (best) organise the material under discussion. In the simplest case, such as in a novel, this may just involve breaking the text into chapters and labelling them Chapter1, Chapter 2, Chapter 3, etc., with a slightly 'more advanced' situation being that the individual chapters may also be given a title. The function of the title is in some way to act as a summary, or some other form of indication, of what the individual chapter or section contains. For any kind of scholarly publication, having titles for chapters, sections, sub-sections, etc., is in fact the norm, and here the title not only acts as the 'summary' but its content is often also repeated somewhere in the immediately following paragraph -usually even the very beginning, if we have a so-called 'topic-sentence' starting the paragraph.

This of course introduces certain redundancies in terms of an analysis of the language used because some of the vocabulary may be repeated without really contributing any additional meaning, and we might therefore sometimes want to get rid of headings for some types of vocabulary analysis. On the other hand, this very redundancy may help us to classify -or even identify the exact genre ofa text better. This is because, due to their occurrence both in headings and the text, some of the key words that we find in the headings may occur with a higher overall frequency in the text, and thus help us to 'summarise' the content. We'll explore ways of investigating this phenomenon in Chapter 9.

Therefore, it really depends on the exact purpose of our analysis, and on our own awareness of these features, whether or not we might want to keep or remove headings. Incidentally, the level of redundancy would increase even further if we analysed the whole text, including the front matter, because there the headings might show up once more inside the table of contents (TOC) of the document, where their meaning is 'purely' to serve as a navigational aid by listing them side by side with their respective page numbers. On yet a different level, e.g. when analysing student or professional writings, the presence or absence of a suitable number of headings -however that could be defined -may well present an evaluation criterion as to the proficiency of writers and their ability to deal with a topic efficiently enough to present it in a logically structured manner, so it may well be worth investigating.

Now that you already have a pretty good idea of what types of features you might encounter in a text and how this relates to aspects of its organisation, as well as issues that may affect its analysis, let's move on to finding and processing some data in electronic form as a first step in laying the foundation for creating your own corpus and analysing it.

Although we all encounter a number of different file formats containing text on a daily basis while using the computer, many people generally tend not to be aware of the fact that the text contained in these files may not always be easy to process. The reason for this is that it's often stored in a particular proprietary format that's only 'understood' by programs designed for dealing with this particular type of file. It's thus often only if we happen not to have such particular programs installed on our computer that we actually realise that there's anything special to these types of files. Why we might need such special programs for displaying the text is because we often want to be able to render it with special types of formatting, such as italics, boldface, etc., use a particular layout, or that we want to be able to generate a table of contents automatically in a word-processing application, where this is based on the headings inside the document. Any document that purely contains text (plus potentially some form of markup, see Chapter 11), is generally referred to as a plain-text document and should be readable (and writable) with any basic text editor (see Section 4.2.1 for a more in-depth discussion of these). Some of the most common file formats containing text which may be of interest to us here are listed in Table

Exercise 6

Use the links in the online materials on the page 'Understanding File Formats & Their Properties' to go through each of the examples of different types of 'text' documents above and see how easy/difficult it is to identify where the actual text is.

If you do have the relevant programs installed on your computer, you can also try to create more complex versions, containing more text and formatting, of the different document types yourself and then investigate them.

Exercise 6 has hopefully already alerted you to the fact that it isn't easily possible to just use any document that we can read in some way on the computer equally well as a source for our corpus-linguistic analysis, simply because it contains text.

Instead, what we've seen is that it's often also necessary to understand the nature of how that text has been produced to some extent, or which particular features the program that was used to produce the text may offer. In addition to this, if we want to be able to exchange documents efficiently, we also need to develop a basic understanding of how exactly text may be represented on the computer, which is what we'll do next.

Understanding Encoding: Character Sets, File Size, etc.

In our discussion of file types, and in Section 2.3 when we discussed issues of encoding for diachronic/historical corpora briefly, we've already seen that not all forms of textual representation are equally useful for corpus analysis. For instance, we saw that if we want to treat a Word or PDF document as a corpus file, we first have to extract its textual contents to a plain-text file in order to be left with any amount of usable text. The reason for this was that the formatting and layout options contained in documents of these types are usually not stored as plain text, but instead are often binary coded, and therefore we have no (easy) way of dealing with these.

ASCII and legacy encodings

The problem we have with encodings is one of a similar nature, only that in this case it doesn't have anything to do with proprietary formats, but rather the way that individual characters are physically represented inside the computer, which is as a special type of number. For a long time, most of the basic Latin(-derived) characters used to be represented on the computer as sequences of bits up to one single byte (= 8 bits) only. This made it possible to represent and store up to either 128 (7 bits; 2 7 ) or 256 (8 bits; 2 8 ) characters, where each character was assigned a specific number in the original ASCII (American Standard Code for Information Interchange; 7-bit) or Latin1 (8-bit) character sets or their derived forms. Within these character sets, the 'ordinary' letters of the Latin alphabet are encoded by a single number each, but with a distinction between upper and lowercase ones, where the uppercase ones range from position 65 to 90 and the lowercase ones from position 97 to 122. Punctuation marks, numbers, and mathematical operators occupy the intermediate ones between 33 and 63.

However, there used to be major differences between the representations of the upper ranges of characters and the individual requirements for them for the different, simpler, Western character sets, and it proved impossible to even attempt to store the huge numbers of characters necessary for writing in non-Western languages, such as Chinese. This is also why, even if we're producing a document that only contains characters of the English alphabet, plus a few special characters, such as special types of (typographic) quotation marks, etc., and save this in one of the so-called legacy encodings (e.g. ANSI/Latin1 for European languages, GB5 for traditional Chinese, GB2312 for simplified Chinese, EUC-KR for Korean, to name but a few), someone reading this on a computer that uses a different character set by default may see some, or sometimes even all, characters misrepresented.

Exercise 7

To understand the issues caused by using different character sets, and viewing them with the wrong encoding, better, let's take a look at the page on encodings in the online materials at

Unicode

In order to solve the discrepancy between character sets, first additional doublebyte character sets were introduced, but eventually the notion of one single character set for all processing needs was put forth and implemented. This 'umbrella' character set is referred to as Unicode, which, despite the unifying attempts, still exists in a number of different flavours that use fixed or variable byte length to encode thousands of characters. The particular flavour we'll want to use for this course is a variable byte encoding called UTF-8, which stores characters in up to six bytes, but generally has the advantage that the basic Latin characters appear at exactly the same positions as in the simple encoding discussed above, so that opening an English text in UTF-8 would essentially be the same as opening it in ASCII or Latin1. This character set is also well supported by many different programs, including browsers, so that it makes information exchange including characters from many different languages, as well as special characters, such as phonetic symbols, etc., much easier, thus also facilitating the creation of multilanguage corpora.

File sizes

Plain-text files in general tend to be much smaller than other files because representing characters, even if some of them may take up six bytes in UTF-8 in some cases, does not require much space. As pointed out in Section 3.3.1, for most characters appearing in an English text, one single byte will be enough. Shorter plain-text files are therefore generally very small, sometimes even less than a kilobyte (kB; 1kB = 1024 bytes). Based on a small selection of four literary files we'll download and analyse later, I tested the approximate ratio of words per kilobyte, which appears to be around 180, so that per 1,000 words we may want to collect for our own corpora, we'd probably require about 5.5 kB of text. In order to collect this much -or rather, little -text, we'd probably need the equivalent of about 2.4 to 3 pages of scanned text, as the number of words in texts roughly varies between 250 per page for double-spaced texts of average font size (i.e. 12 pt) in printed books such as novels, and maximally about 600, which I found in a monospaced article from a scientific journal where the print size was around 10 pt.

This text size doesn't increase dramatically, even if we add other types of enriching information -generally referred to as annotations (see Chapter 11) -to our files, provided that these annotations are also stored in plain-text form, and aren't excessive. To verify this in a very crude manner, I ran another test by comparing the raw and annotated files for my home page (in HTML), one dialogue annotated on a number of linguistic levels by one of my own programs, and one dialogue from the BNC, which contains a rather large amount of meta-information in its header and extensive word-level annotation. It turned out that the web page was only 1.2 times as large as the original raw text contained in it (6 kB: 7 kB), the first annotated dialogue containing minimal meta-information and my own annotations was only approximately 3 times as large (2 kB: 6 kB), but the BNC file was more than 11 times the size of the original source text (30 kB: 340 kB). However, even though the latter two values may already seem relatively high, this is still fairly small compared to the file sizes of some of the proprietary document formats we encountered in Section 3.2.3. We'll explore plain-text-based file formats, such as HTML and XML, that may contain such markup further in later sections, as well as discussing their use(fulness) for linguistic annotation/analysis. Now that you have a basic idea regarding the formats and encodings a text might come in, and the issues involved in being able to work with them, we can move on to finding out how we can obtain our own texts for analysis purposes. As we go about investigating various sources, we'll also learn more about making the texts contained in the different types of documents more 'amenable' to analysis. Perhaps one more thing is worth mentioning before we move on, though, which is that sometimes, if you use special programs or corpus data that other people have collected, you may occasionally encounter files (or file types) with uncommon extensions that are not recognised by any other programs. Due to this fact, if you simply try to open these files by (double-)clicking on them, you'll usually get some form of message saying that your operating system doesn't recognise this particular file type, and asking you to identify a program to open the file with. In most cases, this should not be a problem, though, because, as pointed out earlier, most corpus data is stored in some form of plain text, so you can always try opening these files in your editor first. In case this fails, you then either have the choice of trying to find the program used to create the files or, alternatively, simply not to use these files.

Solutions to/Comments on the Exercises Exercise 5

Looking at an HTML page for the first time may be somewhat bewildering, due to the strange bits of code that are generally contained within angle brackets (such as e.g. <p>), which seem to have nothing whatsoever to do with what the text is all about. Furthermore, most HTML page sources do contain a fair bit of information before the actual start of the text, which is signalled through the <body> tag, so most of what precedes it should be considered 'non-text'. Don't worry, though, if this all still looks like a foreign language to you -you'll soon learn to understand this better, at least as far as you need to in order to be able to make use of the text contained inside an HTML document.

You may well notice that the header (<head> in HTML) generally doesn't really contain any part of the text itself, but simply stores meta-information, i.e. information about the text or relevant to how the browser is supposed to handle the page, for example, in which way to display it. Header elements may for instance be the page title (contained in the <title>…</title> tag, where the forward slash indicates the end of the tag in the closing part), which then appears in the title bar of your browser, or possibly meta-information (<meta>…</meta>), such as the text (content) type or encoding/character set (charset), as well as style (sheet) information/references (<style>…</style> or <link rel="stylesheet" href="./corpus.css" type="text/css" />, in our case) responsible for some of the page formatting.

Exercise 6

When looking through the examples, you'll hopefully realise that there are some document formats that allow you to see the text contained in them quite easily, while others contain too much additional coding describing the layout or formatting of the content to be able to easily detect/identify the textual content. As almost no programs for corpus analysis can deal with documents in 'graphical' formats, such as PDF, or proprietary formats, such as MS Word, the only logical choice for working with corpora is to use either plain text or other types of documents that contain minimal or easily recognisable annotations, such as HTML or XML documents.

Exercise 7

This brief exercise should have demonstrated to you how tricky it may be to work with the wrong encoding, especially when our data may contain characters from a number of different languages. However, what you've just seen on the exercise page is not only an issue in corpus linguistics, but actually represents a much more prevalent problem on the internet. This is because many people still only produce web pages, mainly through easy-to-use, but badly configured programs, in their own local encodings, which is absolutely fine as long as they only use these pages to 'communicate' with other web users in their own country, who are likely to have their computers set to the same code page. When those pages, however, are then opened on a computer in another country, and which is set to a different code page, the result may look very similar to, or even worse than, the result we get when we set the exercise to any encoding that is different from UTF-8.

Sources and Further Reading

Finding and Preparing Your Data

Having the right kind of data to work with is essential in doing any kind of linguistic analysis. However, even the best and most appropriate data may not always fulfil all your needs, or may contain bits of information or formatting that make it difficult to work with. Apart from this, the programs we can use for such an endeavour are not always able to handle all the data you may think suitable, due to some of the above issues. This is why, in this chapter, we'll first explore where to find some basic materials for language research, and then go on to discuss which types of formats may be suitable for language analysis, as well as how we can try, to the best of our abilities, to bring our data into such a format. The data we'll be working with here may or may not be similar to the kind of data you'll be interested in working with yourself, but mainly serves for illustrative purposes, so that you can learn the right preparatory and analysis techniques. Once you start compiling your own data later, you'll obviously need to make your own decisions regarding which data exactly suits your research purposes, and also how much to collect in order to get a representative sample that may reflect all or a specific sub-part/genre/text type of the language you're trying to describe. This obviously requires careful consideration and also to develop at least some initial hypotheses about what you'll encounter in which type(s) of data, which techniques to apply, etc. Therefore, this preparatory process should not be taken lightly, especially because corpus compilation and preparation, if done well, is a very time-consuming effort. And thus, the more time you waste on collecting data that's unsuitable for your purposes, the more time you lose for actually interpreting this data in a linguistically meaningful way, which is, after all, still the most important part of the analysis.

Finding Suitable Materials for Analysis

Retrieving data from text archives

Text archives are internet-based repositories of literary and non-literary texts that have usually been scanned from the original sources and can be retrieved in a variety of different formats or encodings. Many -if not even most -of the texts available through such archives are free of copyright or available under academic licences, but, as pointed out in Section 3.1.4, before you publish anything based on such a text, it's usually advisable to inform yourself about any potential issues, such as how to acknowledge the source of your materials or whether there are any restrictions concerning re-distribution, etc.

Perhaps the two most important text archives for linguists or literary scholars are the Project Gutenberg website (

Obtaining materials from Project Gutenberg

Project Gutenberg is a large repository of texts in many different languages. These texts were/are essentially created by volunteers who scan or type in the materials, thus converting them into an easily readable electronic format, without any special formatting or layout. What this format may look like, and which advantages and issues this form of presentation may have in store for us in terms of learning about language, and its representation in electronic form, is something we'll be exploring throughout this section. For some initial practice, let's start by downloading a text from the Project Gutenberg website and taking a look at it. We'll later do some further exercises using these files as well.

Take a look at the different options available for finding books, then restrict the language to English, and finally click on the letter A under 'Authors'.

Scroll down the list until you find Jane Austen or press 'Ctrl + f' on Windows/Linux, ' + f ' on the Mac, on your keyboard in order to use the browser's find functionality to search for the name. Once you've found the entry for her, click on the link for Emma, making sure that you select the one with the book symbol next to it, rather than the loudspeaker symbol. You'll now be redirected to a new page with a listing for that book.

Take some time to look at the information provided on this page, especially with regard to copyright on the 'Bibrec' (bibliographical record) tab, and then look at the table at the bottom of the 'Download' tab listing all the different formats available. You'll notice that there may be a variety of formats available for different purposes, but the most useful for ours will usually be 'Plain Text UTF-8 '.

Press the right mouse button on the corresponding link. From the context menu that will open, select 'Save Link As …' and save the file to your computer or memory stick, possibly changing the name to something more telling than the original file name suggested. Tip: If the name you've chosen would ordinarily contain spaces, I'd suggest you replace these by underscores (_) because computers are generally better at handling file names that do not have spaces in them.

Repeat the above process with Sense and Sensibility.

Finally, also download the PDF version of the same file. We'll use this later on to see how we can extract text from a PDF file.

Open either one of the text files and scroll through it to see whether you may be able to recognise anything special about the formatting, layout, etc. If not, don't worry, we'll discuss these matters in more detail soon.

Obtaining materials from the Oxford Text Archive

Now that we've already downloaded some basic literary texts, let's explore another archive, the Oxford Text Archive, to see what's available in general there, and to download a whole corpus for later use.

Exercise 9

Open the Oxford Text Archive page at

Browse through the tabs in the catalogue a little, and then explore the options for sorting it, for instance listing all free, unrestricted resources first, or by author or language.

Switch the display option to display 100 entries, just to make it easier to see more corpora at a glance.

Use your browser's find functionality to look for the Uppsala Student English corpus (USE) under the 'Corpora' tab, then click on the id (2457) on the left.

Look at the information presented on the page, especially about availability, and then download the .zip archive of the corpus from the link provided there.

Once you've downloaded the zip file, unpack it and start exploring the contents.

If you're less interested in literary language, but may be more interested in exploring 'real' linguistic corpora, this type of data may already be more useful for you, especially if your main interest is in learner language.

Collecting Written Materials Yourself ('Web as Corpus')

Although you can collect other types of written data in various ways, including scanning or retyping, these days it's becoming more and more fashionable to retrieve written materials in the form of web pages (or other formats) from the WWW, so this is what we want to explore next. Obviously there are some problems with this approach due to the format of the data that will be retrieved, which often includes other types of information apart from the main text, but we'll deal with these issues only partially now, and explore the remaining options a little later.

A brief note on plain-text editors

In order to work with corpus data outside of the specialised corpus analysis programs we'll be discussing later, for instance to prepare it for processing or distribution, it's usually best to use a simple plain-text editor, rather than a word processor like MS Word or OpenOffice Writer, etc. Plain-text editors usually save their files as pure plain text, often using the extension .txt by default, and the more useful ones also allow you to specify a default encoding (which should generally be UTF-8 these days to ensure exchangeability of data), run sophisticated search-and-replace operations based on regular expressions (see Chapter 6), do syntax highlighting for special annotation formats (see Chapter 11), display line numbers, allow the user to run word counts, or even set up macros, i.e. little programs that automate certain tasks, etc. Don't worry if some of these features are still unfamiliar to you. You'll learn to appreciate them more and more once you encounter them in later sections of the book or simply gain more experience in working with such editors.

As most operating systems recognise the extension .txt and will automatically open an appropriate built-in editor when a plain-text file with this extension is clicked, I'd strongly recommend you to use this for your own corpus data, at least for data that doesn't contain any special annotations, even if some operating systems, such as Linux or Mac OS X, may not require it, and default installations of Windows will unfortunately also hide known extensions from the user. The latter, however, can easily be fixed by going to 'Tools→Folder Options…', selecting the 'View' tab and unchecking the option to 'Hide extensions for known file types'.

Because many of the built-in editors, such as Windows Notepad or TextEdit on the Mac, are either not powerful enough (the former), or first need to be configured in special ways to handle plain text by default (the latter), I will make some recommendations for editors I consider suitable for corpus processing for Windows, Mac OS X, and Linux below, and also try to explain some of their advantages for basic corpus processing.

On Windows, one of the most easy-to-use, fast, and small editors is Notepad++ (

On Macs, a highly useful and powerful editor that can handle many different plain-text formats is TextWrangler (

On Linux, if you're running KDE, you can safely rely on the built-in default editor, Kwrite, as it fulfils all needs of the budding corpus linguist, while unfortunately the Gnome default editor, gedit, neither supports regex replacements nor setting encoding. To replace this on Gnome, I'd therefore suggest you install the Bluefish HTML editor, which provides all these options.

The following is a brief summary of desiderata for suitable plain-text editors. They should: r allow the (default) encoding to be set to UTF-8; ideally also to convert between encodings r support regular expressions in search-and-replace operations r be HTML/XML-aware, i.e. allow matching elements and colour coding; ideally allow syntax checking r support setting line endings to Windows or Linux/Mac OS X format

The more you'll work with text editors, the better you'll hopefully come to understand these features, and will ultimately also be able to make your own decisions as to which editor you prefer, at least if there are multiple options available.

Browser text export

Perhaps the easiest option to obtain texts from web pages is to use your browser's text export function, if one is available. Often browsers will not only allow you to save a web page in its HTML-form(s), but also as a plain-text variant. The quality of this type of export may vary from browser to browser, though. Some browsers will remove the HTML and only leave plain-text content, while others may retain some bits of HTML, such as, for example, email addresses contained in mailto links (i.e. those that allow you to fire up an email client when you click on them), etc. As far as the layout is concerned, some browsers will remove almost all layout or structure information, apart from maybe some line breaks, whereas others may indent headings, etc., or insert some empty lines in order to try and preserve at least some of the original layout. It is therefore best to test whatever capabilities your browser has in this respect and download a sample page, which you can then compare with the original. As Chrome and Safari have no support for downloading only the text, the instructions below are split into two sections:

Exercise 10

Open the page on file formats we looked at earlier in Exercise 6.

In Firefox, choose 'Save Page As…', in IE, 'Save As…'. Under 'Save as type:' (or whichever entry is equivalent in the dialogue box on your operating system), select 'Text Files ( * .txt ; * .text)' in Firefox, 'Text Files ( * .txt)' in IE for the type. You may optionally also be able to specify an encoding, and if this is available, you should definitely select 'UTF-8 ' here. Specify a file name or accept the one provided by the browser.

This should copy your text to the clipboard. Open your favourite text editor and paste in the contents by pressing 'Ctrl + v' on Windows/Linux, ' + v' on the Mac.

One thing you may have noticed when reading through the instructions above is that, as far as using keyboard shortcut keys is concerned, whenever Windows/Linux use the 'Ctrl' (Control) key, the same feature is usually triggered by replacing 'Ctrl' by ' ' (command) on the Mac, although the Mac actually does have a key labelled 'control'. This keyboard mapping for shortcuts seems to be a general principle, so if you need to move between platforms for some reason, you can bear this in mind.

Browser HTML export

Although using the text export function will provide you with most of the relevant data you might want to extract from a web page, as we've seen through Exercise 10, it'll also potentially remove certain types of information, such as the indications for headings (as opposed to proper running text), or even add line breaks that weren't part of the original text. In some cases, we may actually be interested in particular types of 'layouting' or formatting information, which is why we might want to download the whole web page, including all of its HTML markup. Luckily, this time, all browsers provide this functionality directly. In order to do this, you just need to follow the instructions below:

Exercise 11

Click on the 'File' menu as before or the icon in Chrome. In Firefox, choose 'Save Page As…', in IE and Safari 'Save As…', and in Chrome 'Save page as…' again. For other browsers, find the equivalent function, which will be named in a similar way.

Under 'Save as type:', select 'Web Page, HTML only' or 'Webpage, HTML only" for the type. In Safari (v. 5) for Windows, select the 'HTML Files' option, on Mac OS X, select 'Page Source' under the 'Format' option. Also make sure that you can see the extension by unchecking 'Hide extension'.

Specify a file name or accept the one provided by the browser. Click on the 'Save' button or press the 'Enter' key.

Open the file in your text editor and examine its format. Pay particular attention to whether you might be able to recognise basic textual divisions, headings, formatting instructions, links, etc.

We'll take a closer look at HTML and its conventions a little later. For now, it should just be enough if you become aware of roughly what the original source format and formatting options for web pages look like.

Getting web data using ICEweb

ICEweb is a small application that I've written in order to retrieve a number of web pages for a given domain, and to process them automatically in order to be able to use the raw text for analysis purposes or to create different types of frequency lists, something we'll discuss later in this textbook. The original idea behind its name is that it was to be used to retrieve corpora of web pages that could form a web-based counterpart to the corpora contained in the International Corpus of English (ICE) we discussed in Chapter 2. You can see a screenshot below. To download a copy of the program, go to

The program itself is relatively intuitive to use once you've launched the executable, which may be named 'ICEweb_win32.exe', or simply 'ICEweb.exe', depending on which version you're using. Once it's running, you normally start by defining -i.e. typing the name of -a new region (generally a continent) in the textbox in the top left-hand corner and creating a directory for this by clicking the button below it. If you're not collecting data from a specific region of the world, you can always set up a different main category instead, though. Next, you add one or more countries/sub-categories to this region/category and create either a fixed genre/directory structure, or a simple one, by clicking the 'create structure' button for each country/sub-category.

You can then use a web browser to identify and collect the web addresses of pages that you want to download. These need to be specified in a file called 'urls.txt' in the appropriate directory, which you can create by clicking on the 'add/edit URLs' button and selecting the directory. This action will automatically open the built-in editor where you can paste in the web addresses, one per line.

Once you've added enough URLs, use the 'start retrieving' button and you'll be able to select a directory/domain for which you want to collect the pages. The download and processing progress will be reported in the text window on the right, and a number of sub-folders will be created once the pages have been downloaded and processed successfully. You can later explore these folders/files using Windows Explorer, Finder on the Mac, or your favourite file manager on Linux. If you want to see frequency statistics for a given directory, you can also use the 'show stats' button and a new text window will open, presenting you with detailed descriptive statistics. Let's practise some of this with a few web addresses I'll provide below:

Exercise 12

If you haven't already done so, download ICEweb and set it up as described above.

Start Save the file and close the editor. Retrieve the pages. If there are any error messages in the window at the bottom, you can usually safely ignore them , but if any appear in the window on the right, this generally means that you've either got a misspelt URL or that the relevant web page is simply not downloadable for reasons related to the server configuration.

Open Explorer or whichever file manager is appropriate for your system to view the output of the program. You should normally see 3 files

You can look through the folders to see whether/how many files they contain, but it's generally best to look at each file, apart from maybe 'dirIndex.html' and 'index.csv', through the built-in editor by clicking on the 'open result file' button and selecting an appropriate file. Try this with at least one of the downloaded HTML files and its corresponding text version.

Please note that the extensions for the files containing the word tokens (.tok) and frequency lists (.frq) are not extensions that are generally recognised by any programs, such as editors, but you'll be able to view them with any text editor if you use the usual file-opening mechanisms. However, if you're working in ICEweb, anyway, it's usually easiest to use the built-in editor. We'll discuss the significance of word tokens and frequency lists similar to the ones generated by ICEweb in Chapter 9, and once you've worked through this chapter, you'll be able to interpret these lists as well, although they, just like the statistics you can view from inside the program, will probably not really tell you much at the moment.

Downloading other types of files

Of course, a corpus downloaded from the web can also consist of other types of text-containing documents, such as those discussed earlier that are in a non-plaintext format, e.g. PDFs, MS Word documents, etc. For those, you'd essentially have to download them manually, but at least we can still discuss an efficient way of finding the data you might want using a little Google or Baidu (if you're in China) trick.

Exercise 13

Open your browser and navigate to the Google/Baidu page, or use a builtin Google/Baidu search bar if available. Type in 'corpus linguistics filetype:doc', and hit 'Enter'.

Look through the list of results and download one or two that seem interesting to you. Hint: don't forget about the context menu… Repeat the same thing, only changing 'filetype:doc' to 'filetype:pdf'. We'll soon investigate ways of extracting the text parts from these documents.

Collecting Spoken Data

As already hinted at through our earlier discussions, collecting spoken data is far more complex than compiling a corpus of written materials (see, for example, also Exercise 2). This is why I can only provide you with some fairly general guidelines here on how to achieve this, apart from pointing out which issues you may encounter when embarking on such an endeavour, and this section won't be accompanied by any exercises, either.

The first thing you obviously need to consider is what type of spoken data you may want to analyse. In general, it won't make sense, apart from actually being illegal (see Section 3.1.4 above for legal points), to simply go out into the street and make random recordings of people. As before, you'll therefore need to consider your data requirements carefully in the light of your particular research questions. Thus, for instance, you may want to record and analyse the speech of native or non-native speakers of a language only, or perhaps make recordings of them in interaction. The first form would then hopefully allow you to investigate one particular population, in whichever recording context you've chosen, while the second should make it possible to compare the two groups (see e.g.

Once you've established exactly what type of spoken data you want to collect, perhaps the first important point to observe is that the recordings need to be of sufficiently high quality to make them useful for linguistics research in the first place. Therefore, the recording conditions must be suitable in order to ensure that there isn't too much background noise, that all speakers can actually be picked up by the recording equipment, that the input signal is strong enough, etc. This is especially important if you're not only planning to analyse what your informants say, but also how exactly they say it. In other words, if your aim is to conduct any phonetic/prosodic analyses, then you'll also need to use relatively high-quality recording equipment that will at the very least allow you make recordings that comprise the necessary frequency range that is significant in producing different nuances of human speech, i.e. between 80 Hz and 11 kHz

In comparison to written data, the digital audio files produced through such recordings tend to be relatively large. For instance, one minute of uncompressed speech that allows us to analyse samples of up to 11 kHz, with 16bit-quantisation and recorded in mono (i.e. one single channel) in .wav format, will take up about 2.6 Mb of hard-disk space. This can be reduced to maximally a tenth (around 270 kB) if we use the common .mp3 compression format that MP3 players/recorders employ, still retaining suitable quality for phonetic analysis.

Once you've stored your sound files on the computer, you can start the highly time-consuming process of transcribing them. To do so, it's advisable to use an audio editor like Wavesurfer (

We'll discuss some of the specific formats for better representing and annotating orthographically transcribed spoken data later on in Chapter 11, and ways of identifying such features in Chapter 9, but for now, I just want to mention some specific points you need to bear in mind when trying to render speech more or less accurately in orthographic form. The first of these is that you should always represent what's been said as faithfully as possible; in other words, for example, if someone uses a contracted form, such as won't, then you should always represent it as a contraction, and only transcribe will not if this has actually been said. As similar point applies to other typically spoken features that are often wrongly assumed to be non-standard, such as the pronunciations of /wÅn´/ or /gÅn´/, which should be transcribed a wanna or gonna, even if this may be frowned upon by prescriptive teachers or grammarians, because they're really absolutely normal features of genuine spoken language, and 'correcting' them to what is supposed to be 'standard usage/representation' would simply constitute an act of falsifying your data. Further issues may arise in representing other features of spoken language, such as minimal responses like aha/uhu/uh-huh or mhm/mm-hm (for yes or yes-like backchannels), huh-uh/uh-uh (for no), etc. As the alternative representations listed in the previous sentence show, there may be multiple ways of representing the same thing, and you should not only find a consistent way of representing these features, but also document their meaning, so that other potential users of your corpus will be able to understand exactly what they represent. The same thing goes for common abbreviations of conjunctions, such as because, which may variably be represented as cos (BrE) or cuz (AmE). In some special cases, it may also be necessary to represent other vocalisations, such as hesitation markers (e.g. em/erm/um), or specific realisations phonetically, possibly in order to convey emphasis appropriately, e.g. /ði˘/ (without following vowel) instead of /ð´/. Other features that may be highly relevant to some types of analyses are pauses, which are frequently indicated in round brackets, e.g. (.3) meaning .3 seconds, or incomplete words, which can be abbreviated by using an ellipsis, e.g. Mon… for Monday or Monica, etc.

In spoken interaction, usually speaker turns, i.e. periods where one speaker talks before the next one takes over, are also indicated, generally prefaced by the speaker name or an identifier in anonymised data. When speakers exhibit overlap, which is quite frequent in multi-party talk or dialogues, this is often indicated via opening and closing square brackets to indicate start and end, respectively. In some cases, where one speaker simply tries to indicate to the other that they're still 'following', we may also get backchannels that should in fact not be marked as separate turns, but nevertheless indicated, which can be achieved by inserting them inside some form of bracketing, similar to the format for pauses above. As pointed out earlier, we'll later discuss more sophisticated ways of rendering such information, and there far too many different possible conventions to list them all here, so I can only refer you to a few sources, such as Edwards and Lampert (1993),

Before we conclude this section, a further brief note about data handling and meta-information is in order. As pointed out earlier, not only is it important when recording informants to obtain their full consent for using the recordings for research, as well as possible distribution, but it is equally necessary to protect their privacy through a suitable anonymisation of the data, for instance using codes to represent personal data, such as names, addresses, telephone numbers, etc., in the orthographic transcript, but also masking these in the audio data if it is to be distributed alongside the transcripts, which it generally makes sense to do if possible. On the other hand, it's usually important to make some personal information, such as the informant's age, sex, provenance, level of education, etc., available to users of the corpus, in order to allow them to conduct research of a more sociolinguistic nature. And, of course, as the compiler of the corpus, you still need to be able to identify and possibly contact your informants later, should follow-up questions arise, so you need to keep a separate file that allows you to look up this information, based on the user codes in your data.

Preparing Written Data for Analysis

In this section, we'll discuss how to prepare our data for analysis. We'll first explore ways of efficiently cleaning up or normalising parts of our data by means of basic search-and-replace operations, using a small number of illustrative examples. Later on, we'll move on to learning about ways of extracting text data from files that contain formatted text, where of course the same, or at least similar, clean-up operations might be necessary after the main data extraction has been performed.

'Cleaning up' your data

As we've already seen, some of the electronic data we can obtain off the web (or elsewhere) can contain unwanted formatting or meta-information, so we always need to scrutinise whatever materials we download carefully. In some cases, it may just be a question of removing a header or some other types of information, or converting some special symbols into a format that's more suitable for our analyses. Sometimes you may also have to correct artificial line breaks, for example, when you've extracted text from graphical formats -such as .ps or .pdf -, or those inserted by some browsers, as we've seen in Section 4.2.2. We need to do this because these line breaks, as indeed anything that doesn't really form part of our text, may in fact interfere with the processing of the text later and even create a number of problems that could affect the meaningfulness of at least part of your data for linguistic analyses.

What exactly you may need to remove from your data depends on the specific formatting or encoding conventions used for the document. Doing this can sometimes involve a substantial amount of manual work, but might also be as easy as opening the document in your editor and using the search-and-replace functionality in order to replace certain codes from the data.

Exercise 14

Download and open the web page 'Cleaning Written Data' (cleanup.html) from the online materials in your browser and also in your text editor.

Call up the search-and-replace function, either from the 'Edit' menu or by pressing 'Ctrl + h', which is the shortcut available in most general text editor s these days, even in the non-Windows world. If you're using Text-Wrangler on the Mac, you need to trigger the replace operation through the 'Find' functionality ( + f) and then fill in the replacement term.

In the 'search box', type &amp; (including the semi-colon) and in the 'replace box' the word and.

Click on the button. Next, change the search term to &ndash; and the replacement to -, and replace all instances again.

You should now have replaced all ampersands and n-dashes in the HTML document. Save it and refresh your browser to see the effect.

Searching and replacing text along the lines of what we just practised above is a very useful semi-automatic means of preparing your data efficiently, but you nevertheless constantly need to be aware of potential errors that might be introduced by replacing the wrong things or replacing them in the wrong order. For example, you might be tempted to remove all single quotation marks in a text altogether because they 'interfere with' the creation of word frequency lists (see Chapter 9), etc., but of course if this is done carelessly and improperly, you may end up taking out all apostrophes, too, in which case you might, for example, end up with a single neuter possessive pronoun form its instead of the contraction it's which actually represents two separate word forms! In order to prevent problems like this, it may sometimes be necessary to use the more time-consuming option of replacing all occurrences individually by clicking on and only replacing the item found by clicking on if you're really sure that you want to replace it.

No matter how much of an unnecessary effort it may seem to you to clean up our data in this way, you should always remember that if you don't do this, then the data you're going to use for your analysis later could in fact contain many errors that will most likely skew your results, potentially making them (highly) unreliable.

An additional step you should also take in preparing your data in this way is to keep track of the way in which you adjust the data to your needs. This should ideally be done in the form of a text file that lists all the separate editing steps, and can later provide the basis for part of a manual of information to be distributed with your corpus if you ever plan to release or distribute it. Such information will then help other users of your data to understand better what to expect from your corpus, or allow yourself to refresh your memory if you should use the corpus data once more after an extended period of time of not working with it.

Extracting text from proprietary document formats

Essentially, the mechanism for extracting text from proprietary formats, such as MS Word or PDF, is always more or less the same, although the exact output and changes you'll need to make to the data later will vary depending on the specific features of the program we're extracting from. The most important thing to remember/look for in such a program is a menu item that either provides us with a 'Save as…' or 'Export' option from the 'File' menu to save the text as plain text, as we've seen for web pages earlier, or some item on a different menu that will probably contain the word 'extract', such as in older versions of Adobe Acrobat or GSview.

Exercise 15

Open the Word and PDF documents you downloaded earlier one at a time and try to extract the text contained in them using one of the mechanisms described above. If you don't have a copy of Word, you can try OpenOffice Writer, which will usually do a very good job of dealing with Word documents, unless they're really complex.

Open the resulting text files and see whether you can identify any other clean-up operations you may need to carry out.

Removing unnecessary header and 'footer' information

Many types of files that you can download, such as the text files we retrieved from the Project Gutenberg website in Exercise 8, do contain some form of metainformation in the form of headers (at the beginning) and/or 'footers' (at the end), concerning copyright, etc. In HTML(-like) files, as we've seen earlier, such sections containing meta-information are clearly delimited by HTML tags, but in other documents, they may be more 'free-form', without any clear indication as to their beginning or end. This is for instance the case with the two literary documents by Jane Austen we downloaded, which contain both header, as well as 'footer', information. As mentioned earlier, this type of meta-information definitely doesn't represent any textual content that we want to keep and analyse, so let's practise identifying and removing this.

The keyboard shortcuts given below for Windows/Linux and the Mac should work in most editors these days; otherwise, if you're using a non-standard editor, you'll need to try and find their equivalents. To make it a little easier to understand what the shortcuts do, just try to remember that the basic keyboard combinations without pressing the 'Shift' key -the one that switches between small and capital letters -will only help you to navigate through the document more efficiently, while combining them with 'Shift' will also select the text spans covered by the shortcuts.

Exercise 16

Open your copy of Emma in the editor and see whether you can identify the beginning of the text body. Also take a note of the contents of the header.

Once you've done so, place the cursor at the very beginning of it, i.e. just in front of the first letter (character).

Press 'Shift + Ctrl + Home' on Windows/Linux, 'Shift + fn + + ←' on the Mac. In most editors, this will select everything from the current cursor position to the beginning of the document.

Press the 'Delete' key on your keyboard. The header should now have disappeared.

Next, press 'Ctrl + End' on Windows/Linux, 'fn + + →' on the Mac. Your cursor should jump to the very end of the document.

Scrolling up through the document, find the end of the text body and place the cursor there, keeping note of the 'footer' contents.

Press 'Shift + Ctrl + End' on Windows/Linux, 'Shift + fn + + →', which should highlight everything to the end of the document, then press 'Delete' again.

Save your document. It should now be in a state where you've at least removed all redundant meta-information.

Look through the cleaned copy and see whether you notice any additional formatting-related problems, and think about whether these could possibly be solved via search-and-replace operations. If you identify any, then try them out, each time carefully checking the results before saving the document. If something goes wrong, you can always undo the last step (using 'Ctrl + z'/' + z'), as long as you haven't saved the document, so there's no need to panic .

Repeat the process for Sense and Sensibility.

Documenting what you've collected

As already pointed out above, it's generally at least advisable to document all the steps you've seen necessary in editing your data to make it fit your research questions and purposes. However, ideally, describing the editing process shouldn't be the only thing you do in this respect; you may also want to retain a certain amount of meta-information about the compilation of your corpus, especially if your plan is to share the data with other people. Therefore, in addition to the file describing the editing process, you'll probably want to keep at least one extra file that lists the contents of the corpus file-by-file if your data contains materials from different genres, text types or domains, or, as with our web page data, that lists information about where the file was retrieved from, when, what the original file name was if you've changed it), etc. ICEweb already caters for some of that information by keeping a little text database that you can open with Excel or OpenOffice Calc.

As I cannot possibly list, let alone even imagine all the particular pieces of information that may be required for your own research projects/questions, I'd suggest that when you embark on creating your own substantial corpora, you go back to the manuals for similar corpora we investigated earlier on, and try to evaluate whether the documentation for these corpora satisfies all your needs, and, if necessary, adapt this accordingly.

Preparing your data for distribution or archiving

Sometimes, in order to be able to exchange your data with other people, or simply to archive it in some way for backup purposes, etc., it's useful to compress this data into an archive. This not only saves storage space, because texts can be compressed quite easily, but also makes it easier to email files, as one relatively small archive file can easily be sent as an attachment instead of sending each file individually and in its original size. There are numerous compression formats, but the most common one is .zip, for which most operating systems not only provide direct support, but also generally have options to create and manage this type of archive from within whichever graphical file manager they offer. The general mechanisms employed in such a file manager are in fact very similar for Windows, Linux, and Mac OS X. On the latter two systems, you can also use the command line to perhaps achieve the archiving task even more efficiently, but in order to be able to do that you obviously need to be familiar with such procedures, which are a) more complicated to learn, and b) too extensive to be discussed within the confines of this book.

Essentially, it's generally either possible to select a number of files or even a whole folder and then instruct your file manager to compress these into an archive, or to create the archive first, then open it again, and keep on adding files to it, often per drag-and-drop. Of course, the latter mechanism, which is only available on Windows in a simple form, can also be used to add, change, or update files at a later point in time. Let's practise both ways by adding the two texts by Jane Austen you downloaded and cleaned up in Exercise 8 to an archive.

Exercise 17

Open Windows Explorer, Finder on Mac OS X, or whichever file manager you're using on Linux, and navigate to wherever you've stored the two files. Tip: The easiest way to open Explorer in Windows is to hold down the windows flag and press 'E' on the keyboard once, while Finder will automatically be running on the Mac and you just need to switch to it. On Linux, start your favourite file manager, for instance Dolphin on KDE, which is the one I tested the following actions with.

Highlight the two files you want to add to the archive. If the files appear immediately below one another, the easiest way to do this is to click on the first one, then hold down the 'Shift' key, then click on the second one. If your computer is configured to open files via a single click, as is probably the case on Linux, hold down the 'Ctrl' key to select the first one.

If the files are not in consecutive order, Ctrl + click on either one of them, then hold down the 'Ctrl' key ( on the Mac), and click on the other. Tip: These two mechanisms should work on more than two files, too.

Next, use the right mouse button to trigger the context menu or use two fingers to tap if you're using a touchpad on the Mac.

In Windows, move the mouse down to the item 'Send to', which will open another sub-menu, where the first entry should read 'Compressed (zipped) folder'. Clicking on this (using the left mouse button) will create the archive, add copies of the selected files to it, and also allow you to (re)name the archive itself. In Linux (KDE), find the menu item 'Compress', then select 'As ZIP archive' from the sub-menu. On the Mac, choose 'Compress n items', where n here refers to however many items you've selected to add to the archive. Finder will automatically create a zip archive for you, which, by default, is simply called 'Archive.zip', and which you can then rename to something more appropriate by first selecting the archive and then clicking on the file name once again (avoiding a double-click, which will extract from the archive instead). On Linux, the archive name will probably be based on the name of the first file you selected, but you can easily change that by using the context menu again and choosing 'Rename' from there.

Test this by clicking on the archive itself, once it's been named. All operating systems will just treat it like any other folder that you can copy files (or other folders) to or delete them. Tip: The easiest way to copy additional files into the archive is to simply open another copy of your file manager and then drag-and-drop from there.

Let's also try the other way, if you're running Windows, creating a new archive from scratch. This basically works in a rather similar way, only that, instead of pre-selecting files, you use the right mouse button somewhere inside a blank space in the folder where you want to create the archive. This will again trigger a context menu, where this time you need to move the mouse cursor down to the 'New' option, which will again present you with the option for creating the archive, only that it should appear as the second-to-last option here.

Once you've created the new archive, give it any (half-way sensible) name you like and copy the two files in there, either via drag-and-drop or copyand-paste. Now that we've discussed most of the preliminary issues in corpus design, and seen how we can actually collect and prepare our own data for analysis, we can soon move on to learning how to analyse linguistic data in various forms. However, before we do so, perhaps a final note on record-keeping is again in order. As I stated before, because many of the steps you may need to take in order to produce your corpus may frequently involve making changes to the original data, it's advisable to document the steps you've taken in your preparation as much as possible, to allow both yourself and any other potential users of your corpus to understand the exact nature of the data. Such documentation should obviously also be included in any distribution, provided that there are in fact substantial changes to the original data. In addition, you should also add any meta-information, and finally, perhaps describe both of the above, at least to some extent, as part of a complete manual of information. This corpus manual will usually be in PDF format, and from here you can always refer to any additional files for reference if necessary.

Solutions to/Comments on the Exercises Exercise 8

This exercise should not present much difficulty to you. Perhaps the only real difficulty if you're not really well-versed in using a computer may be to learn to make use of a few important program features. The first is that you can employ the right mouse button (two-finger tap on touchpads on the Mac) to activate the context menu inside the browser in order to be able to save the material, as otherwise clicking on the link will simply get the text displayed inside a browser window, rather than saved to your computer. Even if this happens, though, it's not a problem, as you can then use the main browser menu and the 'File→Save Page As…' menu in Firefox, 'File→Save As…' in Internet Explorer and Safari. In Chrome, you unfortunately have to use the icon to access the 'Save page as…' option, as no menu bar at all exists in this browser. An added complication in Firefox and Safari is that you may need to switch on your menu bar first before being able to do so, but I'd recommend you do this for consistency, anyway. Activating the menu bar in both browsers can be achieved by using the context menu in the grey area below the title bar of the browser and choosing the appropriate option.

Exercise 9

Again, this exercise should be relatively straightforward to accomplish, although you may need to learn to handle .zip archives if you're not familiar with them yet. If you're using Windows, Explorer has built-in functionality for extracting the data from such an archive, and on Linux and the Mac, (double-)clicking on the file inside the file manager will usually extract the data automatically.

Another thing that you may have been wondering about while exploring the OTA is what some of the abbreviations in the column for 'Availability' stand for. All those that start with 'CC' refer to Creative Commons licences, the nature of which you can explore in more detail at

When you explore the contents of the archive, you'll hopefully also realise that additional information along the lines of what I recommended earlier is in fact included in the form of a plain-text file called 'ReadMe.txt' that describes the contents of the corpus distribution, a manual (in Word format), and an Excel file that contains a database of important details related to the corpus files, speakers, etc.

Exercise 10

It's difficult to describe what exactly you're going to see once you've opened the text version in your editor because the output from different browsers varies so greatly, so I'll just provide a few examples here to raise your awareness of different issues.

Firefox (ver. 25 and above), for instance, will try to preserve at least some of the layout and formatting for the page, so the main heading, which is centred on the page in the HTML and has some spacing around it, will be surrounded by two empty lines and indented somewhat. In general, paragraphs or other objects, such tables, will have spacing around them simulated through empty lines. A longer paragraph will be broken into a number of lines, each ending in a line break, where the maximum line length seems to be about 80 characters. The table structure is mimicked through tabs, which will simply look like spaces, unless you can get your editor to display them as tabs. Hyperlinks are preserved and rendered in angle brackets (<…>), italicised text surrounded by forward slashes (/…/), and underlined text surrounded by underscores (_…_). The label and number for the exercise at the bottom of the page, which are generated automatically in my HTML, are deleted, and the horizontal rule below this paragraph is represented as a series of hyphens. IE (ver. 11) will strip out almost all of the formatting, even removing the spaces between the cells in the table, and thus joining some words that should not be joined, just like in our example of the 'fake compounds' in Section 1.1.2. Strangely, though, it simply 'lists' all the row headers (Format, Extension, and Properties) as separate paragraphs before the remaining cells of the table, which are then presented as individual paragraphs from left to right and then top to bottom, but without any additional spacing between them. It therefore treats the header row conceptually very differently from the rest of the table. It also removes all the hyperlinks and the horizontal rule, as well as the generated content, completely. Unlike Firefox, IE also takes the page title from the <head> section of the HTML page and lists it, with a slight indent of one space, before the first heading. As I generally use (more or less) the same text in the <title> tag of my pages, this effectively duplicates the text of the heading inside the saved text version. Just like Firefox, it also breaks longer paragraphs into shorter lines, thus adding extra line breaks that do not form part of the original text.

The copy-and-paste versions retrieved from Chrome (ver. 31 and above) and Safari (ver. 5 on Windows, ver. 8 on the Mac) seem to be almost identical in that they generally strip all the formatting and hyperlinks from the document and simply leave the text with a minimal degree of formatting. In the case of our test page, all regular paragraphs, including the heading, were rendered with two line breaks following them, apart from the last one in the document. The table here is presented in a different way from the paragraphs, with no extra line breaks following it, but with each cell inside a row separated from the next by a tab and the rows themselves separated by a single line break. On the plus side, neither browser produced extra line breaks.

If you have a number of different browsers installed, you can also use them to download the same page in text format and compare the versions they produce.

Exercise 11

By now, you'll hopefully be able to download/save files to your computer quite easily, so the only difficulty in this exercise could be in identifying what some of the HTML codes mean in terms of creating a certain layout or formatting. What you'll hopefully have observed already is that headings in HTML start with <h, followed by a number. In our document, we only have one heading, and the number here is 1, which means that it's heading level 1, that is, a main heading, similar to a chapter heading in a book. You'll probably also have noticed a number of tags that start with <p, followed by additional class information, but once only as <p>. What all instances of these have in common is that they contain some text, and that this text had </p> at the end, to close the tag. The <p>…</p> tag is perhaps the most common tag in HTML because it encloses paragraphs, which account for most of the textual divisions inside web pages.

Furthermore, you've hopefully observed that the table is contained inside a <table>…</table> tag, with table rows (<tr>…</tr>) inside them, which, in turn, contain cells marked as <td>…</td> for table data or, in the header row, as <th>…</th>. Some other things you could have noticed are that materials that are highlighted, i.e. emphasised, appear in <em>…</em> tags, and that hyperlinks to external pages (or my email address) are enclosed in <a>…</a> tags that contain a so-called href attribute which specifies their location/target.

Exercise 12

This exercise should be pretty straightforward, provided that you have permission to install the program on the computer and follow the instructions as described.

The only problem could be that the files you choose to get are not downloadable via a program. Some web servers can detect such things and treat automated downloads as an intrusion. In this case, unfortunately, you don't have any other choice but to download and process the files manually.

The main purpose of the exercise was actually to demonstrate to you that there are ways of downloading web pages automatically, as well as store and process them in a principled manner, so as to be able to conduct different types of analyses on them later.

Exercise 13

The main point of this exercise was to teach you how you can narrow down a web search to be able to find only particular types of files, rather than potentially having to click through many results pages just to find a few of these documents among a multitude of other -mainly HTML -documents. A by-product of this exercise is, of course, that you should now also have a number of Word and PDF documents that you can extract some text from later.

Exercise 14

This exercise should have made you aware of how (deceptively) easy it may be to convert some copied data into a more appropriate form. The search-and-replace functionality built into many editors or word-processors these days is a very powerful tool to make changes comprehensively and quickly, but, as you've hopefully gathered from my comments, there may well be some risks involved if you're not sure about the exact nature of the data and therefore unable to anticipate any potential issues. Thus, the semi-automated way to search and replace, where you first verify whether you want to replace what's been found, may often be a safer option. If you're too pressed for time, or simply impatient to use this method, then you should at the very least test the results of your clean-up operations on a sufficiently large amount of data.

Exercise 15

Again, the basic operations for extracting the text should be relatively straightforward, especially for Word documents, provided that you either have Word itself or a reasonably good word-processor that can handle Word documents, such as OpenOffice Writer, installed. Of course, more complex constructs, such as tables or lists, will never quite look the same in plain-text format, but the essential thing is that we can somehow get a handle on the text content. PDFs, on the other hand, may present more of a problem as, due to their graphical nature, it's unavoidable that we'll end up with unwanted line breaks within paragraphs which probably need to be replaced by spaces manually as and when appropriate to avoid complications when trying to create frequency lists or otherwise processing the text documents later.

Exercise 16

In doing this exercise, you should not only have learnt how to remove unwanted parts of a document, but also gained more experience in working with editors. Ideally, you'll also have memorised the keyboard shortcuts by now because if you have to process a lot of data, the more efficient you are in moving around a document, the more time you'll save that you can later use for actually analysing and interpreting the data, which is also a very time-consuming process, although it may sometimes seem as if the data preparation is the most extensive part of a project.

Exercise 17

As before, this exercise should be relatively straightforward if you follow the instructions carefully and are working on a Windows system. As pointed out in Section 4.4.5, compressing files in this way is not only useful for archiving purposes, but especially if you want to make your data available to other people online or via email because an archive constitutes a convenient and relatively small-sized package (depending on how much data you have in it) that can easily be copied or downloaded and later extracted again, so it's definitely worthwhile practising how to do this.

Sources and Further Reading

Edwards, Jane & Lampert, Martin. (Eds.).

Concordancing

What's Concordancing?

Concordancing is an analysis technique that allows linguists to investigate the occurrences and behaviour of different word forms in real-life contexts, that is, in situations where they have actually been used by native or non-native speakers. This is quite different from more 'traditional approaches' in linguistics that simply rely on the intuition of native speakers in order to determine 'correct' usage, and especially provides a mechanism for non-native speaker researchers to justify their research claims, or learners to improve their awareness of many different features of language by investigating different word forms in their 'natural' contexts, either with or without the guidance of a teacher. Learners can thus achieve a more realistic learning experience that is at least a little closer to language acquisition, rather than simply learning specific structures and rules. On a more 'commercial' level, concordancing is heavily employed in lexicography in order to select the most frequent, suitable, and representative examples for a particular lexicon entry, as well as to help disambiguate between its different senses. Based on concordances and frequency lists, modern dictionaries often now contain information regarding the frequencies of words in different domains, their typical collocations, register appropriateness, etc. In textbook preparation, concordancing can also help to identify the most important and salient features of vocabulary items and idiomatic structures, as you'll hopefully soon realise through the exercises in this chapter.

Essentially, a concordance is a listing of individual word forms in a given specific context, where the exact nature of the context depends on the requirements of the analysis and which particular program one may be using. The word context here refers to something different from what we discussed above, that is, not the situational usage in a particular place and time, but instead the immediately surrounding text, something we can also refer to as co-text in case of ambiguity.

Exercise 18

As a preliminary awareness-raising exercise, go to the online resource pages and open the page for 'Concordancing', which contains a short sample paragraph, constructed for illustrative purposes.

Try out the built-in concordancing facility by searching for the words 'this', 'very', and 'in' in the paragraph, and observe what happens. Do the results always correspond to what you would expect to happen? Exercise 18 will hopefully already have alerted you to the fact that, in analysing language, especially by computer, we may sometimes get rather unexpected results. This is one of the 'beauties' (albeit also one of the pitfalls) of doing corpus linguistic analysis because it allows us to identify language features that we may never have expected to find, thus providing inspiration for further and deeper research into the regularities and irregularities of language (structure), which generally go hand-in-hand. This is a phenomenon we'll frequently encounter in our analyses from now on, and it requires us to always have an open mind and the willingness to let the data 'drive' our interpretation, rather than trying to force the data to fit the theory, which is something I've frequently observed, for example, when colleagues who were doing literary analysis were not working closely enough to the actual text, but always somehow tried very hard to make the text fit the theory they were using.

The context (or co-text), for a concordance, as in traditional, printed concordances, may be a whole sentence, a paragraph, or simply a given number of characters to the left and/or right of the search term. The latter form is the one most frequently encountered in the results produced by modern concordance programs (concordancers), and is known as the keyword-in-context (KWIC ) format (see Figure

In this format, the search term -which may be either a single word, different forms of a word or even a series of words in a row -is usually displayed in the centre of the display window and may additionally be highlighted in a different colour or format. The results of these searches can usually also be saved to disk, together with additional information, such as the line number or file name where the occurrence was found. Although all concordancers usually provide a minimum of core functionality, such as the ability to produce KWIC concordances, they also often offer many additional options, so that it's difficult to generalise about which types of functionality to expect from any particular program.

Most concordancers are also stream-based, which means that they 'suppress' line breaks by replacing them with spaces, so that the text is essentially read as a continuous stream of words. Thus, words that actually occur on different lines in the text may still be presented as part of the context. In contrast, a line-based concordancer, such as the one built into my Simple Corpus Tool (downloadable from martinweisser.org), will only extract and display the immediate context found on the same line, plus a number of surrounding lines specified.

Concordancing with AntConc

AntConc is a free and highly versatile concordancer that provides support for many advanced concordancing features, including support for non-Latin character sets, as well as additional functionality that we'll discuss in the relevant later sections. The program itself, which is available in versions for Windows, Mac, and Linux, can be downloaded from

Exercise 19

As our first exercise, download and start AntConc now, and familiarise yourself with the elements of the program window a little.

Also have a quick browse through the menus to get a first impression of the options, even though you may not understand what they all mean yet.

When you first start up the program, you'll be presented with an initial screen like the one shown below:   Don't worry if you open a directory and there may be files listed that you don't really want to include in your analysis -you can always remove them from the analysis corpus later. For now, however, we're going to start by selecting only two files for concordancing as part of Exercise 20, which is designed both to give you some initial practice on running concordances, and to illustrate one of the most important linguistic issues you'll encounter in concordancing, that of polysemy, that is, the ability for word forms to either represent multiple meanings or belong to different word classes.

Exercise 20

Search for the folder in which you've saved the Jane Austen texts Emma and Sense and Sensibility.

Open the two files in AntConc. The result should look similar to this. In the search box, type in the word, round as indicated in the following graphic. Note that the option to search for 'Words' is automatically pre-selected by default and that 'Case' is not selected, which means that the search will be case-insensitive. In other words, no distinction is made between capital and non-capital letters, so that you could actually have typed in Round or even rOuNd, too, even if the latter doesn't really make much sense. We'll discuss the third option, 'Regex', in more detail in Chapter 6.

Think about what kind of results you would expect to get from the search (in terms of word classes, etc.), and then click on . You should get a result that looks like Figure

Sorting results

If you think about how long it would have taken you to find all these occurrences manually in both texts, you can see how useful it is to be able to create concordances like this within a few seconds. However, as useful as an ordinary KWIC concordance may be, AntConc also offers us the functionality to create much better views of our search results by providing options for sorting the results based on their immediate left or right contexts. We'll have a quick look at how this is done, and then you'll hopefully understand the usefulness of this option without much further explanation, although, as usual, some will be provided in the solutions section.

Exercise 21

Take a look at the sort options immediately to the right of the search options. You'll see that for 'Level 1', which is already pre-selected, there's '1R' in the box by default, for 'Level 2', '2R', and for 'Level 3', '3R'. This means that, once you click on , the primary sort will be conducted alphabetically on the word immediately to the right of the search term, and in descending order of frequency. However, the sorting is then not yet complete, as it will be continued, based on the next word to the right, and finally, also on the 3rd word to the right.

For our current purposes, we want to sort according to only the word immediately to the left of the search term, so change Level 1 to '1L' and untick the check boxes next to the other two levels.

Before you try this, first change the search term to mind, then click on and finally on . Check the results and try to understand why this feature may be so useful… Tip: If you have problems in getting upper-and lowercase characters sorted separately, open the 'Tool Preferences' for 'Concordance' and check the option for 'Treat case in sort'.

Saving, pruning and reusing your results

Saving

There are essentially two different types of 'Save' option in AntConc, one that allows you to save your results as a regular plain-text file, and one that provides the option to only save the results displayed inside the analysis window temporarily, so that you can compare them to the results of another search side-by-side. The first type is accessible through the 'File→Save Output to Text File…' menu option, which saves the hit number, the concordance line, plus the file name for each hit. Saving our results not only provides a means of keeping a record of what we've found, but also to analyse and/or manipulate the data further, perhaps also using different programs, so let's see how we can produce such output in Exercise 22.

Exercise 22

Save the results of the previous search to a text file. I'd suggest you save all such files to a folder called 'results' inside your 'AntConc' folder, and always give each file a suitably descriptive file name that will later allow you to identify what its contents are. In this particular case, maybe something like 'mind_in_jane_austen.txt' would be appropriate. Once you've saved the file, open it in your editor to view the output.

Unfortunately the search term is not highlighted in a very useful form in the output, although you can achieve some degree of 'highlighting' by changing the delimiters between the different fields of the output. In order to get around this slight limitation of AntConc, you can of course always use your favourite plain-text editor and do a search-and-replace operation where you replace the search term by something like >>> [search term] <<< (leaving out the square brackets). Let's try this:

Exercise 23

Open the file in your editor and run the search-and-replace operation to add the highlighting.

The second 'save' option can be triggered by clicking the button on the bottom right-hand side of the AntConc window and will open a separate window containing the results of the current analysis, so that you can then run the other analysis and compare it to the original one side-by-side. The search terms in the saved window will still be displayed in colour, but are no longer clickable, so direct access to the larger file context, as it's available in the main window, is unfortunately no longer possible.

Pruning

Sometimes, not all the results of a search may be relevant to the problem or topic you're investigating, or you may simply end up with too many examples of a similar nature, so that you need to make some decisions as to which ones of the results you want to keep or treat as relevant to your purpose. If necessary, you should also document the decisions, maybe if you need to explain your choices to other people. For instance, in the grammatically polysemous results for mind in Exercise 21, you might want to only retain the noun usages for your research because you're only interested in nouns, but to discard all verbs, or vice versa.

In such cases, you have two choices for removing unwanted hits. The first one is to save the results to a file, as described above, and then delete all irrelevant material from this file. However, AntConc also allows you to remove some hits before you save the results. You can best do this by holding down the 'Ctrl' key, clicking on the hit number, and then pressing the Delete key. If you want to delete more than one non-consecutive hit, then simply keep the 'Ctrl' key pressed down and select any other hits until you've highlighted all the ones you want to remove, and then press Delete. If you're lucky enough to have all the data you want to delete in consecutive order, then you can also use 'Ctrl' + click on the first one, and then, holding down the 'Shift' key, click on the final one. This technique for making multi-selections is something you should already be familiar with from our discussion in Section 4.4.5 on how to select files for archiving in a file manager.

In case you're now worried that this may destroy your corpus data, there's no need, because whatever AntConc displays as a list of results in its KWIC window is in fact copied from the original file, rather than showing the original data itself.

Exercise 24

Try pruning a few hits in the current result set, perhaps deleting all nonnoun occurrences. Don't worry, you'll always be able to re-run the concordance to get the original results back Save the results of the search that you've just pruned under a suitable file name.

Reusing

Once you've extracted some data from a corpus, or a number of different corpora, there are different options regarding what you can do with this material. In some cases, you may already have identified all the relevant examples, and there's no need for any further analysis if your examples already illustrate all the points you might want to make in your 'research output', whether it's an assignment, an academic paper, a dissertation or thesis, or some teaching materials. This, however, is only the simplest case, and rarely do all those extracted examples simply speak for themselves, but instead may still require a substantial amount of analysis before they really tell the whole story.

Although you can always look through your data and keep notes on any salient features you've identified, as well as perhaps periodically go back to revise these notes if you encounter some new relevant details that may confirm or contradict your earlier observations, this may not be the best strategy for understanding your data completely. Perhaps a better way to analyse some data is not just to work through it, but, once you have all the relevant results in a file, to edit this file and mark up your samples using some simple codes in order to categorise them. This type of basic initial coding needn't be very complex, but can consist of a few easily identifiable markers that you insert somewhere in a text. The key to this is in the two words "easily identifiable" because whatever codes you may insert in your file, those shouldn't easily be confused/confusable with any regular text. Therefore, maybe simply inserting your codes in round brackets may look too much like regular text to be really useful, and even replacing the round ones by other types of brackets may be problematic, for instance, if you happen to be analysing a linguistics text where different types of brackets are used to indicate different levels of analysis, such as, for example, curly brackets to enclose morphemes, or square ones to mark phonetic representation. Thus, you need to find something that distinguishes you code from any ordinary text, ideally by using characters that do not form a part of any normal text. For example, if you're investigating grammatically polysemous words like the ones we searched for above, you can add an underscore and a simple word category identifier -such as N for 'noun', V for 'verb', etc. (we'll soon learn more about this when we discuss morpho-syntactic annotation in Section 7.1) -to each occurrence of the word (form). Once you're finished, you can just load the resulting file in AntConc instead of the original corpus, and filter out or sort the relevant entries in another concordance. And once you've reached a more advanced level in your research, you might also want to consider using a 'proper' markup language, such as HTML or XML, for annotating and categorising your data. Such options will be explored in some more detail in Chapter 11. For now, we only want to practise adding simple word class codes to our results file.

Exercise 25

Open the results file from Exercise 22, and categorise at least the first 30-40 search results, but ideally all, according to their grammatical categories as described above.

Save the file and load it in AntConc. Run a search for the search term again, this time only searching for those occurrences where the word mind occurs as a noun, then as a verb.

Solutions to/Comments on the Exercises Exercise 18

As you'll hopefully observe while doing the online exercise (and can verify again in the exercise paragraph replicated below), the computer search for our fairly common sample words managed to find this, but not This, in as a word, but also many other occurrences of the character (letter) sequence i+n (where both are lowercase characters) that you would probably not have expected to find. This is because the computer does not understand what a word is/may be, and can thus only 'blindly' match characters. The only 'word' that was apparently not an issue in this exercise is very, but I said "apparently" above because of course the same issue as for this also applies to very, only that you may not have noticed it because no form of very with an initial capital letter occurs in the text. This is a short test paragraph to illustrate very basic concordancing. As you can see, in this basic form, concordancing is very similar to a Google search, only that it shows you the results in one or more pre-selected pieces of text, rather than trying to find them online. The other thing is that our expectations concerning the results may differ from the actual output of the concordancer, as you will hopefully have noticed while concordancing on the words this and in. For the former, you might have expected to find both occurrences of the word with and without an initial capital letter, but the very first word of the paragraph did not in fact get highlighted because in general concordances tend to be case-sensitive, so that they will only find exact matches. This unfortunately resulted in you finding fewer occurrences than you would have expected, whereas the search for in gave you many more hits (i.e. search results) than you would probably have expected because the grapheme combination <in> occurs as part of a number of other words in this paragraph. Both of these issues are related to differences in the way that human readers and the computer process texts and we'll explore options for handling them later.

Exercise 19

This exercise was designed to allow you to explore (very roughly) the different types of functionality a concordance program, such as AntConc may provide in addition to pure concordancing. Furthermore, if you've paid close attention to the configuration options, you may already have noticed that the program not only allows you to work with single, or a number of different, files at the same time, but that you also have some degree of control over the particular (plain text) input format and its encodings, something you should by now be able to understand better through the exercises we did in previous sections. As far as the basic concordancing interface is concerned, you'll hopefully already have spotted that you can in fact adjust the context displayed by the concordancer for showing the result, as well as that there are various options for sorting our results, which is something we'll explore in more detail in Section 5.2.1.

Exercise 20

Essentially, the main aim of this exercise was -apart from giving you some initial concordancing practice in AntConc -to further raise your awareness of the fact that words -or rather word forms -in text are basically just sequences of characters for the computer/concordancer. Thus, generally the concordancer will not be able to 'understand' that you may be looking for a word with a particular meaning if there are homographs or polysemous forms, as in the case of round, which may be classified as an adjective, an adverb, a noun, a verb, or preposition. And, although you would most likely have expected to mainly get hits for the adjectival meaning, which is perhaps the most prototypical meaning associated with it these days, you will have seen from the results from the two novels by Jane Austen that the adverb or preposition meaning appear to have been much more common in her day, or perhaps only in her writing, something which only an in-depth comparison with the works of other authors of her period can establish for sure.

Another thing you could potentially have learnt from this exercise is that, because we tend to have particular expectations regarding any data we may be analysing, these expectations may sometimes initially make us blind to alternative options. However, as the examples from the exercise will hopefully have shown you, if we use a concordancer to investigate enough data, we're bound to find examples that may run counter to our imagination, and also help us deepen our knowledge/understanding of how words are used, which is one of the reasons why concordances are not only useful for research in lexicology or lexicography, but also as a means for native and non-native speakers to develop their language skills. Now, let's briefly return to the issue of case-sensitive vs. case-insensitive searches. The computer stores what we commonly refer to as small and capital letters in a different way, so that it becomes possible to search for them separately, for instance if we only want to find words that occur at the beginning of a syntactic unit ('sentence') or proper names. In many cases, however, we just want to find all instances of the word to get an overview of its different functions, and, for this, the position may simply not always be relevant. To test and see what happens if you make the search case-sensitive, go back to your search in AntConc, briefly change the search term to Round with an initial capital letter, and run the search again. This time, you should in fact get no hits at all because none of the occurrences of the word round are in such an initial position in our texts.

Exercise 21

If you've observed the results of our search for the word form mind closely, you'll probably have noticed a number of things. First of all, that -just as in our previous exercise -the word form mind is again grammatically polysemous, i.e. may represent different word classes, in this case either the singular form of the noun or the base/infinitive form of the verb. Secondly, that sorting the output in this way makes it far easier to see which word forms may precede the hit most frequently, and last, but not least, also which word classes/parts of speech may occur most frequently/typically with a given word form. For example, by examining the first 121 hits, you'll notice that more than half of all 225 hits are in fact nouns (apart from number 29, don't mind) because they're preceded by determiners, possessive pronouns, genitive nouns (e.g. Emma's), qualifying attributive adjectives, and quantifiers, while the rest of the examples do contain a few more instances of verbs, characterised by preceding adverbs, such as never, or the negation operator not.

However, as examples 212 -"the Highbury people, but if you call to mind exactly the present line of the path." -& 213 -"bake or boil. William did not seem to mind it himself, he was so pleased to think" -illustrate clearly, disambiguating the word class simply based on the preceding word may not be straightforward, either, as the first of them has the word form to occurring as a position marker, and the second as an infinitive marker. Such issues may also cause problems for approaches to the automated processing of language where such disambiguation is of course also important, but naïve algorithms based on probability-based assumptions regarding the word class of only a single word preceding a grammatically polysemous item would potentially fail, as such probabilities would, in our case, clearly favour the more frequent use of to as an infinitive marker.

Exercise 22

As this is more or less a 'mechanical' task, there isn't really much you can do wrong here, apart from maybe making one or two minor mistakes. The first could be that of not choosing an appropriate output folder to store your results in, which could mean that you may end up spending a considerable time searching for your output files later. The second might be that you don't label your output file sensibly, which will have a similar effect in that you may find yourself searching for the right file for longer than necessary if you have a number of different output files that are not clearly distinguished from one another. Both of these mistakes basically may cause you to lose valuable time that could be spent on actually analysing your data, thus 'throwing away' one of the most important advantages of using corpus linguistics as a methodology, which is that it allows you to save a significant amount of time finding a large amount of potentially relevant and interesting data quickly. In the past, having long file names wasn't even possible, but these days, having a maximally explicit file name that is up to maybe 20 characters long is no longer an issue, although I have occasionally experienced some issues with exceedingly long folder or file names when trying to back up files on even more recent versions of Windows.

Exercise 23

This exercise is very similar to the ones we did in Chapter 4 in order to clean up our data, so it shouldn't really be too difficult to do. The only thing you may need to be careful with is not to accidentally highlight hits that you don't want because the sequence of characters that represent your hit might also be part of another word. In order to avoid this problem, it may be best not to do all replacements fully automatically, but instead use the search-and-replace functionality step-bystep to identify and 'OK' each replacement.

Exercise 24

Again, this exercise is quite straightforward. The only thing that could happen is that you accidentally either delete an entry you hadn't intended to delete, in which case you'll need to re-run the concordance and delete more carefully, or that you may accidentally select too many hits before pressing Delete. In the latter case, you won't need to re-run the concordance, but can simply click anywhere in the hits to remove all selections, although you'll still need to select the ones you want again. In order to avoid any issues with this, it's probably best to only select a few hits each time before deleting, and then start selecting again…

Exercise 25

This exercise was really just meant to demonstrate a) how easy it is to categorise your own data in a simple way, and b) how efficiently you can then work with such pre-categorised data. Of course, adding this type of information can be quite time-consuming, so, in later sections, we'll explore ways of adding similar types of information automatically, as well as looking into ways in which we can exploit such annotated data even more extensively in order to search for and identify more complex linguistic patterns.

Sources and Further Reading

Regular Expressions

Imagine yourself frequently having to look for very similar patterns that differ only in a few minor details, such as in the different forms of a verb paradigm, for example, (to) walk, walks, walking, walked, singular and plural forms of nouns, or words that may start with either a capital or small letter. This is a very common and important task in linguistics, even for languages that only exhibit a moderate degree of inflection, such as English, but much more so for more strongly inflected ones like German, etc. Now, even if you are aware of all the relevant forms you may need to identify, and search for each of these forms separately in a row in a concordance program, you can only save the results, maybe even print them out, and then compare them afterwards. However, this makes your job much, much more difficult and time-consuming than it needs be, and may also lead to your overlooking some details or missing out on important generalisations. Thus, to be able to work more efficiently with a concordancer or similar search tool, it would be very useful to be able to specify more complex patterns that we could then look for all at once, and also make use of the other useful options we've explored before, such as sorting, etc., to help us simplify our analysis procedures.

Regular Expressions (or regexes, for short) are an important and very powerful means of specifying such complex search terms for concordances or computer programs for language processing. Most concordance packages support at least some basic forms of regexes, although they're not necessarily as advanced as the options offered by command-line search utilities, such as (e)grep (global regular expression printer), or programming languages, such as Perl, Python, or Java. Here, we'll discuss the most important general regex concepts in order to demonstrate their usefulness for linguistic purposes, but may introduce further, more complex, options later, as and when required for advanced analyses. We'll start by discussing some of the abstract notions behind regexes, each time accompanied by appropriate exercises that should allow you to observe and test the individual features we're discussing. At this point, perhaps a cautionary note is in order, as the individual features related to regexes that we need to introduce individually will really only make sense fully from a linguistic analysis perspective once we can put a number of them together in order to construct efficient searches. Therefore, the very first exercises, even though I've always tried my best to make them as relevant as possible to linguistics, may not yet appear very useful to you, and you might well be tempted to give up if things appear too abstract. However, if you persist, I can promise you that soon you'll not only be able to understand the value of regexes for achieving even highly complex tasks in linguistic analysis very efficiently, but will also learn to hone your analysis skills related to morphology and morpho-syntax, something you'll definitely be able to profit from in your work in corpus linguistics, as these two areas often form the basis for further linguistic analyses.

Tip: When using regexes in any other program, always bear in mind that there may be fairly large differences in their implementation, depending on the purposes or design decisions made by the author! This is especially the case for using them in search-and-replace operations, where many editors will only offer a reduced set of functionality, or even implement a special syntax for some purposes.

Below, you can see a short test passage. This passage will mainly be used in the online exercises (

Character Classes

Character classes allow you to specify groups/ranges of characters in order to efficiently search for such characters that have specific shared properties, maybe all being vowel or consonant letters (as opposed to the actual vowel or consonant sounds, i.e. phonemes), etc. These classes contain individual characters or ranges of characters, where a range is indicated by a hyphen in-between characters that should be consecutive. These patterns are enclosed in the same kind of square brackets ([]) that we usually use for (narrow) phonetic transcription. Here are some basic examples: r [a-z] (all English lowercase letters); useful, for example, if you want to exclude all proper names/nouns or sentence/paragraph-initial words from a search r [A-Z] (all English uppercase letters); useful for finding proper nouns or initial words r [0-9] (all digits, also often abbreviated \d); useful for finding and possibly removing numbers from word lists r [aeiouy] (all lowercase vowel letters for English); useful, for example, for finding words that begin or end in vowel letters r [Tt] (either <T> or <t>); this will allow us to specify words that start with an upper-or lowercase letter 't', as in This/this, That/that, The/the, etc. r [A-E0-3 ] (all uppercase letters between A and E, all digits between 0 and 3, and a space); potentially useful for finding specific English postcodes

In order to develop a feeling for how to construct character classes and establish their usefulness for identifying basic linguistic features potentially shared by different word forms, let's do a two-part exercise, both off-and online.

Exercise 26

Test the character classes shown above on the sample paragraph provided in Figure

Next, search for the same character classes in the online exercise version at

Feel free to make your own changes to the classes, too, to explore some more.

Other classes that are often predefined as shorthands/abbreviations are:

r \w for all word characters, usually including hyphens, theoretically also matching characters that are not part of the character set used for English, such as, for example, ß, ä, é, c ¸, ñ, 𝜉, etc. r \W for all non-word characters, such as, for example, punctuation r \s for (white)space characters (but sometimes a whitespace itself only) r a single . usually stands for any arbitrary character, unless it occurs inside a character class, in which case it simply means a dot.

Exercise 27

To experiment a little with the character class shorthands shown above, test these on the sample paragraph above and online, and observe the effects.

Each character class, the way we're defining and using them at the moment, essentially represents options that act as placeholders for one single character only. The only reason why all of them will get highlighted in the sample paragraph is because the script that allows you to display them identifies them globally, that is, each and every one of them individually. Just seeing the results may also not yet allow you to see the usefulness of creating/using this type of class, but this will soon become clearer when we introduce quantification.

One special feature to note is that, as hyphens are used to indicate ranges in character classes, if we want to include them as hyphens in a class, we have to place them somewhere where they don't occur between characters, that is, either at the beginning or the end of the class definition.

Negative Character Classes

As we've already seen for \W, sometimes it's useful to simply be able to negate character classes if we want to exclude certain characters/constructs. Unless we have such a predefined group as \W, we need to define this class ourselves, which is done by creating the class in the same way as a positive class, only that the opening square bracket needs to be followed by a caret sign (ˆ), for example, [ˆA-Z] (no uppercase English letters).

Exercise 28

Try to think of some negative character classes and test them on the sample paragraph. If you still have difficulties thinking of any sensible ones yourself, just negate the positive character classes we saw above, and try to understand what's happening.

As you'll hopefully have observed, simply excluding a particular character by negating it may have rather unexpected effects. So, for instance, if we simply exclude a capital <T> and expect now only to find instances of words that start with a <t>, just because the word form we had in mind may have been <the> or <this>, this is clearly wrong because we're only excluding one single character option, rather than the whole set of potential options we may have wanted to exclude. Thus, successfully specifying an exact pattern may often require careful thinking and using a mix of different regex options to constrain our pattern.

Quantification

Now, simply being able to look for single individual characters, or those belonging to a specific class on their own, frequently doesn't make such a lot of sense, so we need a way of being able to specify how often they can occur. This can be done by using quantifiers, which allow us to say whether a given character or class may (or has to) occur zero or more times, maybe even infinitely, in a row. This is a bit similar to using quantifiers in natural language, only that there we tend to quantify noun phrases through 'non-specific' (or underspecified) quantifiers like some, many, or all, as well as using numbers or numeral quantifiers, such as once, twice, three times, etc., to say exactly how many subjects or objects we may be talking about. On the other hand, in natural language we also have quantifiers that signify the absence of something, such as none or no one, which actually have pronoun character. The specific (basic) options for quantification in regexes are: r a * following a character (class)/group means it may occur from 0 to an unlimited number of times; expressed in natural language, this would be from none to infinitely many. r a ? following a character (class)/group means it may be optional or can occur at most once; in natural language: possibly (not) to maximally once. r a + following a character (class)/group means it has to occur at least once but up to an unlimited number of times; in natural language: minimally once to infinitely many. r a set of curly brackets {} following a character (class)/group specifies a more exact quantification c {5} matches exactly five times; or: no more and no less than 5 times. c {5, } matches at least 5 times or up to an unlimited number of times: minimally 5 times to infinitely many. c {5,10} matches between 5 and 10 times.;

At least in this way, we can already specify that we may want to look for something like \s\w+\s, that is, only words, although, of course, we need to bear in mind that not all words are actually surrounded (delimited, in technical terms) by two whitespaces. This equally allows us to take into account dialectal differences, such as the differing British and American English spellings of the word colou?r, but still doesn't quite give us the flexibility we may want in looking for specific word forms or paradigms (i.e. all associated forms of a word), which is why we need to introduce three further concepts below, after doing another exercise.

Exercise 29

Test the two quantification examples shown above using the sample text. The text on the online page is editable, so you can also delete the u and see whether the quantification using the question mark still works, or even try some of your own words, if you know any other differences in spelling that relate to one character only. You can also try to find words where a character is repeated multiple times.

For the example of the whitespace-bounded words, also experiment with the curly-bracket type to practise more exact quantification for the number of characters allowed inside a word. Can you already detect any practical use in this?

Anchoring, Grouping and Alternation

Anchoring

To be able to increase the precision in the search for words, we next need to solve our problem of word boundaries that aren't signalled by whitespace, that is, if a word occurs at the beginning or end of a line or in a different context. In cases like these, we can anchor our search string in different ways. The greatest flexibility in this is provided by marking word boundaries, for example, by writing \band\b if we only want to look for the word and, where \b is the word boundary marker. The backslash preceding the b here is just like the one we've seen in the other abbreviation classes above and changes the 'meaning' of the b from a literal character to something different. Thus, although our example may look like the word band preceded by a backslash and followed by \b, it actually represents a combination of boundary marker + and + boundary marker. To see exactly how this works, let's do another exercise.

Exercise 30

First, try the example of and as described above on the print version of the sample text to see whether you can already spot the odd-one out, then try 'searching' with and without boundary markers in the online version.

Finally, using appropriate quantification, look for words of different length.

If we want to anchor our search term at the beginning or end of the line (or string), we can specify ˆor $, respectively. However, please always bear in mind that this may make sense in a program that reads texts line by line, such as grep, a Perl script, or most of my own programs that allow you to run line-based concordances, but not necessarily in a stream-based concordancer which usually reads and processes all words as a continuous stream of characters/words and may therefore ignore these markers, or may only match at the beginning or end of the whole file!

Grouping and alternation

Enclosing items in round brackets (in the first instance) causes them to be grouped. Thus, for example, (\sapples\s) * would look for the exact group/sequence of characters representing the (white-space-bounded) word apples, as well as specify that the word may occur zero or any number of times in a row. Granted, we generally wouldn't really expect this word to occur two or three times in sequence, unless we may have encountered a copy-and-paste error where the word was reduplicated accidentally, or maybe in a question like "Why do we call apples apples?", so, in this case, we're probably more interested in specifying that the word could, but need not, occur. However, in some rare cases, apart from the question just cited, it may indeed be grammatically correct to repeat the same word form twice in a row, albeit with different grammatical functions and meanings. This could, for example, happen in a relative clause that begins with the relative pronoun that, followed by the demonstrative determiner that, as in the following example taken from the BNC: "I realize now that that is what I want more than anything else" (A08 2983).

Grouping character sequences together, though, isn't the only thing bracketing allows us to do. It also gives us a way to express alternatives within our group, and thus a much greater flexibility in specifying linguistic patterns. If, for instance, we wanted to be able to search for a number of different types of fruit at the same time, we could construct a regex like this: (apples|bananas|grapes|pears), where the pipe symbol (|) separates the alternatives from one another. This way, we can also specify alternatives within alternatives because we can nest brackets within one another, as in, for example, (\b(an?|the)\b (old|young) (wo)?man), which would group and find all occurrences of a/an/the old/young man/woman, where I've indicated the alternatives options using slashes, allowing for either an indefinite or definite article, followed by either one of the two adjectives, and followed by either man or woman.

Exercise 31

Try something similar by specifying a regex that will find all the verbs in the sample text above, then do the same thing for all nouns. First, draft the regex on paper and then try it in the online interface. Are there any unexpected results or difficulties in specifying the patterns? Another 'side effect' of using brackets in many programs/programming languages that use regexes is that whatever is enclosed in a group is captured and can usually be 'referred to' again via a so-called backreference, where each group of brackets, starting with the outermost, can be referred to by a number, generally preceded by a backslash. Thus, if we had only one pair of brackets, as in the fruit example above, we'd be able to retrieve whichever one of the alternatives was found by \1. In the more complex example with the nesting, \1 would contain the whole expression found, \2 whichever article was found, \3 either one of the adjectives, and \4 the initial morpheme of woman, provided that woman was matched in the first place, but nothing if only man was matched.

As concordancers generally only match one search term, though, they are very unlikely to let you do much with backreferences. However, in programming or in converting data from one particular format to another, this is extremely useful for extracting parts of complex matches, and 're-using' them. In some editors that allow regex search-and-replace operations, you can even do things like automatically swap \1 and \2, for instance to move adverbs after auxiliaries, as in for instance turning the phrase I (rarely) (have) seen automatically into I have rarely seen, or even turn some declaratives into interrogatives (or exclamatives), provided that you ignore punctuation and sentence case, as in, for example, I have seen it to have I seen it (?!). If you want to, and your editor, like the ones I recommended above, supports regex replacements, you can try this on the above example for swapping the adverb with the auxiliary. To do this, simply type the initial phrase into an empty document and then search for (rarely) (have) and replace this by \2 \1, making sure that the option for regex replacements is switched on. You'll probably learn to appreciate backreferences much more once you've learnt about how to construct linguistic annotations in Chapter 11, where you'll be able to convert some original data into a special coding format using techniques such as the one above to carry out the conversion more efficiently.

Quoting and using special characters

Now we've discussed almost all the essential details to begin using regexes properly inside a concordancer, apart from one little detail. If certain characters, such as the dot/full stop (.), question mark, or caret, etc., have a special meaning for the program interpreting the regex, how do we actually specify those characters if we explicitly want to look for them? Well, this is done by escaping them, which is generally achieved by preceding them by a backslash (\), that gives them back their literal meaning. Therefore, in order to search for a full stop, you simply write \. (but note that this is not necessary inside a character class, where all punctuation marks retain their 'punctuation meaning').

For linguistic purposes, when we quantify character classes or shorthands, we'll mainly search for and use constructs that represent word characters, for example, something like [a-z]+ or \w+, which is somewhat safer than using a . to simply indicate any character. This is because quantifying the latter will literally find any character at all, including spaces, line breaks, as well as even special control characters that only the computer may understand, or even word boundaries themselves. Thus, if we, for example, want to look for words with particular beginnings or ends and think that, instead of using appropriate constructs representing word characters, we can be lazy and just write the dot followed by an appropriate quantifier, we may end up with rather unexpected results. For instance, if we wanted to look for all pronouns that start with the character sequence <the>, i.e. them, these, their, etc., and just specified \bthe.+\b, assuming that what this would find starts with said combination and ends at the next word boundary, we'd be in for a surprise. This is because what it would in fact generally match is the very first occurrence of the character sequence, followed by the rest of the text, that is, a combination of word characters, whitespaces, punctuation marks, etc., until we reach the end of the text itself, which is -perhaps not so obviously -also a kind of word boundary. This happens because most regex engines assume that quantification is by default greedy, that is, supposed to match as many characters as possible. In order to avoid this problem, and still use our 'lazy' version of a shorthand, we have to make the quantification non-greedy, which is done by adding another question mark after the first quantifier. In our case, we'd then have to write \bthe.+?\b, which would stop the match as soon as the regex finds the first word boundary following the quantified shorthand expression, also finding non-pronouns like theme, thematic, thesis, etc., though.

Constraining the context further

In some cases, rather than just specifying everything we want to find, we also need to be able to indicate that something either should or should not occur in an environment. To some extent, we've already done this when we used word boundaries above, because whenever we inserted a boundary marker in our regex constructs, we effectively said "don't allow another word character to occur here", thereby constraining the options for a match. To return to our example of and from Section 5.4.1, where we wrote \band\b, we stated that there should be no word character preceding the <a>, and no word character following the <d>. This is a very powerful construct, but only allows us to constrain one particular environment, that is, the beginning or end of a word we're looking for. Sometimes, however, we also need to be able to constrain certain patterns inside or around parts of a word or a grouping, in which case we need other ways of being able to indicate that we either don't want something to precede or follow a specific pattern. This can be achieved by using so-called lookaround, which is subdivided into lookahead and lookbehind. The former allows us to constrain what is (positive lookahead) or is not (negative lookahead) supposed to follow, while the latter provides positive and negative options for whatever is or is not supposed to precede, our search term. As groupings normally capture, and we don't want whatever is supposed to be constraining our grouping to become part of the match, and thus be highlighted by the concordancer, these constructs actually use a special syntax that excludes them from being captured, which is that the opening bracket of the grouping parentheses is immediately followed by a question mark, i.e. (?…).

To illustrate the usefulness of such a feature, let's assume you want to teach students vocabulary about different types of containers to store things in, in which case you may want to run a search for compounds involving the words box and case to find all singulars and plurals. From what we already know, we can easily achieve looking for the stems of these compounds by writing the following regex: (boxe?|case)s?. However, using this expression, we're not only bound to find many occurrences of the stems themselves (which are relatively uninteresting), but at the same time also retrieve instances of the word case in its legal meaning or as part of the prepositional phrase in case. To avoid this, we can use negative lookbehind like this, (?<!\b)(boxe?|case)s?, stating that the stem should not be immediately be preceded by a word boundary. The counterpart, positive lookbehind, can for instance be used to find all words at the beginning of a sentence, using the following expression: (?<=[.!?] )\w+\b. As you'll hopefully already be able to guess, this finds all words that follow a major punctuation mark and a space, but unfortunately misses all first words in paragraph-initial sentences, as these are not preceded by punctuation. Now, unfortunately, we cannot simply combine these searches because the one limiting feature of lookbehind in most regex implementations currently is that they have to be fixed, so that we have no option but to run both searches and then combine the results, if necessary.

Lookahead, probably because it's easier to implement, doesn't suffer from this limitation, so that we can specify more complex expressions, such as the counterpart of the one we just tried, in one go, i.e. \b\w+(?=([.!?]|$)), which finds all words that occur at the end of a sentence and also the end of a paragraph, along with, unfortunately, Roman ordinal numbers and some abbreviations that may occur in the middle of the sentence. Negative lookahead basically allows us to exclude unwanted constructions, such as parts of a paradigm. Thus, if we wanted to find all but the ing-forms of the verb want, we could say \bwant(?!ing)\w * \b, where we first ensure that <ing> cannot follow the stem, and then say that any other word characters may or may not follow before the boundary. Of course, depending on your corpus, you may also find some rather unexpected words that have nothing whatsoever to do with the verb want; for instance, because my test corpus for trying out regexes contains more 'archaic' language, I also found the adjective wanton, as well as some other constructions, this way. In comparison to the genuine expected results, though, those were extremely small and easy to weed out after sorting.

Further Exercises

In order to do these exercises, you should have AntConc installed on your system or on a memory stick. The exercises are based on the Project Gutenberg e-text of Wuthering Heights, which you can obtain from Project Gutenberg (

Exercise 32

Start AntConc and type in hand as your search term. View the results to see how many different words or word forms you get. Tip: It's best if you sort your results.

Next, check the tick mark for 'Regex', run the search again, and observe/interpret the results to see what additional useful or not-so-useful information may have been added to the results.

Next, in our first attempt to narrow down our search to forms we know belong to either the noun or verb paradigm, specify hand(ed|ing|s)?

instead. Check the results and see how/whether they differ from what you found before.

Change your search term to include a \b at the end of your search term and observe the difference. Now, add \b at the beginning of the term, too, and compare the results. Last, add [ˆ-] at the beginning of the term and check the result again.

Some more, diverse, exercises:

Exercise 33

Do a search for all occurrences of the 'verbs of belief', guess, suppose, think, believe and assume.

In the next step, add all possible inflectional endings to the search. What will you fail to catch with this? Do the same for see and understand. Do you encounter similar problems here? For all the above exercises, it obviously doesn't make sense to only do them purely mechanically, but you should always analyse and evaluate the results very carefully. This generally involves keeping a record of how you achieved the results by setting up and possibly modifying your regex until you've managed to find all the expected results, keeping track of interesting, but unexpected, ones as well as saving some suitable examples and discussing them with regard to their (morpho-) syntax, semantics, pragmatics, possible frequency distributions, etc.

Solutions to/Comments on the Exercises Exercise 26

Character classes clearly represent one of the features referred to above, where we would get too many, and probably also linguistically uninteresting, search results, if we simply look for these in a corpus. However, what you'll hopefully have learnt from the online demo display is that a character class on its own simply represents alternative options for finding a single character, which is why I've chosen to surround all instances by a little extra space to make the individual characters stand out more clearly. When using character classes, and also partly when looking at the most salient examples in the list of basic examples above, it's perhaps easy to be confused into thinking that character classes always represent consecutive ranges, but of course they can represent any kind of grouping that may be (linguistically) meaningful in any way, as in, for example,

Exercise 27

In my description of the \w word characters definition, I deliberately used the word "theoretically" when stating that this construct should also match other characters that occur in words, in whichever way or character set they're transcribed. Unfortunately, though, JavaScript, which I used to create the online exercises, only seems to take characters from the traditional Latin (ASCII) character set as being word characters, which is why the phonetic characters are also not selected. However, when you use the 'any character' construct, i.e. the single dot (.), the phonetic characters are at least also recognised as characters. I also tested the same feature with some Chinese characters, which are clearly word characters, but again these were not recognised, which again proves that we always need to test any regex constructs we use in different programs to see whether they're going to be recognised properly. In this case, JavaScript has shown that its implementation of regexes is still very limited, but many other languages or programs -including AntConc -understand many more advanced regex options.

One further thing that may have been confusing when you tested for whitespace is that around all the spaces you will probably have noticed strange so-called 'pipe' symbols (|). These indicate word boundaries and their being displayed here is due to some tricks I had to use in writing the script to make it display everything the way I wanted it displayed, so you can safely ignore them .

Exercise 28

As stated below the exercise itself, it's quite easy to get confused about what negation does to a character class, partly because we may sometimes get the impression that we're actually just turning around a definition if we negate a range inside a character class. However, although, for instance, negating the original character class [A-Z] (i.e. all English uppercase letters) to [ˆA-Z] will predominantly have the effect of now showing us/selecting all lowercase letters, it will also show/select non-word characters, such as punctuation, spaces, or numbers, simply because they're obviously also non-uppercase characters. Thus, one thing you may need to do when defining/using a negative character class, apart from thinking about it carefully anyway, is to not think in terms of binary oppositions.

Exercise 29

Essentially, the two basic examples in this exercise have allowed you to explore a number of things: a) how to specify one optional single character through the use of a question mark: the difference between colour and color, or the presence/absence of a 3rd person singular or plural s, provided that you add a \s after the question mark, which will then find only those instances that are not followed by a punctuation mark, b) how to possibly identify words that contain multiple occurrences of the same character, such as double f, s, or l, or even words with duplicated vowel letters such as the word good, c) how to identify words of different length, but only if they're in fact surrounded by whitespace.

Examples of type a) allow us to work with different dialectal variants, or sometimes also to cover spelling variants in historical texts, where the spelling may not yet have been standardised, so that even one and the same text may contain alternate forms that represent exactly the same word meaning. In addition, we can already model very basic morphological features, such as the plurals referred to above, or vowel alternation in irregular verbs that may signify differences in tense, for example, give vs. gave, or in a small number of singular-plural distinctions for nouns, such as in man-men. Examples of type b) make it possible to identify phonotactic features, such as the presence or absence of reduplication and its effect on pronunciation, while type c) may be useful for selecting or extracting words of different length in order to establish potential correlations between word length and complexity, or, if we assume that shorter words are indeed less complex, to extract simpler vocabulary from texts in order to teach it at less advanced learner levels. Obviously, though, to what extent we'll in fact be able to use these regular expression constructs in the ways described here again depends on which particular program we're using and how regexes are implemented there.

Of course, although we've already made great progress towards identifying words, we still have the issue of words not surrounded by whitespace to consider, but we'll soon find a solution for this, too.

Exercise 30

When scanning the printed sample text visually, you'll hopefully already have spotted that the character sequence <and> not only occurs in the conjunction, but also as the final part of the word understand. If not, then the online exercise will have revealed this very quickly, although you may still have had to look carefully to see it. Of course, now knowing that this sequence may occur inside another word, we could make it easier to highlight such examples simply by adding a \w in front of the character sequence <and> in our search, thereby specifying that it has to be preceded by at least another word character. However, this still wouldn't allow us to identify all occurrences where the sequence occurs at the very beginning of the word, so we need to be even more precise and formulate our regex like this: \b\w * and\w * \b . If you cannot immediately recognise what this means, then I suggest you spend a few minutes thinking about this and maybe even test it on a longer text in AntConc, as described in Section 6.5.

As I hope you'll also have seen, by using the boundary markers in combination with the shorthand for word characters, you'll now definitely be able to look for words of fixed or variable length properly, where, for example, \b\w{2}\b will find all words that are exactly two characters long, \b\w{2,5}\b between two and five characters, and \b\w{2,}\b at least 2 characters long.

Exercise 31

Your first attempt at finding all verbs will probably be to simply identify all existing verb forms in the sample paragraph and group them together in one fixed regex that matches everything exactly as it occurs in the text, which would probably look like the following string: (allow|test|displaying| \bbe\b|contains|Can|understand|based|concerning|make), where hopefully you'll already have realised that the sequence <be> can also be part of another word and thus we'd at least have to add boundary markers around it to only find the base form of the verb. However, if you think about this more closely from a linguistic perspective, doing this only allows you to find what's really there, but wouldn't be suitable for identifying all the different potential word (paradigm) forms of the verbs we may be interested in if we were actually researching them. In order to properly investigate all the different verb forms, we'd minimally have to try and specify a regex that groups the base forms of the verbs together, and then adds another optional grouping that expresses all the possible verb endings that could be present, and which may then initially end up looking somewhat similar to the following, already considerably more complex, construct:

(allow|test|display|\bbe\b|contain|Can|understand|base|concern| make)(e?d|ing|s)?. This regex, though, still has a number of problems, not least of all that it would also potentially find the word shallow, so that we'd at least need to use a boundary marker before the word allow, too. Other things the expression may also find, and which we may or may not actually want it to find, are the 'negative counterparts' of some of the verbs (or nouns) that carry a negative prefix, for example, disallow, misunderstand(ing), debase(d), unconcerned, and unmake. If we want to exclude some of these, but still not the noun forms understanding, base(s), concern(s), or make(s), not to forget capitalised can, which was yet another issue above, then we'd at least have to add a boundary marker before the whole group, and ideally also after it, thus yielding something along the lines of or at least close to this: \b(allow| test|display|be|contain|Can|understand|base|concern|make)(e?d| ing|s)?\b. To make this expression completely flexible to cover almost everything we want it to find, we'd then still need to adapt it a little more by allowing for initial small (lowercase) and capital letters: \b([Aa]llow|

Now, if you have a keen eye for, or at least some substantial training in, morphology, you'll also have spotted that, among all the regular verbs, we have two irregular ones that we haven't dealt with properly. The first one, make, is relatively easy to fix because we only need to add a character class that specifies the consonant options, i.e. [dk], as well as make the final <e> optional, so that we can also 'create' the present participle form, which, incidentally, we also have to do for the verb base. For the verb be, though, we have no choice but to list the full paradigm. So, the 'final' version of our regex would then be: \b([Aa]llow|

(e?d|ing|s)?\b. Now, I put the word final above in scare quotes because of course this regex still wouldn't be able to handle the negative contraction forms of the BE paradigm, which, however, I leave up to you to add as another exercise. Please also note that our regex would of course 'overgenerate', that is, match more than just legitimate words, e.g. also errors like maked, which could still occur, for example, in child language or learner interlanguage. One other thing we need to be aware of here is that the above regex could still be made more compact, albeit less readable, to the novice, if we grouped all the words that start with the same letter together, which would obviously lead to more brackets and pipe symbols within brackets.

I'll leave the second part of the exercise, to identify all the nouns, to you. Now that you're aware of the main issues, you should probably be able to handle this one relatively 'easily' if you bear in mind that nouns do have singular and plural forms, etc. Unfortunately, though, given our current means, and even using the most sophisticated regexes, we still have no means of distinguishing between grammatically polysemous nouns and verbs. To solve this problem, though, we'll soon look into ways of enriching our data with word-class information.

Furthermore, now that you've gone through a lot of trouble to painstakingly create two sets of fairly complex regexes, you'll probably want to test them on more than just my short paragraph, so I'd suggest that you open a file in AntConc, paste the regexes in the search box, tick the 'Regex' option, and see what the results will be…

Exercise 32

When you do a basic word search for hand in AntConc, not using the regex feature, you should get 94 hits altogether. To distinguish between your examples, you can simply sort on the first word to the left (Level 1: 1L). This should allow you to quickly establish that there are no occurrences of hand as a verb, but instead a number of different noun uses, ranging from the basic noun (phrase) use (a hand, her hand, an unformed, childish hand), via temporal/locative prepositional/circumstantial phrases (at hand, on either hand, at your left hand, by the hand), idiomatic/metaphorical expressions (take him in hand, try my hand at, came to hand; He shall have his share of my hand [i.e. be beaten], offering the right hand of fellowship), to textual deixis (on the one hand). Furthermore, there's also a single compound noun, minute-hand.

Once you tick the 'Regex' option, you'll probably be very surprised that you suddenly end up with 199 hits instead of the original 94. Sorting the hits again, this time by the word itself (Level 1: 0), and then by the first word on the left (Level 2: 1L) quickly shows us that now we not only find many more forms of the noun, including plurals, plus a few ED-forms of the verb (commonly referred to as 'past participles'), which we did want to find, but also quite a few words that simply contain the character sequence <hand>, such as, for example, handsome or even chandelier, which we definitely didn't want to find.

When trying to narrow down our search to noun or verb forms of hand by specifying the regex hand(ed|ing|s)?, unfortunately we still get all of the 199 hits that the most basic regex string has already given us, so we definitely need to explore better ways of limiting our search via additional regex options. The most basic thing to do here is to 'say' that the verb/noun 'endings' we specified really have to occur at the very end, and 'word end' here means 'word boundary', i.e. \b. Please note that, even if you may have assumed that we could simply replace the ? quantifier in the regex by a + to force the endings to occur at least once, this simply would not work because we would then be excluding all uninflected forms.

Adding the boundary marker at the end now already reduces the number of hits to 159, that is, excluding 40 unwanted hits, but still leaves us with some unwanted cases, beforehand, close-handed or minute-hand. Adding another boundary marker at the beginning of our regex now eliminates the 2 occurrences of beforehand, reducing the number to 157, but still leaving us with close-handed and minute-hand because hyphens are generally considered to be parts of words, for instance many compound nouns, so, in order to get exactly the results we want, we also need to exclude the hyphen, thus making the final regex \b[ˆ-]hand(ed|ing|s)?\b and only leaving us with 155 relevant results.

Exercise 33

As in the previous exercise, we'll develop the solution here as far as possible, in a step-by-step manner in order to 'fine-tune' the regex as much as possible. I'd thus suggest that, at every stage, you try out the adjusted regex and observe the results closely to enable you to understand the process better. To begin exploring the options here, you'll probably want to start with the simple pattern \b(assume|believe|guess|suppose|think)\b. To make it easier to read, I've already put the words into alphabetically sorted order, which has the added advantage of speeding up the search process a little because regexes actually try to match in alphabetical order. This first attempt will give us 210 hits, but unfortunately miss all the inflected forms.

In the next step, to add inflectional endings, and having noticed that the final <e>s in assume and believe and suppose may actually need to be made optional in the groupings for the inflections for the sake of creating a more consistent pattern, we can already change the pattern to \b(assum|believ|guess|suppos|think)(e?[ds]?|ing)?\b. This will give us 284 hits that are already fairly close to what we want to be able to find, but still of course misses the ED-form of think, thought.

Adding an option for this will then provide us with an improved solution for the regex, \b(((assum|believ|guess|suppos|think)(e?[ds]? |ing)?)|thought)\b, increasing the number of hits to 414, and indicating the importance of the past tense form of think in the text. However, after sorting the list on the search term and scrolling down to the end of the hits, we can see that our attempt to cover all inflectional verb endings has inadvertently led to our also finding a number of occurrences of the plural of the noun thought, which is definitely something we'd like to avoid if possible. Now, obviously, we don't really want to have to specify all the verb paradigms for each verb separately, but, on the other hand, cannot simply exclude the 3rd person singular present {s} morpheme because it's such an important part of the paradigm for verbs. Luckily, though, the {s} morpheme is not a part of the paradigm for the past tense forms, where we only have thought in both the simple past and perfect tense forms. Therefore, what we can do here is use a little negative lookahead to exclude the <s> from our pattern definition for the past tense form, yielding \b(assum|believ| guess|suppos|th(ink|ought(?!s)))(e?[ds]?|ing)?\b, and reducing the final number of hits to 400.

In this way, we've now managed to exclude all the plural forms of the noun thought, but are still left with a number of occurrences of the singular noun, as in:

1 every thought 2 an evil thought 3 Catherine's first thought 4 there lay immense consolation in that thought 5 ere they had a thought to spend for any 6 Hareton seemed a thought bothered 7 while I can bestow a thought on the matter 8 for every thought she spends on Linton 9 Our first thought 10 my great thought in living 11 no thought of worldly affairs 12 the slightest act not prompted by one thought 13 sometimes shuddered at the thought of.

Looking at this list, we may assume that, in order to improve our search results, we could adjust to restrict our search to only finding instances of thought not preceded by a determiner or quantifier, which, from a linguistic point of view, is obviously correct. However, unfortunately the one construct we know that should allow us to express such restrictions, that is, negative lookbehind, is (currently still) restricted to a single fixed pattern, so we could maximally exclude one of our options, in this case possibly <a> because it occurs marginally more frequently than any other form, but still only three times. Although, of course, we could easily add the negative lookbehind for <a>, this seems hardly worth it, especially because the effect is negligible, and because sorting the hits according to the first word on the left will relatively easily allow us to identify and delete all noun occurrences of thought. For the moment, we therefore simply need to accept that we have no ideal way to exclude the nouns form our search until we learn how to work with morpho-syntactically annotated versions of our data, which will then allow us to only look for verb forms much more easily.

A similar problem to the one above could also exist for the singular and plural forms of the noun guess because they look identical to the base form of the verb and the 3rd person singular present, but luckily an investigation of the data reveals that those don't actually occur in our text. If they did, we'd have to resort to the same solution again.

Regarding the search for see and understand, the first thing to note would be that the past tense forms are both irregular, so that the regex 'suffix' pattern would only need to include the 3rd person singular and ing-form, with one minor addition, the <n> that allows us to create the ED-form of see, while all other irregular forms would need to be specified as part of the stem allomorph pattern as in \b(s(a|ee)|underst(an|oo)d)

Sources and Further Reading

Understanding Part-of-Speech

Tagging and Its Uses

This chapter will explore one of the main breakthroughs in corpus linguistics, morpho-syntactic annotation, which is also referred to as Part-of-Speech (or PoS) tagging. This kind of linguistic technology makes it possible to enrich data with information that facilitates extracting and analysing specific word forms or constructions in a way that's more advanced than what we've discussed previously. However, despite this obvious advantage, we still need to be cautious and aware of its limitations when using it. Thus, the following sections will try to provide you with a rough overview of what exactly PoS tagging is and how it can be carried out, where its strengths and weaknesses lie, and how you may be able to use it with your own data.

The following is an extract from the beginning of the Brown Corpus that illustrates what basic PoS-tagged text may look like, although the exact format and conventions for representing the tags may vary for different annotation schemes:

The_AT Fulton_NP1 County_NN1 Grand_JJ Jury_NN1 said_VVD Friday_NPD1 an_AT1 investigation_NN1 of_IO Atlanta_NP1's_GE recent_JJ primary_JJ elec-tion_NN1 produced_VVD no_AT evidence_NN1 that_CST any_DD irregulari-ties_NN2 took_VVD place_NN1 ._.

For the moment, you don't need to understand what the tags mean, as we'll soon explore which different bits of information may be contained in a PoS tag.

As we've seen in some of our previous exercises and discussions, frequently one and the same word form may have different meanings or functions, that is, be semantically or grammatically polysemous. Very frequently, this polysemy is in fact of a morpho-syntactic nature because these differences in meaning depend on the word class associated with the word form in a particular context, as well as potentially its inflection. Remember the example of the duplicated word form that in Section 6.4.2, where the first occurrence was a relative pronoun and the second a demonstrative determiner? This type of grammatical polysemy is actually far more likely than you may assume, although it rarely occurs in such reduplicated word form contexts, but more frequently in the shape of words that are commonly assumed to be the products of zero-derivation or conversion. The following table, based on DeRose

As stated above, some of the high potential for grammatical polysemy in English stems from what is generally described as zero-derivation in morphology, that is, the ability of certain words to change their word class without any form of inflection or affixation. Most of the instances of this tend to involve noun-verb pairs that need to be disambiguated by their context in writing. Sometimes, in spoken language, we also have the option to recognise them through their differing stress and/or pronunciation patterns, for example, house (noun; /haUs/) vs. house (verb; /haUz/) or insult (noun; / ̍ Ins√lt/) vs insult (noun; /In ̍ s√lt/), etc., but of course, without additional grammatical information, this would only be possible in phonetically transcribed corpora.

For this and other reasons, it's highly useful to have corpora enriched with morpho-syntactic tags. At least some of the existing corpora that we'll be using here already do contain such information, and we'll also explore ways of getting our own data tagged to some extent later. Being able to make use of such extra information will not only help us to distinguish between word forms that are grammatically polysemous, but also to look for collocational or colligational patterns (see Chapter 10), in other words, to investigate formulaic language or language patterns more easily.

A Brief Introduction to (Morpho-Syntactic) Tagsets

To be able to understand exactly what type of information we can usually get from many tagsets, let's start by exploring a relatively simple tagset for English, the Penn Treebank Tagset, depicted in Table

Exercise 34

Look through the list of tags in Table

Think about whether there are any tags that may be unusual, either in the sense that they represent independent categories or that they may be 'lumping together' categories you'd normally have seen as being separate.

As you'll hopefully already have observed when going through the tagset, typically a morpho-syntactic tag will consist of one (in the case of punctuation in the Penn tagset) or more, often three, (capital) letters or special characters, beginning with one that represents the 'major PoS', that is, noun, verb, adjective, adverb, etc. Usually, and whenever possible, the designers of tagsets try to choose a mnemonic for this initial letter, based on the first letter of the word class itself (e.g. N for n oun, V for v erb). But of course, due to the fact that the names of some word classes start with the same letter (e.g. article/adjective/adverb; pronoun/preposition/particle), this is not always possible. In this case, an alternative letter that doesn't constitute the beginning of any word class will be used, for example, commonly J for ad j ectives, R for adve r bs, where at least the letter used tends to be part of the word class name. The second character then often represents a form of sub-categorisation -such as N or P for nouns, where the N can be seen as a mnemonic for ge n eral and P for p roper -, if applicable, whereas the third (or sometimes whichever is the final one) often stands for other properties of the word, such as number or person, etc. Those are just very general rules, though, and depending on the exact number of distinctions made -i.e. how fine-grained it isa tagset may also contain tags that are much longer. This is especially the case for more strongly inflected languages, as more distinctions can, and need to be, made.

Let's explore this a little further by looking at another of the currently bestknown tagsets, the CLAWS (Constituent Likelihood Automatic Word-tagging System) C7 Tagset, which is already far more detailed at 152 tags, exceeding the 48 tags observed in the Penn tagset by 104 tags.    As you should have been able to observe easily, the Penn tagset is much more compact, whereas the C7 tagset makes a lot of rather fine-grained distinctions. This makes it easier to search for highly specific grammatical phenomena, but also forces the user to have a high degree of awareness of which categories exist and what their exact meaning is. As we don't have enough space here to discuss all the differences in detail, I'll just point out a few exemplary ones, and briefly discuss their potential significance here.

Where the Penn tagset occasionally conflates categories -sometimes for reasons that aren't immediately obvious, such as, for example, to use the same tag for 'subordinating conjunctions' and prepositions -C7 goes exactly the opposite way, making much finer sub-distinctions. Even just for the conjunctions, it has 5 subcategories (CS, CSA, CSN, CST, & CSW) that are applied in cases where the word forms themselves are grammatically polysemous, for instance using CST to mark that in its function as a conjunction (which, incidentally, subsumes relative pronouns), as opposed to that of a demonstrative determiner (DD1). Regarding prepositions, it distinguishes between general ones (II), and again specific forms that may be grammatically polysemous, namely for (IF, as opposed to CS), of (IO, as opposed to as adjective, in e.g. matter of fact, out of date, or adverb, in e.g. of course, sort of, in multi-word units), or without (IW, as opposed to as adverb, e.g. in without giving away).

Whichever tagset is more useful, though, really depends on the kind of analysis that is being conducted. The Penn tagset, for example, was specifically 'optimised' to be used as a basis for further syntactic analysis (see

Tagging Your Own Data

Now, let's start taking a look at how you can make use of tagging in your own data, beginning by exploring how you can actually get your texts tagged in the first place. Unfortunately, most of the best-known taggers that have been developed over the years, such as, for example, the CLAWS system, for which a number of tagsets, such as the C7 discussed above, have been developed, are generally not freely available to the public or may require the purchase of a commercial licence for tagging suitable quantities of text. Furthermore, even though some tagger implementations are available as freeware/open source programs, these frequently come in the shape of programming modules or command-line based programs that many linguists are not accustomed to using, may necessitate additional software to be installed on your computer, require special configuration, or produce an output that's not easily adaptable to one's needs without much effort. In other words, free taggers often lack simple, easy-to-use interfaces that make them accessible to the average researcher.

As a further exercise, let's download a freely available and easy-to-use PoS tagger, and see how we can tag our own data and post-edit it.

Exercise 35

Go to the Project Gutenberg website and download a copy of The Adventures of Sherlock Holmes.

As tagging longer texts may take a fairly long time, let's first prepare a relatively short sample. Make a copy of the file you just downloaded, calling it 'adv_of_sherlock_holmes_beginning.txt'. Tip: Whenever you make copies of files, always use the appropriate functionality inside your file manager.

Open the copy in your editor and place the cursor at the end of line 66, which should give us enough text to evaluate the tagging. Tip: In many editors, pressing 'Ctrl + g' will allow you to 'go to' a particular line.

Delete the rest of the text, using the time-saving methods we learnt earlier, and save the text. Download my 'Simple PoS Tagger' from

Run the program by (double-)clicking Tagger.exe. Choose 'File→Load input file' from the menu or press 'Ctrl + f' on the keyboard, then find the file you just prepared. Select, and open it, so that it gets displayed in the window on the left-hand side.

Click on or press 'Ctrl + t'. This will automatically tag your file and display the result in the right-hand window, as shown in Figure

Whenever you find a tagging error, correct it by selecting the erroneous tag and clicking the relevant button in order to insert the right tag. Also think about whether you can imagine why the tagger has made such an error.

Once you've corrected a number of errors, you can update the colourcoding display by pressing 'F5' to refresh the view.

Once you've finished, save the tagged file, either by choosing 'File→Tagged file→Save tagged file' or pressing 'Ctrl + s'. You can also save the file if you're unable to finish the corrections directly, and open it again in the interface later by choosing 'File→Tagged file→Open for editing' or pressing 'Ctrl + e'.

Another way to get a sizable amount of text tagged is to use the CLAWS trial service. This allows you to get a maximum of 100,000 words annotated in either the shorter C5 or the full-length C7 tagset. While this may seem a lot, just remember that a 150-page novel may already contain about 60,000 words, so this will definitely not be enough for larger files or projects…

Exercise 36

Open the CLAWS trial page at

Open our Sherlock Holmes sample and copy and paste the contents into the text box on the page. Make sure the output style is set to 'Horizontal', and select the C7 tagset.

Run the tagging operation, and copy and paste the results into another text file in your editor.

Save the file and compare the results to the output produced by the Simple PoS Tagger, focussing on mainly higher-level category tag elements to ensure comparability.

Having now compared the results produced by two different taggers, you'll hopefully already have developed some basic sense of how reliable such programs in fact are -or rather, how unreliable they may be under certain circumstances. As this book is only of an introductory nature, though, I cannot really describe all the reasons for this in proper detail, but, in the rest of the chapter, will try to present some of the causes for this in a highly, but hopefully not over-simplified, manner.

In the solutions to the exercises, I've already hinted at the fact that, sometimes, the taggers may not have had 'knowledge' of an appropriate rule or 'understood' the complexity of particular linguistic structures. Just as in the preceding sentence, I often used words that refer to meaning or understanding of linguistic rules in scare quotes, to indicate that, frequently, the notion of linguistic rules and knowledge in tagging may not really represent a proper kind of understanding that is/can be applied to morpho-syntactic annotation. What you may, perhaps, naïvely have assumed, without any prior knowledge of how modern taggers work, is that most of them use a lexicon to look up words and then apply some linguistic -in technical terms referred to as symbolic -morphological or syntactic rules in order to assign a PoS tag. This, however, is not true for many modern PoS taggers because they often primarily rely on lexica in combination with statistical rules of co-occurrence for fixed, and highly limited, sequences of words extracted from existing pre-annotated data. And, although the CLAWS tagger is exceptional here in the sense that it uses a hybrid approach combining probabilities and a substantial number of symbolic rules -which already allows it to perform somewhat better than the Simple PoS Tagger -, it's still limited by the fact that the probabilities do not reflect exceptions or longer sequences of words, such as complex NPs, well. Hence, although the results of modern taggers are fairly remarkable, with claimed accuracy rates generally between 95-98%, as we've seen from our short exercises, it's always advisable to check their output as thoroughly as possible before using it for any kind of analysis, especially when this is carried out on a more quantitative than qualitative basis. Furthermore, we also need to be aware of the fact that taggers do not always perform equally well on all types of texts or genres. Therefore, because most tagsets and taggers are generally designed for written language analysis, their adequacy tends to drop strongly when used with spoken data, partly because some of the PoS categories may not always be appropriate. We'll learn a little more about that issue when we practice annotating some spoken data in Chapter 11. However, even for written language, the accuracy of a tagger across different text types/genres may vary strongly. According to

Some languages, such as Chinese, are also more difficult to tag because recognising word boundaries is an issue, due to their not using spaces between words. We'll discuss issues like this further in Chapter 9, but, for the moment, suffice it to say that the PoS tagging of such languages normally requires programs to artificially introduce spaces during the so-called tokenisation process. As a basic rule, morphologically (slightly) richer languages, such as German, tend to be somewhat easier to tag, because they often exhibit less grammatical polysemy due to case-marking or other features, such as the capitalisation of nouns in German, although of course such morphological features may also be ambiguous at times. Morphologically highly complex languages like Korean, for example, again present more issues because they tend to fuse a number of items related to grammar or honorifics into agglutinated word forms, a process that may also involve a certain amount of morphological adjustments.

Solutions to/Comments on the Exercises Exercise 34

Most of the tags listed in the tagset should probably be relatively familiar to you, apart from maybe UH, interjection, which basically represents something like exclamations or expressions of surprise, such as maybe wow, blimey, crikey, shoot, etc., or RP, particle, which represents words you may be more familiar with as prepositions, but are generally used with so-called 'phrasal or prepositional verbs', rather than NPs, as in, for example, run off, give up, etc. Another, possibly unfamiliar, category is PDT, predeterminer, which most grammars would probably categorise as quantifier.

What may be a little more surprising to you is to find separate categories for EX (existential there), TO (to), POS (possessive ending). This is basically because they may -or do -fulfil different functions from their adverb and preposition, or contracted BE-form 'look-alikes'. In other words, there in an existential construction no longer functions as a locative adverb, to as an infinitive marker is nearly twice as frequent in the BNC than in its function as a preposition (16,470 vs. 9343; see

Some other categories that probably look relative unfamiliar to you, but are easy to understand when we think about the diversity that exists in general (written) corpora, are SYM (symbol), FW (foreign word), and LS (list item marker). The first refers to mathematical and other symbols that may (predominantly) occur in scientific or academic texts, the second can easily occur in humanities writing, newspaper reportage, or data that includes code-switching (such as the ICE corpora), while the final one can also occur in a large number of different types of written data, including online materials, where numbered or bulleted lists are frequently used to 'summarise' facts.

In terms of the punctuation marks, which are obviously not words but still need to be included, it's interesting to note that all final punctuation is here subsumed under the full stop, and both colon and semi-colon are represented by a colon. While some punctuation labels are thus 'conflated', some others appear to be more fine-grained than necessary, for example, through making a distinction between straight double quotes, and opening and closing typographical ones, something that seems to be motivated by the nature of the data used when establishing the tagset. The same goes for the use of a hash mark (labelled 'Pound sign', as it's an American tagset), which could normally have been subsumed under SYM, but must have occurred sufficiently frequently in the original data and represented an important enough feature to make the distinction necessary.

Another thing that's interesting here is that, on the surface, no distinction is made between the right closing single quote and an apostrophe marking contractions. However, from some original data I obtained that was annotated with the tagset, it appears as if the apostrophe in such constructions (like it's) is actually counted as part of the paradigm for BE and therefore rendered as 's/VBZ.

Exercise 35

The first thing you'll probably notice in terms of tagging errors is that most of the Roman numerals, apart from, strangely, "IX", have been tagged as proper nouns (NNP), apparently because the tagger has mistaken them for initials. All of these should be changed to CD for 'cardinal number', and if they're followed by PP ('Punctuation, sentence ender'), the full stop and the tag deleted.

In the first title, "A" has been marked as a proper noun (NNP), while it should be a determiner (DET). In the second one, "Red_NNP -_PPS headed_VBN League_NNP" has apparently been interpreted as compound proper noun (NNP), which would -theoretically -be acceptable, but what is problematic here is that the hyphen in red-headed has been treated like a dash, consecutively marked as punctuation (PPS), rather than a hyphen, and hence the whole word has erroneously been split, with headed marked as a past participle (VBN), rather than as being part of a compound pre-modifying adjective. In general, probably either the whole expression should be tagged as a proper noun, or "Read-headed" as adjective (JJ) and "League" as a common noun (NN). Similar issues affect the rest of the titles, presumably due to the capitalisation of content words that 'confused' the tagger into believing that they were parts of names. As we've just seen, dealing with proper names may involve difficult decisions, even for human annotators, but the problem here still remains that some parts of what are potentially compound proper names are inconsistently marked as NNP, while others are marked as general nouns, etc. We'll soon discuss why such issues may arise in tagging.

Moving slightly further down the text, we can see that, again, the determiner "A" in the first section/chapter title has, again, erroneously been tagged as NNP, but what's more surprising is that the word "Scandal", which was previously tagged as NN, is now tagged as NNP, too. Next, the cardinal Roman number "I.", which was previously tagged as NNP, is now confusingly marked as PRP, that is, a pronoun, apparently because the tagger confused it with the first person singular personal pronoun. This should, again, be changed to CD. Overall, we've already seen that the tagger seems to have difficulties in making the right decisions when it encounters a number of words with initial capitals or that are completely capitalised, as may frequently occur in headings.

In the main text, things seem to be going better. The first real errors we encounter are that both eclipses and predominates are tagged as plural nouns (NNS), while they should be 3rd person singular present tense verb forms (VBZ), which indicates that the tagger seems to 'know' no rule that states that subject pronouns are generally not followed by nouns, but by verbs. Next, we find the combination "that_IN one_CD", where that is tagged as a conjunction, when in fact it's a demonstrative determiner. Here, the tagger has apparently 'overlooked' the fact that that is grammatically polysemous. Now, we may assume that one has been tagged correctly as a number, or numerical quantifier, to be more precise, if we simply go by form, rather than considering the function of word forms. However, as it's not followed by a noun here, it actually constitutes the head of the noun phrase, and it would hence probably be better to tag it as a noun or pronoun. The next error we encounter is abhorrent being marked as a general noun (NN), when it's really an adjective. The final issue we'll discuss here is that most in "the most perfect" has been identified as a superlative adjective, when in fact it modifies perfect and is therefore a superlative adverb and should have been tagged RBS. I'll leave the identification and possible explanation of the rest of the errors to you here, but include a (hopefully) fully corrected version below:

Exercise 36

To do the comparison between the two results, it would be best if you either printed out both versions and then marked up whatever errors may exist or what either tagger may have done better than the other in colour, for example, red or orange for errors, and green for better performance. Alternatively, you could also use Notepad++ or another editor that allows you to display two files side by side and then examine the files in parallel. In Notepad++ you can achieve a split screen by opening both files and then clicking on one of the tabs and selecting 'Move to other view'.

The CLAWS output is a little more difficult to read through because the (trial) tagger unfortunately didn't preserve the original line breaks, apparently in an attempt to either end the line at a sentence end or to truncate it below 80 characters to ensure convenient on-screen display/formatting.

Concerning the actual tagging performance, the first thing you'll probably notice here is that CLAWS already performs somewhat better on the Roman numerals, although its input format guidelines (available on the web page) indicate that these should have been entered without a following dot to be recognised properly. Only 6 out of the 14 Roman numerals indicating section titles have been mis-tagged, mainly as singular proper nouns (NP1), but some also as singular general nouns (NN1), and "V." strangely as a general preposition (II), perhaps because the tagger 'assumed' that it may be an even further abbreviated form of vs., hence 'contrasting' the two titles. The dots following the numerals are sometimes integrated with the numerals, but sometimes also tagged separately as sentence-final punctuation.

In the first title, both in the list of sections and in the heading for the first section, the initial determiner has again been mis-tagged, only this time not as a general noun, but a special form thereof, the name of a letter of the alphabet, that is, potentially part of a spelling sequence. Thus, both taggers had difficulties with this, but 'chose' to somehow identify the word as a noun form. In the second title/heading, CLAWS fares better at marking "Red-headed" as a compound adjective because it doesn't split at the hyphen.

CLAWS also correctly identifies the two word forms eclipses and predominates further down as verb forms, but makes the same mistake as the other tagger in mis-identifying that in "that one" as a conjunction (CST), rather than a determiner, despite that fact that it's identified "one" here as a pronoun (PN1). In the same sentence, we find the grammatically polysemous form "cold" having been incorrectly tagged as its noun representation (NN1) because it immediately follows the possessive pronoun "his" (APPGE). The reason for this error seems to be that CLAWS was unable to identify the remainder of the sentence as being the rest of a complex NP, presumably because it doesn't 'understand' comma-separated lists that well, and thus 'mis-took' the comma as a phrase boundary, in which case the annotation would have made perfect sense.

Unlike the other tagger, CLAWS correctly classifies most in "the most perfect" as a grading adverb (RGT), but is inconsistent in marking reasoning and observing, which form part of the complex compound noun reasoning and observing machine, because it marks the first one as a noun, while the second one is tagged as a verbal -ing form. As before, this seems to be due to its inability to recognise complex NPs. A little further down save in "save with a gibe" is marked as the base form of the verb (VV0), while in fact it's used as a conjunction meaning 'apart from', the conjunction "But" in "But for the trained observer" incorrectly identified as part of a multi-word preposition (II21 + II22), and "Grit" at the beginning of the next sentence again as a verbal base form -which, theoretically, ought to then cause the sentence to be an imperative, whereas it's in fact a noun. A little further on in the same sentence, we find "than" marked as a conjunction (CSN), when in fact it's part of a comparison and should thus be counted as a preposition. A similar thing happens with "but", marked CCB, in the next sentence, which, in this case actually has the -perhaps slightly old-fashioned -meaning of 'only', and is therefore an adverb.

As before, for reasons of space, I won't discuss the rest of the text here, but leave it up to you to identify any further potential issues… Sources and Further Reading

While smaller corpora may often not allow us to make as many generalisations about language as we might want to, modern mega corpora, such as the BNC, ANC, or COCA, offer remarkable possibilities for investigating general aspects of language and drawing more valid conclusions from them. As they're very costly and time-consuming to produce, though, one of their potential drawbacks is that their full versions may not always be available for free, and obtaining them may be prohibitively expensive for the individual (non-funded) researcher. Furthermore, even if they are obtainable, there may be a number of issues that make it difficult to handle them for the average corpus user. First of all, working with them, due to their sheer size, may require a relatively powerful computer with adequate memory and processing power. With increasing sophistication of PCs, this is turning into less and less of an issue. What may be more problematic, though, is that such mega corpora are often annotated in ways that make them difficult to use without dedicated software that has been written specifically for analysing them or displaying their contents in a way that's still easy enough to digest. Such software may, in the worst case, only run on very specific computer systems, or require complex installation procedures or changes to the system configuration that most linguists will be unable to carry out. Luckily, these are problems that have already been overcome to some extent by the advent of web-based interfaces to these mega corpora, which, even if they may not allow us to do everything we might want to do with such a corpus, already provide many facilities for investigating the data in relatively complex ways that will probably satisfy the needs of most researchers. In the following sections, we'll explore two of these web interfaces, one that allows us to work with the BNC for general British English, and the other with the COCA, for investigating features of American English. In doing so, we'll also make use of the knowledge you gained in the previous chapters for specifying linguistic patterns and working with PoS tags, in order to fine-tune our searches.

Searching the BNC with BNCweb

What is BNCweb?

BNCweb is a web interface to the BNC which is fairly straightforward to use. Essentially, being a concordance facility, too, some of its basic features are rather similar to the ones we've already discussed for AntConc. In order to be able to use BNCweb for free, you need to register at

The following illustration shows the startup screen you'll see once you've logged in successfully. I'd suggest that you take a few seconds to try and familiarise yourself with this, and to see whether you can already understand what kind of functionality you may be able to expect from the interface. As BNCweb in fact allows you to access the BNC not in its original form, but through a database interface, in technical terms, you don't run a search on the BNC, but formulate a query, as searches are known in database terminology. This is why you can see the word query appearing a number of times in Figure

In the latest edition of BNCweb, there's only one top-level option available for doing queries, the 'Standard Query'. As a sub-type of this query option, there are also two highly useful menu options for querying in only spoken or written texts. In addition, these also provide the user with a convenient interface for selecting a number of different textual properties, as we'll see later.

Basic standard queries

The standard query in BNCweb is similar to a combination of the basic and regex searches in AntConc and, in order to use it efficiently, you need to be aware of a few conventions. Before we start exploring these, though, let's begin with a very basic search, just so you can get a feel for the new interface.

Exercise 37

Type the word assume into the query box and run the query by clicking on . As before, pay particular attention to issues of polysemy when you look at the results. We'll later explore ways of disambiguating these, at least with respect to grammatical polysemy (though not for this example). The default view we get here is not the familiar KWIC view, but something called 'Sentence View'. This, however, can easily be 'fixed' by changing the default through adjusting the 'User settings' from the main page, or, temporarily, by clicking on .

Exercise 38

Experiment with the two different views by switching back and forth between and . Also try hovering with your mouse over the individual hits and the filenames on the left-hand side to see what happens.

Navigating through and exploring search results

At the very top, you can see a display showing your query string, the number of matches/hits, how many texts these occurred in, and some basic statistics. The 'bar' below it contains the navigation links that allow you to navigate through all the results pages, where |< means 'go to the very first page', << 'one page back', >> 'one page forward' and >| 'go to last page'. As you can see, you can also 'jump' straight to a particular page if you remember where you've found specific results.

Exercise 39

Practise navigating through the pages of results, jumping to the next or previous page, or the very beginning or end of the set of results.

As you'll have observed, the concordance output itself consists of the number of the hit, the name of the file it was found in -as a hyperlink -and the concordance result. As the order of the hits that are returned is exactly the same as the ordering in the BNC itself, simply looking at the hits that are returned may sometimes give a misleading picture, though, because the initial ones will all be from the general domain of informative written texts. In order to try and remedy this potential bias, you can click on the button, which will 'jumble up' the results for you.

Exercise 40

Try this and see whether it changes your impression of the earlier results.

As the results are now in random order, if we do want to know which particular category of the corpus (or 'genre') the individual result was found in, we need to check the category details. This can be done by following the hyperlink behind the file name on the left, which will switch to a full, static, display of all the file details, rather than just a tooltip when you hover over the link.

Exercise 41

Click on one or more of the links, and explore the exact nature of the information you can get on the file the search term was found in.

The dropdown list to the left of the button allows you to access the set of options shown in Figure

Exercise 42

Select 'Download…' from the dropdown list and click on . Take a look at the 'Output format options', try to understand them, and choose the ones you think are most appropriate for you.

For the moment, ignore all the category meta-information options listed in the section in the bottom half of the page.

Change the name of the output file to something sensible and click . Once the file has been saved, open it in your editor to see what the output looks like.

Next, let's practise the other option for 'saving' our results.

Exercise 43

Return to the query by clicking on the back arrow in your browser (or pressing 'Alt + ←' on the keyboard in Windows/Linux, ' + ←' on the Mac).

Choose the 'Save current set of hits…' option and click on . Enter a suitable name for the query. If you want to choose a multi-word name, use underscores (_) instead of spaces, and click on again. Go back to the main BNCweb page. The easiest way is to keep 'New query' selected in the dropdown box and click on . Click the 'Saved queries' link below 'User-specific functions' on the top left-hand side to verify that the query result has been saved correctly, then click on the link with the query name to see the original results re-displayed.

More advanced standard query options

The next thing we might want to be able to investigate in BNCweb is how to evaluate different options or alternative constructions. The mechanisms for this in BNCweb are sometimes misleadingly similar to the use of regular expressions we've learnt in Chapter 6, but the most basic forms employ a different system referred to as wildcards, whereas genuine regular expressions are in fact a feature of the CQP (Corpus Query Processor) syntax that BNCweb uses internally for its queries (without you necessarily noticing it), or that you can use to write more advanced and complex queries yourself.

Wildcards

Wildcards look like the basic quantifiers we're familiar with from regular expressions -i.e. ?, * , and + -, but they don't actually quantify the characters or character classes that precede them, and instead are comparable to the regex combination of a dot (any character) + quantifier, though not always the same. To make this easier to understand, let's take a look at the different options, including some (non-exhaustive) examples of possible results: r ?: represents any single character, but not an optional one! Therefore, yell?? finds yell ow , yell ed , yell er , etc., but not yell or yells. r * : 0-(theoretically) 'unlimited', but only within single words! Hence, call * finds call, call s , call ed , call er , call ing , call ous , call igraphy , etc. r +: 1-'unlimited', again, only within single words! So, call+ finds call s , call ed , call er , call ing , call ous , call igraphy , etc., but not call. Now, let's put theory into practice and try this out on an example that illustrates spelling differences between British and American English.

Exercise 44

Type colo * r into the query box, think about which result(s) you would expect, and then run the query.

Rather than having to scroll through the list to find potential different variants, you can use a handy feature called 'Frequency breakdown', triggered from the dropdown menu on the top right-hand side, to explore this.

Select this and click on .

The results probably aren't quite what you expected…

Exercise 45

Next, try replacing the * by a ? and see what the result is, again using the 'Frequency breakdown' feature to see how this changes the results.

Unfortunately, the results may still not conform to our expectations, but we'll soon learn how to fix that

The wildcard syntax in BNCweb also has a number of shorthand expressions that, again, sometimes look misleadingly similar to regex shorthands in that they have lower-and uppercase variants. However, while, as we've seen, in regexes lowercase refers to a 'regular' character class and using uppercase indicates the 'negation' of this character class, in BNCweb lowercase indicates the occurrence of single characters and uppercase of multiple (potentially unlimited) ones. For instance, \w finds a single word character, and \W multiple word characters, so that our 'colour' examples from above could also have been written colo\Wr instead of colo * r, or rather colo+r, to be more precise, and colo\wr instead of colo?r. For further options, see

One other important thing about wildcards in BNCweb is that they can also represent word slots, if separated from other words in a phrase query. Thus, a ? can stand for a single-character word token, punctuation or apostrophe, while * can stand for a word that may or may not occur, and + essentially means that a word has to occur in a slot.

Word and phrase alternation

BNCweb allows us to express alternation in a number of different ways. This can sometimes be bewildering to the beginner, so we'll explore the options step-bystep. In the Simple Query Syntax, which is the one we want to use at this level, we have two different options that allow us to use wildcards in a more complex way, one form that is normally used to express alternatives within a word, and one that's used for specifying phrases. The first one essentially looks very similar to the way in which we defined character classes in regular expressions. It uses the same form of square brackets to express alternative variants, only that, this time, they're not restricted to single character only, and all alternatives in this form of notation have to be separated from each other by a comma. Therefore, to find the antonyms thoughtful and thoughtless at the same time, we could write thought[ful,less]. In addition, it's also possible to specify nothing as an element within an alternation by writing a comma followed by nothing inside the brackets.

Exercise 46

Try expressing the different spelling variants now by using simple alternation.

Next, try to see whether you can use alternation to express all three different spelling variants of ice cream: ice cream, ice-cream, and icecream.

To be able to complete the above task, we need to employ phrase alternation, which is meant to allow us to specify searches for a number of words at a time, and looks rather like the kind of alternation we know from regular expressions. As this option is specifically designed to look for phrases, that is, combinations of words, it also allows us to specify spaces. As with regexes, we need to use round brackets and pipe symbols to delimit the alternatives, only that here, we cannot use the round brackets to group only part of the search term. Instead, each alternative has to be spelt out fully inside the brackets, as the alternative representation of our earlier example of antonyms, (thoughtful|thoughtless).

Exercise 47

With this knowledge, try to now write a query that will find all the three variants, including the one with a space.

Once you've succeeded, take a closer look at the frequency breakdown.

The results should present a relatively clear indication as to which form of the compound is the most commonly used, and therefore probably also the one that should be taught to learners. Frequency breakdowns of this kind can often help us in making decisions as to which forms can be seen as more appropriate, and maybe in which context, too. They therefore represent a highly useful tool, both for pedagogical purposes as well as helping us personally to identify the right grammatical or stylistic choices, something that can equally well be used by advanced learners of a language to enhance their awareness of such choices, and native speakers who want to get around the limitations of a standard word-based thesaurus.

The following represents a brief summary of some of the linguistic points of interest the different wildcards and alternation features in BNCweb may be used to search for/investigate: One further feature you should be aware of involving the bracket notation is that, just like in proper regular expressions, the round brackets can be quantified to allow for optional or multiple element slots.

Restricting searches through PoS tags

As we've seen a few times previously when concordancing in AntConc, grammatical polysemy may 'interfere with' our search for a particular word form that can represent different parts-of-speech. As the BNC data available through BNCweb has been PoS-tagged using the C5 tagset, a tagset that is more complex than the Penn one, though not as much as its 'big brother' C7, this becomes much less of an issue. Now, we can simply restrict our searches to forms that belong to a particular PoS category only, which then only leaves us with cases where there's basic semantic polysemy to contend with, which may or may not be a problem, depending on the particular goal we're pursuing.

However, a word of warning is in order before we start using this feature. As constructing the BNC was a major exercise involving the digitisation of very large amounts of text, sorting out meta-information as much as possible, and PoS tagging and annotating the data in a number of ways, the care taken in checking and correcting the final result of the tagging has, at least to some extent, been sub-optimal. Thus, we'll frequently find word forms that have either been tagged with two possible options for tags, even when, for the human annotator, only one would have been correct and possibly even easily identifiable, or many tags that are simply incorrect. This 'feature' does affect the reliability of the results that you can get out of BNCweb and other interfaces to some extent, the more so if you rely only on quantitative analyses of the results extracted. Therefore, when using such data, especially in the form of frequency lists, you should always be a little bit suspicious and try, as much as possible, to double-check the results to see if they may be affected in any way by this issue. Nevertheless, this now shouldn't discourage you from using the BNC, as it still represents a highly useful and amazing resource for investigating relatively recent British English in a form that is highly representative and will probably remain unmatched for a very long time.

After this brief excursion, let's return to investigating how we can make use of PoS tagging in BNCweb. The general notation that allows us to look for a combination of word form and tag here involves specifying a word form, followed by an underscore (_), followed by a PoS tag. As it turns out, the wildcard features we've just learnt about can also help us to a great extent in simplifying the specification of PoS patterns, so that we don't need to remember or specify all the different possible variant tags for a particular PoS category if, for instance, we want to look for all verb, noun, or adjective forms. Let's see how this will allow us to restrict our searches for a few grammatically polysemous word forms we've already encountered.

Exercise 48

To ensure that you don't get biased results, based on the order of the texts in the corpus, first go to the 'User settings' from the main query page and change the 'Default display order of concordances' to 'random order' and click on . Run a search on the word form mind, initially without specifying a PoS tag.

Inspect the results visually by reading through them and trying to identify roughly what kind of a distribution in terms of nouns and verbs you may have in your random sample.

Next, to confirm your intuitions -and to verify the tagging -, hover over the hits to see which tag CLAWS assigned to them, and whether this is always unambiguous.

Select the 'Frequency breakdown' option from the options menu in the top right-hand corner and click . Next, select the option 'Frequency breakdown of word and tag combinations' and click again. This will show you a breakdown of how often the word form occurs with a particular PoS tag, at the same time allowing you to see which tags it may occur with in the first place.

Investigate the different tag options first, then start a new query where you use a combination of word form, underscore, and a suitable wildcard for extracting all verb forms at the same time.

Do the same thing for round, first exploring the different word-class options through the breakdown, and then extracting only those that are adjectives.

To get a better sense of the query options and the C5 tagset, you can also take a look at -and possibly download -the 'Simple Query Syntax help', a PDF hyperlinked to the main query page.

Headword and lemma queries

So far, we've learnt that we can specify different word forms/phrases via wildcards and/or tag restrictions. However, since it's somewhat of a nuisance having to do this all the time, apart from being error-prone, it's quite useful to be able to specify a search for all different forms of a headword or lemma. Here, the distinction between the two is essentially that the headword encompasses all the occurrences of a base form, regardless of PoS, while the lemma always represents a combination of base form + PoS tag (forms). Let's explore the two different options, beginning with searches for the headwords run and take. To be able to do this, we simply need to enclose the particular base form in a set of paired curly brackets ({…}).

Exercise 49

Start by searching for the headword run.

Explore the results through a frequency breakdown (with tags), as we did in the previous exercise.

Repeat the same for the headword take.

To limit our searches to lemmas by specifying a PoS, BNCweb provides us with two separate options, both based on the following simplified (representation of a) tagset: To specify the lemma form, BNCweb allows two different variants. The first one requires both base form and simplified tag to be included inside the same set of curly brackets, but separated by a forward slash, e.g. {make/N} to find all occurrences of the noun make. The second one allows us to specify the headword and the simplified tag separately, both enclosed in curly brackets, but this time separated by an underscore, e.g. {make}_{N}. Essentially, the two options produce the same results, but I wanted to introduce the second option to you here because enclosing the simplified tag in curly brackets in this way also allows us to use it when we're not looking for lemmas, but for sequences of words where we may only want to specify the word class, rather than a word form + tag, and use a wildcard to find any word that occurs with this particular word class.

Exploring COCA through the BYU Web-Interface

Unlike the BNC, which is now freely available in its offline version, too, the COCA -at least in its free online version -can only be accessed through its interface at Brigham Young University. As there's also a database behind the interface, many of the operations that you can carry out in COCA resemble those in BNCweb. Unfortunately, however, the syntax for anything but the most basic word searches is fairly different, so that we need to learn new ways of doing old things, at least to some extent. Furthermore, the logic behind the interface, depicted in Figure

As the COCA is a monitor corpus, you also need to bear in mind that the overall results you might get will probably change over time. Thus, for instance the number of hits I'll be reporting below, as well as their relative frequencies, may well change over time, due to changes in the language itself, and should only be considered accurate at the time of writing this book.

The basic syntax

As hinted at above, the syntax for basic word and phrase queries is somewhat similar to BNCweb in that typing in a word or a number of words separated by spaces and clicking on will find instances of this word or phrase. If the display option is set to the default, LIST, something that looks quite similar to the frequency breakdown in BNCweb will appear in the top window on the righthand side, which can be seen in Figure

Luckily for us, though, wildcards work in the same way as in BNCweb, so there's really nothing new to learn there, and we can now focus directly on identifying the options and differences to BNCweb in the query syntax. For instance, alternatives can be specified as list separated by forward slash (or pipe), but this time without any brackets around them. For example, thoughtful/thoughtless finds both antonyms at the same time, as well as providing a convenient frequency breakdown (see Figure

As a general rule, any searches that go beyond simple words and phrases involve the use of (possibly multiple) square brackets ([…]) and allow the user to find PoStagged words, words followed by a particular PoS tag, or lemma forms (see Figure

The following list represents all the basic word-level options, which can be 'picked and mixed' for phrase queries again. Please note that I sometimes change the slot for illustrating a given feature to either occur on the left or right if multiple slots are involved in a query. Plus symbols used below do not appear in queries, but simply indicate combinatorial options in a more abstract form: r word form(s): finds exact words or phrases only r word form(s) including wild cards: finds variable words or phrases r [base form] finds lemma: e.g. finds all word forms mind, minds, minding, but only as verbs (equivalent to lemma search in BNCweb) r [=base form] finds 'synonyms', e.g.

Comparing corpora in the BYU interface

Exercise 50

Start by running a lemma query on the word movie, as depicted in Figure

Investigate the results by first clicking on the hyperlink for the singular form, then doing the same for the plural, and last, but not least, by ticking the two boxes next to the separate forms and clicking on to get all results displayed at once. The obvious advantages the COCA interface has over the BNCweb search results here are that a) we get a frequency breakdown of all forms immediately, rather than having to switch to a separate view first, and b) the results can be displayed in the bottom frame for individual sub-results immediately without having to open them in a new window or needing to navigate back a page each time we want to look at another sub-result. This is especially useful if we want to investigate lemmas with many different potential forms, as, for example, for verbs. The main disadvantage of the COCA interface, on the other hand, lies in the fact that it's not possible to get the frequency breakdown according to tags, which makes it more difficult to discern between polysemous forms.

In the next step, we want to see how we can compare this result to occurrences in British English, where the original word for what's called a movie in American English is generally assumed to be film, although some older people may also refer to films shown in the cinema as pictures. The latter basically comes from the same expression as its American counterpart because the word movie is derived from the expression moving picture(s), where the first component has simply been clipped in American English, while in British English, the second part of the compound noun was originally retained.

Exercise 51

In order to compare the results to the equivalent data from the BNC, click to open the dropdown list that initially reads SIDE BY SIDE. Here, it may be a bit misleading at first that you don't use COMPARE, but this would simply switch us to the other corpus.

Select BNC and wait for a few seconds for the result to appear.

The initial result of the side-by side-comparison should look like Figure

Exercise 52

Click on the dropdown list that initially reads '--START --' and select 'COCA'. This will return you to the basic single-corpus COCA interface.

Run a search for the lemma of film, this time constraining the results to nouns, as, of course, film can also be a verb and we're not interested in this.

Next, switch to the side-by-side comparison again and investigate the results.

As you've hopefully seen, this kind of comparison between two different varieties is quite easy to carry out in the BYU web interface, and you can also use your knowledge of the query syntax to investigate further varieties through the Corpus of Global Web-Based English (GloWbE), which is accessible through the same interface.

We'll explore further analysis options in COCA in some of the following sections, but before we do so, I still want to mention a further display option COCA offers, the KWIC display, which can again be selected directly from within the display options in the left-hand search frame. Choosing this option will initially provide you with a selection of 100 random samples (adjustable to a maximum of 1,000) that are distinctively colour-coded for the main content word PoS categories, but unfortunately not for function word categories, which are marked with a single colour. The output can also be sorted using variable sorting options for a visual indication of the PoS of a search term(s), plus surrounding tags. This type of display may already be highly useful for identifying most of the potential for grammatical and semantic polysemy, but the maximum number may not allow you to extract enough samples in cases where a search term has a high degree of polysemy on both levels.

Solutions to/Comments on the Exercises Exercise 37

As you'll hopefully see from the examples on the first page, the verb assume here has two distinct meanings, one that represents a verb of cognition (similar to think), and the other a behavioural verb that may mean to 'adopt a position or role', either literally or metaphorically. The text displayed here just looks like any plain text from a book, article, etc., with only the hit highlighted and formatted as a hyperlink.

Exercise 38

Toggling the view to KWIC will provide you with a display that's similar to that in AntConc, with the keyword centred in the window. When you hover over the hyperlink for the hit itself, you should be able to get a tooltip that shows the hit and displayed context as morpho-syntactically annotated data where the hit is highlighted in red. Hovering over the file name on the left displays a different tooltip, this time providing fairly detailed information about the text the hit was found in.

Exercise 39

This exercise should present no difficulty, so there's nothing to discuss here

Exercise 40

Because you'll now get random results, obviously there's no guarantee that you'll always be able to observe any difference. However, what may well happen is that some of the results illustrate the different meaning potentials more clearly, so, for instance if you were trying to find suitable examples for teaching or creating a textbook, repeatedly running the same query may be a useful option.

Exercise 41

As you'll have seen, the file information for every file in the BNC contains various types of information extracted from its header, partly referring to textual statistics, such as the length in words, and approximate sentence length, the full file name, as well as certain types of bibliographical information. However, what you may alsosadly -notice fairly frequently is that, although there's the basic provision for a possible piece of information in the left-hand column of the table displayed, often the information itself is actually marked as 'unknown' in the right-hand column.

Exercise 42

In choosing your options, you first need to make a decision as to whether you want any meta-information included, so you either need to have some of the options selected in the bottom half of the page or simply leave all un-ticked. I'd suggest that you initially remove all absolutely unnecessary information, such as meta-data or information required for re-importing into BNCweb, etc., so that you end up with a relatively simple list that primarily contains the hits with the query results marked, similar to the output we got from AntConc.

In order to get the most plain -but still useful -form of output, make sure that none of the boxes in the second half of the page are checked and also choose 'no' for 'Download both tagged and untagged version of your results'. In order to make it easier to see the hit, keep the 'Mark query result…' option set to 'yes', though. Under the options for choosing the operating system, make sure that you select something appropriate, as this'll affect the line endings in the output. If you're only viewing the result in a good text editor, this won't really make any difference, but if you may be planning to use other programs to further process your results automatically, it may make a difference if these programs expect operatingsystem specific line endings, and thus potentially lead to errors. Keep the option 'Write information about order…' set to 'no' as well because it only describes the order of the basic output fields, that is, number of hit, file name, unit number where the hit was found, possibly speaker ID (for spoken language), left context, hit, and right context. The output should then look like the short example excerpt provided below:

1 A01 407 Many people wrongly <<< assume >>> that all they have automatically goes to their loved ones . 2 A04 170 The description is rather slender , but Pater was able to <<< assume >>> some existing knowledge on the part of his reader : [ We all know the face and hands of the figure , set in its marble chair , in that circle of fantastic rocks , as in some faint light under sea . ] 3 A05 1282 It was lively enough to marry Bellow to a [ stylish Radcliffe graduate ] of whom Roth had been [ enamoured ] --if we are to <<< assume >>> that The Facts has not imagined the connection . 4 A06 1358 Here is an example of an impro exercise for two actors : [ An actor is asked to <<< assume >>> the character of a close family friend who arrives at the house with the news of the death of the wife 's husband in an accident . 5 A06 1413 Read newspapers , and do n't <<< assume >>> that the whole world is as interested in acting as you are .

If you choose the option for downloading the tagged version, too, you get something that looks like this:

1 A01 407 Many people wrongly <<< assume >>> that all they have automatically goes to their loved ones . Many_DT0 people_NN0 wrongly_AV0 <<< assume_VVB >>> that_CJT all_DT0 they_PNP have_VHB automatically_AV0 goes_VVZ to_PRP their_DPS loved_AJ0 ones_NN2 ._PUN 2 A04 170

The description is rather slender , but Pater was able to <<< assume >>> some existing knowledge on the part of his reader : [ We all know the face and hands of the figure , set in its marble chair , in that circle of fantastic rocks , as in some faint light under sea . ] The_AT0 description_NN1 is_VBZ rather_AV0 slender_AJ0 ,_PUN but_CJC Pater_NP0-NN1 was_VBD able_AJ0 to_TO0 <<< assume_VVI >>> some_DT0 existing_AJ0 knowledge_NN1 on_PRP the_AT0 part_NN1 of_PRF his_DPS reader_NN1 :_PUN [_PUQ We_PNP all_DT0 know_VVB the_AT0 face_NN1 and_CJC hands_NN2 of_PRF the_AT0 figure_NN1 ,_PUN set_VVN-VVD in_PRP-AVP its_DPS marble_NN1 chair_NN1 ,_PUN in_PRP that_DT0 circle_NN1 of_PRF fantastic_AJ0 rocks_NN2 ,_PUN as_CJS in_PRP some_DT0 faint_AJ0 light_NN1 under_PRP sea_NN1 ._PUN ]_PUQ Here, the output's basically the same as before, only that the PoS tagged version follows immediately after the non-tagged output. This, however, creates a bit of redundancy, and it would be nice to be able to save the tagged version only, but unfortunately, there's no provision to do so. Therefore, if you only want to retain the tagged part, you need to delete the non-tagged one manually.

Later on, depending on how much information you might need about a text, you can also output as much meta-information as required (or is actually available), in case you need to determine and/or distinguish which particular type of texts the results come from. When outputting meta-information, to keep it easily readable, I'd suggest you always go for the 'full values' option because otherwise you'll simply have to look up what each number means. This really only makes sense if you're planning to put the result into a relational database for complex analysis and annotation, and where you'll automatically be able to look up what the numbers mean from a lookup table.

To illustrate the difference, here are two short samples that only contain the first hit and all associated meta-information: This first sample contains only the reference numbers, while the next one contains the full textual descriptions, where 'n/a' (not applicable) essentially indicates that those fields don't apply to written, but only spoken, texts.

A0407

Many people wrongly <<< assume >>> that all they have automatically goes to their loved ones .

Written

Exercise 43

Essentially, doing this exercise should be straightforward. The only thing that may conceivably go wrong here is if you try to save the query with a name that contains spaces, in which case you'll get an error message saying: "Names for saved queries can only contain letters, numbers and the underscore character ("_")! Please use the "back"-button of your browser and change your input accordingly."

Exercise 44

The results of this exercise may initially be somewhat surprising to you because you may have expected to simply get the British and American spelling variants of this word. Instead, the query will have returned any word that starts with the grapheme combination/character sequence <colo>, followed by any number of characters, and ends in an <r>, because the wildcard asterisk ( * ) means 'zero or more characters'.

Exercise 45

Based on your knowledge of quantifiers, you might have thought that you could find the two spelling variants by replacing the * by a ?, but unfortunately this still isn't how wildcards work. You may again be surprised by the result because you probably would have expected this to work, when in fact this query only returned examples of the British spelling. The reason for this is that the wildcard ? always stands for any character and is thus the equivalent to the dot in regular expressions. Given our current means, it may therefore seem as if we can't really express what we want using wildcards because the + always represents at least one character, but potentially an unlimited number, too, so we clearly cannot use it in this case, either. However, luckily for us, there is a way to solve this problem, which is through expressing simple alternation.

Exercise 46

Using simple alternation in the query string colo[u,]r, you can now get the two spelling variants (only) displayed. Apart from the fact that you can even find 115 examples (1% of the total forms) of the American spelling in the British corpus data at all, the results should not be surprising now. You may now be tempted to investigate further why you can find these American spelling variants in the corpus, and of course the easiest option for this is to click on the link for 'colour' in the frequency breakdown to get to the concordance for the results, and then checking the meta-information for each result. Unfortunately, however, doing so remains relatively un-insightful, as in most cases you'll only find information about the author's domicile being marked as 'unknown', or, surprisingly, in some cases as 'UK and Ireland'.

Of course, one thing we haven't done yet here is to ensure that we really retrieve all forms of the noun paradigm by including the plurals, which we can do using the same mechanism and writing colo[u,]r[s,] instead. This, however, doesn't change the distribution, so the results, apart from now being complete, aren't really any more interesting.

Solving the second part of the exercise may again pose more difficulties than expected. When you try to achieve this task, you'll probably at best manage to create a query that will find two of the above forms, and instead get an error message saying "bracketing is not balanced". This is because the space is not part of the wildcard/simple alternation syntax, so we need to look for alternative ways of solving the problem.

Exercise 47

With a little bit of experimenting, you'll hopefully have come up with the solution (icecream|ice-cream|ice cream). This does indeed allow us to find all three alternatives at once, but we can still make it more compact by writing (ice[-,]cream|ice cream) instead, where the first part allows us to express the hyphenated compound and the one completely written together as one word at the same time.

Exercise 48

As your results are randomised now, I can only discuss the solution in a more abstract way here, but hopefully, this'll reflect your results well.

First of all, when you do the visual inspection, you may notice that the random sampling could give you a little bit of a skewed impression because it might just so happen that BNCweb has only returned either a majority of noun or verb hits. However, even if this may be good enough for finding suitable examples to illustrate the usage of the word forms, we don't really get any idea as to whether it's used more frequently as one or the other, and it certainly doesn't allow us to easily extract only noun or verb usages. You may also notice that, occasionally, as I've pointed out before, CLAWS has assigned two tags to a word form (e.g. VVB-NN1), even if you might think that definitely just one of the two can be correct.

When you display the 'Frequency breakdown', the first result will not be very exciting because, as we've specified only one word form, it only displays that, together with the 'No. of occurrences', and the 'Percent' value, which will be 100%. What may be a little puzzling, though, is that the word form displayed is actually the one with an initial capital, which is perhaps not the most representative way of displaying it because this exact form is bound to be much rarer than the non-capitalised one.

Once you've specified the option for viewing according to 'Frequency breakdown of word and tag combinations', your display will show you the exact word forms in combinations with all the tags -or 'ambiguous' tag combinationsaccording to their frequency of occurrence. The results, which you can also download by clicking 'Download whole table', should look similar to Table

However, investigating frequency distributions wasn't our main objective here, so let's return to the main task at hand, exploring how we can actually make use of the tagging in our searches directly. First of all, when you take a close look at the listing of words + tags in the table, you'll notice that they're in fact hyperlinks that, once clicked, provide you with a concordance of exactly the combination specified, so that you can already narrow down your search in this way. This is still limited, though, because the different tags for verbs and nouns are further sub-divided in the output, so, in order to be able to find only verb forms, you need to use a wildcard query like mind_V * . This will then retrieve all hits where the tag starts with a verb marker as the first element, including those that have an ambiguous tag where the verb tag is listed first, but excluding the ones where the noun tag comes first. In order to also capture the latter, we need to make use of 'word-level' alternation in a slightly more complex form, specifying mind_[V,N??-] * .

Exercise 49

As you'll be able to see when you run the search, you'll immediately get a number of different forms of the verb and noun paradigms for the basic concordance. When you then switch to the frequency breakdown, you may be surprised a little because you not only get the regular forms of the two word categories displayed, but also two slightly more unusual forms, the archaic form runneth, and runnin', which represents the pronunciation variant with so-called 'g-dropping', where the final consonant is not realised as a velar, but an alveolar nasal. Now, you might expect that all these forms actually come from the spoken part of the BNC, but closer inspection of the meta-information reveals that they're in fact from written materials, where authors have simply tried to represent a stigmatised pronunciation form. The last interesting 'form', or rather feature, we can observe here is that there are 5 instances of run with the UNC (unclear) tag. Closer examination by following the hyperlink reveals that these are in fact by no means 'unclear' examples, but in fact represent interrupted words that occur in the spoken part of the BNC. However, despite the fact that interrupted words are very common in spoken language, even that of highly fluent speakers, the CLAWS tagset provides no tag for this, something that is probably due to the CLAWS tagsets originally having been created for the morpho-syntactic annotation of written language, and later adjusted for spoken language to some extent.

The headword search for take doesn't really have any surprises to offer. The distribution indicates very clearly that the noun form is comparatively rare, and that there's a clear ranking in terms of frequency for the verb form, with the infinitive being the most frequent, followed by the simple past, the ED-form, and the ingform. We again find one unclear form, but this time it doesn't mark an incomplete word, but instead a form that should probably be marked as an infinitive, despite the fact that it's missing the infinitive marker.

As before, for both headword searches, we again get a considerable number of ambiguous tags

Exercise 50

Provided that you've used the correct syntax to search for the lemma, [movie], COCA should have found 53,369 instances of the singular form, and 24,991 of the plural. This exercise, on its own, doesn't really reveal anything remarkable. All we can really observe here is that, out of the overall 78,360 hits, the singular form accounts for slightly more than 2/3 of all results, while the plural is far less frequent. Rather than seeing the relative frequency in terms of percentages as in BNCweb, though, the bars on the right-hand side provide us with a visual indication of the extent to which each sub-form of the lemma contributes to the total.

Exercise 51

Other than what was already discussed in the chapter itself, you should have been able to observe that, in the BNC, the singular form is also more common than the plural, with 1,716 hits, and the plural only amounting to 1,013, so that the total is 2,729. In other words, the distribution of singular to plural forms is somewhat similar to that in the COCA, although the singular forms still make up less than two thirds.

Exercise 52

The correct version of the lemma query, which excludes anything but (common) noun forms, should be [film].[nn * ]. This should first yield 57,917 hits for the singular, and 17,421 hits for the plural in COCA

Basic Frequency Analysis -or What

Can (Single) Words Tell Us About Texts?

The methods described in this chapter can be considered a starting point for providing us with some quick hints as to which particular type of language, or perhaps even genre, we're dealing with in our corpus analysis by investigating how frequently certain words occur in a corpus. In other words, what we want to do here is to try and develop an understanding of how much, but perhaps also to some extent how little, lists of single words can tell us about the texts they occur in. This type of analysis will then be continued in the next chapter, where we'll discuss fixed or variable combinations of words that may be equally, or sometimes even more so, relevant to a particular type of text or genre.

In order to develop this understanding thoroughly, as so often in corpus linguistics we need to look at this task from at least two different angles, a theoretical and a practical one. We'll start with some theoretical considerations first, and then see whether or how this may affect the way we carry out frequency analyses, or need to interpret them.

Understanding Basic Units in Texts

Every text has multiple levels of meaning, and these levels tend to be -at least to some extent -linked to the 'physical', structural, units we may encounter, ranging from single 'words' via phrases, clauses, sentences, etc., to whole texts. There's frequently no direct one-to-one mapping, though, which means that we need to make certain decisions as to which sizes of chunks of texts may be relevant to various types of analysis. In order to develop some more concrete notions of what the different potential units of meaning may be, and how they relate to the structural units we can investigate, we'll start with a bottom-up description of potential units, working our way up from the level of the 'word'. In later sections, we'll then move on to discussing longer sequences of units, both fixed, as in, for example, idioms or proverbs, and flexible, as in different types of more or less formulaic phrases.

What's a word?

One of the fundamental issues that we encounter when processing texts is the question of what exactly we should treat as a word. Initially, we may naïvely start out with the idea that words are entities in texts that are separated by either whitespace or punctuation. Unfortunately, this is even the case for the PoS tagging of the BNC, where the guidelines state that "our tagging practice in general follows the default assumption that an orthographic word (separated by spaces, with or without punctuation, from adjacent words) is the appropriate unit for word-class tagging"

We thus have three different (primary) means of creating a compound. As we've already seen when we examined phrase-level alternation in BNCweb, for the word ice cream we can find all three of these different variants within the BNC:

1 icecream: 28 matches in (17 texts), 2 ice-cream: 368 matches (174 texts), 3 ice cream: 471 matches (203 texts).

Based on this data, it's certainly the last variant that's the most frequent one, but also the one our naïve detection algorithm would fail most miserably on. We can find further similar problematic examples for other types of composite words as well, such as for the word however, with how-ever occurring only once, how ever used as a true adversative conjunction occurring at least 6 times, and howeversubsuming the true adversative and its 'comparative' use -59,730 times. And even for the negated version of the word smoker, which may at first glance seem to be quite uncontroversial, we find the three variants non smoker (3x in 1 text), un-hyphenated nonsmoker (7x in 4 texts) and -most frequentlynon-smoker (55 in 36 texts).

Therefore, the choice of how to represent composite words clearly isn't fixed by a rule, but essentially seems to be a case of whatever form becomes conventionally more or less accepted by a majority of people in/for its written form. This is a particular problem we have in dealing with written language or, more generally, language that's represented in some kind of orthographic form. In spoken language, in contrast, this issue doesn't normally arise because a) words there aren't generally separated by pauses -which the space is to some extent the equivalent of in English and other Western languages -and b) the prosody/stress patterns usually help us to recognise which units belong together or not, at least for relatively proficient speakers.

In other languages, such as Chinese, defining a word by the spaces surrounding it makes even less -or almost no -sense at all, as most sentences there do not even contain any spaces between characters. As a matter of fact, in the majority of cases, a Chinese word is made up of two characters, with the 'exact' ratio of characters to words apparently being around 1.7. In French, to cite an example from a language that's closer in nature to English, noun compounds such as, for example, machine à laver ('washing machine') are generally formed from a noun + PP involving the prepositions à or de, while in German, extremely long compounds without any spaces can be created, although this is often exaggerated jokingly, as in the example of Donaudampfschifffahrtskapitänswitwe ('widow of the captain of a steam boat that runs on the river Danube').

Returning to English, further, but similar, problems are caused by other multiword units (often abbreviated MWUs), such as phrasal (prepositional) verbs, for example, give up/in or get along with. Only that, in this case, we don't need to deal with three different potential forms for one 'word' but a sequence of up to three orthographic units belonging together that act as one. The same goes for multi-word conjunctions, such as as far as, as if, provided that, etc. (c.f.

With contractions -such as can't, won't, she's, he'd, etc. -, on the other hand, we really have the opposite problem, that is, we only have one single word form, but might in fact want to interpret this as two different words. In this case, if we purely look at individual (untagged) words and don't actually analyse the context, we won't necessarily be able to group the second part (i.e. the clitic) with its appropriate full counterpart in a frequency list (something we'll discuss in detail in Section 9.2). For instance, we wouldn't know whether a d following an apostrophe actually represents a form of had or would or an s stands for a third person singular form of be or a possessive marker, etc. If frequency lists are ordered alphabetically, the same issue doesn't necessarily arise because then the un-contracted and contracted 'first parts' will appear close together, but if they're sorted in terms of their frequencies, we may overlook that one form or the other could occur more frequently, as we'll soon test practically in AntConc.

In this context, problems with the semantic content of frequency lists also already become apparent to some extent, due to the polysemy of the little function word clitics indicated above. Of course, cases of polysemy aren't restricted to function words, something we've seen from the very beginning when we started exploring concordances, but may also occur with content words. Here, for instance, we have the well-known example of bank as a noun, either denoting a 'financial institution' or the 'sloping land/grounds alongside a river'. It may get even worse if we further add the potential for grammatical polysemy, such as in our example word bank, which may not only be a noun, but also a verb, where again it can have two or more different senses, i.e. 'turning steeply' for planes, 'having a bank account with' or 'counting/relying on', each time in conjunction with the highlighted preposition. And, of course, a simple listing of the single word form without context in a frequency list would (normally) not allow us to disambiguate our examples, as would for example be possible through sorting a concordance of the word by its left or right context.

All of the above are issues that are often largely neglected in the analysis of corpus data, especially in more automated and quantitatively oriented types of corpus analysis, where the norm still seems to be to assume that the traditional 'definition' of a word is relatively unproblematic, and that synonymous expressions generally consist of single words only. Now, while of course it's generally not possible for us to directly change the design of any corpus tools we may be using to allow us to deal with this issue, we at least ought to bear this 'handicap' in mind in many of our analyses, and see whether at least some of the tools allow us to avoid any of these problems, or whether we may be able to find a way to work around certain issues by manipulating our data ourselves in simple ways.

Types and tokens

The problems we discussed earlier in deciding what exactly constitutes a word all relate to the issue of how to assign a frequency count to a suitable representation form of a word, something slightly similar to creating the entry for a headword in a dictionary. Such a representative instance/word form in a frequency list is referred to as a type, and each individual occurrence of a particular type as a token, which is why splitting a text into individual word-level units is also referred to as tokenisation. As we've already seen above, there are quite a few cases where we may have difficulty in assigning multiple words to one single type, but it isn't only for multi-word units that we encounter such problems. Even for single units that are clearly delimited by spaces or punctuation, we may end up having problems assigning them to one and the same type because of such issues as polysemy discussed above, but also alternative spellings (colour vs. color) due to dialectal or historical variants, typos (teh instead of the), or capitalisation/non-capitalisation.

For the latter, we can even distinguish two different types: capitals indicating 'sentence' beginnings or proper names vs. words that are completely in uppercase, such as in emphatic headings or attempts at representing increased loudness in spoken materials. Here, we may, for example, encounter the rendering of more loudly spoken/shouted passages in fiction, or even in spoken and orthographically transcribed corpora, such as the Hong Kong Spoken English Corpus (HKSCE;

In some cases, though, we may even deliberately want to force the grouping together of disparate forms. This may for example be the case if we want to group all forms of a certain paradigm together, such as all the forms of the suppletive verb be or all different forms (infinitive, third person singular present, present/past participle) of other verbs. This is referred to as lemmatisation (c.f. also Section 8.1.8, where we looked at lemma queries in BNCweb), and many programs that produce frequency lists offer this kind of facility. A similar thing can be done by expanding abbreviations to their full form, but with both lemmatisation and expansion, one always has to bear in mind that the individual forms may potentially also cause a change in meaning of the whole textual unit in which they occur, or may have been used deliberately as a (stylistic) feature of a particular type of genre. For instance, some publishers force their book or journal authors to use word sequences like that is instead of the more academic abbreviation i.e., which has direct implications for the writing style and -to some extent -even the length of the article/book. Now that you know about some of the issues that may affect our counting of words in a text/corpus, we can start looking at frequency lists properly. Basically, these lists can be created in two different ways by a program (or a person), either by producing a token list first, then sorting it and counting how many times a given word form occurs in a row, or, more efficiently, by keeping track of each word form encountered and increasing its count as it recurs in the text. Both ways of course require the text to be suitably cleaned, normalised, and tokenised into words in the first place, which is at least part of the reason why I placed such a lot of emphasis on cleaning up our data in earlier sections of this book.

And of course, because frequency lists consist of a combination of word types and their associated frequencies, we also have multiple ways of sorting and displaying our results. The most useful output format in many areas of corpus linguistics for such a list is generally to have a list that is first sorted according to the frequency in descending order, that is, with the most common words occurring first, and then 'alphabetically' if the same number of tokens occurs for more than one type. However, we may also sometimes want to look at the least frequent words first, assuming that they can possibly tell us something very specific about the terminology used, for instance in a highly technical text that contains a number of specialised words. If we're working in the area of lexicography, though, and are trying to create a comprehensive dictionary of a particular type of (sub-)language, we may well want to start with an alphabetically sorted list first, and then investigate the individual types according to their specific features that would allow us to classify and describe them optimally. To investigate this further, we'll now look at what exactly frequency lists may look like, and what they could be useful/used for, from a more applied angle.

Word (Frequency) Lists in AntConc

We'll begin our exploration into word (frequency) lists by creating a basic list of a small corpus in AntConc to see what such a list may look like, and also whether we can directly observe some of the problematic issues described above in our data.

Creating a word frequency list in AntConc is a very simple task. All you need to do to create a basic single-word list is load a corpus, select the 'Word List' tab, and click Start . The output of this tool consists of either three or four separate subwindows, depending on which options you've chosen for it under the program's 'Tool Preferences', and can be seen in Figure

The first window from the left lists the rank of the word inside the frequency list, the second the frequency itself, the third the word form, and the fourth, if present, the lemma associated with the word form. The latter, however, is only shown if the option for this is activated in the 'Tool Preferences' for word lists, and a suitable lemma list loaded. Additional information indicated in Figure

Open AntConc and load the corpus. As we're not interested in lemmas at the moment, turn off the 'Lemma Word Forms(s)' option under 'Tool Preferences→Word List'. Also turn off the option 'Treat all data as lowercase' in the same dialogue.

Switch to the 'Word List' tab or press and create the list, keeping the default set to 'Sort by Freq'.

Scroll through the list and try to understand what type of texts we may be dealing with here, both in terms of text type/category and domain/topic. Which particular words can help you to identify these and why?

If you're unsure what a particular 'word' entry in the list means, click on it and this will take you to a concordance of that entry. To get back to the original list, simply go back to the 'Word List' tab.

Keep the list open, as we want to explore further options using the same data later.

Exercise 53 was designed quite deliberately using this type of data in order to demonstrate the potential usefulness of word frequency lists for identifying distinctions between spoken and written language, as well as finding indicators for possible domains. However, be warned that if you create frequency lists of more general written data, it will probably be much more difficult to discern which words may be particularly indicative of certain genres or domains.

The high incidence of function words is of course something you'll be able to observe in both spoken and written texts. Perhaps the only types of texts/textual units where this is not so are the telegram, the heading, and the bullet point, where function words are deliberately left out in order to save space or to create a more 'immediate', summarising, effect by only including content words.

As pointed out in the general discussion on frequency lists earlier in this section, it's fairly difficult to define what exactly a word is, and we may at least partly have to accept simplified definitions or ones that are different from what we might find acceptable. This is especially true when working with software written by someone else, and possibly also for a particular purpose we may not even be aware of. Thus it's generally advisable to first check any output of a frequency list produced by some program to see whether it may exhibit any unusual features that could influence the analysis negatively. You should probably minimally verify how the program deals with contractions or hyphenations to be aware of what exactly the author's definition of a word is, as well as whether that particular definition fits your own purpose.

As will hopefully have become clear from the discussion of Exercise 53, the default frequency list in AntConc treats clitics, such as 's (but without the apostrophe) as separate words, which is often what we want because they're in fact abbreviated words that have been fused with a preceding word. This, however, makes contractions less easy to spot, let alone count, for the untrained observer, when they're still a very useful indication of spoken language, until such time as more people begin to realise that expanding and not using them in written language is actually counter-productive because it not only disrupts the reading flow to some extent but also creates the impression that at least some of the function words that would normally be de-emphasised as clitics may in fact be stressed, which is what using their full form indicates phonetically. Yet, in order to ascertain the 'spokenness' of a text/corpus until such time, we may often want to handle contractions as single units.

One of the great advantages of AntConc here is that it in fact allows you to (re-)define what exactly constitutes a word token for your own purposes by editing the definition of characters that are allowed to occur inside a word. Figure

You may be tempted to simply tick the check box next to 'Use Following Definition', as the definition below this at first glance appears to include all the characters used in the English alphabet. This, however, may in fact exclude some of the rare accented words borrowed from other languages -although this is not applicable to our current data -and is therefore best avoided. Don't forget to apply the setting, too. Now, simply click again to re-create the frequency list including the two extra characters, and observe the changes in the list by scrolling through it.

As before, keep the list open, so we can re-use the data.

An important corollary of being able to control the parameters for creating frequency lists is that, whenever you're reporting any results of frequency analyses The above exercise should have demonstrated quite clearly what kinds of differences to our analyses changes in token definitions might make, but still cannot show us a full picture of all the advantages provided by creating customised frequency lists on the computer. In order to understand this better, we also need to take a look at the sorting options provided by AntConc. Unlike the sorting options we had for concordance lines, where we were able to sort according to n number of words to the left or the right quite freely, in this case, we have a more limited set of options, based on the options for combinations of output for types and frequencies, as already mentioned above.

Out of these options, we've already used one, sorting by frequency, when we created our first basic frequency lists. However, what you may not have noticed when looking through the results is that there's an implicit secondary sort order, which becomes relevant when we have the same number of types for different word forms. You can see this, for instance, when you take a look at the hits ranked numbers 28-30 in the 'Rank' window. All three of the words thereat, boxcars, and oranges -actually occur with the same frequency of 123 tokens, yet at 'ranks' at the top, while the other two are 'ranked' lower, despite having the same frequency. The reason for this is that, in order to be able to distinguish between the types, the secondary sort order also sorts them alphabetically. Now, as we learnt in Section 3.3.1, the computer normally distinguishes between upper-and lowercase characters by using different (ranges of) numbers to refer to them, where the uppercase characters normally always get listed first, so we should expect to find any instances where types have the same number of tokens, but differ in case, to exhibit this behaviour. However, if we scroll further down the list until we find ranks 112-116, which all have the same token frequency of 20, we find that guess is in fact listed above OJ because it starts with letter g. This is because AntConc already automatically corrects the computer's 'natural' sort order in order to allow us to see types that occur with the same letter together, something that's more natural for human 'consumers' of such frequency lists. If you do want to insist on seeing things the 'computer way', AntConc also allows you to do so by checking the option 'Treat case in sort' in the settings for the 'Word List' tool. Doing this will then sort the list 'asciibetically', that is, place OJ before guess. Let's explore the remaining options through another exercise.

Exercise 55

Return to the original frequency list sorted by frequency.

Under 'Sort by', switch the option from the dropdown list to 'Sort by Word' and click on . Scroll through the resulting list and observe the effects to see what you can learn.

The remaining option under 'Sort by', 'Sort by Word End', is useful if you're carrying out morphological analyses on corpus data, as it groups together words with the same endings, so that for instance plural forms of nouns or third person singular and other forms of verbs will end up being grouped together. You can test this, if you want, by applying this sort option, then typing ing into the search box, and clicking on . This'll search through the word list for the first occurrence of the string ing, which, in most cases, will be followed by instances of what's commonly referred to as a present participle, although a better, more neutral term for it would be ing-form. In some cases, though, the words below this will also be instances of deverbal ing-adjectives or simply nouns or other word forms ending in the grapheme sequence <ing>. As English these days exhibits relatively little inflection, using this feature may not appear very useful, but for other, more morphology-rich, languages this represents a highly useful way of investigating morphological regularities, as well as productivity.

Despite the fact that we've now explored all the options from the dropdown list, there's still another possibility we've so far left unexplored. This is the 'Invert Order' option that appears in the form of a check box above the dropdown list. This allows you to perform reverse sorting for all the options in the dropdown list, that is, from z-a for alphabetical sorts, from 1 to the most frequent (n) types in frequency sorts, and z-a for endings, too, where the latter for instance sorts all negated and contracted forms together quite nicely.

Once you're happy with the results of your frequency list, no matter which output format you've chosen, you can save the list to a text file again, just like you were able to do with the concordance output. This way, you can not only keep a record of it, but also analyse it further in a spreadsheet application, such as Microsoft Excel or OpenOffice Calc, or compare it to lists from other corpora, something we'll also explore later on.

Stop words -good or bad?

Almost all texts, apart from maybe certain text types including telegrams and recipes, tend to have a rather high occurrence of high frequency function words, something we've just seen during our first explorations of frequency lists. Since these words don't actually tell us much about the lexical richness or the content of a text/corpus, anyway, they're often regarded as redundant and thus lists that exclude them, at least in theory, ought to help us develop better insights into the nature of any text or corpus under investigation. Words that contribute little to the semantics of a text are also referred to as stop words, and are often compiled into stop word lists that are excluded from frequency counts.

Table

The top 15 types above already contribute to more than a quarter of all word types in section A. This is in line with

However, there are certain problems in simply excluding specific types of function words from frequency counts. Whereas it's relatively safe to exclude articles/determiners or pronouns from frequency lists, we already need to be somewhat more careful with the auxiliaries (be/have; modals), since some forms may actually represent full verbs, such as have, or even nouns, for example, being, and, for example, question particles like which may well be relative pronouns that are important parts of the content.

Just as with auxiliaries and pronouns, we also ought to be very careful when eliminating prepositions and conjunctions from our frequency lists because they may equally tell us something about the domain or genre of a particular text. Imagine, for example, a text from the domain of finance about developments on the stock market, where certain values rise above/to or fall below/down to certain thresholds, etc., where the verbs on their own may not give us enough grounds to distinguish the type of domain, just like the verbs that form part of phrasal/prepositional verb combinations are often semantically relatively empty.

We can avoid at least some of these problems in using stop word lists by tagging our data grammatically before excluding any stop words, but there may not be such a simple solution in terms of deciding which of the semantically ambiguous types of potential stop words ought to be in-or excluded from our lists.

Defining and using stop words in AntConc

As we've learnt in our discussion of stop words above, the large number of function words that's so typical of most spoken and written texts to some extent 'obscures' the content words that are deemed most relevant for the recognition of genres/ domains. This is why most search engines on the Internet, for example, tend to have a list of these stop words that they exclude from their searches and possibly also the production of indexes that these searches are based on. We can do a similar thing in AntConc if we change the 'Word List Preferences' under 'Tool Preferences' to include a stop word list, as shown in Figure

To use a stop word list, first tick 'Use a stoplist below'. As you can see, AntConc then offers us two different ways of including stop words, one by specifying an existing file that includes the list, and the other by adding words to the list on an ad hoc basis by typing them in the box next to the label 'Add Word' and adding them to the list using the button. Unfortunately, when using the latter option, you have to type and add each word individually, so it's often best to prepare a list beforehand and then add to this manually if you find that it doesn't filter the list enough yet.

In my brief list shown in Figure

Exercise 56

Experiment with this yourself by first specifying different function words you want to exclude from the word list manually.

Gradually widen the list of exclusions to encompass other words that aren't normally classified as function words. Each time you've added a few words, check to see whether your list has become 'more explicit'.

Once you're happy with the results, copy your list into a new text file and save it as stop_words_trains.txt.

Be warned, though, that sometimes excluding too many function words, such as possibly prepositions or conjunctions, may also skew your results because the very frequency of particular word classes may be highly indicative of particular genres/domains, as discussed previously, and illustrated by the prepositions to and from playing a rather important role in our dialogues. In cases where you think that the exclusion of specific (function) words may have such an effect, you can of course always create and load an alternative stop word list. As a matter of fact, you might want to create separate lists for many different types of data you work with.

From all the problems we've seen above, it may seem as if single word frequency lists are actually best avoided, but nevertheless, they may provide us with at least some superficial information as to lexical density or makeup/type of a text/corpus. In information retrieval, a frequency list, if properly constructed and filtered, may also provide the basis for accessing indexes of search engines by ranking pages according to the frequencies of occurrence of individual or combined search terms. In stylistics, a word list created for the corpus of all writings of a particular author may also tell us something about their preferences for expressions, so that this may help us in deciding whether we should attribute a particular piece of writing whose origin is deemed debatable to this particular author or not, as is, for instance, done in the area of stylometry. And, of course, word frequency lists also provide the basis for many probabilistic approaches to language processing, such as establishing collocation measures or conducting probabilistic PoS tagging, some of which we've already discussed before, and others we'll turn to soon.

Of course, suitably constructed word lists aren't only useful for analysing individual texts or corpora from a purely linguistic point of view, but can also have other practical uses. In language teaching and learning, they can for instance be used by teachers to analyse materials and select appropriate or required vocabulary items, or by students to identify vocabulary they may need in order to cope with specific types of language materials, for instance in English for Academic or Specific Purposes (EAP/ESP). Attempts at creating such pre-fabricated word lists for EAP from corpus materials have already been made in the past. One particular example here would be the Academic Word List (AWL;

Word Lists in BNCweb

Unfortunately, the COCA interface doesn't allow us to create any frequency lists, so, out of the two online interfaces we've explored so far, we can only investigate how to do this with BNCweb.

Standard options

BNCweb allows you to create different types of frequency lists, either from the whole of the BNC or individual parts of it, by following the link to 'Frequency lists' in the 'Main menu' on the left-hand side of the browser window. Due to the fact that 90% of the BNC contains written materials and only 10% spoken ones, creating frequency lists of all words in the BNC, or even of individual words or word classes, rarely makes sense because this would simply give us a wrong impression of how frequent particular items may be in a heavily skewed selection that isn't really representative of the whole language. This is why, in order to be able to make sensible use of frequency lists in BNCweb, we first need to learn how to restrict our selection(s).

To be able to limit frequency lists to either the spoken or written part of the BNC already makes far more sense because it allows us to understand these different categories of language better, and also compare frequencies across them. Creating such a categorised list is relatively straightforward, as can be seen in Figure

Most of the options here should be more or less self-explanatory, predominantly related to restricting or sorting the output in ways already discussed. One of the most useful features in this setup, though, is probably that you're able to restrict the choices in different ways that can also be combined with one another. This will, for instance, allow you to investigate the frequencies of individual word classes, possibly in combination with different patterns (specified under 'Word pattern'), such as pre-or suffixes, etc., or even to state how many times such a word must occur minimally. Such features may for instance be useful in cases where you're investigating issues of productivity in word-formation through affixation, or if you want to create word lists from particular frequency bands for graded vocabulary acquisition. Let's experiment with this part of the interface a little, so you can get a better feel for what it may help you to achieve.

Exercise 57

Try out the different individual options first to get a feel for the results.

Each time, step through at least a few of the results pages by clicking on >>, and see whether you can make any interesting observations.

Next, try to combine some of the options, and see how this changes the results.

As the options for headwords/lemmas are essentially the same, they should not require any further discussion here, and I'll leave it up to you to explore them. And, before we move on to the next section, it's perhaps also worth mentioning here that different types of frequency lists can also be downloaded directly from

Investigating subcorpora

One special feature of BNCweb that potentially allows you to compare your own corpus data with that of the BNC is the ability to create subcorpora, based on different selection criteria. Figure

Exercise 58

Select 'Spoken meta-textual categories' from the dropdown list and click on . First explore all the options, and try to develop an understanding of what they might mean.

Once you've finished exploring, select 'Dialogue' from 'Interaction Type', and 'Leisure' from the 'Domain' options, respectively.

Click on . Explore the list of texts and then tick the option for 'include all' in the top right-hand corner.

Make sure the option for 'New subcorpus' is still selected in the dropdown list and click on . On the following page, name the subcorpus 'dialogues_leisure' and click on 'Submit name'. You should then get a page confirming that the new subcorpus has been created, including information on the number of texts and words it contains.

Return to the main page by clicking on , then click the link to the 'Frequency lists' page.

From the 'Range of texts' dropdown list, select the subcorpus we just added and create a frequency list.

Select 'Download whole FrequencyList' from the options, click on and save the list as bnc_dialogues_leisure_frequency_list.txt to your results folder.

When I first started writing this book, downloaded lists based on user-defined subcorpora in BNCweb, unlike those based on pre-defined sub-parts of the BNC, were not created with the sorting option we'd like to have by default, which is according to frequency in descending numerical order. This initially made such lists rather less than useful for our purposes, so in order to 'fix' this problem, I created the next two exercises. And even though the issue has now been sorted out in the interface itself, I decided to keep them, as they allow you to learn how you can import lists into a spreadsheet application, such as MS Excel or OpenOffice Calc, and then re-sort the data in order to achieve a similar flexibility to that we have in AntConc.

The procedures described here, along with the screenshots, are based on Excel 2010, and different versions of Excel, or different spreadsheet applications like Calc, may provide other options which unfortunately cannot all be covered here. However, the basic logic behind importing the list into a spreadsheet application and sorting it will always be the same, and should also prove useful for other analysis purposes, such as, for example, comparing frequency lists from different corpora, as we will see in Section 9.5.

Exercise 59

Start Excel (or Calc) and click on 'File→Open' (or press 'Ctrl + o').

Select the appropriate file type that allows you to import text, generally * .txt and/or * .csv. The latter extension stands for 'comma-separated values', but usually also covers tab-delimited data. In Excel, the option should read 'Text Files ( * .prn; * .txt; * .csv)' and in Calc 'Text CSV ( * .csv; * .txt)'.

Select the frequency list you just saved and click . Excel's (or Calc's) Text Import Wizard will start and display the dialogue shown in Figure

Next, check to see whether all fields have been split correctly by scrolling through the whole list and fixing any potential errors.

Once you're happy with the results, save the file, ideally using the same file name you used for the text file, apart from the extension.

We now have our frequency list stored in a very convenient format, as spreadsheets not only allow us to re-sort our data easily (and repeatedly, if necessary), but also because this makes it possible to investigate and enrich the data in various ways. Lists in spreadsheet format can, for instance, be filtered quite easily, cells colour-coded to indicate interesting features, or even comments or new category labels added to classify different types of words. If you want to, you can also cut less useful entries, such as maybe those pertaining to numbers or stopwords, from the list and paste them to another if you're not sure whether you might need them again later. Unfortunately, we don't have space to discuss all of the above options here, so I suggest you find suitable (online) resources about working with spreadsheets and/or experiment with such options yourself. For now, we'll focus on how to sort the list in order to bring it into a more suitable format for our purposes.

Exercise 60

Select all data on the spreadsheet. The easiest way to do this is to click in cell A1 (the one that reads 'Number'), then press 'Ctrl + Shift + →', followed by 'Ctrl + Shift + ↓', which should highlight all consecutive cells that contain frequency data, but of course you can also use the mouse if you want to.

Activate the 'Data' tab in Excel, then click on 'Sort', and set the options as depicted in Figure

Going through all the meta-textual categories on the restrictions pages for spoken and written language may sometimes involve making a lot of decisions and also doesn't allow you to select mixed data from both media. This is why, for simple domain-dependent tasks, it may occasionally be easier to use either pre-defined categories or make a fine-tuned selection from the 'Genre labels' page accessed through the 'Make/edit subcorpora' options list.

If you look at Figure

Exercise 61

From the 'Make/edit subcorpora' page, choose 'Genre labels' from the dropdown list, and click on . Open the 'Genre' dropdown list and see whether there may be a suitable pre-defined category. Strong hint: Remember, we want essays written by university students.

Select the appropriate category and then click on . Add all the files that were found to a new subcorpus named university_essays.

Create a frequency list based on the new subcorpus, import it into a spreadsheet, and sort it as we just did in the previous exercise.

The last form of creating a subcorpus we'll discuss here is to use a 'Keyword/title scan'. 'The Keyword(s)' search option allows you to select files based on library keywords or BNC-specific ones. We'll ignore this option here, though, and will instead use the 'Title word(s)' scan with the option 'any word' to search for tutorials and lessons as depicted in Figure

Based on the above exercises, it may now appear as if subcorpora created in this way are only useful for comparing data from the BNC with your own, but of course they can also be created in order to carry out analyses or comparisons in particular domains within the BNC itself, both in terms of frequencies and other options, such as 'simple' concordancing. To do the latter on a subcorpus you've created, you can simply select your corpus from the BNCweb start page from the dropdown list next to where it reads 'Restrictions'.

Keyword lists

Another way to identify genre/domain relevant vocabulary is to generate keyword lists (see

A further distinction can be drawn between positive and negative keywords, where the former represent types with an unusually high frequency in the source corpus, while the latter are types with an unusually low frequency in comparison to the target corpus.

Keyword Lists in AntConc and BNCweb

Keyword lists in AntConc

As of version 3.2.3, AntConc makes it possible to create keyword lists in two different ways, either by specifying a list of files that 'act' as a reference corpus, or selecting a frequency list created from such a set of files previously. The latter has definite advantages in that you don't need to select and load a number of different files each time, but only a single one, which is also much easier to exchange with colleagues who may not have access to the original data you used, or, in our case, to use a frequency list based on part of the BNC. Figure

Exercise 63

Import the frequency list you named 'bnc_tutorials_and_lessons.txt' into the spreadsheet application as we've done before.

AntConc uses a different concept of 'word' from BNCweb, based on the token definitions you've chosen, and may thus throw an error when it encounters something that doesn't fit this definition, aborting the keyword list comparison. Because of this, it's best to prune the list as described in the solution to Exercise 60. If you still want to retain the original list without pruning, though, you can use a little trick and simply add a # symbol in front of the number indicating the rank, and when you later save the list as text, all lines marked thus will be excluded from the analysis when you use it in AntConc. To compare the two lists, you can either use the button on the 'Word List' or the 'Keyword List' tab, and then switch to the other tab, ideally positioning the two windows side-by-side.

As Exercise 63 will have shown you, the keyword list, at least in our case and for positive keywords, may not necessarily provide you with more information than a frequency list that has been filtered well through the use of stopwords. However, unless you need to prune the reference list extensively, it can certainly allow you to identify some key terms much more quickly, and may therefore be seen as an alternative way of looking at single-word lists for identifying genredependent or semantic features of a corpus. In addition, the ability to highlight negative keywords in AntConc may also allow you to investigate under-use of specific vocabulary relatively easily, for instance when comparing learner data with that produced by native speakers, etc., an option that, obviously, a pure frequency list of only the source corpus is unable to provide.

Keyword lists in BNCweb

BNCweb provides two different 'views' of what may be 'key' in different subcorpora/domains, one where the differences between the occurrences of words are listed side-by-side, with the 'dominant' side highlighted, and the other where it's possible to display only lists of words that occur in one subcorpus/domain or the other. The options for this are depicted in the top and bottom halves, respectively, of Figure

Exercise 64

Go to the 'Keywords' page and select the subcorpus 'university_essays' we created in Exercise 61 for the first frequency list, and 'Written component' for the second.

Leave the 'Compare' option set to 'Words', and all the analysis options set to their defaults. In our exercises, we'll actually trust them to be correct and will never make any changes to them.

Click on and try to understand the results.

Let's discuss the output briefly to clarify some of the points you will hopefully already have noticed. In order to do so, we'll use a screenshot (Figure

To see how the list comparison feature works, let's use a slightly different approach.

Exercise 65

Start a 'New keyword calculation'.

Change the frequency list options to

Run the comparison by clicking on and evaluate the results. Do you notice anything unusual? To check on 'strange items' in the list, you can use a right mouse click on the frequency to display a concordance of the item in a new tab. Please note, though, that for some special characters this link may not work, in which case you may need to change the URL part behind where it reads 'theData=', either simply putting a backslash after the = sign, or sometimes, if the special character has been encoded, replacing everything between 'theData=' and '&chunk' by a backslash followed by the character in question.

As the previous exercises have hopefully shown, keyword analysis does have at least some potential in identifying domain-specific content, although it doesn't necessarily always perform better than a well-executed single-word analysis. The latter will generally take a little longer to set up, but may in fact also force the researcher to pay closer attention to the data. One further caveat in the automatic generation of keywords based on statistical measures has been very clearly summarised in

and

[…] that there is no statistical defence of the whole set of KWs, but only of each one, though the more there are the higher the chance that some of the comparisons came up by chance, and that it is not certain that the order of the items in the set itself reflects their importance. The implication of those conclusions is that KWs are pointers which suggest to the prospector areas which are worth mining but they are not themselves nuggets of gold.

It's thus well worth bearing the above-mentioned factors in mind when conducting any kind of statistics-based keyword analysis, and especially when reporting on the presumed importance of particular keywords for a given text/corpus.

Comparing and Reporting Frequency Counts

While the keyword analysis procedures in AntConc, BNCweb, and other concordancers/interfaces perform comparisons of frequency lists with the express purpose of identifying keywords or unique word types, sometimes we simply want to compare the distributions of types in two different sets of data. In order to do so, we may need to norm our frequency counts to make them comparable because otherwise the size of the corpora may provide us with misleading information.

In order to be able to norm the data, we essentially need to establish the relative frequency of the tokens and multiply these by a sensible common factor/denominator. I deliberately said "sensible" here because the literature frequently only refers to fixed factors, such as per thousand/ten thousand/one million words, disregarding that, for corpora whose overall size is less than these factors, this would be mathematically inaccurate and exaggerate the relative size by interpolating values that do not actually exist (cf.

Of course, when comparing individual items in two texts/corpora directly, one doesn't even need to normalise in this way, but can easily get an indication of the differences by looking at the ratio of the relative frequencies, something that, unfortunately, none of the textbooks I consulted suggests, although, as we saw in Section 8.2.2, the BYU interface uses this in the comparison across corpora. For instance, the ratio for the above example, which we obtain by dividing the relative frequency of text 1 by that of text 2, would be 1.6, which clearly indicates that modals are more than 1½ times as frequent in text 1. Conversely, if the ratio had been below 1, we could easily have observed that they were more frequent in text 2. In order to test these two different options and to see how we can use them, let's do another exercise, based on more data from BNCweb, where we investigate potential differences in the use of positions in economics texts.

Exercise 66

Open BNCweb and create two more subcorpora. Name these commerce_general and commerce_in_newspapers, based on the genre labels 'W:commerce' and 'W:newsp:other:commerce', respectively.

Open your spreadsheet program and create a header row that looks like this:

rank_g type freq_g freq_n rel_g rel_n ratio rank_n type_n freq_n Notice that freq_n appears twice. This is deliberate, as we'll later transfer frequencies from one column to another.

Save the spreadsheet and call it 'norming', plus suitable extension, depending on which program you're using.

For both subcorpora, extract and put the top 50 items into the spreadsheet via copy-and-paste. When you paste the data, make sure you use the 'Paste Special…' option and select 'Unicode Text' (or 'Text') because otherwise the numbers in the frequency column may not be interpreted as numbers by the spreadsheet, but still retain some HTML coding. Paste the data from the general subcorpus into the cells below rank_g, 'type', and freq_g, and the other data into the corresponding rows rank_n, type_n, and freq_n, for now leaving the cells in between the sets empty.

Sort both lists alphabetically, but independently of one another. For the general set, you should sort according to type, and for the newspaper data, according to type_n.

Next, get the token count from the 'Make/edit subcorpora' page and paste it into the spreadsheet, ideally at the top and to the right of the second frequency list, as we may need to shift some items in the list down later to align the data. Once you've pasted a total, click in the box immediately above cell A1 of the spreadsheet and type n_general and n_newspapers, respectively, followed by pressing the Enter key. This will name the cells for you and later make it easier to calculate the relative frequencies.

To align the data, check to see which types in the columns type and type_n are identical, and transfer the corresponding values from the rightmost freq_n column to the one on the left.

When you find any differences, either in the general or newspaper list, insert new rows for the types in the spreadsheet by clicking on the row number where the difference occurs, clicking the right mouse button, and choosing 'Insert' (Excel) or 'Insert Rows' (Calc), which will create a new blank row above the selected one.

Transfer the type that only exists in one list into that row, making sure that the rank is also moved to the appropriate place for the list it occurs in, and setting the rank and frequency in the list where the type's missing to 0.

Continue this operation until all the cells below the headings type_n and freq_n are empty, then delete these columns. If you think you've made a mistake, don't panic, but simply use 'Ctrl + z' to undo the last few operations.

Save the file.

Next, place the cursor in the first empty cell in the rel_g column, 3 across from the word about, type =, then click in the cell below freq_g that contains the frequency for this word, type /, and click on the number that contains the total for the tokens in the general corpus (Excel), or type n_general (Calc), then press the enter key. The formula in the formula bar when you click in the same cell again should then read =C2/n_general. You'll probably need to adjust the decimals display for the cell until you see something meaningful (i.e. other than just 0) because the resulting number will be quite low.

Repeat this step for the frequency in the newspaper corpus, ensuring that the formula bar reads =D2/n_newspapers.

Click in the relative frequency cell for about in the general corpus. You should see a black frame with a small filled square also black now. When you hover the cursor over that square, it should turn to a black cross. Once you see this cross, click and hold the mouse button down and then drag all the way down the same column to the cell across from the word worth. When you release the mouse button, the spreadsheet application will automatically have calculated and filled in all the relative frequencies for the general corpus.

Repeat the same process for the newspaper corpus.

Save the file again.

In the final step, we'll calculate the ratio. This can easily be done by clicking in the cell below the heading ratio, again typing =, clicking the cell containing the general relative frequency to select it, typing / again, and then selecting the cell containing the relative frequency for the newspapers, and pressing enter. Now you can simply select that cell again and drag downwards to get the spreadsheet program to automatically calculate all the remaining ratios for you. Don't worry about any errors that read #DIV/0, as these are only due to instances where the newspaper subcorpus didn't have any tokens at all, for which there's obviously no ratio Once you've finished, don't forget to save the file again, and then see whether you get any interesting results.

One thing we also need to bear in mind when discussing or reporting on frequency comparisons of these two types, either presenting normed frequency counts or ratios, is that, while they're fairly uncontroversial for comparisons of single texts, for comparing corpora, both forms assume a certain level of homogeneity of the data and do not take any variability or dispersion, that is, the distributions across different files, within the individual corpora into account.

As the above discussion should have indicated to you, it's very important to have all the relevant information, not only about the normed number of tokens, but also about overall size (and composition) of all corpora involved in the comparison in order to be able to judge frequency comparisons properly. This is why we should usually ideally also report the raw frequency and the corpus sizes along with any normed counts, which will then enable fellow researchers to judge our results fully.

Investigating Genre-Specific Distributions in COCA

The BYU interface makes it fairly easy to pick two sub-genres and compare the frequencies of specific words or word classes, although it unfortunately doesn't allow you to create, save, or download frequency lists as BNCweb does. On top of that, being a monitor corpus, it also makes it possible to track potential changes across different periods in the same way. To see how the former works, let's do a short exercise exploring the distribution of modal verbs in two different types of academic writing.

Exercise 67

Open the COCA interface.

In the display frame on the left, keep 'LIST' selected, then click in the 'WORD(S)' box to select it.

Next, click on 'POS LIST' and choose 'verb.MODAL'. This should put [vm * ] into the 'WORD(S)' box above.

In the two list boxes below, scroll down until you find the sub-section for academic texts, which start with 'ACAD', followed by a colon.

In the box on the left-hand side select 'ACAD:Humanities', and in the one on the right-hand side 'ACAD:Sci/Tech'.

Change the 'SORT BY' options to 'FREQUENCY'. Click and observe the results. To verify the results for any particular type in either subcorpus, you can use the hyperlinks in the columns labelled 'TOKENS 1' and 'TOKENS 2', respectively to have KWIC concordances displayed in the frame in the bottom half.

Unfortunately, the BYU interface doesn't allow you to save these results to disk, due to copyright restrictions, so, to store the results on your own computer, you'd need to use the copy-and-paste options we explored earlier for extracting parts of frequency lists from BNCweb, and then re-arrange the results a little in your spreadsheet program, as the headings will unfortunately be mis-aligned, due to the web layout.

Solutions to/Comments on the Exercises Exercise 53

The first thing you should perhaps have observed before even looking at the list itself is that the number of word types is 567, and that of word tokens 17,081. We'll learn more about what this may mean in Section 9.1.2.

If you've studied the list fairly closely, you should have recognised a number of things: In terms of text type/category, the fairly high number of tokens for words, such as okay, so, uh, um, etc., clearly indicates that we're here dealing with a corpus of spoken texts.

The initially perhaps odd-seeming type sil is further proof of this, as it represents an abbreviation for 'silence', that is, a pause of undefined length, while utt represents markup for an utterance. You can verify this for sil by clicking on it in the 'Word' window, which will take you to a concordance view of the item, where you can also see that this annotation normally appears in angle brackets in the source texts. If you do the same thing for utt, you should be able to notice that each occurrence is followed by the number of the utterance in the dialogue, a number of spaces, then a colon, sometimes followed by space + s or u + colon, then again a number of spaces, and finally the utterance itself. Here s and u represent codes for the speakers, and they only occur when the turn changes from one speaker to the other. However, s also appears more frequently (834 times) in our list than u (only 666 times), which would be strange in a dialogue because we'd expect both speakers to contribute more or less equally to the discourse, so we need to look for a different explanation here. Again, clicking on the item and investigating the concordance lines will soon tell us that s isn't only used to mark a particular speaker, but of course also represents the clitic (contraction) forms of is (as in that's) and us (as in let's), although there are no possessive markers in the corpus. It also occasionally represents the first letter of interrupted words, where it's followed by a hyphen to indicate the ellipsis.

Apart from the high incidence of discourse markers and fillers referred to above, as well as certain contracted forms like gonna or wanna (cf. Section 4.3), that are such typical indicators of spoken interaction, the number of function word tokens is also fairly high.

There are also a number of indicators that, taken together, may help us in identifying the topic/domain of the dialogues: frequent reference is, for example, made to place names (Corning, Elmira, Bath, Dansville, Avon); the preposition from occurs 136 times, while to occurs even more frequently (699 times; of course, probably at least partly due to its function as infinitive marker); the verbs take and go occur 162 and 118 times, respectively. There are also a number of general nouns that might help us to identify the domain more precisely, but this is something we'll hopefully be able to make clearer soon, once we filter the list successively. For the moment, we can at least say that the dialogues involve some form of movement between different locations, and that certain things, such as oranges or bananas, are being transported.

Exercise 54

The first thing you can observe here is that the number of word types has increased to 644 (from previously 567), but the number of tokens in fact decreased to 16,582 (from previously 17,081). If this hasn't happened for you, you've probably not applied the settings properly before re-creating the list So, how do we explain this when it would appear that, by allowing words to be joined together, we should in fact end up with both fewer types and fewer tokens? The explanation lies in the fact that, by allowing word forms to be joined, we make it possible for word forms that previously may have only existed independently, such as, for example, it, that and ll in it'll or that'll, or he and s in he's to now occur as parts of combinations that may exist as independent types, whereas previously they would have been subsumed as tokens under the individual types of their parts.

Another reason for why the number of tokens has risen here is that the hyphen in fact appears to be ill-defined; as in most instances in this particular type of data, it isn't actually a true hyphen as it would appear in hyphenated words or at the end of a line that has been hyphenated, which would not occur in this type of data, anyway. In terms of regexes, a true hyphen would be defined as \w-(\w|$), that is, a 'dash' that has to be preceded by a word character and either followed by another word character or the end of a line (and one on the next as well), while our definition here essentially only covers the former and a 'dash' that appears to occurs independently, which is how it appears in the list. However, upon closer inspection of the data, it turns out that the hyphens that are supposed to signal incomplete words in the Trains data may also be preceded by what the transcriber assumed to be the completion of the incomplete word, but surrounded by round brackets, which do not form part of our definition of what a word may contain.

Furthermore, if you scroll down the list until you reach rank position 78 (can) and 79 (can't), you'll now be able to observe what I referred to earlier, i.e. that, generally, contractions in alphabetically sorted lists tend to be sorted together with their un-contracted, or positive, counterparts. A perhaps even more interesting example of this can be found at positions 116-118, where, apart from the negated contraction form of could, couldn't, we also find the unusual form could've, which serves as a perfect example of a transfer from phonological to orthographic form gone wrong, as it's actually incorrect in missing an indication of the vowel (shwa), since phonetically/phonologically, this would be [kUd´v] and not [kUdv], as the (conventionalised) spelling suggests.

Through being able to observe the above, it should now definitely have become even easier to recognise the spoken nature of the dialogues, both in the high number of contractions and false starts, where the latter signal the kinds of disfluencies that are an integral part of normal, unplanned language, as speakers have to sometimes modify their utterances when they realise that they may have said something wrong, or maybe need to formulate what they want to say in a different way to make it more explicit.

Exercise 55

The first thing you should note when looking through the alphabetically sorted list is that now the hyphen (due to our current word definition) appears at the top of the list, followed by the lowercase letter a and its uppercase equivalent A and the lowercase a plus hyphen (a-). Now, whereas before the 'Rank' field correlated with higher frequencies, here this is no longer the case. Interestingly enough, though, AntConc does appear to have a secondary sort order based on the frequency because otherwise uppercase A would have to appear before lowercase a, and the latter is only ranked higher because it has a frequency of 161 as opposed to a single token of the former. And, just in case you're curious to find a single letter A in the data (as you should be), you can investigate this through the concordance by clicking on it. If you do this without first checking the option for 'Case' next to the 'Search Term' box, you may be in for a surprise because you'll suddenly end up finding all 162 tokens for both upper-and lowercase forms, which is of course not very useful. This is because the default option for the concordance module is to ignore case.

Scrolling further through the list, you'll probably notice many incomplete words, all indicated via a hyphen at the end, as well as a few combinations of 'stranded' characters that, upon closer inspection, will again turn out to be instances of words where the transcribers have tried to complete originally incomplete words to the best of their understanding. This goes to show that, by observing items in a frequency list, we may often be able to see things we might have overlooked or ignored while concordancing, simply because the results would have been easier to understand. Something similar goes for observing instances of information that describe non-verbal, vocal events in the dialogue, such as clearthroat or laughter, which appear out of place in comparison with the rest of the vocabulary.

Exercise 56

Of course, ultimately, the list that you'll have constructed will reflect your own choices. However, as a general rule, some of the words that you might have chosen to exclude apart from the ones I mentioned earlier will probably have been discourse markers, such as so or right, fillers, such as uh and um, response markers like yes or mm-hm, conjunctions like and, indications of non-verbal behaviour, such as, for example, brth, that signals audible breathing, references to meta-information regarding the dialogues themselves (e.g. Dialogue, Estimated, files, Length, Number), etc. Many of these may not be considered stop words in a general sense, and would therefore not be applicable to other types of files/domain, but are highly particular to this specific type of dialogue.

Exercise 57

I've deliberately left the choices for this exercise open to encourage you to explore the different options, but just in case you run out of ideas, here are a few options/combinations you could investigate. For instance, it might be interesting to investigate nouns first. As the default option for the sorting is 'descending', this will automatically show you the most frequent ones first. If you use the same option I'd set, that is, for only spoken parts of the BNC, you may be in for a bit of a surprise, as you may not just be getting an indication of the most popular 'topics' (people, references to times, money, thing(s), etc.) through this, but also find a number of unusual little words there, such as way, bit, sort, and lot. Under normal circumstances, we'd probably not expect to find these nouns to be among the most frequent ones talked about in conversations. Upon closer inspection, though, it turns out that they simply form parts of longer multi-word expressions that tend to characterise spoken language, such as in a way, this way, no way, for way, bit mainly occurring in a bit, sort as part of the hedging device sort of, and lot in a lot, all of which don't signal any specific topic information. Switching to written language, we do get some of the items, namely people, way, as well as references to times/dates, recurring, but most other nouns are already far more indicative of content, referring to such topics as family life, politics, locations like London, business, etc. I'll leave it up to you to investigate this in more detail. If you do so, though, and want to draw conclusions about written language only from this, please also bear in mind that the written part of the BNC also includes fiction, where at least some parts 'model' spoken language, as in the direct speech of characters in novels or plays, etc., so that we should perhaps not see everything here as exclusively written.

Of course, you could also attempt to investigate the nouns in the BNC from a different angle, which would be to look at the rarest ones first. In order to do so, we could simply switch the 'Type of ordering' option to 'ascending' and then see which rare, or perhaps exotic, nouns we may find. Unfortunately, though, this won't have the desired effect, as the first 3,900 word forms will represent numbers (with or without a dollar prefix), foreign words (apparently mainly Czech), etc., which may in many instances -at least as far as the numbers are concernedwell act as NPs, but not actually constitute nouns themselves. Later, predominantly mis-tagged forms contain leading quotation marks or hyphens, or further numerical/measurement units that are at best pre-modifiers, rather than nouns, where in many cases a number has not been split off from its following unit of measurement. Other similar errors continue up until we reach approximately 12,000 word forms or so, as, up to this point and a little further on, most of the types constitute hapax legomena, that is, word unique forms. From roughly this point onwards, at least some of the following hapax legomena appear to be proper names, so this is perhaps where the tagging errors gradually begin to peter out. And even if this number represents only 0.01% of all the words in the written parts of the BNC, the number of potential errors, which appear mainly due to tokenisation errors, is staggering, particularly when considering that this affects only one of the parts of speech represented in the corpus. You may then think that perhaps if hapax legomena are such a problem, you could start investigating instances of types that occur at least twice. However, sadly, these are beset by similar problems, again mostly related to mis-categorised numbers, such as recurring years or even apparently numbers indicating section hierarchies in documents (such as 4.

For verbs, perhaps the most obvious thing to investigate would again be the most or least frequent types, but, as these still exhibit far more inflectional options than other parts of speech, it may also be interesting to investigate inflectional suffix patterns by selecting the 'ending in' option for 'Word pattern', or possibly verbs that have potential negative prefixes by choosing the 'starting with' option. The final option of this set, 'containing', is probably not exclusively relevant for verbs, but could be used to investigate derived forms of nouns, verbs, adjectives, or adverbs based on restrictions for particular stems.

Most other PoS categories can be investigated in similar ways, either looking for pre-or suffixes, most/least frequently occurring types, etc.

Exercise 58

You should already be somewhat familiar with the composition and features of the BNC from Exercise 4 in Chapter 2, as well as some other discussions in Chapter 3. Just as a reminder, though, under the 'Overall' group, 'Demographically sampled' refers to materials collected from tape recordings made by individuals in private settings, and 'Context-governed' to data transcribed from public speeches or other events, news or sports commentaries, lectures, classroom interaction, etc.

Both of these have their individual sections for restrictions further down the page, to allow you to narrow down your choices, based on domain information for the context-governed type, and age, social class, and sex of respondents -not the speakers, whose characteristics can be selected separately -for the demographically sampled data. For instance, for the former category, you could here choose to only select educational or business materials, depending on whether you may be investigating issues in EAP or business-oriented ESP. For the latter, you could opt for only investigating the speech of men or women, or speakers from the same or different social classes, or create subcorpora for both, and then compare the language in some way, which makes it possible to carry out corpus-based sociolinguistic analyses on the data, as do the options for restricting speakers to particular education levels, etc.

To carry out genre-based analysis, you'll probably want to focus on making particular selections from the 'Genre' section, which, to some extent, overlaps with the restrictions for context-governed materials.

Provided that you don't forget to change the option for creating a new corpus, there should be no issues in completing this exercise and creating a suitable frequency list.

Exercise 59

This exercise may have seemed rather complicated to you, as it involves working with yet another program and also many, perhaps complicated-seeming, steps. However, as we've already seen above, in order to get the full picture, we cannot always only rely on one single program or interface, and frequently need to process our data further in various ways.

The first thing we can see when looking at the top few items in the list is that, apparently, BNCweb treats all punctuation characters as word types, and hence also tokens. This is odd because, although punctuation may well convey linguistic meaning in some sense, it still doesn't carry any truly semantic meaning, but its function is more pragmatic in nature. We'd thus probably want to 'throw away' all types that constitute punctuation marks or mathematical operators, which also includes some character reference entities like &lsquo; (left single quotation mark), &mdash; (m-dash; dash that has the length of the character m), and &rsquo; (right single quotation mark). The latter, strangely, are still part of the text representation of the XML version of BNCweb, despite the fact that they can easily be encoded in UTF-8, which is, after all, the assumed default encoding for XML.

Likewise, when scrolling through the list, you may notice that there are some forms preceded by apostrophes, which either represent true clitics or sometimes just indicate non-standard pronunciation forms, such as h-dropping. The question here is whether we'd really want to keep them in this form or possibly associate them with their full, non-contracted equivalents by increasing their counts and deleting the contracted forms, or keep on treating them separately. A little further down, we also find a series of numbers, some of which represent years. However, in many contexts, number tokens are really not terribly meaningful, either, so we could opt for removing them to prune the word list, especially because most of them represent singletons, anyway.

At the end of the list, you'll also find some entries related to anonymisation in the data, where names, addresses, or telephone numbers were marked as having been elided in order to protect the anonymity of people or institutions. In many cases, these may well represent nouns, but as we have no way of identifying how many tokens they genuinely represent and which types they belong to, it may sometimes be safer to ignore and delete them. The same goes for instances of

Of course, any operations where we modify the data or remove items may also have an effect on further processing, as they'll affect the overall type and token counts that may, for instance, be relevant in comparing our list to lists from other domains, something we'll explore soon. Issues like this may affect both manual and automated processing, but can have more of a negative effect on automated and unsupervised analysis procedures because, there, possible issues will never be spotted, and the potential relationships identified between words may become skewed. Now, if you're worried about deleting data from the list, you can of course take advantage of the spreadsheet application in a different manner. As spreadsheet files normally consist of multiple worksheets, rather than completely deleting items you want to remove from the list for various reasons, you can copy/cut and paste them to an extra spreadsheet and preserve them there, just in case you may decide later on that you want to insert them again. That way, you also won't have to go through the same routine to re-create the list in the online interface, should you notice that you've accidentally removed data you'll later need. And, if you're worried about not being able to get an exact token count of all the data after making modifications, the spreadsheet will also help you there because all you need to do in order to obtain this is to place the cursor in the field immediately below the count for the final token in the list and use the AutoSum function (symbolised by ) to automatically count the total for you.

One other thing you'll hopefully be able to see easily when looking at the list is that BNCweb has downcased all words, and that there was also no option to preserve the original distinctions between capitalised and non-capitalised forms, which now makes it difficult to distinguish between common nouns and proper names. Of course, you could fix this manually, but, since we've now separated the data from our ability to concordance on it, we'd have to go back into BNCweb to look at the original frequency list. One way in which we could have avoided this particular issue would of course have been to create the list including tags. This way, the different PoS categories, insofar as they were correctly tagged in the first place, would have automatically been counted separately. I'll leave it up to you to create such a revised list, and see how this may affect your interpretation of the data.

Exercise 60

The only two things you could possibly get wrong in this exercise are that, perhaps, you don't select all the data and get an incomplete sort, or that you don't use the right (custom) sort button and instead sort only according to one field. However, as you can hopefully see, the options we want to use here may be labelled somewhat differently in Excel/Calc, but are otherwise exactly like what we'd get if we sorted according to descending frequency in AntConc, as our secondary sort also takes the alphabetical order into account.

When you look at the results, you should also be able to notice that, similar to the results we had for the Trains corpus, most of the high-frequency function words get sorted to the top, as well as first and second person pronouns, and fillers like erm, etc. If you didn't delete the contraction forms in the previous exercise, these will also appear near the top. Otherwise, of course, we don't get such a clear indication of the topics as in the relatively small Trains data, which was restricted to a very narrow domain, along with a very specific task.

Exercise 61

This exercise should be relatively straightforward, provided that you pick the option 'W:essay:univ' from the dropdown list. And, of course, you need to make a decision about whether to include tags again or not, but, other than that, the steps required are just the same as before and should require no further comment.

Exercise 62

As before, there should be nothing special about this exercise.

Exercise 63

Provided that you've swapped the second and third columns correctly inside the spreadsheet application, as well as loaded the data file for the reference corpus properly in the preferences, the only real difficulty in this exercise is to remove all the items that don't look like proper words successfully, so that AntConc can load your frequency list. If you've followed the guidelines I gave above for Exercise 60, you'll already have eliminated most of the likely candidates for errors. Other things you'll need to delete are single letters followed by dots, as well as abbreviations, such as i.e. or e.g., and any genuine words that are followed by dots, as the dots don't form part of the general token definition we're using, and most of these forms (apart from the abbreviations) are probably the results of tokenisation errors in the BNC, anyway. You should definitely also delete all numbers, unless you want to change the token definition to include those, but, as I pointed out before, numbers may take many different forms and their meaning may be difficult to identify.

Once you've created the keyword list, you should immediately be able to see that some of the words we'd previously only been able to identify through the basic frequency list after repeatedly manipulating the stop word list should now more or less automatically have 'jumped' to the top. If there are still some words in the list that seem odd, perhaps because they are part of the meta-information of the dialogues or indicate paralinguistic features, such as breathing, etc., you can easily eliminate them from the list in order to clean it up further by manipulating the stopword list, then re-creating the Trains frequency list, and re-running the keyword analysis.

Exercise 64

Even just a cursory glance at the positive keywords in the top 31 types of the keyword list already demonstrates the academic nature of the vocabulary quite clearly, containing vocabulary roughly related to scientific/psychological experiments, politics, as well as linguistics. The terms themselves may also be polysemous or refer to different domains, so it's useful to be able to use the hyperlinks behind the frequencies to generate concordances for disambiguation. For example, subjects predominantly refers to 'people involved in experiments', rather than the plural form of the 'syntactic role' category, while memory may refer to either 'brain capacity' in psychological experiments or to '(random-access) computer storage', etc.

In terms of function words, the high keyness of the and this points to the highly packed nature of written academic language, which contains many (definite) noun phrases and other types of deixis.

Looking at the negative keywords, we can easily notice that personal pronouns are much less frequent, partly due to the impersonal, non-interactive, 'objective' nature of most types of academic writing. This becomes especially clear if we investigate the low frequency of i (lowercased, as all examples are automatically), which indicates a strong dispreference for authors to refer to themselves, in particular also because not even all instances of I always constitute tokens of the personal pronoun. Sadly, in very few cases here do we actually get a strong expression of an author's voice.

Exercise 65

Essentially, the option we just explored has relatively little to do with keywords as calculated through the options from the top part of the same page, as all it really does is eliminate all word types both corpora share, and then display whatever remains as a frequency list. Therefore, the list produced by BNCweb actually also contains one redundant column, depending on whichever list wasn't selected, and where all the type frequencies are set to 0. The idea behind seeing such words types as key is of course based on the notion that non-shared items are always key for a particular corpus, which may not necessarily be the case, even though they do help us narrow down the options for identifying true keywords without the use of statistics. The smaller the corpora compared are, the easier it'll of course become to narrow down such selections, but essentially, the technique itself is similar to creating basic stopword lists, only that, in this case, a word list from a whole corpus is used as a stopword list.

Although the comparison of such wildly different subcorpora in terms of size is, admittedly, not very useful in general, the list of unique items in the written component immediately reveals a number of interesting features of the BNC tagging and composition, or rather, the way BNCweb allows you to work with them. An extraordinarily large part of the unique types revealed as 'keywords' here constitutes items of punctuation, proper names (genuine names, abbreviated letters), cardinal numbers, dates, and measurements, many of these features that we've already previously identified as having the potential to skew our frequency analyses, and thus being able to filter these out in some way would be nice. Unfortunately, though, BNCweb doesn't offer us any direct way to do so, so we'd need to export a tagged list and then filter it ourselves in some way, for instance by excluding specific tag patterns in Excel or by writing a small program to remove words tagged with unwanted tags.

Exercise 66

As before, there are many steps in this exercise, and you need to be very diligent in moving around some of the data once it's been pasted into the spreadsheet. However, you'll hopefully soon realise how programs such as Excel or Calc make it much easier for you to calculate the values you want automatically, which saves a lot of time and is much less error prone (if done carefully) than calculating everything by hand. But of course, if you get some of the steps wrong, there's also great potential for producing erroneous output, so we'll go through the individual steps here again and I'll try to point out potential issues or traps you might fall into, along with further opportunities the spreadsheet offers to visualise and investigate the data better.

The first part, creating the subcorpora, should be relatively straightforward, as we've practised this before. Creating the header row in the spreadsheet and saving it should also not present any problems.

The first issue could in fact arise when you paste the data from BNCweb into the spreadsheet. If you don't use the 'Paste Special…' option, which in Calc even triggers the text import wizard, you'll end up with frequency values that the spreadsheet cannot interpret as numbers because they still contain some (hidden) HTML formatting. In this case, there's simply nothing I've been able to identify that will make it easy for you to convert all the data short of editing all the cells manually . Sorting the lists (individually) should again be easy, as this time we don't even need to specify any additional sort key. Pasting the token counts from the 'Make/edit subcorpora' page also presents no problem because, this time, we only have one single piece of 'text' without HTML codes, so there's nothing to misinterpret for the spreadsheet program.

Naming the totals should not be particularly difficult, but if you should have a problem using the 'name box' above cell A1, you can right-click on the cell in Excel and choose 'Define Name…' or use 'Insert→Names→Define…' in Calc.

To transfer and align the data is probably the most difficult and time-consuming part of the exercise because it's easy enough to make mistakes when creating new rows and moving the data around, as well as adding the noughts to the relevant cells where a type doesn't exist in one of the corpora. Incidentally, the reason for why we added noughts for the ranks in the first place is that we'll later be able to sort the result table according to the rankings in either of the corpora and compare them, too, in which case all non-ranked, that is, non-occurring types will be sorted together. The same applies to sorting the data by frequency if we wanted to compare raw frequencies for whatever reason, or simply identify non-occurring types in either corpus.

Provided that you type the formulae into the formula bar correctly and select the right cells, calculating all the relative frequencies accurately and efficiently should also not present any problems, although, if you haven't used spreadsheets extensively, filling cells by dragging may take some getting used to. If you feel uncomfortable using the mouse for this, there's also an alternative way of filling the cells down, which is to click in the first cell that already contains a formula, then holding down the shift key, and using the down arrow to highlight all cells you want to fill. Once you've selected all the cells, you can press 'Ctrl + d' in Excel or use

For example, if you have a formula that sums two adjacent cells A1 and B1 using the syntax =A1+B1, and then drag down, the spreadsheet will 'assume' that you'll next want to add A2 and B2 automatically, and adjust the formula accordingly. However, by naming the totals, and using the names inside the calculations for relative frequencies, we've effectively mixed relative references and absolute ones in one and the same formula, as a named cell always refers to a constant value contained in that cell. Thus, by dragging down the formula we created to calculate the relative frequency for about in cell E2 (=C2/n_general), we effectively changed it to =C3/n_general for the next cell below, etc. In determining the ratio, however, we used relative references for both parts of the equation.

Once you've finished all the calculations, you can analyse and evaluate the results. As I said before, there are now a number of options for sorting according to different fields, for instance comparing the ranks in one corpus against another or, perhaps more importantly, seeing whether certain words dominate to some extent in one corpus in comparison. To do the latter, all you have to do is to sort according to the ratio. If you sort our data in descending order, all the types that only occur in the first corpus will automatically appear at the top, due to the division by 0 error I referred to in the instructions. This will then be followed by all instances where the relative frequency is higher in the first (general) corpus, and you can easily identify these 'dominant' words due to the fact that they'll have a ratio above 1. However, even without sorting in this way, or sorting according to another column, the spreadsheet application would still allow you to visualise this easily by applying conditional formatting to the ratio column, where you could, for instance, highlight cells with values above zero with a green background and those below with an orange one, or any other colours you prefer. This is similar to the + and -symbols display in the keyword analysis in BNCweb, and represents just one of the ways in which we can visualise differences in the data easily for a quick overview. I'll leave it up to you to find out how exactly this can be achieved in whichever application you're using.

The analysis of the data, sorted according to the ratio, reveals a few interesting features, although overall it may not provide us with any deep insights. Looking at the words that exclusively occur in the general corpus, we can see some interesting types, namely concerning and regarding, that have been classified as prepositions despite the fact that they don't look like typical prepositions because they are in fact ing-forms, that is, they clearly still retain some verbal character. Closer inspection of the original data in BNCweb further reveals that at least some of their tokens, especially for concerning, would perhaps better be classified as conjunctions, especially when occurring in clause-initial position. The occurrence of v. demonstrates that parts of the general subcorpus contain references to legal matters, where the type is one of the two possible abbreviations for versus (the other being vs.). Interestingly enough, the second form, which in fact occurs twice in the same subcorpus, is not used in a legal context, so the first form may (tentatively) be seen as a potential identifying factor for texts from the legal domain. The most striking difference in type frequency for types occurring in both subcorpora is that for upon, which occurs more than 10 times as frequently in the general corpus (ratio 10.604). This can be interpreted as a difference in the levels of formality in the two subcorpora, as well as again partly to the occurrence of legal texts, which tend to be more formal (or formulaic) in nature, anyway. This would seem to be corroborated by the fact that among is somewhat underrepresented in the general corpus (ratio 0.712), which, however, exclusively has the alternative, and more formal form, amongst instead, as well as the relatively high level of occurrences of within (ratio 2.727) as an alternative to the less formal in. Of course, as with all analyses of word lists, even though the above results may lead us to assume that economics-related newspaper reportage may be less formal in style than other types of writing in economics, this assumption would still need to be substantiated further by more careful qualitative analyses based on concordances.

Exercise 68

This exercise is much easier to do than the previous one, so there's relatively little potential for getting anything wrong. Perhaps the only things that could happen here are that you either inadvertently select the wrong display option, or the wrong sub-genre from one of the list boxes. In the latter case, the interface will unfortunately not allow you to spot that error easily because it'll only refer to the two selections you've made as SEC 1 and SEC 2 in the comparison frame.

Looking at the results, we can first notice that there are specific tokenisation and tagging issues that result in inappropriate 'compound' forms. These, we can perhaps safely ignore, bearing in mind that the modals contained in them would normally contribute relatively little to the overall token counts of the genuine types.

The remaining examples can be investigated by clicking on the links referred to in the instructions, which I'd strongly advise you to do before making any conclusions about the comparison results, as at least the results for 's mainly yielded results that contained the contraction Let's, where the clitic isn't even a verb, but the reduced form of the pronoun us, apart from the fact that it's clearly debatable whether the reduced form of the auxiliary BE should ever be considered a modal. The latter also applies to used as part of the combination used to, where the sequence represents an alternative way of expressing past tense, rather than any form of modality. These two cases clearly represent issues related to automatic PoS tagging and the theoretical assumptions and rules behind it.

The next thing to observe in the lists is that we have what appear to be nonwords, namely ca, wo, sha. However, if you call up the concordances for these, you'll soon find out that they represent the initial parts of the negative contractions can't, won't, and shan't, which have been separated from the negation 'clitics' in the tagging process and are being treated as individual tokens. Incidentally, the same thing also applies to BNCweb, due to the use of CLAWS in the tagging of both corpora.

Having identified these potential issues, we can now focus more on the actual genre similarities and differences. Here the most common 'general' modals like can, will, would, may, must, might, which tend to be more epistemic in nature, that is, frequently reflect instances of hedging or stance, are almost equally frequent in both sections. In contrast, though, the light-green highlighting in the table on the left-hand side clearly (along with its mauve counterpart on the right) indicates that the humanities writing contains more instances of deontic modals, that is, those that express permission or obligation, such as shall, ought, and need. The latter, which are sometimes also slightly more old-fashioned, for example, shalt, shall, or also to some extent dare, are apparently due to the fact that the humanities section contains religious texts that are highly formulaic in nature, as they contain biblical quotes of deontic nature, such as thou shalt…, etc., which are not present in the science/technology writing, which tends to be less 'prescriptive' in general. One particularly interesting type in the table in this context is wilt, which, on the surface, would appear to be an archaic form of WILL, so we'd expect to find more occurrences of it in the humanities texts. Upon closer inspection, however, it turns out that this type in most instances in both corpora in fact represents a typo, that is, the misspelt form of will, or in the science writing a mis-categorisation of the nominalised form of the verb wilt in two cases. Although it's of course somewhat easier to confine our corpus-based investigation to single lexical items, single words and their frequencies aren't the only interesting things we may want to analyse with the help of corpora. This would be restricting ourselves to more or less a kind of lexical/semantic analysis that largely ignores the fact that words really only gain their 'true meanings' in context. Even if (sorted) concordance lines can already represent an extremely valuable asset in a teaching context because learners -as well as teachers -can investigate words as they're really used in authentic materials, such an analysis may be rather time-consuming. Furthermore, the fact that we may be able to find some patterns easily -and then perhaps focus on only those -may make us overlook the true flexibility inherent in language in 'playing' with these patterns to change their meaning in an appropriate and sometimes fairly subtle manner. These choices we have when using language can really only be investigated through finding ways of expressing this flexibility on the paradigmatic and syntagmatic axes in our corpus searches. On the other hand, if we move away from the analysis aspect for a moment and think about the teaching/learning side, we'll soon realise that finding out about groups of words that co-occur is of great importance in developing the learner's sense of idiomaticity.

Sources and Further Reading

Understanding Extended Units of Text

People who tend to 'stray' on the theoretical side of linguistics often appear inclined to assume that the unit of text we should deal with is the sentence, without necessarily being able to define what exactly this is. Is it simply everything from a capital letter to the next full stop, exclamation or question mark, provided that we even follow Western writing conventions? And how do we deal with embedded sentences, or co-and sub-ordination, reported speech, or parenthetical structures separated by hyphens or commas from the main proposition?

From these questions, it may be clear that it's by no means obvious -or at least should not lightly be considered obvious -what the real unit in text could be. Certainly the 'utterance' A cup of coffee

The most likely candidate for a textual unit seems to be the c-unit, defined by

Exercise 68

Try to identify the c-units inside the following piece of dialogue by inserting whatever you think may be an appropriate punctuation mark between them in pencil. Pauses in this excerpt are indicated by #, and beginnings and ends of overlapping speech by opening ([) and closing (]) square brackets, respectively. A & B represent different speakers, where each speaker indication in the text is followed by a dot and the turn number.

Justify your choice each time, thinking about why you've chosen to break the unit at this point, and also why you've decided to use this particular punctuation mark. Was your decision based on your knowledge of writing conventions, syntactic, semantic, or pragmatic criteria, or maybe a mix of the former? As the above exercise will hopefully have shown you, while we may have relatively clear-cut syntactic categories/units in written language, spoken language, which is, after all, more dominant in our daily lives, is not as easily segmentable. Therefore, analysing and understanding it may require a rather different approach, as well as a need to re-think many things we may have learnt about the analysis of language in the past. What therefore seems to be even more important than recognising the c-unit as the correct unit is the fact that one should always be aware that there are certain boundaries in text that form natural borders between the units of text we may want to see as belonging together, an issue that gains in importance once we move away from looking at single words only.

Text Segmentation

As the previous exercise has shown, finding individual larger units of/in text is quite difficult to achieve. One might assume that punctuation, something we use all the time in order to delimit 'sentences' from one another, is fairly unambiguous, but, as you may also have noticed, once we start exploring all the varied functions the different punctuation marks may perform in a text, we may be quite surprised by their capacity for creating ambiguity. To illustrate this further, let's take a brief look at some of the general and special potential functions of different punctuation marks:

r full stops may indicate (declarative) 'sentences

We already encountered some of these issues in Exercise 35, where, for instance, most of the Roman numerals in capitals were tagged incorrectly, due to not having been identified as numerals followed by a dot indicating their ordinal nature. Despite the potentially problematic practical issue of how to deal with the above options, it's important for us to try and interpret all the different uses of punctuation marks as accurately as possible if we do want to determine the correct unit for analysis, since straying too far may result in odd results when we want to handle the larger units of texts we're now going to discuss. Luckily for us, many of the potential difficulties shown above can be avoided if tokenisation is performed using appropriate regexes in most cases. We'll discuss options of how to actually mark (up) and distinguish larger units of text in Chapter 11, when we talk about annotation.

N-Grams, Word Clusters and Lexical Bundles

Moving beyond the level of single words, we usually start by looking at two-wordunits (bi-grams), next three-word-units (tri-grams), and so on (see Section 10.7). When we talk of units larger than a single word, we can also refer to them collectively as n-grams, word clusters, lexical phrases, or lexical bundles, although these expressions carry slightly different connotations, depending on the context they're used in or the research conducted on them. The main thing that unifies them is that they all represent sequences of words that tend to occur directly in a row, that is, without any intervening elements between them.

N-grams may already constitute basic level collocations, such as strong/black coffee, etc., but certainly don't need to, as for example in the determiner-noun sequence the green as part of the noun phrase the green house. When dealing with semantically and syntactically meaningful n-grams, we need to be especially careful not to move across any c-unit boundaries because otherwise our combinations of words no longer make any sense. For example, if we have two consecutive units like There was a green house. The house was in a terrible state., and we ignore the full stop at the end of the first unit, we may either produce a bigram house the or a trigram house the house, neither of which will really be a sensible combination, and both are therefore not worth interpreting in terms of their linguistic functions.

Incidentally, you may have noticed that the words in the n-grams shown above are all in lowercase, so that they don't really always represent the original forms. This is generally done in order to be able to group words with or without initial capitals together, just as we've seen for sorting above, and represents one of the 'shortcuts' in language processing that could potentially lead to errors of analysis if not borne in mind.

One further pitfall in analysing n-grams may also be to forget that there could actually be n-grams within n-grams. In other words, it often makes sense to look for the longest (sensible) string, which could always contain one that's one word shorter. Coming back to our example from above, the tri-gram definite noun phrase the green house also contains the bi-gram green house, which we may want to distinguish from the indefinite noun phrase a green house. Here, we can also begin to see the importance of retaining function words in our n-gram counts because otherwise we run the risk of losing potentially valuable distinctions, a feature that becomes even more important when we start looking at collocations or idioms, where, for example, we say that somebody hit the roof because they were angry, but not they hit a roof.

The term cluster can be seen as a kind of neutral 'umbrella term' for both ngram collocations, that is, meaningful sequences, and those sequences of words that are less meaningful in terms of their syntactic or semantic functions, which are generally referred to as 'lexical bundles' (c.f.

Exercise 69

To do this exercise, go to

To develop a better understanding of what clusters may look like and how they're built up, let's take a look at an example of how n-gram lists are generated, based on the contents of this paragraph.

Click on the button that reads 'Click to generate n-gram list' and look through the results.

Next, change the number in the dropdown list to 3, 4, and 5, respectively, each time observing the output. Every time you do this, think about how meaningful/useful the resulting combinations may end up being.

The frequency of occurrence of the n-grams you'll have observed doing the above exercise can also be counted, just as in basic frequency lists. In this way, we can identify the most common clusters, and, for example, evaluate whether they may be semantically or pragmatically meaningful or useful to learn for L2 learners, or be indicative of a given textual domain, etc.

Exploring (Relatively) Fixed Sequences in BNCweb

BNCweb provides us with many different options for investigating words in context, involving a mix of the various options we've explored before, such as looking for only the (fixed) word forms themselves, this time in combination, enhancing this by restricting/expanding word forms in context by using wildcards or headword/lemma searches, as well as using regex quantification in conjunction with grouping. These queries are generally referred to as phrase queries in BNCweb. Additionally, we can also do contextual or so-called proximity searches, where it's possible to search for words that occur within certain textual units or within a certain number of words of one another, etc. The range of potential options is so great that we'll only be able to explore a small part of this to give you a 'taste' of what you can achieve if you explore them in more detail.

Simple, Sequential Collocations and Colligations

'Simple' collocations

We'll begin our exploration by looking at what I've termed 'simple' collocations, to contrast them with what would appear to be more complex phenomena we can only identify easily via statistical measures of co-occurrence. We'll discuss the other 'type' in more detail in Section 10.8. In general, as the name collocation implies, we're here dealing with a phenomenon that describes which words tend to occur in proximity (co + location) to one another because they have some kind of 'affinity' to, or 'affiliation' with, one another. To identify such affinities at the most basic level, one can start by looking at the sequential co-occurrence of words in context. The differences in meaning lie in the 'combinatorial options' on the paradigmatic axes of the different words that co-occur with one another, either in relatively loose combinations, such as those involving nouns and their pre-or post-modifying adjective phrases, or in terms of relatively fixed combinations, like idioms or proverbs. In other words, the question here is to what extent the words in a particular context are actually (freely) exchangeable with other words.

In order to investigate the different options available for specifying variable sequence patterns, we'll now do a series of exercises, beginning with an analysis of expressions of 'voice quality' in literary texts.

Exercise 70

Select 'Written restrictions' under 'Query options' on the main page.

To practise narrowing down your searches some more, restrict the query options for 'Medium of Text' to 'Book', the 'Derived text type' to 'Fiction and verse

Gradually refine the query to limit it to particular parts of speech occurring between the fixed parts. You can also use the simplified PoS tags after the underscore, and don't even need to add any word or wildcard before it.

Extend the query to include two different intervening pre-modifiers coordinated by and.

As you'll have seen, this approach already works quite nicely with relatively fixed phrase query patterns, such as the ones we tried above. However, sometimes you may also want to be able to restrict certain options for words, rather than using wildcards or PoS tags, or to make certain parts optional, so let's just try a few relatively simple options for grouping, alternation and quantification:

Exercise 71

Try to re-write the last example to make the co-ordinated second modifier optional.

Extend the previous query to alternately look for in or with as the initial preposition. Can you observe any differences?

Next, try the somewhat more complex string (in|for) (a|the) (long|short) (time|period).

Finally, try to create a pattern that will find one of the three prepositions in, on, and at, followed by up to three intervening words, and then a noun. Hint: You need to use exact regex quantification here. What do you think you can find with this and what could the results of this query be used for?

In addition to exploring the various phrasal constellations we've just investigated above, this type of flexibility also makes it possible for us to research idioms to some extent. When doing this, one important feature to verify is to see whether there are any words in such expressions that may potentially be replaceable by other words with similar meanings, for instance definite articles by indefinite ones, etc. If such variability is found, then the most interesting feature to explore is whether it's relatively free -something that is quite rare in idioms -or whether the idiom has deliberately been 'manipulated' by the speaker/writer in order to achieve a particular (pragmatic) effect.

Colligations

To illustrate colligation, that is, the co-occurrence ('linking') of specific word classes or lexical items with particular parts of speech, we can look at a special feature of BNCweb, which is the way the clitics in contractions are handled. As discussed before, contracted forms in English still tend to occur mainly in the spoken medium because the conventions of what is often deemed to be 'good writing style' continue largely to forbid their use in the written medium, despite the fact that this really doesn't make sense because contractions do in fact aid the natural reading flow. Because of this conventional restriction, we'll confine our searches to spoken parts of the BNC later.

The option to produce contractions in the strict grammatical sense, which is what we want to investigate here, rather than just any abbreviated word form, is limited to a closed set of words, belonging to few word classes, and co-occurring with an equally limited set of word classes. To be able to investigate this, we can divide our task into two separate stages. In the first, we'll use BNCweb to identify all possible forms of contractions in order not to miss any if we simply 'query our intuition'. In the second, we can then construct one or more appropriate search patterns that'll allow us to create concordances that illustrate which word classes co-occur with which type of construction.

Exercise 72

Open BNCweb and set the 'Restriction' option for a new query to 'Spoken restrictions'. Think of possible options for contractions you know and define a wildcard pattern that'll find them. BNCweb defines certain clitics in contractions as separate words, so you need to bear that in mind.

Test your pattern and use the 'Frequency breakdown' option to identify unique 'types'. If you're not happy with the results, refine your pattern iteratively.

Investigate the individual clitics by accessing the concordances from the breakdown page and hovering over the hit to get the pop-up display of the PoS-tagged context. Opening the concordances in a new tab each time would definitely make sense here, as you then only need to close the tab again to return to the list of clitics. In addition to straightforward concordancing, though, also explore other ways of investigating the results, such as those that the sorting options/restrictions for the concordance lines offer.

The above exercise should have given you an insight into how particular clitics in contractions can combine with other word classes on the syntagmatic axis, although the options for each one of the parts of speech that can occur to the left of the clitic obviously represent items that are exchangeable on the paradigmatic axis.

Contextually constrained and proximity searches

Apart from the various options for using wildcards to specify optional or nonoptional slots in a query, BNCweb also allows us to look for words/expressions in certain contexts or within a specific number of words of a search term. This, for example, enables us to look for search terms in specified environments, such as at the beginning or end of a paragraph, a sentence-like unit (s-unit), or a speaker turn in the spoken part. Furthermore, it also makes it possible to construct patterns where we're not exactly sure about the potential position of one term in relation to another, and want to be able to roughly investigate a range of possible options. What exactly this means will hopefully become clear soon.

The context options we have are indicated via XML tags (explained in more detail in Chapter 11), e.g. <p> for paragraph, <s> for s-units, and <u> for turns, etc., where 'u' apparently means 'utterance', although it may, strictly speaking, possibly consists of a number of these. Searches can either be defined to occur within paired tags, e.g. <u>(+)+</u> to (theoretically) retrieve all individual turns from the spoken part, or as occurring at the beginning or end <u> + search term to retrieve items that occur in turn-initial position. Let's try this in a brief exercise.

words, if we take colligation to be the co-occurrence of PoS categories, this particular class would colligate with itself in these cases. In the examples we saw earlier, though, the types we encountered usually contained one element that was part of a phrasal/prepositional verb construction. In another type, such combinations generally create a complex meaning, as in, for example, up until or down below, where the resulting meaning is a mix of the semantic properties of both elements. In some cases, though, especially in American English, we may encounter sequences of prepositions/particles that actually contain redundancy because one of the parts alone would already express the same meaning, such as in off of.

To investigate this feature, and to see whether such redundancy also exists in other combinations, we could now try to create lists of all multi-preposition sequences. Unfortunately, though, the BYU interface won't allow us to search for these, throwing the following error message "All of the "slots" in your multi-word search string occur more than 10,000,000 times in the corpus (e.g. [n * ] [be], or [j * ] [nn * ]). Please re-do the query so that at least one of the slots has a frequency lower than 10,000,000". Thus, unlike in BNCweb, where we were able to run the query and then thin it down, there's in fact no way to run this query at all if we don't know how to restrict at least one element. The alternative presented for the COCA is to download n-gram frequency files by following a link presented further on in the error message. This obviously isn't as useful because there's no way to directly investigate the context by clicking a link whenever you find an interesting example, but of course better than nothing -or looking through hundreds of concordance lines generated for single prepositions in the hope of finding suitable combinations, which you could still do by using the KWIC display option and sorting according to first word on the right of the node.

Once you've downloaded the relevant offline file(s), you may now think that you can just load an n-gram file into AntConc and specify a suitable regex pattern to extract all multi-preposition sequences. Unfortunately, though, as useful as AntConc is for most purposes, since it's a stream-based concordancer it'll ignore all line breaks and match more than we want to see in our concordance, so using it here is not an option.

Theoretically, you could also import the data into a spreadsheet program and filter it according to the last two rows, but, in practice, this failed for me in both Excel and Calc as the number of rows (1,048,720 for bi-grams) unfortunately exceeds the rather high limit of 1,048,576 rows the spreadsheet can handle for each individual worksheet in it. In our particular case, filtering may still work if you cut and paste the final row into another worksheet (or delete it altogether) and insert a header row at the top of the data and then use the 'AutoFilter' function in both programs, because, essentially, all the rows that didn't get imported into the worksheet will contain combinations that start with the letter z and English has no preposition starting with z. To automatically filter here, you'd have to specify patterns where the PoS columns 'begin with' i.

However, the above clearly isn't a very 'clean' way of doing this, and also relatively complex. The only really 'clean' way to solve this using filtering would be to import the data into a database, which is too advanced a topic here. A much simpler way to search the data, though, is to use a line-based concordancer, such as my own Simple Corpus Tool, where you can both look at the whole file easily once it's been loaded and also devise a suitable regex that will match two occurrences of the tag used for prepositions occurring in a row at the end of the line. Click on the hyperlink to open the file in the built-in editor. As it's a rather large file, it may unfortunately take a considerable time to load.

Exercise 75

Look at the way the file is structured, think of a suitable regex that would help you match two prepositions in a row, and close the editor.

Change the 'Lines after' context to 0 on the tool bar.

Type the expression into the box next to the label 'Term 1' and press the enter key or click on . Once the results have been loaded into the frame on the right-hand side, investigate them, always adjusting your regex, if necessary.

The previous exercise was mainly designed to show you that line-based concordancers can give you an alternative view of some types of corpus data, as well as obviously to help you get sensitised to the issue of multi-sequence prepositions a little further. However, to get some more experience in investigating some limited options for these in COCA, let's do another exercise there revolving around off as the first element in such a sequence. This will also allow us to see whether it may be involved in other redundant structures or belong to any of the other categories we explored before.

Exercise 76

Open the COCA interface again.

Run a search for off followed by any preposition.

Investigate the individual results in the KWIC view frame to see whether they may contain any redundancies, are part of phrasal/prepositional verbs, or form genuine multi-word prepositions.

To potentially distinguish between the three options if they should cooccur in the same KWIC window, you can also make use of the columns labelled A, B, or C for each concordance line. Clicking on each of these will highlight the particular row in a different colour and therefore allow you to visually distinguish between the samples at a glance.

In cases where the frequency of the colligates isn't too high, we can obviously also investigate colligations in a similar way as in BNCweb.

N-grams and Clusters in AntConc

We've already seen how relatively fixed, largely pre-defined sequences of words can be investigated using BNCweb, and to some extent COCA, but of course this only works if we want to explore interesting sequences that we're already aware of in more detail. In order to find new sequences of interest, such as lexical bundles or collocations that involve words occurring next to each other that we were unaware of before, we need to learn about some other features that are (currently) only available in

The simplest way of investigating collocation in unknown sequences is to use a computer program to produce a series of n-grams or word clusters, as described in Exercise 69. The 'n' here theoretically means 'any number of', but usually n-grams are restricted to relatively short sequences, such as bi-or tri-grams, because finding n-grams can be very expensive computationally as long lists have to be kept in memory, and their size grows with increasing length of a text/corpus, and each increment of n. Thus, if you're working on a relatively sizable corpus, be prepared to wait for a few minutes for n-gram calculations to finish.

We can create two basic forms of n-grams/clusters in AntConc, one where we produce lists of sequences of all words in the text indiscriminately, which takes the longest, and another where we create clusters around a given search term. Obviously, the lists for the latter are much shorter and much more concise because they 'centre around' the search term. Let's try out both variants.

Exercise 77

Download the following texts from the Project Gutenberg website and put them in a folder called 'lit_selection': The complete poems by Emily Dickinson, Frankenstein, Much Ado About Nothing, The Taming of the Shrew, The Tempest, Twelfth Night.

Clean up the files as we practised before.

Start AntConc and open the folder containing these samples. Also move the full copy of the Sherlock Holmes text here. Please note that the selection we've now created is deliberately mixed, and in no way represents any balanced sample! Make sure you set the 'Token (Word) Definition' in the 'Global Settings' to include hyphens and apostrophes.

Click on 'Clusters/N-Grams' or press to jump to the appropriate tab.

Select 'N-Grams' and note how the label next to this turns to 'N-Gram Size', too.

Set both the 'Min. Size' and 'Max. Size' under 'N-Gram Size' to 3 to produce only tri-grams.

Click and wait for AntConc to produce the list. Select 'Sort by Range' from the 'Sort by' options and click . This will sort the results based on how many of the documents in the corpus the n-gram occurs in, that is, the dispersion, in descending order.

Scroll through the list briefly and try to identify some interesting constructions. Also, pay attention to the overall number of n-gram types and tokens AntConc has identified.

So far, the composition of our small 'corpus' has been fairly heterogeneous, which has had a clear effect on our results in making it difficult to identify any interesting recurring 'themes'. However, if we change the composition of the corpus to make it more homogeneous, this ought to change very quickly.

Exercise 78

Select all non-Shakespeare texts in the 'Corpus Files' window and close them via the 'File' menu.

Create a new n-gram list and explore the results.

As you'll have seen from the first two exercises, n-gram analysis will often highlight typical grammatical or interactional structures, but possibly also some authoror period-specific language or recurrent semantic themes. The more limited the topic contained in the corpus, the easier it'll become to identify the latter. So, for instance, if you'd run this same n-gram analysis on our Trains data that contains task-oriented dialogues of a highly limited nature, you'd have been able to identify some of the key terms and expressions used there much more easily through n-grams than through the single-word lists we analysed above.

If you're interested in identifying the 'neighbours' of individual specific words, that is, potential collocations or colligations, it's far more useful to be able to specify the search term and investigate its behaviour with regard to other words that may surround it. Let's try this, using the 'all-time favourite' fair.

Exercise 79

Add the remaining literary files again. It's probably easiest to close all files first and then open the directory containing all the literary samples again.

Uncheck the box for 'N-Grams', and type in fair as your search term. Change the 'Cluster Size' option to 2 for the minimum and 3 for the maximum. Also ensure that the option to 'Treat all data as lowercase' in the relevant 'Tool Preferences' is unchecked and that the 'Sort by Freq' option is selected.

Click . Scroll through the results, at least some of which should immediately make some sense to you.

Pay particular attention to the length of the clusters and their frequency. Can you identify a correlation?

Experiment with the two additional options for 'Search Term Position', 'On

Having just covered the ways in which we can identify collocations 'manually', we can now turn towards exploring methods that involve statistics to try and 'predict' which words collocate more frequently and/or more strongly with one another.

Investigating Collocations Based on Statistical

Measures in AntConc, BNCweb and COCA

Calculating collocations

As you'll have seen from our two previous exercises, n-grams/clusters may sometimes already provide us with a fairly good measure of collocation. However, they most easily show us only words that co-occur in a relatively fixed and linear order, and frequently also include purely grammatical elements that may form a part of lexical bundles, but which may tell us rather little about the actual content of texts or the specific semantic content of the search terms we want to find collocations for. Another approach, which usually looks at a larger span of potentially discontinuous words, is that of calculating the degree of relatedness of words that occur near a particular node word. The following illustration shows how this relationship may be represented by referring to the negative (left-hand) or positive (right-hand) positions relative to the node. .1 only illustrates a span of 3 words to the left or right, but the calculations can be, and are commonly, also carried out taking larger spans into account. Due to the increasing number of calculations, though, this will usually also prolong calculation time. Calculating the degree of relatedness may be carried out using a variety of different statistical measures, most of which have advantages and disadvantages, the latter because they usually assume that words are randomly distributed, which is rarely the case in language (cf.

Here, we'll only discuss two of the individual measures that are implemented in both AntConc and BNCweb, MI and the t-score, and briefly evaluate their relative merits. For a very interesting and much more in-depth discussion of these two measures, see

MI is a measure that tries to compare the ratio of O to E directly using the formula log2 O/E, and, according to Hunston "is a measure of strength of collocation"

The t-score, on the other hand, mainly takes into account the frequency of cooccurrence of node and collocate. It's therefore less sensitive to corpus size, and hence probably more reliable for smaller corpora. According to

BNCweb adds further options for calculating collocations. For a list of these, see the illustration of the dropdown menu in Figure

Computing collocations in AntConc

Let's now test MI and the t-score in AntConc, using the complete set of literary data again, to see whether we may be able to observe some of the features discussed above.

Exercise 80

Open AntConc and select the folder containing the literary data, but restricting your selection to the Shakespeare plays again to make the selection more homogeneous.

Select the 'Collocates' tab, type in fair as your search term and set the 'Min. Collocate Frequency' option to 2, in order to avoid one-off constructions, also known as singletons or hapax legomena.

Set the 'Window Span' options to 4L and 4R respectively. Make sure that you have the collocation measure in the 'Collocates Preferences' set to 'MI' initially and that the 'Sort by Stat' option is selected.

Start the analysis. If you haven't previously created a word list, AntConc will inform you that it needs to do so and automatically invoke the corresponding tool.

Since we want to compare the results obtained through both measures, we first need to store the results of the MI analysis, then produce the list for using the 'T-Score', and save that, too. The best way to do this is to save both sets of result to text files as we did earlier for our concordancing results. However, just to get a quick impression, you can also use the quicker, easier and less permanent option produced by the button, which will create the extra window containing the MI results that you could then keep open and then re-run the analysis using the 'T-Score' statistic to compare the results side by side.

Open the text files in your editor Next, either simply separate the lines that contain scores above the cutoff points by spaces or some other marking from the rest of the results, or even delete all results below the cut-off points.

Finally, compare the results, and try to understand why the items appearing in the list may be considered collocates of fair, and, based on the information provided above, how they may be influenced by the statistical measures we used.

As you'll hopefully have noticed, apart from simply giving us the option to look at/sort by the statistic or frequency of the collocates, AntConc also allows us to see or sort by whether the results occur on left-or right-hand side of the node, and with which frequency. I'll leave it up to you to explore the usefulness of this feature.

Computing collocations in BNCweb

As we've seen earlier, BNCweb offers a much larger choice of collocation measures than AntConc, with its default set to log-likelihood. Unfortunately, though, there's no facility for creating n-gram lists, probably because these could potentially get very large, working with such a big corpus. All collocational analyses have to be conducted by running a query on the node word first. Once you have the results of the query, the dropdown list on that page will then allow you to select 'Collocations…' to produce a list of collocates. Let's try this using the same word we used above.

Exercise 81

Run a query on fair.

Select 'Collocations…' from the dropdown options list and click on . Investigate the settings on the next page briefly, see whether you might want to change any of them, and then click . Look through the list of collocates returned and identify the ones you're already familiar with and the ones we haven't encountered before. To get a concordance for the collocate, this time you need to follow the hyperlinked frequency in the column for 'Observed collocate frequency', while the link in the 'Word' column provides a breakdown of the statistics for the word form, including scores for the other statistical measures available and distribution of collocates within the given span.

Experiment a little with the options for changing the parameters to see whether you can understand what kind of an effect they have, and how the results will change/look different based on this.

When you try the option(s) for collocations on 'POS-tags', especially take a look at the results for punctuation to see whether these make sense to you.

If you want to compare results produced on a corpus analysis in AntConc with those drawn from a subcorpus of BNCweb, you should definitely select either the 'Mutual Information' or 'T-score' options there, depending on the size of your samples. However,

Computing collocations in COCA

Calculating collocations in COCA works in a relatively similar way to that of BNCweb, so I'll just introduce this briefly here without any additional exercise. Perhaps the main positive difference is that you can control some of the options directly from the display frame of the BYU interface, rather than having to run a query on the search term first. Thus, in order to investigate the collocates of fair, you simply type fair in the box next to 'WORD(S)', and then click on 'COL-LOCATES'. Doing so will add another textbox, as well as two dropdown lists to the interface, which will allow you to select the left and right span size. If you enter a specific word in the search box, the interface will allow you to calculate an MI score for the node and that particular collocate, while selecting from the 'POS LIST' options will allow you to look for colligations directly. If you don't put anything in the textbox, a * will be added automatically when you run the query, and you essentially get the same effect as in BNCweb, where the query will simply find all potential collocates.

As with other queries in the interface, you can also restrict your collocations search to particular genre categories from here directly, and, of course, once you've calculated the results for the COCA, you'll immediately be able to compare them to other corpora again.

Perhaps the only major limitation here, though, is that the BYU interface only provides a single collocation score measure, which is MI. assign to the greeting formula ought to be a full stop or an exclamation mark. Clearly, it's formally an expression of well-wishing that is similar to a polite exclamation. On the other hand, we normally also associate exclamations with imperative structures, that is, commands, for which it may be more appropriate to use the exclamation mark to indicate their force, so in this case, I've opted for the full stop.

Finding a suitable punctuation mark may be easier for unit 2, which simply consists of a single noun phrase, and clearly is a kind of statement that just provides the name of a company. This, of course, grammatically doesn't correspond to a well-formed statement, as it's elliptical and doesn't even contain a verb. Its 'wellformed version' would probably be This is Virgin train line, with a subject, finite verb, and subject complement.

In contrast, unit 3 does at least contain a verb, even if it's non-finite, and we'd probably expect a completely well-formed version to look similar to unit 2, only that the verb in this case consists of a split construction of auxiliary + non-finite ing-form, that is, This is Sandra speaking.

Unit 4 is obviously a question, so you'll probably have chosen a question mark like me. Normative grammarians may frown upon the preposition preceding the wh-word, though.

My having split off the filled pause/hesitation marker er as unit 5 may come as a surprise to you, but as it occurs at the beginning of a turn and has the function of indicating hesitation on the part of the speaker, this is justifiable, although, admittedly, not everyone involved in transcription may agree with this practice, and some people might prefer to integrate it with what I've numbered unit 6, which is clearly a directive, as indicated by please, that is, a mild form of command that justifies the use of an exclamation mark.

In turn 3 by speaker A, unit 7 is a discourse marker that indicates that a new stage in the dialogue is beginning. As such, it's prefacing the yes/no question in unit 8, and clearly has an independent function that justifies treating it as a separate unit.

Unit 9 is a short acknowledgement, the equivalent of a yes-answer, so clearly a special form of statement, which warrants the use of the full stop here. The same goes for the negative response no in unit 11.

As you'll have seen from the above discussion, throughout the extract we encounter a number of units that'll look familiar to us from traditional grammar, but also a considerable quantity of elliptical structures that range from single words as signals of acknowledgement, agreement, negation, or hesitation, to more phrase-like structures that may even contain incomplete words, such as in unit 16, where speaker A begins to ask for a confirmation of what she assumes to be the month of travelling. However, before she can actually complete her question, speaker B completes this name for her collaboratively. At other times, such as in unit 24, a speaker may simply repeat part of the information provided by the previous speaker, possibly in order to confirm that they've heard and absorbed the information correctly.

Exercise 69

As you should be able to see from experimenting on the web page, when generating n-gram lists, each time a window of n words is selected from the text, then the position shifted to the right by one, extracting the next n words, and so forth. This seems to be a relatively straightforward process, involving no particular issues. However, if you look closely at the example sentence, you'll realise that it actually contains a number of clauses, with the main (finite) one being "let's take a look at an example of how n-gram lists are generated". Now, based on the experience developed in Exercise 68, you'd probably want to assume that this, as well as the other two, dependent clauses, are also independent c-units, so that, technically, we're crossing over a c-unit boundary as soon as we create the bi-grams up let and generated based. And when crossing over a boundary for a new unit of sense, we'd probably want to avoid assuming that there's a close association between the final word of the first unit and the first one in the second unit, although, of course, there may be some cohesion between some units, for example, when the first unit ends in an object NP and the second unit picks this up as an subject in the form of a pronoun. At any rate, even in the latter case, we'd have an association between the NP and the pronoun, but not necessarily the final word in the NP and the beginning of the next unit. Unfortunately, at the moment, most corpora generally don't indicate c-unit boundaries, so that, at best, we'll generally be able to analyse texts according to the 'sentences' marked up for them. An exception in this respect is the SPAADIA corpus (see

Exercise 70

Selecting the restrictions for this exercise should be quite straightforward again, so I won't say anything further about this here.

The most basic form of this query would be in a * voice, where the * stands for any word or character. However, this wouldn't catch any intervening words that start with a vowel letter, so we'd have to re-write this slightly to make it in a[n,] * voice to include the optional n.

As the whole phrase is a prepositional phrase, in its basic form the intervening parts of speech could minimally be an adjective or an adjective pre-modified by an adverb, so we could extend the query by either saying in a[n,] _{A} voice or in a[n,] (_{ADV})? _{A} voice, where the element for the adverb slot has been made optional by including it in round brackets and using the ? quantifier. This should result in 899 hits which, if you consult the frequency breakdown, will tell you that in a low voice is by far the most frequently used expression, accounting for 157 hits (17.46%), while the frequency for the next two expressions, both with 36 hits (4%), in a small voice and in a loud voice, is already considerably lower. We can therefore conclude from this that low collocates with voice most strongly in descriptions of voice quality.

Extending the query to again include optional adverb results in in a[n,] (_{ADV})? _{A} and (_{ADV})? _{A} voice. This query, involving a phraseological pattern with more complex and longer pre-modification, now only yields 16 hits, none of which are repeated. This goes to show that, the more specific we want to be in characterising different features of NPs, the less we can rely on 'ready-made' chunks, such as the ones collocations offer us.

Exercise 71

This exercise draws on much of the knowledge you gained in previous sections on BNCweb, as well as regex quantification, but extends the scope of our queries to considerably more variable and longer phraseological units. Re-writing the last example from the previous exercise should be fairly easy, only that now the optional element contains not just a single word, but an AdjP that is (still) optionally pre-modified by an adverb, and which should be written like this: in a[n,] (_{ADV})? _{A} (and (_{ADV})? _{A})? voice.

The extended query string including with along with in should be (in|with) a[n,] (_{ADV})? _{A} (and (_{ADV})? _{A})? voice. When you do a frequency breakdown of this, you should be able to notice that there are far fewer examples starting in with, and that most of those tend to describe fixed vocal characteristics of people, rather than specific ways in which people are reported to be saying things.

In investigating expressions related to durations, using the query string (in|for) (a|the) (long|short) (time|period), close observation should show you that, while both constructions have relatively similar meaning, those with for are used much more frequently. This may indicate a slightly stronger preference for such expressions to collocate with for, even though this would of course require more in-depth investigation, including potentially further expansion of the nouns expressing durations, such as for example duration itself, which I leave up to you for additional practice.

The final part of the exercise focusses more on identifying general differences between the three prepositions at, in, and on regarding their usage. The string (at|in|on) ( * ){1,3} _{N} finds a very large number of mainly prepositional phrases based on the three prepositions. To be able to do a frequency breakdown here, you'll need to reduce the number of examples, which is referred to as 'thinning' in BNCweb, as the interface won't allow you to work with the whole set of data, due to its size, and the computation time and processing power involved. You can reduce the number of results by selecting the 'Thin…' option from the dropdown menu after running a query. In most cases, you'll probably want to keep the default setting set to 'random (selection is reproducible)' and then go for a percentage of 5%-10% of the data, at least for the above exercises. Thinning down the results to a manageable amount (randomly) and extracting suitable items from them can then yield useful samples that may for example be used to investigate differences in their usage for creating improved teaching materials, based on real-life data, or directly in the classroom for awareness-raising activities.

Exercise 72

For this exercise, you may again need to thin the query; 10% should do. As the thinning is random, of course the results you obtain may look somewhat different from the ones I'll describe, as well as from those of your classmates, if you're doing this exercise in a classroom.

Your attempts at creating a wildcard query will hopefully have lead you to define a contraction as anything that contains an apostrophe followed by a one or more characters, i.e. * '+, or possibly also * ' * . A more general wildcard pattern that may only be looking for '+ (or ' * ) will fail here because it can only identify word tokens that actually consist of the apostrophe followed by any number of characters, but will not include anything preceding the apostrophe, due to restrictions of the wildcard syntax. It will thus, for instance, find forms like 's (representing multiple PoS) but fail to identify any instances of the negation particle (tagged XX0), as doing a 'Frequency breakdown of word and tag combinations' will easily confirm. Likewise, if you postulate that something has to occur in front of the apostrophe by using +'+, you won't find any of the tokens where the clitic is treated as being independent.

Once you've run the appropriate query, and switched to the word + tag breakdown, you should at least be able to identify the following options for PoS categories involved in genuine contractions as the right-hand element, discarding other abbreviated forms or representations of 'non-standard' speech: r be ('s_VBZ, 'm_VBB, 're_VBB), have

As going through the rest of this particular list is a bit tedious and timeconsuming once you reach colligations with he, you'll probably be tempted to assume that, from now on, you'll most likely only find pronouns, anyway, so you might be tempted to stop here, even though you shouldn't really. However, because we can assume that pronouns will make up a major part of the colligations, we can use an alternative way of investigating those colligations other than pronouns. To do so, we can use the 'Tag restriction:' feature on the sort page, select 'any pronoun' from the dropdown list, and then check the box next to 'exclude'. This would then at least have shortened the list by removing everything tagged as a pronoun, although you'd still have to click your way through lots of occurrences of noun colligations and other noun-like tags, such as those for cardinal numbers, which may act like nouns syntactically. The ideal situation would of course be that we could exclude tags based on regexes, but unfortunately BNCweb currently doesn't offer this functionality.

If we start by excluding pronouns, the first word class we find when we start ignoring everything 'nouny' is adverbs (although some of them may have been mis-tagged). Having realised that some adverbs may in fact be colligates of this clitic, we can now go the other way and, instead of excluding a particular word class, restrict our sorting to display only adverbs (using 'any adverb') to the left of the clitic. One of the possible adverbs you'll probably find here is else (tagged AV0), although this may be a doubtful classification, as it generally appears to have a nouny function again before clitics because it normally occurs with indefinite pronouns like someone, and the two elements together constitute an NP. Furthermore, you'll find deictic adverbs like here, there, today, tomorrow, and tonight (AV0), the cohesive conjunction so, also labelled as a general adverb (!), and the question words how, when, where, why (tagged AVQ). Regarding the latter, you may now, quite rightly, expect to find at least the other two question words what and who in the same concordance list, as they can certainly be followed by the same clitic, but, due to the tagging rules of CLAWS, these are in fact classified in different ways from the other question words. As before, I'll leave it up to you to find out how and also reflect on why this was done, using the query string (what|who) 's… Going through the rest of the items of clitics that are parts of genuine contraction should now be relatively straightforward, as I've already pointed out the various issues you may encounter, and ways of efficiently identifying particular occurrences of lexical items for a given PoS category. The only interesting thing to perhaps point out to you here regarding contractions involving n't is the high frequency of occurrences of ain't you'll encounter, which clearly reflects the nonstandard language occurring in the spoken sections of the BNC. What's also interesting here, but from a tagging point-of-view, rather than a sociolinguistic one, is that all left-hand elements of this contraction have been tagged as 'unclear' (UNC), probably due to the fact that the CLAWS grammar couldn't disambiguate between the different forms it may represent in standard language, such as isn

Exercise 73

The first part of the exercise shouldn't prove particularly difficult, at least not as far as retrieving the units is concerned. However, if you switch the display from random to corpus order, you'll notice that, apparently, not all u-units are in fact retrieved because the KWIC display actually starts with the second unit. Therefore, the number of hits (699,885) reported in the info bar at the top is most likely unreliable. However, for the purpose of this part of the exercise, this isn't really relevant because you can still get a rough impression of the length of the individual turns, especially if you switch from

Bearing in mind that some occurrences of well may represent both the beginning and end of a term at the same time, that is, in cases where well constitutes the only c-unit in a turn, you should be able to observe that the word form can have two rather distinct meanings. In initial position, the meaning and function is that of a discourse marker (DM), and either tends to indicate the beginning of a new sequence in the spoken interaction or the beginning of a response on the part of one speaker, where that particular speaker wants to preface this response by indicating that they don't agree fully with what has been said before. In final position, in contrast, well represents the genuine adverb counterpart of good or possibly also part of the MWU as well. Unfortunately, the CLAWS tagging simply 'lumps' all these meanings together, using a single general adverb tag for all of them, which again proves the point that taggers like CLAWS are really optimised for written language, but often still have a number of problems when it comes to dealing with spoken language appropriately.

Exercise 74

The basic query that allows us to look for the relevant patterns is _{V} >>3>> _{PREP}. As this'll produce a rather large amount of hits, though, you'll need to thin it down in order to be able to perform any sorting operations. To get a wide range of examples, I'd suggest that, this time, you thin to the maximum allowed number of hits, 250,000.

Starting our investigation with particles occurring one position to the right of the node verb form, we can choose the position '1 Right' and set the restriction to 'any preposition'. This will immediately return some fixed collocations with verbs such as think, talk, know, say, or ask in combination with about. Please note that, essentially, all these verbs are semantically relatively empty, that is, would largely be considered de-lexicalised verbs, and often only acquire a specific meaning in conjunction with such a preposition. Of course, you may also encounter atypical combinations, such as consider about in "PRACTICAL POINTS TO CONSIDER ABOUT_PRP-AVP A HOME BURIAL" (ACM 711), as is in the nature of corpus data. As you'll see from the typical examples here, the verbs involved in this construction are predominantly verbs of saying, perception, and mental reflection. This would probably be even easier to see if BNCweb allowed us to do a secondary sort, but unfortunately, it doesn't do so. In rarer cases, we may also find even more idiomatic collocations that do involve verbs of action, as in the example "Anne said that she would like to come up with him, potter about_PRP Dundee while he's having his medical [pause] then they will come" (KP8 2581), where of course about has the meaning of 'around', rather than specifying an object or expressing an approximate value via a prepositional phrase. The same also goes for examples involving look about, etc. As it might get a little tedious again looking at endless examples of about, you can also skip ahead a few pages by typing in a page number next to the button in the navigation bar above, and clicking the button. From about page 24 onwards, I managed to find examples involving above this way, but remember, as we've thinned the examples randomly, your frequency distribution may be somewhat different. For above, one thing that again becomes immediately clear is that certain verbs of 'forced' or independent 'movement', such as rise, raise, increase, clearly collocate with this in establishing a sense of directionality, while others, such as whirl, billow, tower, or perch signal descriptions of positions. I'll leave the remainder of this sub-part up to you to complete and learn from, or use as a basis for creating suitable materials for teaching.

While investigating position '2 Right' with the same restriction, you'll have to ignore many examples where the node verb form may in fact be an auxiliary, followed by a finite verb and then the preposition, as these examples are really only similar to what we just investigated. At other times, they may illustrate collocations that are similar again, but allow for intervening objects (e.g. in "that man in London you told us about_PRP"; HA7 3272), adverbs (e.g. in "It also follows that one cannot talk specifically about_PRP 'marked theme' in FSP theory"; FRL 1446) or adjective complements (e.g. in "But I have nothing to feel guilty about_PRP-AVPI"; JXS 1530) to occur in between the verb form and the preposition. Things do get even more interesting in terms of the collocations when we start encountering verb + multi-preposition constructions, as in, for example, "That is what Robert and my agent are on about_PRP-AVP" (ADA 596), or "What're those dogs goin' on about_PRP?" (J13 1864), "What are you so fed up about_PRP?" (C8E 655). As before, you'll probably have to skip ahead quite a few pages until you find examples of other prepositions, or try to use the 'Starting with letter:' option to isolate a few more interesting examples other than those where the preposition starts with a. Alternatively, you could of course also thin the original query down to a manageable size and hope that the random selection won't give you too many of the most frequently occurring prepositions.

Investigating the '3 Right' option will be even more time-consuming, due to the relatively longer span, which leaves even more room for options of syntactic constructions occurring in between. Nevertheless, in some cases, it may well be necessary and rewarding to carry out in-depth investigations of this kind as it's well known that particles may occur that far apart from the verbs they go with. However, we'll soon investigate other ways that may make it easier to check these co-occurrences. And, of course, you could always also use smaller corpora (or parts of the BNC itself) and investigate these in a concordancer like AntConc, where the sorting options make things easier for you.

Exercise 75

Downloading and unzipping the frequency files and the program should hopefully have presented no problem, as should loading the file in the built-in editor. As you may have noticed, this editor is really not optimised for handling large files, so to just view the frequency list without the ability to concordance on it, a dedicated editor, such as Notepad++, would be far better. On the other hand, using such an editor would only allow you to search through, but not concordance, on the file.

The regex that should allow you to extract only occurrences of two prepositions should be something like \bi.{1,3}\b\si.{1,3}$. If we were sure that we could exclude ditto tags, we could also have used \bi[ˆ\d]{1,3}\b\si.{1,3}$, but in our case, this would actually have excluded off of, which happens to be marked with a ditto tag.

I'll leave it up to you to find some interesting combinations, but remember, whenever you identify something here that may be interesting, you need to go back into the BYU interface and investigate it there. And if you find that it's too difficult to spot potential candidates for investigation, due to the limited context, you can of course also download the files for larger n-gram combinations (up to 5) and adjust your regex accordingly.

Exercise 76

Although this exercise will quickly present you with a wide range of choices representing combinations of off followed by another preposition, provided that you used the right query, off [i * ], very few of these will actually contain any examples of the kind of redundancy described in the exercise above. Probably the only real examples are off of, as well as off off, where perhaps some of the examples are used jokingly. However, not having found many other examples here still doesn't invalidate the observation that this feature does occur in American English, as we've shown the redundancy to exist. And, of course, it may not be limited to instances involving off. On top of that, the exercise has helped us to distinguish more between the different types of multi-item prepositions, which has hopefully helped you understand better how they work.

One thing that still remains to be done, though, is to verify the question whether this may be a typical feature of American English only. Obviously, as one of the strengths of the BYU interface is that it allows us to compare different corpora, this is something that can easily be tested by running a comparison with the BNC, where it turns out that the combination off of is nearly 4.5 times as frequent in the COCA, while the frequencies for off off are too small for a genuine comparison.

Exercise 77

As you'll probably notice while doing this exercise, n-gram lists, although they can produce very interesting results, may take a while to interpret properly. Furthermore, instead of revealing interesting combinations of content words, you'll often find more grammatical constructions or combinations of function + content words, especially if the corpus is not very homogeneous, as in our case. With this particular selection, what you'll probably be able to identify at best is some recurring grammatical constructions, such as the beginnings of statements or questions, as well as some occurrences of 'archaic'/'literary' usages. For instance, there's a relative overabundance of constructions with shall, or occurrences of I will not instead of I won't, where the latter would be much more common in modern drama/literature. Other things you can identify are I pray you/thee vs. I prithee, instances of reported speech (said X, etc.), as well as a few proper names, such as (Mr.) Sherlock Holmes, etc. The overall number of types (194,570, at least based on my token definition) is also fairly high, reflecting the variability of expressions.

Exercise 78

The results of the new trigram list should now have become much easier to interpret, and you should be able to identify many more archaic usages in terms of expressions, such as by your leave or didst thou not, absence of contractions, etc., much more easily than before. Other things that will crop up frequently are bits of information related to the plays that form part of this 'corpus', such as references to the author, to acts and scenes within the plays.

Exercise 79

This exercise, as deceptively simple as it may look, can actually demonstrate a number of important features of how to identify and analyse collocations to you. First of all, keeping the 'Search Term Position' set to 'On Left' will help to mainly identify noun or NP collocates, while changing this to 'On Right' reveals other types of combinatorial options, such as combinations of determiners, possessive pronouns, intensifying adverbs, etc. with the node.

In terms of the length of clusters, it should immediately become obvious that, as soon as we move from bi-grams to tri-grams, the frequency drops, as does the range, in fact. This is not surprising because the longer a cluster gets, the more specialised its composition, along with its meaning, also gets.

In terms of the composition of the corpus and its relation to the individual clusters, you'll hopefully notice very quickly that, with collocates occurring on the right, our results contain a relatively high number of proper names. This is due to the imbalance in the data, where the three Shakespeare texts still clearly illustrate the Elizabethan concept of 'fair' predominantly related to the appearance and beauty ideal of women, where light-coloured skin was not only one of the most valued properties, but also associated with good character, honesty, and virtue. This feature still remains 'alive' in modern English, but in a highly limited way, where fair, related to appearance, still collocates with nouns like hair and skin. You can see the difference between the texts from the Elizabethan period and the somewhat more modern other texts more clearly if you remove all the Shakespeare texts from the selection. Then, even though the ideas of beauty and virtue will still occur in a number of the examples, the modern meaning of 'fair' as 'just and equal', 'unbiased', or 'considerable' will become more and more apparent, even despite the fact that much of the language in the remaining texts is still more old-fashioned and/or poetic.

Exercise 80

In doing this exercise, it's perhaps even more important to follow all the steps carefully than before because relatively minor changes may well influence the results. As we saw earlier, the original literary selection was (deliberately) very heterogeneous, which did allow us to identify features related to language change nicely. However, this very heterogeneity could skew the results of our general collocational statistics rather strongly, especially in such a small corpus. Thus, it's important that we try and compare 'like with like' in restricting the selection to a single author/period now to get some more specific information about the collocations that are more characteristic of the actual texts.

Also, if you forget to set the minimum collocate frequency to 2, you may get a number of results related to singletons, which we clearly want to avoid, since generalising from single examples has no genuine predictive or explanatory value. Yet another potential source of 'error', or rather confusion, could be that you might somehow inadvertently have set the sorting option to 'Sort by Freq'. This is, of course, one valid way of looking at the data, but unfortunately doesn't illustrate the differences in the use of statistics very clearly. This is because the words then wouldn't appear ranked according to their relative importance according to the statistic used, but we'd have to look very carefully at the values in the 'Stat' column to compare the difference and judge whether it may be significant.

If we trust the cut-off points given in the literature, 3 for MI and 2 for the t-score, we should in fact prune the lists before comparison, which leaves us with a list of around 60 collocate candidates for the former, but only 15 for the latter, although of course the same number of collocates originally gets identified by AntConc since they all occur within our span. Perhaps the very first thing to notice here is that, as we saw in the quote from Hunston earlier, the t-score does indeed seem to provide information about the "grammatical behaviour" because most of the words in the pruned list are definitely function words. In contrast, the majority of words MI identifies are content words, such as proper names, common nouns, adjectives and verbs. Here, especially the high number of proper nouns is, as we saw earlier, highly characteristic of the Elizabethan English used in plays such as Shakespeare's. And, if we try to interpret the MI list further, most of the words that occur at the top turn out to be words that either refer to or characterise people or general nouns -in other words, word classes that form parts of noun phrases. Having identified the potential significance of the MI results for our data, we now need to think about how the t-score results may relate to these. Starting from the very top, with and, we can observe that some of the instances are simply due to issues of cohesion, that is, they are co-ordinating conjunctions that represent the beginnings of new clauses, in which case it would be nice if we had some way of preventing collocations from being identified across syntactic unit boundaries to improve the precision of our analyses (see Exercise 81) because those aren't really collocates at all. In a number of instances, though, and co-ordinates adjectives in predicative structures that, again, help to characterise people, for example in fair and virtuous, which actually occurs twice in the data, each time characterising a different woman, Katherina or Bianca, or fair and fresh and sweet, which once more represents an epithet of the former. The definite and indefinite determiners, as well as to some extent the possessive pronouns my and thy, again sometimes form parts of characterising noun phrases, where fair is used attributively in these cases. Therefore, essentially, all the cases of collocations identified through the tscore stat that I've just discussed, strictly speaking, represent cases of colligation.

Exercise 81

When you look at the 'Collocation Settings' page, you'll notice that the option to 'Calculate over sentence boundaries' is, very sensibly, set to 'No' by default. In BNCweb, even being able to do this is only possible because the whole corpus is marked up for s-units. As pointed out before, though, in very rare cases where we want to investigate cohesion, looking across boundaries may be justifiable. Unfortunately, when working with most corpora, and in most concordance programs so far, the option for handling data involving a measure of the syntactic units they occur in is still absent, something we just saw in the Exercise 80. In addition, as

Starting from the top collocates, you should be able to see that enough, trading, share, amount, fair, play, say, trial, etc. all relate to the meaning of 'just and equal' we identified before as being perhaps the most common contemporary meaning associated with the word, whereas the one associated with isle still expresses the idea of beauty left over from Elizabethan times. The collocate hair is also already known to us, and related to the older meaning, but antiques, Hannover, trade, Frankfurt, and CeBIT (some already corrected in capitalisation here) all refer to a more modern institution, that of the 'trade fair', while Vanity of course is part of the title of the book Vanity Fair by Thackeray, where the meaning of fair represents a slightly older variant of the modern 'trade fair' and probably closer to the modern meaning of 'fun fair'.

Experimenting with the options should allow you to find that BNCweb also lets you list the PoS categories that collocate with the node by selecting 'collocations on POS-tags' next to 'Information'. In other words, what we're really looking into here is a form of colligation, which, however, is probably a little too finegrained for most purposes. Changing the minimum frequency options for either the co-occurrence of node and collocate, or the collocate only, theoretically makes it possible to thin down the results further, but in practice probably only works for relatively high values, and may thus impose artificial restrictions. The 'Filter results by' options allow you to either quickly select a word form from the list of collocates produced, or even calculate a statistic for a form that hasn't been identified during the analysis. In addition, you can also restrict the output to a given PoS tag category to either disambiguate a grammatically polysemous word form provided on the left, or to filter the list of collocates by PoS.

When changing the options for the statistical measure, you'll probably have observed some similarities and differences across the measures. While LL and most of the other statistics tend to emphasise the content word collocations in our example, two of the measures, 'T-Score' and 'MI3' sometimes rank function words a little higher, while a raw frequency ranking places predominantly function words at the top of the list, as is to be expected.

The final part of this exercise is designed to raise your awareness of one of the issues in the handling of types and tokens in the BNCweb interface, as well as the underlying CQP architecture. Here, sadly, the designers of the architecture have introduced a serious flaw in the system that may well affect the overall calculations of the collocation statistics very strongly, which is to treat punctuation tokens (and their types) as equivalent to words. In doing so, they've effectively introduced two sources of error that affect different parts of the interface, and hence the quality of the results. At the level of collocations, the very fact that punctuation occurs with a relatively high frequency in any orthographically transcribed corpus like the BNC almost guarantees that it'll be treated as collocating with genuine word types, something that simply doesn't make sense because the semantics and pragmatics of punctuation are very different from, and completely incomparable to, those of ordinary words. At the same time, the high frequency of punctuation tokens will affect the calculations of relative and normed reported frequencies throughout the whole corpus, which will again have an effect on the calculations for collocations, too. Unfortunately, even pointing out this issue to the designers/developers of CQP so far hasn't led to any changes in this respect

Sources and Further Reading

Understanding Markup and Annotation

We've already looked at more basic forms of enriching our data in earlier sections, as well as at PoS tagging as a fairly important and advanced one, but now, in this chapter, want to turn towards developing both an understanding of, as well as get some practice in, using relatively sophisticated ways of making linguistic data more useful than it would be in its raw form. As we've seen before, this makes a lot of sense because it not only allows us to distinguish features on different linguistic levels more easily, actually making them countable, but also to possibly exclude some parts of the data from our specific analyses, for instance by ensuring that we don't perform n-gram/collocation analyses across syntactic boundaries. Before we can take any detailed look at the technical aspects of such an advanced enrichment process and the best type(s) of format for this, though, we still need to deal with a few terminological distinctions, and try to understand the historical developments and motivation underlying the different formats.

As in the heading of this chapter, we frequently encounter two different terms when talking about the process of storing and enriching linguistic data, markup and annotation. While the term markup is sometimes used to indicate the physical act of marking specific parts of a text using specific symbols, and, in contrast, annotation may often refer to the interpretative information added, the two may also be used synonymously. This is also roughly the way in which we'll use them here. As such, they can refer to either a particular format for enriching a text, so as to r categorise parts thereof, or r make these parts more salient, or to the actual process of applying this form of annotation.

We'll see some examples and different ways for applying markup to text later on, but for now, we're more interested in the reasons that exist for applying markup to our data in the first place, and roughly which different types of linguistic annotation currently exist. Despite a somewhat strong over-emphasis on morpho-syntactic annotation,

As you can see in the above example, the syntactic annotation actually starts out from PoS information and then adds labelled bracketing for the syntactic categories. For reasons of space, we won't look at further examples of other types of annotation here now, but will later work more extensively on a sample that combines some of the different levels. Before doing so, though, we'll first try to develop an understanding of how the different formats that are mainly used today have come to exist and developed into their most up-to-date forms.

From SGML to XML -A Brief Timeline

In the 1960s, first attempts at standardising markup for information exchange began to be made. This was deemed necessary in order to be able to exchange various types of documents efficiently and without the need for extensive reformatting of the partly idiosyncratic/proprietary coding used by different authors, manufacturers, or publishing houses. However, it wasn't until 1986 that the SGML (Standard Generalized Markup Language) standard was in fact ratified by the International Standards Organisation (ISO). This first official type of markup language, however, as we'll see further below, was relatively complex and also had a number of serious drawbacks.

After Tim Berners-Lee had invented the World Wide Web (WWW) in 1989, it was necessary to develop a new, and simpler, markup language in order to take full advantage of the new hypertext medium. Thus, in 1992, HTML (Hypertext Markup Language) arrived on the scene and became popular very quickly. It's since then gone through various stages of development, the most recent versions being HTML 5 and XHTML. HTML in its more modern variants represents a simplified version of SGML, where many of the unwieldy features and shortcuts that made the processing of SGML rather difficult (see further below) were eliminated. In recent years, it's also further been enhanced by the addition of Cascading Style Sheet technologies (CSS1, 2 & 3) that make it possible to separate form and content better.

Because the formatting and linking capabilities offered by HTML were not always sufficient for all needs in document handling, and SGML proved too unwieldy and error-prone, a new hypertext format, XML (eXtensible Markup Language) Version 1.0, was eventually created and released by the W3C (World Wide Web Consortium) in 1998. As can be seen from its name, XML allows users to create their own extended markup frameworks, and thereby makes it possible to increase the separation of form from content even further than was already possible by using HTML and CSS. Furthermore, XML can not only be used in conjunction with CSS, but also has its own stylesheet language XSL, which far exceeds the capabilities of CSS in that it also provides mechanisms for transforming documents from XML into other forms of XML, as well as various other types of formats, for display and processing. It also offers further improved processing and hyperlinking facilities in the form of XPath and XPointer technologies. Due to these advantages, we'll explore the use of XML further in Section 11.2.

XML for Linguistics

Why bother?

So why should we actually be tempted to 'mess around' with our nice and clean data and possibly go through a lot of trouble in adding markup? Well, for one thing, markup helps to structure information, for example, in separating documents into appropriate sections/divisions with headings, sub-headings, paragraphs, etc. We can also include certain types of meta-information that we talked about before, such as, for example, information about when and where the material has been collected, who has edited it and in which way, etc. The best examples we've seen for this were the meta-textual choices BNCweb allowed us to make for selecting specific parts of the BNC for different analysis purposes, which were all made possible by the fact that the BNC (in its most recent version) is marked up in XML, albeit with somewhat over-elaborate header information, as we'll soon discuss. On the other hand, adding annotations also allows the annotator to highlight interesting or important phenomena by using colour coding or other visualisation techniques, or simply helps to render a historical document in a relatively faithful and searchable manner online.

What does markup/annotation look like?

As pointed out in Section 11.1, all the formats we'll be discussing here are essentially plain text-based, and thus constitute 'human-readable' formats where the text itself contains different types of additional information, sometimes related to its structure, and sometimes to its linguistic content. Some of these formats are rather constrained in the types of information that can be added to a document, while others are more flexible, but sometimes even the less flexible ones can be 'coerced' into allowing us to add suitable types of annotation.

Although there are actually many different ways of marking up a document, one fairly standard method is the use of the kind of tags that we use for writing HTML documents. These are actually more appropriately referred to as elements. Elements, in their most basic form, are generally represented in markup languages by pairs of opening and closing angle brackets, i.e. < & >, with the name of the element appearing in between the two. There are three different forms of such element tags:

1 opening tags (<element_name>), e.g. <p> for the start of a paragraph; 2 closing tags, where the name of the element is preceded by a slash (</element_name>), e.g. </p> for the end of a paragraph; 3 and 'empty' tags, where the closing bracket is preceded by (a space and) a slash (<element_name />), e.g. <pause /> to represent a pause of undefined duration.

The first two are used to 'bracket', that is, enclose, elements that contain some form of textual content, such as the sentences inside a paragraph, the words inside a sentence, etc. The third type usually either specifies formatting instructions, such as line breaks, etc., contains links to external resources, such as a style sheet that specifies the layout and formatting options, or can be used to include other information that doesn't require a containing element, such as, for example, a comment on a specific piece of data. We'll see more examples of these later.

Opening and empty tags can also contain attributes, which are typically specified as attribute-value pairs, joined by an equals sign (=). Usually, these days, the value also needs to be quoted, using either single or double quotes. Attributes often specify sub-types, identifiers (commonly labelled id), counters (typically labelled n), or other features associated with a certain element. Therefore, a paragraph with the number 5 may be represented as <p n="5">…</p>, where the ellipsis (…) stands for the text contained inside it, or as <para n="5">…</para> or even <paragraph n="5">…</paragraph>, if you want to be even more explicit about it being a paragraph. The following is a brief sample paragraph that illustrates what a paragraph annotated in XML might look like in terms of elements and attributes.

<paragraph n="01"><unit n="01"><word n="01" pos="DD1">This </word> <word n="02" pos="VBZ">is</word> <word n="03" pos="AT1">a</word> <word n="04" pos="NN1">sample</word> <word n="05" pos="NN1">paragraph</word> <word n="06" pos="VVG" >illustrating</word> <word n="07" pos="DDQ">what</word> <word n="08" pos="NP1">XML</word> <word n="09" pos="NN1"> formatting</word> <word n="10" pos="VM">may</word> <word n="11" pos="VVI">look</word> <word n="12" pos="II">like</word> <punc type="stop" /> </unit> </paragraph>

Exercise 82

Go through the above sample paragraph and make a list of how many elements there are and which type they belong to.

Next, also count the attributes and try to explain what they're used for.

11.2.3 The 'history' and development of (linguistic) markup

Although we've already talked about the history of markup languages before when I presented a brief timeline, we now want to return to this briefly and illustrate the differences, advantages and disadvantages of the different types.

We'll begin our little survey by presenting a short sample of SGML (Figure

In our example, we can also see that all attributes occurring in start tags are not quoted, but at least those can be handled relatively easily because they occur inside a tag and are also clearly marked by the equals sign, at least where there's an explicit attribute-value pair(ing) present, such as for "complete" in the "<div2" tag. The lack of any explicit attribute inside the <w> elements, indicating words, however, fails to clearly label the attribute name for the values specified inside each tag, which we therefore need to infer to be something like pos because the morpho-syntactic tags indicate this.

SGML also has two other major disadvantages compared to HTML or XML, the first being that it absolutely requires a DTD (Document Type Definition) specifying its structure, in order to allow any type of serious processing, and -last but not least -that it's not supported by any 'standard' (browser) software.

One big advantage, at least in comparison to HTML, is that a large set of tag definitions/DTDs for linguistic purposes, such as for the TEI (Text Encoding Initiative), were originally designed for SGML, although more and more of these have been or are being 'ported' to XML these days, too.

Although HTML is a direct descendant of SGML, it only provides a limited set of tags, which, on the one hand, makes it less flexible than XML, but, on the other, also much easier to learn. It's widely recognised by standard (browser) software and the DTD(s) are already built into these browsers, although they can also be explicitly specified.

HTML itself defines a rather limited set of options for specifying structural properties of documents, such as elements for paragraphs (<p>), different levels of headings (<h1> -<h6>), larger textual divisions (<div>), smaller -inlinetextual sequences (<span>), different types of lists, certain types of formatting options (e.g. bold: <b>, italic: <i>, etc.), as well as some processing instructions, such as for line breaks (<br/>).

HTML itself is largely standardised and also technically extensible via CSS to some extent, so that it's already quite useful for the presentation and visualisation of some linguistic content. The option to include dynamic content via client-side scripting, which can be achieved with relative ease, can also be seen as an advantage, although cross-platform development for different operating systems and browsers is somewhat hampered by inconsistent or incompatible implementations of the DOM (Document Object Model).

XML, however, is much more versatile than HTML because it was designed to be extensible by the user in providing the ability to completely define one's own tags. It's much easier to process and far less error-prone than SGML because some of the shortcuts we've seen before are no longer allowed.

All XML documents minimally have to be well-formed, that is, no overlapping tags (as in HTML, e.g. <b>…<i>…</b>…</i>) are allowed. Furthermore, end tags are required for all 'non-empty' elements, so they no longer need to be inferred. Empty tags also differ from their SGML/HTML equivalents in that they have to contain a slash before the closing bracket, i.e. <element_name />. Unlike in older forms of HTML, where case did not matter, XML is case sensitive, so that linguistic tags like <turn>, <Turn> & <TURN> for representing individual speaker turns in dialogues are all treated as being different from one another.

XML has been designed to be Unicode-aware right from the very beginning, so as to allow for markup using different character sets, also within one and the same document. If no encoding is specified, it always defaults to UTF-8, so that all basic ASCII characters occurring in English documents are always displayed correctly, even without explicitly having to convert existing ASCII encoded documents to UTF-8, as the basic code points are the same.

XML describes content (like SGML), rather than layout (like HTML), so that the exact rendering of a document needs to be specified via a style sheet because otherwise the browser/application displaying it wouldn't know how to achieve its task. If no style sheet is explicitly provided, most browsers will try to render the XML content using their own default style sheets, though. At the time of writing, the only major browser that didn't do this was Safari, so if this is your default browser, as for example on Mac OS X, you'll unfortunately need to install an additional one to be able to display the results of the exercises further down. I'd suggest that you use Firefox in this case, following the instructions at

Apart from the well-formedness criterion described above, the document structure of an XML document can also be more rigorously constrained by specifying either a DTD or a schema that it needs to conform with. If an XML document conforms with one of these two types of specification, we talk of a valid document. We won't go into issues of designing DTDs or schemas here because they're fairly complex, but will at least have a look at some of the rendering options for XML documents using style sheets.

XML and style sheets

Style sheets in general allow the author to present/publish material in a visually more appropriate and/or appealing format, that is, specifying line spacing, indentation, positioning, etc., very similar to the formatting options you see in the text you're currently reading. Style sheet languages come in different flavours for different markup languages, namely DSSSL for SGML, CSS 1, 2 & 3 for HTML, and both CSS & XSL for XML. In our discussion below, as well as for our exercise(s), we'll concentrate on the essential aspects of CSS because it is simpler and easier to learn than XSL.

CSS1 already allowed the user to apply relatively 'low-level display formatting' such as layout, colours, positioning, drawing boxes around elements, etc. CSS2 & 3 expanded on these features by adding the ability to specify formatting options for publishing to different media -i.e. screen vs. printed paper, etc. -and advanced selection rules/mechanisms.

At the most basic level, CSS works by pairing the name of an element with different styling properties that the browser is supposed to associate with it. This is best done inside the style sheet definition, but may also happen inside the XML file itself. Such a pairing is similar to the attribute-value pairings we've seen above, only that in CSS, each definition starts with the name of the element defined and this is then followed by a listing of its individual properties inside a set of paired curly brackets ({…}). These brackets may in turn contain a number of attributevalue pairings, where the attributes are separated from their value(s) by a colon, with each property definition ending in a semi-colon. Therefore, for example, to define a basic paragraph layout, we could write a definition like the following, which has already been styled to look like the resulting paragraph it may be applied to in an XML (or HTML) document: In the simple example in Figure

While most of these properties used above should be relatively self-explanatory, the very first one, 'display: block;', may require some explanation: essentially, in defining display properties, we can make a distinction between block(-level) and inline display, where use of the former causes the element that is rendered to be separated from the surrounding content by clear spacing, breaking up the flow of the content, giving the visual appearance of separating it from the surrounding (con)text, as in the paragraphs in this book, while the latter means that the element remains embedded in the context, that is, does not interrupt the flow, such as in this highlighted part in this paragraph.

When it comes to specifying colour-values, there are two options for doing so, both of which are demonstrated in Figure

XSL (eXtensible Style Sheet Language) provides similar options to CSS for formatting XML display, but also offers much more complex selection mechanisms, as well as allowing reuse of 'text objects', for example, for producing tables of contents, etc., via so-called XSL Transformations (XSLT). Layout design for other (printed) media is also supposed to be enhanced through XML Formatting Objects (XSL-FO).

Of course, XML doesn't need to be viewed inside a browser or transformed in any way, but can either be viewed, or even -at least to some extent -meaningfully searched, in one of our basic editors, or manipulated in other ways through programming languages, too. Furthermore, concordancers, such as AntConc, generally also allow us to search for XML tags because they simply represent plain searchable text, but may also sometimes provide options to hide them when we don't want to display them. Thus, it's not always necessary to use a style sheet, etc., to generate a specific kind of display format for a browser, but being able to use a browser for display simply provides one convenient option for exchanging and viewing our data using a specific formatting.

'Simple XML' for Linguistic Annotation

In this section, we want to explore how to use a form of XML that I refer to as 'Simple XML' in order to do annotation on multiple levels. The following exercise is designed to provide you with an introduction to this which is broken down into a series of steps that all successively allow you to deal with a particular XML-related or linguistic issue in creating a well-formed XML document.

Exercise 83a

Download our practice file from

Open the file in your plain-text editor. Add the line <?xml version="1.0" encoding="UTF-8" ?> at the very beginning of the file.

Save the file as 'practice.xml', ensuring that the encoding is 'UTF-8', and without Byte Order Mark (BOM).

Keep the file open.

The line we just inserted identifies the document as being in XML for any application that's capable of displaying it, at the same time specifying its encoding as UTF-8. The question marks at the beginning and end identify the tag on this line as a processing instruction, in our case specifically the one referred to as the XML declaration. Please note that, so far, what we've done has not turned the file into valid XML yet as there are still some parts missing, so we're really just 'pretending' that the file is XML.

The next step is to create a 'container' element for our dialogue. This container element is also known as the root element, and every well-formed XML document needs to have one. We'll label the root element for this document 'dialogue' in order to identify our text as a dialogue, and we'll also give it two attributes with the names 'corpus' and 'id' and corresponding values of 'test' and '01' respectively.

Exercise 83b

Wrap the whole text following the XML declaration into the container tag.

Add the attributes specified above and their values.

By wrapping our text in the container element, we've effectively created the first (top) level of a hierarchy or categorisation, as well as a file that is now, at least technically, valid XML, despite the fact that it hasn't been sub-divided into meaningful parts yet.

We can now proceed to subcategorise the elements contained within the dialogue. If you look at the text again, you'll notice that the next level below the dialogue in our text is the speaker turn, so it would be quite logical to use this label for a tag surrounding each turn. As our turns have already been transcribed with a speaker id (A or B), followed by a number for the turn, we can identify these as suitable attributes characterising the turns.

Exercise 83c

Wrap a 'turn' element around each turn, keeping tags and text separate. If you feel adventurous, you can also try and save a fair amount of time here, experimenting with using regex replace operations and backreferences in your editor. Always remember, when something goes wrong, you can usually undo it, in the worst case simply closing the file without saving, then reopening it and starting all over again.

Save

Exercise 83d

Based on your knowledge of syntax, mark up all stretches of dialogue that you can identify, using the element names <decl>, <q-wh>, <q-yn>, and <imp> for the 'traditional' syntax types. For simplicity, and to improve readability, always separate each c-unit text from the tags by line breaks again, so that both container tags and text end up on separate lines.

In addition, mark up areas of text that represent terms of address (e.g. Sir or Madam), discourse markers (e.g. well, ok, right, aha, etc.) and yesand no-like answers, using the tags <address>, <dm>, <yes>, <no>, as well as syntactically incomplete/ungrammatical units as <frag> for fragment.

Save the file and view it in your browser.

Now that we've finished marking up the syntactic level, we can move on to the pragmatic one. Here, we want to add speech-act attributes (sp-act) to the syntax elements to try and express what the intentions of the speakers are. The potential options you could use for speech acts, based on the syntactic categories, are: r q-wh: reqInfo (request for information), reqDirect (request for directive/instruction) r q-yn: reqInfo, reqDirect r decl: state (stating/informing), agree, reqInfo, reqConfirm (requesting confirmation) r imp: direct (giving a directive/command), suggest (making a suggestion) r dm: init (initiating/initialising a new (sub-)topic), acknowledge (acknowledging) r yes: agree, accept r no: negate, refuse r address: refer frag: greet, intro (introducing oneself or a third party), thank, bye (saying goodbye), refer (providing deictic information)

Exercise 83e

Add what you think may be the appropriate label for each speech act to the individual syntactic units. When you do this, please remember that our interpretations may sometimes be a little subjective, so it's best to compare your results with a colleague to see how much consensus you can reach.

As before, check the result in your browser.

Colour Coding and Visualisation

In this section, we want to explore some of the basic and slightly advanced features of CSS, which will enable us to style our XML file in a useful manner, based on its elements and sometimes even specific properties, such as speech acts, etc. We'll begin by defining some display properties for the whole dialogue, and then gradually move on to define options for the relevant elements and attributes.

Exercise 84a

Create a new file in your editor. Type in dialogue, followed by a set of paired curly brackets. The first properties we want to set here are the background colour (backgroundcolor), the font size (font-size), and the left margin (margin-left), so you can add each one of these within the curly brackets, each time followed by a colon, a space, and a semi-colon, as illustrated in our discussion of CSS earlier on. Now, set the properties as shown in Table

font-size 1.25em

This sets the font size to 1¼ times the original font size, which is usually around 10 points in the browser. As em is a relative value, this allows you to increase or decrease the display size in the browser in relative steps by pressing Ctrl+ or Ctrl-, respectively.

margin-left 2.5%

This sets a left margin/indent for the page, mimicking the margin on a printed page, so that the text isn't squashed against the left side of the display window.

Save the file as 'dialogue.css' in the same folder as the dialogue file.

So far, we've only created the basic style sheet, but we also need to 'let the browser know' that it's supposed to use it with our dialogue file, so we need to create a link between the two.

Exercise 84b

Open your XML dialogue file again and insert the line <?xml-stylesheet type="text/css" href="./dialogue.css"?> between the XML declaration and the dialogue tag, then save it.

Load the XML file in your browser and view the result.

You may now be surprised because, apart from the few general formatting options we've just set for the dialogue itself, all the levels in our hierarchy that we've so painstakingly set before will have disappeared and the text simply runs on without any indication of where one turn or syntactic unit starts and ends. This is the case because the browser now assumes that, since you've 'told' it that you want to style the XML yourself, it should no longer apply its own built-in style sheet, but instead leave all the styling up to yours. Therefore, in the next few steps, we'll have to learn about some further styling options that'll allow us to do just that. We'll start by (re-)formatting the turns.

Exercise 84c

Go back to the style sheet and create a new definition for turns, setting the properties for display, color, margin-left, line-height, and textindent to block, black, 5.5%, 1.5em, and -3.5em, respectively. What these mean should hopefully be relatively self-explanatory, based on what we learnt earlier.

Now, save the style sheet, go back to the browser, and refresh the page to see the result.

Each turn should now appear in a block of its own, but we still haven't got any indication of the speaker id, so we don't really know who's talking when. Although it could potentially be an interesting exercise for students in classroom activities to identify this, it's not really what we want to see here, so we need to find a way to access the speaker attribute information to use it in our style sheet. Luckily for us, in our style sheet definitions, we can not only refer to elements, but also to attributes, so we can easily style turns by speaker A (i.e. the agent) differently from those by speaker B (the caller). In order to do so, we'll not just make use of the speaker attribute, but will also exploit another feature of CSS (2/3), which is that we can generate content before or after each element automatically.

To be able to access information about the speaker attribute, we need to use an attribute-value pair with an equals sign, similar to the way we define attributes in XML, but without the quotation marks around the value. This needs to be enclosed in a pair of square brackets, so writing turn[speaker=A] will allow us to refer to all turns produced by speaker A, which we could already exploit in order to style these turns differently in terms of background colour, etc., if we wanted to. However, instead of changing display properties like this, we'll manipulate the actual textual content by prefixing each turn with the contents of its 'n' attribute, as well as with information regarding the roles of the speakers. To achieve this, we need to access the so-called pseudo-class called 'before' of the relevant turn, which is appended to the earlier definition we created using a colon as a connector, thus yielding turn[speaker=A]:before. Knowing this, we now only need to learn how to manipulate/refer to the text we want to pre-pend to the turn. This can be achieved via its 'content' property, which, in its most basic form, is simply a string of text enclosed in double quotes, so we can write turn[speaker=A]:before {content: "Agent:";} to make the word Agent followed by a colon appear before each turn produced by speaker A.

Exercise 84d

Try adding the definition for speaker A, the Agent, to your style sheet.

Do the same for speaker B, the Caller.

Test the result in the browser.

The only thing left to do now is to incorporate the information from the 'n' attribute, that is, the turn number, into the pre-pended content. To refer to the value of the n attribute within the content property, we can use the general style sheet syntax for accessing such attribute information inside property definitions, which is to use attr, followed by the name of the relevant attribute in round brackets, i.e. attr(n) in our case. This should be placed outside the quotation marks in the appropriate place, and thus we'll write turn[speaker=A]:before {content: attr(n)" -Agent:";} to complete our definition, where we've also modified the content inside the quotation marks slightly to separate it from the turn information.

Exercise 84e

Amend your prior definitions for both speakers and test this inside the browser again.

Having formatted the turns, we can now move on to dealing with the syntactic elements occurring inside them. Unlike the turns, which we'd styled as block-level elements, we'll format these as being inline and with a left margin of .5%, for which you should already have enough knowledge to write the individual definitions. As this is a kind of formatting that we'll equally want to apply to all such units, it would be cumbersome to have to type this out repeatedly, so we can take advantage of a feature of CSS that allows us to list the different elements a definition applies to by separating them from each other by a comma, just like in an ordinary written list.

Exercise 84f

Add the style definitions for all the syntactic element classes.

Test the result.

Please note that, so far, we've only set two general properties for the syntax elements, but still need to set their individual display options, so as to be able to recognise and distinguish them visually from one another. Creating the definitions that are individual to each syntactic category essentially entails finding suitable background and foreground colours to make the different types either maximally distinctive or, in contrast, to possibly emphasise common features, such as the similarity between the different types of questions, which can be expressed by using the same background colour for both. For some of the other features, we can use specific types of colour coding semantics to signal 'open' and 'closed' options. For instance, to signal the open-ended nature of wh-questions, we can use green, while we can use red to indicate the limited options of yes/no-questions. For yes and no answers, we can employ a similar type of traffic signal analogy and encode them like one-way street signs. Table

Exercise 84g

Add the above definitions to the style sheet and test the results.

Another thing we want to do here, apart from applying colour-coding semantics to our units, is to actually add the punctuation that was missing from the original transcripts. We can do this by using the counterpart of the before pseudo-class, after, in order to generate it automatically, based on the syntactic class. This works in exactly the same way as when we pre-pended content to all turns.

Exercise 84h

Define yes, no, decl, frag, and address, as being followed by a dot, the questions a question mark, discourse markers a dash, and imperatives by an exclamation mark in the style sheet.

There are still a few more syntactic elements that we haven't defined any colourcoding for. Simply create definitions for these using the following values: r discourse markers: white font, #ff8000 background r declaratives: blue font, white background r imperatives and fragments: background #d9ffff r imperatives: red font r fragments: font colour #4b4b4b.

Exercise 84i

Create the remaining colour definitions in the appropriate places inside your style sheet and check the results again.

One very important feature of the style sheet rules is that the definitions inside the style sheet are applied by the browser in exactly the order that they were written in. This effectively means that any later re-or additional definition for already defined specific elements or attributes will override earlier existing definitions. We can exploit this feature in order to display requests for information contained inside fragments in the same font colour as wh-questions.

Exercise 84j

Create a definition for the font colour of both wh-questions and any items that have a speech act reqInfo in the appropriate place, setting them to #009b00.

In order to refer to the speech act attribute-value combination, you need to use a similar syntax to the one we used for our definitions of the different turns, only without an element name in front of the square brackets.

Finally, we also want to override the punctuation mark that occurs after all syntactic units that may contain requests for information, which may also include declarative questions, apart from the fragments we just handled.

Exercise 84k

Based on your knowledge from the earlier tasks in this exercise, you should already know how to achieve this, so just write an appropriate definition and test the results again.

When examining the results so far, you may have noticed that all empty elements were also removed from the display. Unfortunately, as CSS definitions are generally designed to help us render the content occurring inside non-empty elements, we cannot simply write a definition that will display the empty elements because they technically speaking don't contain any content. This is why we have to use a little trick to have them displayed inside our browser, which is that we simply 'recreate' them using the after pseudo-class. All we have to do to achieve this is first to declare them as inline elements and then write the appropriate content definitions, where we use the same text as in the actual elements, but get their relevant attribute values using the attr() syntax we used previously for retrieving the turn numbers. As we also want them to be unobtrusive, we'll use a grey font colour (#808080) for all empty elements.

Exercise 84l

First, define all empty elements, pause, unclear, overlap, and backchannel, as inline elements with the font colour specified above.

Next, write the individual definitions for them, using the after pseudoclass with an appropriate CSS content attribute that uses the correct attribute of the XML element to extract and display its value.

Test the result in your browser again.

The two lengthier exercises above were essentially designed to provide you with two different perspectives on the use of annotations. The first one basically gave you a means of designing your own ways of classifying your data in a sensible manner, which then enables you to use the corpus-based analysis methods we've discussed throughout the course in order to extract and count relevant information. The second one was designed to allow you to develop alternative views of your data, something that may help you, or any students or colleagues you may create teaching materials or presentations for, to quickly visualise different features that might be present in individual texts. This may, for instance, make it possible for students to notice the number of discourse markers or syntactic fragments that occur in spoken interaction, or identify other important elements of such types of interaction. For analysis purposes, both techniques, if used sensibly, will often lead to a cyclical refinement of the categories identified and their respective annotation forms, enabling the researcher to 'fine-tune' the analysis results and thereby also the conclusions that can ultimately be drawn from the corpus data.

More Complex Forms of Annotation

The form of annotation I introduced you to above already contains many different bits of information, so that it may appear fairly complex to you. However, I still refer to this as 'Simple XML' because it remains easily readable due to the relatively low number of container elements and the separation of text from nonempty tags. This separation, incidentally, is possible because most XML processors simply ignore any whitespace they 'perceive' as redundant. Because these processors generally parse the XML and often produce tree structures based on the hierarchy of the elements, they'd thus have rendered our sample in the same way, even without the additional line breaks I suggested you insert for readability. Some proponents of 'pure' XML technology would even frown upon what we've done here and argue that XML was meant to be processed by the computer, anyway, and could then be rendered in whichever way it would be best viewed. However, this argument ignores the fact that, before any annotation is finished, it repeatedly, and often for very long periods of time, needs to be read and edited by humans, so that readability does indeed represent an issue in annotation. Even the best fully automated annotation still needs to be checked for errors by human editors, although, as we've seen in the many examples of mis-tagging in the BNC and COCA, this is often not done for reasons of time and cost involved, especially the larger the amount of data processed becomes. Furthermore, any process of rendering, combined with editing and saving, involves making modifications to the data in forms invisible to the user, and may be error-prone because it repeatedly needs to save the changed data and then update the view again. In addition to this, using such software also ties the average user unnecessarily into using often complex annotation tools that themselves represent a relatively steep learning curve, apart from further potential issues regarding platform availability and setup.

Adding further levels of annotation to corpus data almost always leads to added complexity in the data, although, for instance, striking the right balance between using an appropriate number of container elements, empty elements, and suitable attributes can already help us to go a long way, as hopefully Exercise 84l has shown you. Further decisions of course need to be made regarding how much meta-information each file in the corpus absolutely needs to contain, or whether there isn't a choice to relegate some of this information to external header files (cf.

The BNC data we've been working with represent a different way of handling annotation, in that they, for instance, contain a relatively complex header that contains a high degree of meta-information, which can be seen in Figure

Another popular format, especially in language engineering circles, is the socalled standoff format

From the above discussion, you'll hopefully have seen that, especially for smaller corpus projects, the motto should always be 'the simpler, the better'. And one always ought to remember that, even though one might never know how a corpus may potentially be used by others, not all types of annotation need to be included in all corpora, simply because this may be possible or relatively easily achievable. Therefore, for instance, a very good, and still valid, example of the use of different versions of one and the same set of corpus data is the SEC (see Section 2.3.1.2), which actually exists in five different forms.

Solutions to/Comments on the Exercises Exercise 82

The elements you should be able to identify in the first part of this exercise are <paragraph>, <unit>, <word>, and <punc />. The first three of these represent container elements for the different syntactic/textual levels described, while the last one is an empty one, representing punctuation as a non-word.

In terms of attributes, you should be able to find 27, where 'n' represents numerical identifiers for all the 14 textual elements, 'pos' the 12 PoS categories for the words, and 'type' which type of punctuation is present at the end of the syntactic unit.

Exercise 83

(a) Inserting the XML declaration and saving the file should generally present no problem, but ensuring the actual encoding for the file in UTF-8 (without BOM) may present a little bit of an issue, depending on which editor you're using. If your editor is encoding-sensitive, like Notepad++, it'll normally have retained the original encoding of the text file, which was already the correct one. If you need to change the encoding, though, where exactly this can be done, and also how you can see this, may vary. And, unfortunately, there's no way for me to tell you how exactly this will work in your editor if you're using a different one, so I can only give you some tips as to where such option can possibly be set. In some editors, the encoding can be set in the 'Save' dialogue, together with the file name. For instance, in Windows Notepad, you can select the option for UTF-8 under 'Encoding' in this dialogue, although, unfortunately, there's no way to specify the additional option to exclude the BOM we don't want, and which is in fact unnecessary and, if present, may also cause display issues in some browsers. In other editors, such as Notepad++, there are additional options available via a dedicated 'Encoding' menu item, where you can specify what to encode a file in or even to convert between a limited set of encodings. In Notepad++, you can also specify the default encoding for any files you create under 'Settings→Preferences→New Document', where I'd recommend you set the option for 'UTF-8 without BOM', as well as use the additional setting 'Apply to opened ANSI files'.

(b) Hopefully, you remembered that only the start tag can have any attributes in it, so that our start tag should now read <dialogue corpus="test" id="01">. Our end tag, which you need to insert after the last line of the dialogue, will then be </dialogue>.

(c) Wrapping a <turn> element around each turn should be quite straightforward. For instance, the tag for the first turn ought to look similar to this: <turn speaker="A" n="01">. Of course, you can also opt for single quotes, if you want to, as long as you remember to be consistent with your opening and closing quotes. If your browser indicates an error, go back to the XML file and first verify whether you've set all start and end tags correctly, as this is generally the most common error. One of the most common error messages you'll probably encounter will be something like "XML Parsing Error: mismatched tag. Expected: </turn>". The browser will usually also try to provide a line and column (i.e. character position) number for where the error was found, but this will often not be very helpful because XML parsers are unfortunately not very good at locating such errors, so that, frequently, the error location pointed out is only towards the end of the file where the browser (finally) 'notices' a mismatch between start and end tags. While trying to fix potential errors, however, some editors may provide a certain degree of help through syntax highlighting. For instance, when you click on the start tag in Notepad++, it'll normally highlight both the start tag and its corresponding end tag, so if you go through the file from the top, you can perhaps identify the error this way, even if it may be a rather tedious process, especially if the file is long. Anyway, once you think you've fixed an error, switch back to the browser and refresh the display.

Using regex replacements here, rather than tediously adding all the tags manually, is possible because the turns are basically all represented in our file as single lines that start with a speaker id, followed by a dot, the turn number, a colon, a space, and then the rest of the turn. Thus you can construct a regex expression that captures the speaker id to be reused as \1, the number of the turn as \2, and everything following the colon and space until the end of the line in \3. And, to be able to automatically keep the turn tags separated from the text, we can add new lines in the appropriate places. To do all this, you need to define one character class enclosed in round brackets to match the speakers, either A or B, at the beginning of the line, ideally escape the dot that follows, then capture one or more digits for the turn number, again enclosed in round brackets, match a colon, one or more spaces (just to be on the safe side), then as many characters as possible, again in round brackets, until you reach the end of the line. To avoid any potential unwanted spaces at the beginning or end of the line, you can also add some optional whitespace. Therefore, the search term, with regex search option switched on, would be: ˆ\s * ([AB])\.(\d+):\s+(.+)\s * $. To achieve the replacement, including line breaks, the code construct would then be <turn n="\2" speaker="\1">\n\3\n</turn>.

(d) This part of the exercise may initially be more difficult because you'll have to think 'out of the box' in terms of new and unusual syntactic categories, but you'll soon get used to this. The beginning of the file should now look like this:

<?xml version="1.0" encoding="UTF-8" ?> <dialogue id="01" corpus="test"> <turn speaker="A" n="01"> <frag> good afternoon </frag> <frag> Virgin train line Sandra speaking </frag> <q-wh> for which journey do you wish to purchase a ticket </q-wh> </turn> <turn speaker="B" n="02"> <frag> er Euston to Manchester please </frag> </turn> (e) Again, this part may be a little difficult if you've never worked with/on speech acts before, but most of the ones listed should be relatively intuitive. One thing the list of speech acts I gave you doesn't include is a complete set for speech acts signalling responses, although some of them, such as, for example, 'accept' or 'refuse', already contain such information. Other speech acts, though, may be responding in a similar way, but this response may be paired with one of the other options. Thus, a 'reqInfo' speech act by one speaker will often trigger an 'answer' by the other, but this answer is generally a statement ('state') in our above taxonomy. Thus, to be more explicit, we could in fact also allow more than one speech act to occur in our annotation, which would result in 'answer-state' for this particular example.

(f) Replacing the pauses so that our empty tag will, for example, either look like <pause /> or <pause length="9s" />, should be quite easy. The empty element for the 'unclear' 5 syllables should, logically, become <unclear length="5 syllables" />. Regarding overlap tags, you'd actually be quite right in assuming that, theoretically, these should be container elements because they mark up specific spans of text. However, as XML forbids the use of overlapping tags, and overlap passages span across different speaker turns, using <overlap>…</overlap> elements would break the document's well-formedness, so it's best to use the empty tags <overlap type="start" /> and <overlap type="end" />, possibly in combination with a number ('n') attribute. As backchannels constitute text where another speaker simply provides feedback to the current speaker who holds the turn, but doesn't really interrupt to take over, it also makes sense to incorporate this via an empty element. As before, when we created the turn elements, because there are easily recognisable patterns, you can save a lot of time if you're able to construct appropriate regex replacement patterns, so you should definitely try this.

For a complete solution of the annotated dialogue, see Appendix B. As an alternative to some of the steps we modelled as regexes above, and for slightly more convenient manual annotation of the remaining XML structure, you could also use an annotation tool, such as my Simple Corpus Tool, which actually includes an editor that'll allow you to add these tags and attributes through the click of a button. For a fully automatic large-scale annotation of the syntax, speech acts, etc., you can also try my Dialogue Annotation and Research Tool (DART), which not only allows you to annotate hundreds of dialogues in this way within minutes, but also to post-edit/correct the annotations, as well as to carry out similar analysis operations to those we learnt how to perform in AntConc, including concordancing, n-gram analysis, etc.

Exercise 84

(a) The display options for the whole dialogue you set should hopefully look as follows, where I've simply written everything on one single line for compactness: dialogue {background-color: #ffffdd; font-size: 1.25em; margin-left: 2.5%;}.

As with XML, in CSS slight mistakes in the syntax may cause errors, so if the display you get doesn't seem to reflect our properties, you need to check your style sheet for errors.

(b) Adding the style sheet reference should need no further comment. (c) The definition for the turns should look like this: turn {display:

block; color: black; margin-left: 5.5%; line-height: 1.5em; text-indent: -3.5em;}.

(d) This part of the exercise is probably a bit more difficult, especially because there are some fairly complex new CSS terms and concepts involved. However, with a little bit of effort, you'll hopefully be able to set the appropriate display entries for both dialogue participants to:

r turn[speaker=A]:before {content: "Agent:";} r turn[speaker=B]:before {content: "Caller:";}.

(e) Again, this part may sound a little complex, but with a little experimenting you'll hopefully manage to produce the following definitions:

r turn[speaker=A]:before {content: attr(n)" -Agent:";} r turn[speaker=B]:before {content: attr(n)" -Caller:";}.

(f) Adding the general definition for all syntactic element classes is quite straightforward. The line required in the style sheet is: address, decl, dm, frag, imp, no, yes, q-yn, q-wh {display: inline; margin-left: .5%;}.

(g) This should again be straightforward and not require further comment. (h) For adding the punctuation marks, you should end up with the following definitions: r yes:after, no:after, frag:after, decl:after, address:after {content: ".";} r q-wh:after, q-yn:after {content: "?";} r dm:after {content: " -";} r imp:after {content: "!";} (i) As the basic colour options for the remaining syntax elements are quite straightforward, I won't provide a separate solution for these here.

(j & k) Changing all wh-questions and any other elements that contain requests for information as a speech act to a separate font colour can be achieved via: q-wh,

(l) We can set the unobtrusive font colour for all empty elements in one go, using the following definition: pause, unclear, overlap, backchannel {display: inline; color: #808080;}.

The individual definitions for displaying content should look as follows:

r pause:after {content: "<pause length='"attr(length)"' />";} r unclear:after {content: "<unclear length='"attr(length)"' />";} r overlap:after {content: "<overlap type='"attr(type)"' />";} r backchannel:after {content: "<backchannel content='"attr(content)"' />";} For a complete version of the style sheet, see Appendix C.

Sources and Further Reading

Conclusion and Further Perspectives

Throughout this book, I've tried to give you many opportunities to develop an awareness, coupled with some basic experience, of how linguistic data can be used to solve linguistic puzzles or questions. At the same time, whenever appropriate, I've also tried to point out other potential applications for such data, for example, in the development of teaching materials/textbooks, grammars, or direct application in the classroom, but of course such a list will always be incomplete as there are too many applications of corpus linguistics to be listed exhaustively. To get a better overview of other applications, you can -now that you should have more than at least a basic understanding -turn to the additional literature I've listed for the different sections, especially the many other, generally more theoretically oriented, textbooks, or the two handbooks I listed in the Introduction,

For now, all that remains for me to do is to summarise what we've tried to achieve here, and to give you some further pointers on where to get information other than from publications.

In the first two main sections of the book (Chapters 2-4), we started out by investigating the different forms language data may come in, especially in the shape of existing corpora, and then moved on to developing an understanding of how you can complement such data by collecting your own, including which problems and pitfalls you may encounter in this endeavour, focussing on the nature and sometimes 'messiness' of electronic data. Here, I already tried to give you at least some sense of how data that hasn't been prepared well may cause specific issues and errors in later analysis stages. Apart from teaching you how to prepare your data, this was essentially also a plea for showing diligence in preparing corpora, especially if your intention may be to distribute your data to a wider audience.

In the next major section (Chapters 5-10), we then investigated various techniques for analysing language data using established methods of corpus linguistics. Here, as much as possible, I've tried to show you complementary techniques for working both with your own data and larger, general, corpora through webbased interfaces, thereby also allowing you to some extent to test findings derived from your own data against reference corpora, or to compare linguistic phenomena across different corpora. Throughout this section, we encountered various issues with tools and methods, again partly illustrating the effects of data where flaws in the basic compilation of the corpus may cause potential errors in the result, but partly also pointing out potential shortcomings in the particular tools at our disposal. What you'll hopefully have come to realise from this in particular is that no tool, even if it may have been designed with the best of intentions by the author(s), and to incorporate as many facilities as possible, and even if it may be highly customisable, such as AntConc, is ever perfect for all our potential purposes. Thus, we constantly need to be aware of such shortcomings, as well as try and find new and creative, but nevertheless solid, ways of overcoming these issues.

One of the major issues we've repeatedly encountered, especially concerning the mega corpora we've worked with, is that the creation of large-scale resources may frequently lead to the compilers taking shortcuts when it comes to ensuring the quality of the data in terms of tokenisation and annotation. Furthermore, the design of the annotation and interfaces available may sometimes exhibit flaws from a linguistic perspective, as we've, for instance, seen for the CQP architecture behind BNCweb, which treats punctuation tokens in exactly the same way as genuine words, thereby potentially skewing all the statistics produced by the tool. Another issue we've encountered in this context is that often tools designed for analysis almost force us into accepting the 'orthographic word' as the correct unit of analysis, which is e.g. exemplified in the fact that the BYU interfaces don't allow us to compare 'words' with 'phrases' directly. These are just some of the issues we need to constantly be aware of when we use such tools, so the idea that 'bigger is better', even if it is indeed often important to work with very large amounts of data for such research as collocation analysis in order to be able to find rarer combinations, may not always be fully justified if the quantity of data isn't equally matched by quality. Having criticised some of the larger corpora and their interfaces rather heavily, I now need to relativise this again, as, of course, creating such large and complex resources still remains a highly laudable effort despite these apparent flaws, and having these mega corpora and associated interfaces already takes us a very long way in advancing our knowledge of language and its uses, apart from providing us with huge amounts of extremely valuable real-life materials for teaching and learning, and other applications of or in linguistics.

In the final brief section (Chapter 11), I've tried to provide you with a short, but nevertheless highly practical, glimpse at what current technology in the form of XML has to offer to linguists who want to enrich their data and/or visualise important facts inherent in it. Here, I've deliberately advocated a form of annotation I call 'Simple XML', which still makes it possible to read and edit the corpus data, or annotate it further, but without the need for complex interfaces many less computer-literate corpus linguists may have a hard time to even install, let alone use without accepting a steep learning curve.

This book being (only) an introductory textbook, the topics we've covered can obviously not be exhaustive, in particular because of the more practical nature of our approach. I've therefore had to restrict my discussion of relevant topics in corpus linguistics to what I consider the most essential ones for beginners. There are, however, also many other applications of corpus linguistics I've been unable to cover, but which may be of interest to you now that you've mastered the basics, and which I'd like to mention here briefly.

Diachronic/historical corpus linguistics is an important area that aims at answering many of the as yet unanswered questions in language history, including differences in style, vocabulary, grammar, and even pragmatics. In translation studies and teaching, parallel corpora, that is, corpora that cover similar textual matter or even represent aligned translations of texts, continue to have a very strong influence. In L1 Acquisition, there's a substantial body of data that has been collected following the standard of the Child Language Data Exchange System (CHILDES;

Of course, this use of statistics, as we've seen to a very small extent when we looked at collocations, etc., is also partly shared with corpus linguistics, and methods and textbooks in this domain, such as

As this brief overview has shown, there are still many more advanced topics in corpus linguistics for you to explore. However, for the moment, I'd first like to suggest that you consolidate what you've learnt from this book further, and devise your own ways of dealing with any issues you may encounter in your own research. If, however, somewhere along the line you should realise that what the tools available have to offer you is not enough, you can always investigate learning how to write your own analysis programs, as this will liberate you from many of the constraints described above, and will also give you a degree of flexibility in your research that no single tool can offer. The idea of doing this may at first appear daunting, but I can assure you that, if you've gone through this book carefully and now feel relatively secure in conducting the types of analysis described here, taking this next logical step will already have become much easier… Last, but not least, I should also point out that there aren't only book or journal publications relevant to corpus linguistics, but also online discussion groups on various social or professional media, such as Facebook or LinkedIn, and especially the corpora mailing list at

Glossary ACE: the Australian Corpus of English, also known as the Macquarie Corpus (see Section 2.3.1.1) alignment: establishing a link between the source text and the translation, usually at the sentence, phrase or word level ANC: the American National Corpus (see Section 2.3.2.2) annotation: the process of enriching corpus data with (interpretative) linguistic information (see Chapter 11) ARCHER: a diachronic corpus (see Section 2.3.3) ASCII: American Standard Code for Information Interchange (see Section 3.3.1) BNC: the British National Corpus (see Section 2.3.2.2) BNCweb: the web interface of the BNC (see Section 8.1.1) BoE: the Bank of English (see Section 2.5) Brown: the Brown University Standard Corpus of Present-day American English (see Section 2.3.1.1) balance: the notion in corpus building, especially for reference corpora, that a corpus should contain as many representative pieces of texts, from all representative genres, as well as possibly all relevant media (i.e. generally written vs. spoken vs written-to-be-spoken); often also linked to representativeness (see Section 3.1.3) CES: the Corpus Encoding Standard character encoding: a computer-based system of representing characters as numbers (see Section 3.3) colligation: the association/collocation (see below) of a PoS category or word with a particular word class (see Section 10.5.2) collocation: the characteristic co-occurrence of lexical patterns involving two or more words within a certain (limited) distance of one another, usually up to 4-5 words to the left or right of the word under investigation (the node); these can be investigated/identified 'manually/visually' or by means of statistical test for co-occurrence (see Sections 10.5.1 & 10.8.1) concordance: a listing of a word/expression in a corpus, generally within a specific context (cf. KWIC) (see Section 5.1) concordancer: a software package that generates concordances, as well as possibly displays of other linguistic features, for corpora (see Section 5.1) corpus (pl. corpora): a collection of texts in machine-readable form (see Section 2.1) corpus-based (research): research that primarily uses corpora to verify the researcher's intuitions corpus-driven (research): research based on few, if any, prior intuitions, where the results of corpus analysis directly lead to new insights and theories dispersion: a term in descriptive statistics which refers to a quantifiable variation of measurements of differing members of a population within the scale on which they are measured (see Section 10.7) ditto tag: in corpus annotation assigning the same part-of-speech code to each word in an idiomatic expression (see Section 9.1.1) Document Type Definition (DTD): a separate document in markup languages such as HTML, SGML and XML that is linked to a content-bearing document and defines which elements and attributes are allowed to occur within the document, thereby constraining available options and making it possible to test the validity (as opposed to well-formedness) of the document (see Section 11.2.3) EAGLES: Expert Advisory Group on Language Engineering Standards; an initiative of European researchers that made recommendations regarding standards and guidelines for best practice in Language Engineering EAP: English for Academic Purposes error-tagging: the process of adding error-related information to a corpus FLOB: the Freiburg-LOB Corpus of British English, an update of the LOB corpus in the early 1990s (see Section 2.3.1) frequency: the number of occurrences of a linguistic feature in a corpus (see Section 9.2) Frown: the Freiburg-Brown Corpus of American English, an update of the Brown Corpus in the early 1990s (see Section 2. 2) log-likelihood: statistical measure used for collocation or keyword analysis; also referred to as G 2 markup: a term often used synonymously with annotation to refer to adding enriching information to a corpus; sometimes specifically used to refer to the physical act of marking specific parts of a text using specific symbols (see Chapter 11) meta-data: a term used to describe data about data, typically the contextual information of corpus samples (see Section 3.2.1) MI: mutual information, a statistical measure of co-occurrence that measures the strength of collocations (see Section 10.8.1) MICASE: the Michigan Corpus of Academic Spoken English (see Section 2.4.1.1) monitor corpus: a corpus that is constantly updated and enlarged with new corpus materials (see Section 2.5) MWU: multi-word unit; a unit of sense that consists of multiple words (see Section 9.1.1) node: the central word or search term in a collocation or concordance (see
Introduction

As defined by

As highlighted by

What is a Corpus?

Corpus is the main data that a corpus linguist (or a researcher aiming to explore the use of language) needs to investigate a specific area of a particular language(s).

For this reason, sampling is an essential issue that a corpus compiler needs to consider. Secondly, the corpus should be created by considering a specific idea. This means that the researcher should have a clear mind about what to do with the corpus being collected. In addition to these aspects, the corpus needs to be in electronic form since it will be almost impossible to cope with such vast data without the help of technology. Finally, the corpus should serve as a linguistic study. As mentioned above, without narrowing down the scope of a corpus, it is impossible to create a corpus representing everything related to all areas of language and texts appearing in the language.

Corpus as Data

Researchers may use corpora for many purposes. The purpose of the study determines the corpus type the researcher will come up with at the end of the compilation of the texts. For example, suppose a researcher wonders about the use of language among teenagers or children. In that case, the researcher will collect the data from that target group. At the end of the data collection process, a corpus such as CHILDES, which aims to explore the language acquisition process of the children, will be created. No matter the purpose of a corpus collection, all corpora need to provide metadata to ensure that other scholars can also benefit from the corpus. Otherwise, the corpus itself, without the metadata, will be useless since the researcher will not be able to know the background information of the corpus.

As metadata is vital for understanding the corpus itself, metadata can be defined as the data about data

In addition,

Analytic metadata provides information about how the corpus developer interpreted and analyzed corpus constituents.

Descriptive metadata provides classificatory information derived from internal or external properties of the corpus components.

Administrative metadata provides documentary information about the corpus itself, such as its title, availability, revision status, etc.

Editorial metadata provides information about the relationship between corpus components and their source Corpus Analysis & Linguistic Theory

Observational Adequacy: It focuses on describing the grammatically well-formed sentences in a language. For example, according to observational adequacy, the first example is well-formed, while the second is not grammatically well-formed. However, the reason why the second example is not well-formed is not explained at this level.

He studied for the exam *studied for the exam

Descriptive Adequacy: It focuses on describing whether individual sentences are well formed and specifying the abstract grammatical properties that make the sentences flawless. For instance, the second example above is not well-formed, while the first sentence is well formed. Thus, it can be stated that sentences in English require an explicit subject.

Explanatory Adequacy: The description or theory not only reaches descriptive adequacy but uses abstract principles that can be applied beyond the language being considered and become part of UG. For example, by analyzing the samples above, it can be generalized that English is not a language permitting 'pro-drop'.

Due to these levels, it was claimed that corpus linguists give more importance to descriptive adequacy while generative grammarians focus on explanatory adequacy. However,

Corpus linguists have harshly criticized generative grammarians as their discussions generally gather around the advocacy of a language system that is highly abstract and decontextualized. Corpus is used to verify any linguistic hypothesis's falsifiability, completeness, simplicity, strength, and objectivity.

As generative grammarians focus on more abstract discussions of language,

As corpus linguistics is accepted as a methodology in linguistic research, corpora have been used in many studies. Some of the study areas exploiting are summarized below.

Grammatical Studies

According to

Reference Grammars

The purpose of using corpora in reference grammar studies is to gather information on the form and use of various grammatical forms of a specific language and to use this information for writing a reference book. Longman Grammar of Spoken and Written English (1999) by Biber et al. can be taken as an example to illustrate how the corpus is used. While writing this book, they used Longman Spoken and Written English Corpus. This grammar book provides extensive information both on the form of English structures and their frequencies. Also, the use of specific structures in different genres of spoken and written English is shared with the users of the book.

Lexicography

Language Variation

The use of language may vary from one genre to another or from one region to another. Even socioeconomic status may play an essential role in language variation. To explain whether the language differs in different contexts, researchers may benefit from corpora, an excellent source to reveal such differences. Corpora are widely used in sociolinguistics, especially when the effects of socioeconomic status are taken into account. Furthermore, in the description and explanation of the use of language among specific people, such as teenagers, corpora help researchers a lot during the course of detecting language varieties. For example, researchers can use the Corpus of London Teenage English to determine the characteristics of teenager English.

Historical Linguistics

When the aim is to trace the historical change of a language, a corpus divided into periods should be used. In diachronic research, scholars may focus on the specific usage of a word or a structure. Also, the effects of demographic information on the use of specific linguistic constructions could be explored with the help of a corpus designed as a historical or diachronic corpus.

Contrastive Analysis and Translation Theory

To use corpora for contrastive analysis and translation theory, researchers need to use a parallel corpus, meaning that the same text has been translated into various languages. For example,

Language Acquisition

The corpus created by collecting data from children acquiring their first language help researchers explore the process of first language acquisition. Similarly, researchers can use data collected from children or adults acquiring their second language to reveal the stages and difficulties they face during the second language acquisition process. CHILDES (Child Language Data Exchange System) can be a good corpus for researchers focusing on acquisition.

Learner Corpora

The importance of learner corpora is also understood thanks to corpusbased studies related to language acquisition. For this reason, some corpora such as ICLE and Longman's Learner Corpus have been created to understand the systems of language learners and reveal their interlanguage systems.

Language Pedagogy

Learner corpus is used to find out the strengths and weaknesses of the language learners. The results received from a study based on learner corpus can be used for educational purposes. As researchers can understand the needs of students thanks to such studies, they can review the curriculum, materials, and even teaching methods thanks to learner corpus-based studies.

Even though it is thought that corpus is quite helpful in language teaching, scholars still do not fully come to an agreement on this issue. As indi-cated by

Types of Corpora

Depending on the purpose of the study, corpus type will change. Some of the types of corpora are defined below.

A specialized corpus is a corpus type represented by a collection of texts compiled from a particular genre (newspaper articles, agreement letters, academic articles, lectures, essays, etc.). Specialized corpora should serve the needs of a researcher who plans to study a specific genre in their study. In addition to the genre-based specification, a researcher may restrict the corpus to a time frame, a social setting, or a given topic. For example, the Air Traffic Control Speech Corpus and the Corpus of Early Modern English Tracts are specialized corpora.

A general corpus comprises texts represented by various types, including written or spoken language. Compared to a specialized corpus, a general corpus is usually much larger. Since it can be used to produce reference materials, it is sometimes called a reference corpus. The Corpus of American Contemporary English is an example of a general corpus.

Comparable corpora consist of two or more sub-corpora complied from different languages or varieties of a particular language. They are designed to contain the same proportion of texts (i.e., newspaper texts, essays, novels, conversations, etc.). Translators and learners can use comparable corpora to figure out similarities and differences in each language. International Corpus of English created for comparing the use of English throughout the words is an example of comparable corpora.

Parallel corpora should consist of at least two sub-corpora compiled from different languages, including source and target texts or texts produced simultaneously in two or more languages (e.g., EU texts). Parallel corpora can be used by translators and learners to find potential equivalents in each language and to investigate differences between languages. For instance, English-Norwegian Parallel Corpus was created for contrastive analysis.

A learner corpus is a collection of texts from learners of a particular language. Researchers can use learner corpora to focus on various aspects of learner language, such as differences among learners, frequency and type of errors, etc. This corpus type might be helpful in foreign language learning studies and language pedagogy.

A historical or diachronic corpus is a collection of texts from different periods. It helps to trace the development and change of a language over time.

A monitor corpus is a corpus allowing researchers to track current changes synchronically in a particular language. It rapidly increases in size since it is added annually, monthly, daily, etc. The proportion of text types has to remain constant so that each year is comparable with every other.

Balanced or representative corpus consists of texts selected in predefined proportions to mirror a particular language or language variety. If a corpus is divided into word samples representing different types (or genres) of the language, it can be labeled as a 'balanced' or 'representative' corpus. Brown Corpus and British National Corpus can be accepted as balanced corpora.

Collecting and Computerizing Data

As corpora should be created by compiling written and/or spoken sample texts in a pre-determined way, the researcher needs to plan exactly what will be included in the corpus carefully. Also, researchers should choose the kinds and amounts of texts and from whom they will collect data according to the criteria set before compiling corpora. However, the researcher should also be ready for changes during the course of data collection since the process of text compilation may not be as smooth as expected. For this reason, the data collection process changes must be accepted as natural and inevitable. The researcher needs to be flexible in terms of restructuring the corpus creation process.

While collecting spoken and written language samples, researchers need to be ready for numerous obstacles and complications. As

People generally prefer communicating via speech. Many types of speeches range from spontaneous daily conversations to radio and television interviews. Due to this variety and the logistical difficulties that might be faced while recording speech samples, it is accepted that collecting samples of speech is much more complex than collecting samples of written language.

The biggest issue regarding the collection of speech samples is the naturalness of the speech. The issue of natural speech collection becomes more critical when the aim is to collect spontaneous multi-party dialogues. Such natural speech samples should be collected carefully. Otherwise, the results of the recordings will show that the speech is not natural. While recording the participants, researchers should also ensure that interlocutors are not affected by the presence of such devices. Hence, researchers need to ensure that the interlocutors do not feel threatened to prevent the effects of being observed or recorded by an outsider. Once they do not feel comfortable due to knowing that they are recorded, they will adjust their speech because of the 'observer's paradox'. To avoid this, for example, London Lund Corpus was created through the recordings of the participants who were not informed before the recording process. Even though the speech will be quite natural in this way, most linguists criticize collecting speech samples without informing the interlocutors before starting to record their voices. It is also emphasized that researchers should take both verbal and written consent from the participants before collecting data.

However, it should also be kept in mind that it might be almost impossible to record natural language samples when the participants are informed before they are recorded. To avoid this,

As for collecting writing samples, we should accept that it is less complicated when compared to collecting speech samples. The first thing that needs to be taken into account is the copyright issue. If a written text is planned to be used in a corpus, researchers should take the required permissions from the author of the text, which will prevent copyright issues that might be faced in the future. Even though the authors allow researchers to use the text in their corpus, some of the owners of the writing texts may not allow researchers to share the corpus with other researchers worldwide. For example, English -Norwegian Parallel Corpus is not available online. If you want to conduct research with this corpus, you have to go to the University of Oslo to reach the corpus.

In addition to copyright issues, digitalizing handwriting is another problem with collecting written language samples. Even though software programs are helping you scan and transfer the writing into a digital platform, these programs, such as OCR, do not transfer the information correctly. For this reason, the researcher has to deal with a vast amount of editing. Due to this fact, it is suggested that using electronic texts in a corpus may facilitate the researcher's duty.

Computerizing Data

Computerizing the data is the labor-intensive part of compiling corpora. After collecting raw data, researchers need to transcribe speech which is an extremely lengthy process as it requires the transcriber to listen to the same segments many times to reach a proper transcription. Even though computerizing written texts seems more manageable as the written data could be transferred to a computerized format by scanning, there will be many scanning errors that the researchers should correct manually. For this reason, most researchers generally give up scanning and typing the texts as it is not as time-consuming as editing the scanned written texts

While computerizing data, researchers should bear some criteria in mind. They should keep the speech transcriptions and the written data in ASCII (text) format as it is the standard format used in corpus linguistics. Also, researchers can use ASCII files in parsers, concordance programs, and taggers. Similar to ASCII files, researchers have been using Text Encoding Initiative (TEI) as this system developed a formalism used for identifying the specific character set used in a given electronic format.

Even though the basics of computerizing spoken and written data are the same, depending on the type of data (speech or written), the methods used for computerizing differ. While transcribing speech, for example, the researcher has to use transcriptors such as Sound Scriber, VoiceWalker 2.0, and Backbone Transcriptor. Such programs help the researcher control the recording and the transcription on the same page. In addition, they offer some shortcuts that might help the researcher add labels to specific speech events. Also, the recording could be slowed down, re-listened, or stopped by using some buttons on the keyboard. They also include transcription conventions and help researchers add pre-set formulas to the transcription. For example, the Backbone Transcriptor includes the transcription conventions given below; <unclear/> indicates that the researcher could not recognize what had been uttered. <unclear>word</unclear> indicates that the researcher could not exactly recognize the recording but has a guess about it. <trunc>word</trunc> indicates that a word has been truncated. <foreign>word</foreign> indicates that the speaker uses a foreign word. This tag shows that the speaker employs a 'code-switch'.

( ) indicates that the researcher has written a comment about the recording.

# shows the section boundaries.

<break/> indicates that the speaker stops syntactic structure and starts a new utterance.

These transcription conventions can be helpful when the data are transferred to annotators.

Annotating Corpora

Once the data are computerized, researchers should annotate the data according to the needs of the study. The first step of annotation is called the structural markup, which gives descriptive information about the text; thus, structural markup is used to create the text's metadata. Most corpora use Standard Generalized Markup Language (SGML), Text Encoding Initiative (TEI), or Extensible Markup Language (XML) to have a unified structural markup system. Among these systems, XML systems are used frequently since they include both SGML and TEI.

While annotating the data, a label is attached to each linguistics item indicating its grammatical class or part of speech. Even though researchers can do tagging manually, it is now possible to train computer programs to tag utterances or sentences automatically. Parsing can also be used while annotating a corpus, done through grammatical markup inserted by a software program called a parser that automatically assigns labels to forms beyond word level (phrase, clause, etc.). Stanford Parser, for example, is widely used for the syntactic level of analyses of the corpus. Similar to taggers, once the parsers are trained, they automatically annotate the text for you. In addition to grammatical tagging and parsing, it is also possible to do semantic, discourse, and problem-oriented tagging.

Analyzing Corpora

Once the corpus is created and annotated, the most crucial part will be using the corpus for analysis. To use the corpus for analysis, the researcher needs to frame a research question that will help the researcher set the parameters of the process of the corpus analysis. After deciding on the research question(s), the researcher should test the corpus to determine whether it can answer particular research questions. If the corpus is not suitable for the research questions, the corpus should be changed or retagged by the researcher. After testing the appropriateness of the corpus for the research, the researcher needs to find suitable software tools to conduct the study, code the results and then analyze them through appropriate statistical tests. For example, if the purpose is to study the use of specific collo-cations, lemmas, or n-grams, the researcher can use concordance programs such as Wordsmith or AntConc. Then, the researcher should transfer the results obtained through the concordance programs to statistical software programs such as Excel or SPSS. Depending on the purpose of the study, the researcher should run appropriate statistical tests and analyze the results accordingly.

Conclusion

Corpus linguistics is a field within the study of general linguistics, and it can be accepted as a methodology rather than a paradigm within the area of linguistics. The corpus studies are crucial since they either test the validity of a language theory or hypothesis or help researchers create a language theory based on corpus analysis. Even though many people assert that corpus-based studies are essential in explaining the actual use of language, it should be kept in mind that corpus studies are challenging and require a lot of time and energy.
Preface

Since the 1990s, linguistics has progressively experienced a fundamental methodological turning point. Following the works of American linguist Noam Chomsky, it changed from the essentially rationalist discipline it had been since the middle of the 20th Century, and gradually (re)opened up the empirical approaches represented by corpus linguistics and experimental linguistics. Over the past decade, this transition has accelerated even more, in such a way that the majority of linguistic works published in international journals currently make use of empirical data. Thus, linguistic corpora have gradually established themselves as fundamental tools for linguists, and their use has spread to other fields in linguistics, including those traditionally favoring a rationalist approach, such as syntax. The development of corpus linguistics has led to the creation of new methods for collecting and analyzing linguistic data, which were made possible thanks to the development of computers and the arrival of the Internet. This new direction in linguistics has encouraged spectacular advances for dealing with the multiple facets of human language in all its complexity from a scientific perspective. Our book intends to introduce such a wealth to readers who are not particularly used to reading linguistics-oriented literature.

In our times, the ability to quantitatively analyze corpus data has become an integral part of the linguist's toolbox. Nevertheless, the use of such data is based on precise theoretical and methodological principles, which require a thorough understanding. This turning point in linguistics implies the need to introduce the new generations of students to the use of these methods which will help them understand the issues underlying their use in scientific literature, to critically assess the results obtained, and to use them in the context of their academic work. Our book is intended as an educational support for students and, in general, for all those wishing to learn the use of corpora in linguistics.

The material introduced in this book does not presuppose prior skills other than basic linguistic knowledge, as well as a minimum command of the most common computer tools, such as spreadsheet software. This book has been designed as study material for teaching corpus linguistics at university initiatory phases, as well as a tool for students wishing to be trained in the use of corpora. Students will be able to work independently thanks the revision questions presented at the end of each chapter, and the detailed answers provided.

As it is an introductory work, this book is necessarily partial and does not deal with all the questions raised by the use of corpora in different linguistic disciplines. It does not cover certain advanced analysis methods which require a high level of computer and statistical skills for data analysis. However, further readings are suggested at the end of each chapter that will enable those who wish to deepen one or other of the aspects presented to go a step beyond.

Finally, this book places a special emphasis on French as an object of study. While it is true that corpus linguistics has imposed itself in an incontestable manner in the English-speaking world and that a significant proportion of French-speaking researchers currently use these methods, the teaching of corpus linguistics still remains marginalized in France. Therefore, this book also aims to highlight the vitality and richness of corpus studies devoted to French, as well as identify the most important resources which have been developed for this language, in the hope of making a contribution to the rise of this discipline for the study of French.

Defining elements

The term corpus has a Latin origin and means "body". A text corpus literally embodies a set of texts, a collection of a certain number of texts for study. For example, it is possible to collect a series of newspaper articles and make a corpus of them in order to study the specificities of the journalistic genre. In the field of language teaching, it is also possible to collect texts written by students having different levels, and to build a corpus of these writings in order to study the typical errors that students produce at different learning stages. A methodology using data from the outside world rather than using one's own knowledge of the language is called an empirical methodology. Corpus linguistics can be defined as an empirical discipline par excellence, since it aims to draw conclusions based on the analysis of external data, rather than on the linguistic knowledge pertaining to researchers.

Working with corpus linguistics therefore implies being in contact with linguistic data in the form of texts, and also in the form of recordings, videos or any other sample containing language. Most of the time, these samples are collected in a computerized format, which makes it possible to study them more effectively than if they were on paper. Let us imagine, for example, we wish to know how many times and in what passages Flaubert evokes the feeling of love in his novel Madame Bovary. If we have a paper version of that book, finding these passages will be a long and tedious task, which will require going through the entire text. However, having a computerized version would make the task much easier. We simply need to look up for the terms love, in love or the verb to love in its different forms with the search function of the word processor so as to locate the appearances and easily count them. For most of the questions addressed by corpus linguistics, it would be impossible to search through a paper database, and that is why having computerized corpora becomes essential.

The problem of manual tracking and counting of occurrences is all the more acute since corpus linguistics is often based on large amounts of data which have not been drawn from a single book, in view of observing the multiple occurrences of a certain linguistic phenomenon and thus apprehending its specificities. For example, let us suppose that we wish to know whether Flaubert talks about love in his work. In this case, focusing solely on Madame Bovary would induce a bias, because this novel is not representative of the whole of his work. So, in order to be able to answer this question, it is necessary to go through the entirety of his novels, making the task even more complex to perform manually. Let us now imagine that this time we want to know whether the French authors of the 19th Century all deal with the question of love as much as Flaubert does. In this case, it would be impossible for us to look up the occurrence of terms related to love in all of the novels written by French authors in the 19th Century. In order to avoid this problem, it would be necessary to collect a sample of texts, representative of the works of this period. We will discuss this topic in Chapter 6, which is devoted to the methodological principles underlying the construction of a corpus. For the moment, the important point to bear in mind is that corpus linguistics often resorts to a quantitative methodology (see section 1.5) so as to be able to generalize the conclusions observed on the basis of a linguistic sample to the whole of the language, or belonging to a particular language register. As we will see in the following chapters, corpus linguistics may be of use in all areas of linguistics, for instance in fundamental (see Chapter 2) or applied (see Chapter 3) linguistics. For example, it is crucial in lexicography, since it makes it possible to make an exhaustive inventory of a language's lexicon. It also makes it easy to find examples of uses in different types of sources (literary, journalistic and others), while bringing to light the expressions in which a word is frequently used. In other words, it makes it possible to establish very useful phraseology elements for dictionaries. For example, it is useful to know what the word "knowledge" means, but it is just as important to know that this word is frequently used in phrases such as "acquire knowledge" or "having good knowledge of", etc. Corpus linguistics is a particularly effective method for establishing the frequent contexts in which a word or an expression is used. But corpus linguistics is also used for conducting research in fundamental areas of linguistics such as the study of syntax, since it makes it possible to identify the types of syntactic structures used in different languages. For example, by making a corpus study, it is possible to determine in which textual genres the passive voice is most commonly used. Finally, thanks to the existence of a corpus of oral data, corpus linguistics also makes it possible to answer questions related to phonology and sociolinguistics. For instance, it makes it possible to establish the area of geographical distribution of certain pronunciation traits, such as differentiating the short /a/ form in the French word "patte" (paw), from the long /ɑ/ form in the word "pâte" (pastry). Answering these different questions requires the use of different types of corpora, as well as having available data regarding their contents. For example, in order to determine the geographical area of diffusion of a certain pronunciation trait, it is necessary to know where each speaker having contributed to the corpus came from. This type of information is called corpus metadata. We will review the main types of existing corpora at the end of this chapter, and discuss the issue of metadata in Chapter 6.

To sum up, in this section, we have defined corpus linguistics as an empirical discipline, which observes and analyzes quantitative language samples gathered in a computerized format. In the following sections, we will discuss in depth the different central points of the definition, indicated in bold, in order to better understand the theoretical and methodological anchoring of corpus linguistics.

Empiricism versus rationalism in linguistics

Corpus linguistics is an empirical discipline, which means that it uses data produced by speakers in order to study language. This methodology is opposed to the rationalist method, which functions by looking for answers by relying on one's own linguistic knowledge, rather than looking for it in external data. Let us take an example. In order to determine whether the phrase "When do you think he will prepare which cake?" is grammatically correct or not, the use of empirical methodology would go through large corpora to find whether this syntactic structure is used by English speakers or not.

If sentences following such a syntactic structure never or almost never appear in the corpus, linguists might conclude that this sentence is only rarely used in English. Rationalist methodology, on the contrary, might respond to the same issue by relying on the intuitions of linguists. In this particular case, they might wonder whether they could produce such a sentence or not, whether it seems correct or incorrect depending on their knowledge of the language and might infer a grammaticality judgment from it. Grammaticality judgments are often classified into three types: correct, incorrect or marked, in the event that a sentence may seem possible, but sounds unnatural. This example illustrates a fundamental difference between empirical and rationalist methodology. While the rationalist methodology leads to the formulation of categorical judgments, the empirical methodology provides a more refined answer to this question, since the observation of corpus data offers a precise indication of frequency, rather than a result in terms of absence or presence. This is one of the reasons why many linguists currently consider that the empirical methodology better matches a scientific approach (in the sense of confrontation against the facts) than a purely rationalist method for studying language.

Nonetheless, the choice between the use of empirical or rationalist methods is not limited to the field of linguistics. Certain scientific branches such as physics, chemistry, as well as sociology and history are essentially empirical disciplines. In fact, both physicists and historians base their insights on external data, which they collect in the world, in order to build a theory, test it and draw conclusions from it. On the other hand, other disciplines such as mathematics or philosophy are traditionally based on a rationalist approach, since mathematicians and philosophers use their own reasoning to build theories and to draw conclusions, rather than from the collection and observation of external data. Philosophers often resort to thought experiments, but these are not experiments in the empirical sense of the term, because they are based on the reflective abilities of researchers.

Chomsky's arguments against empiricism in linguistics

Although corpus linguistics has experienced a strong growth over the past 20 years, the empirical grounding of linguistics is not new. Linguists have long used observational data. In the 19th Century, for example, linguists used to work on the comparison of Indo-European languages in an attempt to reconstruct their common origin. Research was based on existing data about the languages spoken in Europe such as German, French and English. Similarly, in the first half of the 20th Century in the United States, the so-called distributionist approach to syntax focused on the study of sentence formation in syntactic structures as they appeared in text corpora, and from there, tried to infer language's general functioning. Around the late 1950s, the use of corpora in linguistics was almost completely interrupted in certain fields such as syntax, following the works of the American linguist Noam Chomsky. In fact, Chomsky defended a strictly rationalist methodological approach to linguistics, and fiercely opposed any use of external data. The objections made by Chomsky against the use of external data in linguistics have been numerous. We will briefly review them, to show in what ways most of them have lost their raison d'être in the context of current research.

Chomsky's first objection to the use of corpora, which is also the most fundamental one, is that corpora contain language samples produced by speakers. According to him, linguistics should not focus on the linguistic performance of speakers, but on the competence they have in their mother tongue, something he calls their internal language. Now, here is the problem. When people speak, what they produce (their performance) does not necessarily reflect what they know about their language (their competence). For example, under the effect of stress or fatigue, speakers sometimes produce verbal slip-ups or make language mistakes. From time to time, almost everybody happens to badly conjugate an irregular verb and mistakenly produce the form "he eated" instead of "he ate". However, if the person who produced this wrong form were recorded, and then asked whether he or she thought he or she had spoken correctly or not, we can almost be sure that he or she would realize his or her mistake and would be able to state the correct form, "he ate". Conversely, a speaker could pronounce a word like "serendipity" after having heard it from somebody else's lips, but without really knowing its meaning. These examples illustrate the fact that the words speakers "utter" are not always a true reflection of their linguistic competence. In this way, according to Chomsky, the fact of studying corpora places linguists on the wrong track, because they lead them to consider language from the point of view of "production", which merely represents a biased reflection of the rules of language.

According to Chomsky, another problem related to corpus linguistics stems from the fact that corpora are not representative of the language as a whole. He illustrates this problem in an extreme way, by picking the case of an aphasic speaker recorded in a corpus. Linguists analyzing this corpus would draw totally incorrect conclusions about the language in question, since this person does not represent the linguistic competence of a typical speaker. Furthermore, even if we were not to include an atypical speaker, a corpus could never represent more than a tiny language sample when compared to all the oral and written productions in any language. It is for this very same reason that it is impossible to conclude that a word simply does not exist in a language just because it is absent from a corpus. It could simply never have been pronounced in such particular context, while it could exist in other language registers or have been mentioned by other speakers not included in the corpus. This problem is particularly acute in the case of rare linguistic phenomena, such as infrequent words or little used linguistic structures.

This limitation has led to Chomsky's third criticism of corpora, namely the fact that a corpus can never contain the whole of a language and that, therefore, the above-mentioned biases are not solvable. According to him, this problem is all the more serious because even if a corpus were of a very large size and included a representative portion of the language, it would not be fully analyzable by linguists, given the fact that it is impossible to manually analyze the content of billions of sentences.

Chomsky's last two objections have largely become obsolete due to the advances made in computer science. In fact, the size of corpora has increased exponentially over the past 20 years, and corpus analysis tools have also made considerable progress. It has thus become possible to analyze very large amounts of data, which represent a much more accurate mirror of the language than when Chomsky formulated his objections. We will return to this in section 1.4, devoted to the connections between computer science and corpus linguistics. In addition to these technological advances, theoretical and methodological advances have also largely made it possible to eliminate or control the other types of biases mentioned by Chomsky. For example, good practice for building a corpus is to accurately document the type of language it contains. This helps to avoid analyzing the language of a single aphasic subject by mistake, for example, as Chomsky might suggest. It is nonetheless true that a corpus can only show that which it contains, and therefore the absence of evidence that a word or a structure exists in a corpus cannot constitute definitive proof of their absence from the language. Thus, for certain research questions relating to rare or hardly observable phenomena in a corpus, it might be advisable to complement research with another empirical method, namely with the experimental method. As we will see later in this chapter, this method shares the use of a quantitative methodology with corpus linguistics.

In conclusion, we should point out that the rationalist method suggested by Chomsky is also accompanied by biases and limitations which are not negligible and can be corrected by the use of empirical methods. In particular, this method leaves a large space for the subjectivity of linguists while it overestimates the linguistic skills of speakers. Indeed, the use of grammaticality judgments presupposes that all speakers have a definite and consistent intuition regarding all the sentences in their mother tongue. However, such is not the case. If all English speakers agree that a sentence like "Mary dog her walks" is incorrect in English, whereas the sentence "Mary walks her dog" is correct, judgment will not be so unanimous in the case of complex sentences, as the one mentioned above: "When do you think he will prepare which cake?". These divergences become problematic as soon as these judgments are used for building a linguistic theory. What is more, while it is likely that many English speakers would reject a sentence such as "He does be working" for being grammatically incorrect, in certain areas of the English-speaking world (such as Ireland), this sentence would be acceptable. By resorting to many different speakers and including them in reference corpora of speakers coming from different geographical areas, corpus linguistics makes it possible to respond to this problem in a much more satisfactory way.

What is more, in many areas of linguistics such as lexicology, language acquisition and sociolinguistics, the idea of relying on the internal judgments of linguists is simply not conceivable. No one can study children's language by remembering how he or she spoke as a child, or make assumptions about language differences between men and women by imagining how he or she would speak if he/she were a man or a woman. In all these fields, the use of text corpora has been obvious for a long time and corpora use was never interrupted as a result of Chomsky's work. The paradigm shift in recent decades has taken place in areas where it is conceivable to use a purely rationalist methodology, for example syntax.

Finally, it is important to remember that the role of linguistic theory and the intuition of researchers is not absent in most corpora studies. Indeed, a majority of linguists consider corpora studies as a tool, making it possible to validate or invalidate hypotheses on language, formulated in advance, on the basis of scientific literature and their linguistic intuitions. We will see many examples of this approach (empirical validation) throughout this book. This corpus-based research approach is opposed to an approach which considers corpus data as the only point of reference, both in a theoretical and a methodological sense. In this approach, linguists begin their research without an a priori and simply let hypotheses emerge from corpus data (this is called a corpus-driven approach). This approach is almost unanimous among linguists working with an empirical methodology. On this point, we agree with Chomsky's metaphorically explained opinion where he states that working with linguistics in this way would be the equivalent for physicists of hoping to discover the physical laws of the universe by looking out of their window. Observing data without a hypothesis often leads to not being able to make sense of data. It is for this reason that the approach that we will adopt in this book corresponds to a corpus-based approach, considering these as available tools for linguists to be able to test their hypotheses.

Corpus linguistics and computer tools

As we have seen above, corpus linguistics, as performed nowadays, cannot do without computers. Even if works related to corpus linguistics have existed for a long time (such as the indexing of the Bible by theologians or the file-based construction of dictionaries by scholars like Antoine Furetière in French or Samuel Johnson in English), this discipline was only able to properly take off after the arrival of computing.

Corpus linguistics depends on computer science for various reasons. The first one, which we have already mentioned above, is related to the need for computerized texts in order to be able to carry out truly quantitative research. Nevertheless, looking for elements in a corpus, even a computerized one, by using a simple word processing tool is rather inconvenient. Going back to the example of the search for terms related to love in Flaubert, which we discussed earlier, we find that the use of the search function of a typical word processor quickly reaches its limits. First of all, in order to verify that all occurrences found when looking for the verb to love correspond to expressions of love as a feeling rather than to modal uses as in the phrase "I would love it that you kept quiet", it is necessary to examine each occurrence and thus browse the entire text. Second, to find all the occurrences of the verb to love, it is necessary to perform a different search for each verbal form, for example love, loved, etc. It is for this reason that other computing tools, specifically devoted to corpus linguistics, have been developed.

In particular, concordancers are useful for searching all the occurrences of a word, plus their context of use and for displaying the results line by line in a single query. These tools also make it possible to establish the list of words contained in the corpus, together with their frequency, and to generate a list of keywords matching the content of a corpus. In the case of corpora containing texts as well as their translation, certain tools called aligners make it possible to align the content of the corpus sentence by sentence. That being done, bilingual concordancers search directly for the occurrences of a word in one of the two languages of the corpus, and simultaneously extract the matching sentence in the other language. We will learn how to use these tools in Chapter 5, which is devoted to the presentation of the main French corpora, as well as the tools for analyzing them.

Then, in Chapter 7, we will also see that in order to answer certain research questions, it is necessary to annotate the content of a corpus. For example, let us imagine that we wish to study the different contexts in which we can use the causal adverb since. If we only look up the word since in the corpus, we will also find occurrences which do not correspond to the use of this word as a causal adverb, but to its use as a preposition, for example in "I haven't seen Mary since Christmas". So, to be able to correctly look up the uses of since we are interested in, we should only keep those which are adverbs and exclude prepositions. This search can be greatly simplified if the corpus has been annotated by determining, for each word, its grammatical category. This operation, called part-of-speech tagging, can be performed automatically by certain software.

Another problem might arise if we decide to study the use of relative phrases such as "the girl who is intelligent" or "the violin which was left on the bus". For this study, a good starting point would be to look for relative pronouns such as who or which in order to find occurrences of relative sentences in the corpus. The problem is that these pronouns are also used in interrogative sentences such as "Who do you prefer?" or "Which hat is yours?" In this case, looking for the grammatical category of the word will not solve the problem, because they are both pronouns. In order to find only the occurrences of who and which as relative pronouns, we should use a corpus in which the syntactic structure of each sentence has been analyzed in such a way that we can assign a grammatical function to each word and group them into syntactic constituents. Tools for analyzing the syntactic structure of sentences have also been developed in the context of works for automatic language processing. These automatic analyses still require human checks so as to avoid any form of error, but their performance is continually improving. The arrival of these tools has greatly accelerated research in corpus linguistics. We will discuss this issue in Chapter 7, which is devoted to annotations. But corpus linguistics was not only developed thanks to the creation of such tools. Above all, it is the general development of computers and the digital revolution which have made the greatest advances possible. In fact, the increase in the computing power of machines -as well as in their memory -has made it possible to build ever larger corpora. Until the 1980s, a corpus of a million words was considered to be a very large corpus. For instance, the first reference corpora (such as the Brown corpus developed for American English in the early 1960s) were about this size. At the same time, the arrival of cassette recorders to the market enabled the first creations of oral corpora containing an exact transcription of spoken speech, rather than a synthesis taken in shorthand.

The marketing of scanners in the 1980s later made it possible to digitize a significant amount of data and corpora began to reach larger sizes, up to 20 million words. Then, with the democratization of computer use, the amount of digitally disseminated texts greatly accelerated the growth of corpora.

Finally, since the beginning of 21st Century, the wide dissemination of documents online via the Internet has given another dimension to the size of corpora available to researchers. At present, the Google Books corpus, for example, contains more than 500 billion words, which represents approximately 4% of all the published books of all time

Quantitative versus qualitative methods

We have seen that computers help us to work on very large corpora and automatically count word occurrences, find keywords, etc. The need to use a large amount of data and the desire to quantify the presence of linguistic elements in a corpus corresponds to a quantitative research methodology. This methodology involves observing or manipulating variables, as well as the use of statistical tests. The main objective is to test a limited number of variables, in a highly controlled environment whenever possible and on a language sample that can be representative of the phenomenon studied. This can later make it possible to generalize the results obtained to the whole language or to a part of the target language (e.g. journalistic language). These methods nonetheless imply a certain form of reductionism and a simplification of reality. Ultimately, the addition of studies with well-defined and properly controlled variables may provide a global and realistic picture of a phenomenon. Let us take an example. Suppose we want to test the hypothesis that women talk more about their feelings than men. To test this hypothesis by means of a corpus study, we should first make sure that we are comparing records of men and women produced in the same context, for example, in the context of friendly discussions around a topic, or a face-to-face interview with a researcher. We will also need to make sure that the corpus collected in this way includes approximately the same speaking time or the same number of words pronounced by men and women. This control over the linguistic context and the duration of interactions helps us to ensure that men and women have had fairly equal motives to pronounce words related to emotions/feelings, and as many chances of doing so. Second, we would have to choose a list of words to search within the corpus, representative of the vocabulary related to emotions, for example verbs such as to annoy, adjectives like furious or nouns like anger. Then, by comparing the number of times these words have been produced by the two groups and by validating the significance of the differences observed between the groups through statistical tests, we would be able to provide an answer to the research question. In this study, we have sought to reduce the number of confounding variables by controlling the context of production of the statements, as well as by limiting the word choice in the examined vocabulary. It is precisely this limited and reductionist aspect that the opponents to quantitative methods criticize, thinking that the constructed and unnatural context in which structured interviews take place does not reflect the richness of natural and spontaneous exchanges between speakers.

The other major methodological paradigm includes so-called qualitative studies. The main objective of these studies is holistic: they aim to study a phenomenon understanding it as a whole, as detailed and as thoroughly as possible, but in a small number of people. Due to their nature, qualitative studies are interpretative. In linguistics, research paradigms involving a qualitative methodology typically resort to the administration of questionnaires with open questions, interviews, observations or introspective techniques, such as think-aloud protocols. For example, in order to study the differences in the way of expressing emotions between men and women, a qualitative methodology could involve asking a reduced number of speakers, for example three men and three women, to describe the way in which they express their emotions, either by talking freely with the experimenter or by talking to each other. The analysis would then require an in-depth study of some of the examples found interesting during the discussion.

One of the main criticisms aimed at qualitative methods is that they are very subjective in nature, insofar as they are largely based on the interpretations made by linguists and the subjective impressions of a few speakers. Thus, the specific cases they describe cannot often be generalized to a population, which, by the way, is not the aim pursued by such studies. Rather than the generalization of results, these studies are based on the possibility of making a transfer from a particular situation so as to understand another one with which it shares common traits. For example, an in-depth case study on the difficulties of expressing emotions in an aphasic patient may help to highlight similar difficulties existing in other patients with the same disorder.

To summarize, each of the two methodological paradigms introduced in this section has both advantages and disadvantages. Quantitative methods enable the generalization of results to the whole of a population, whereas qualitative methods offer a more detailed and nuanced panorama of a real case. Recently, the complementarity between these approaches has started to be broadly accepted in research and many studies are crossing the two types of methodologies, in order to benefit from their advantages and limit their disadvantages.

For example, if we want to know whether learners of French as a foreign language at an advanced level are able to use collocations as native speakers do (collocations such as "prendre une décision" -to make a decision -or "pleuvoir à verse" -to pour with rain), we can search for occurrences of these expressions in text corpora produced by learners and compare the number of times these expressions appear -and their frequency -in a corpus of similar textual productions made by native speakers. By comparing these frequencies through statistical tests, we will know whether learners actually use these expressions as often as native speakers do, or not. Even if we find a difference between the two groups, something which this study will not tell us is why learners do not use these expressions as often as native speakers do or which expressions they use instead. To find out, we can complete this study with a qualitative analysis, by observing, for example, which words often accompany the occurrences of the noun décision in French, which are not the verb prendre. If we observe that several times the verb used is faire (make), rather than prendre (take), a decision in English-speaking learners, but not in German-speaking learners, we will conclude that these errors could come from a problem of transfer from their mother tongue and, more specifically, from the expression to make a decision in English.

In summary, a corpus can be analyzed using a quantitative or qualitative methodology. While we acknowledge the use and importance of combining these two approaches, in the rest of the book we will focus on the quantitative approach to corpus linguistics, which poses its own theoretical and methodological challenges.

Differences between corpus linguistics and experimental linguistics

Corpus linguistics and experimental linguistics share very important methodological properties, since both are empirical in nature and both generally involve a quantitative rather than a qualitative approach. However, these two types of approaches differ in one very important point. On the one hand, corpus linguistics focuses on data observation as found in collections of texts, recordings, etc. On the other hand, experimental linguistics points to the manipulation of one or more variables in order to study their effect on other variables.

Let us imagine once again that we are interested in the types of language errors produced by learners of French. By means of a corpus study, we will be able to identify all the types of errors produced and then quantify each of them: for example, 30 spelling mistakes, 12 lexicon errors, 20 syntax mistakes, etc., made every 100 words. Then, by applying statistical tests, we will be able to determine whether one of the error categories is significantly more frequent than the others. We will also be able to compare the number of errors produced in each category by students of different levels and, thanks to statistical tests, determine whether students make significant progress faster in certain categories than in others. In contrast, what a corpus study will not help you to do is establish with certainty the factors influencing the number of errors. The corpus only shows you the result of the speakers' production, but not what led to these results. In order to determine the factors that lead learners to make mistakes or not, we will need to resort to experimental methodology.

When we conduct an experiment, the goal is to manipulate the possible causes and then to observe their effects. Going back to our example research question, we may wonder what makes some students produce more errors than others, and in certain contexts, what makes the same student produce more errors than in other contexts. As regards the difference between students, we may think that one possible cause is the level of general intelligence of each student, the assumption being that overall smarter students should produce fewer errors than less intelligent students. The level of intelligence thus constitutes the cause that we will manipulate in order to observe its effect on the number of errors produced. In order to measure the effect of the intelligence variable, we will first need to measure the students' intelligence, for example by means of an IQ test. We will then use the result of this test to determine whether the students who have a higher IQ are also the ones who make the fewest language errors.

In the case of the second research question, which seeks to determine why the same student makes more mistakes in certain contexts, we may assume that stress promotes the production of errors. In order to test this hypothesis, we will have to conduct an experiment in which half of the students are placed in a stressful situation such as an examination context or, for instance, a test with a limited amount of time to complete the task, whereas the other half of the students are placed in a low-stress situation, for example, without any time constraint, performing a task which does not involve marked assessment, etc. Then, we will compare the number of errors in the two groups so as to determine, by means of a statistical test, whether the students under a stressful situation make significantly more errors than the other students, or not. In the two examples of studies that we have just discussed, the approach is the same: to identify a possible cause and to assess its effect through experimental manipulation. Conversely, a corpus study focuses on linguistic productions without manipulating the data before collecting them.

The study of linguistic productions in a corpus and the manipulation of experimental variables both have their advantages and disadvantages. On the one hand, corpus linguistics has the advantage of favoring the observation of natural data, that is, those which are not influenced by an experimental context. A corpus of journalistic texts includes real productions by journalists, which are not produced for the purpose of being observed. Likewise, a text produced by a learner is also natural, insofar as it is produced in its usual conditions, without there having been any particular manipulation. In addition, the use of corpora favors the observation of a very large amount of linguistic data, whereas experiments are based on a limited number of linguistic items for the task to remain feasible for participants, who would not be able to read thousands of sentences at a laboratory, for example. Finally, once a corpus has been created, it can be used for numerous research questions without requiring any additional time or financial costs. On the other hand, experiments require significant time resources as well as the usual obligation of having to financially compensate participants for their cooperation.

Experimental studies also have definite advantages over corpus studies. The first advantage, mentioned above, is that experiments allow us to test the existence of a causal relationship between two variables, such as the fact of being stressed and producing more errors. Corpus studies do not make it possible to draw this type of conclusion. Second, while an experimental paradigm can be developed to test almost any kind of phenomenon, there are some rare linguistic phenomena which may be absent or too little represented in a corpus to be examined in this way. For example, if we want to decide whether learners are fluent in French idioms such as "mettre le feu aux poudres" (to stir up a hornet's nest) or "avoir un poil dans la main" (to be extremely lazy) through a corpus study, we will have to look for them in a corpus of learners' productions. Now, it is quite possible that these expressions are never found there, but this does not necessarily mean that the learners do not know how to use them. It only means that they did not have an opportunity to produce them in the corpus. Using experimental methodology, we will be able to test whether learners have mastered these expressions. For instance, we can encourage them to read the expressions and then ask them to choose, from among several definitions, the one corresponding to their meaning. Finally, experimental linguistics makes it possible to study the linguistic competence of speakers, through different language comprehension tasks which can be more or less explicit or implicit, such as the conscious evaluation of sentences, their intuitive reading, etc. Corpora can only reflect the linguistic productions of speakers.

To conclude, corpus studies and experimental studies can often be used in a complementary way, and, when put together, they represent powerful tools for answering a good number of research questions.

Different types of corpora

As we will see in the following chapters, corpora represent linguistic samples of a very varied nature, and it is precisely this variety that makes it possible to answer diverse research questions in all fields of linguistics. In this last section, we will introduce a first classification of the types of existing corpora, in order to be able to refer back to it in the following chapters.

The first distinction we can make among all the existing corpora is the one that classifies them into a sample corpus and a monitor corpus. Sample corpora are those in which data have been collected once and for all, and which no longer evolve thereafter. For this reason, they are also known as closed corpora in the specialized literature. The advantage of these corpora is that they have been designed to contain a set of texts representative of the language, or a part of the language to be studied, with a balanced representation of the different text genres, for example. Thus, these corpora make it possible to draw conclusions which can be generalized. On the other hand, their main defect is that they age quickly and do not follow changes in the language. Therefore, sample corpora need to be recollected at regular intervals.

On the other hand, monitor corpora are never finished and constantly continue to integrate new elements, which is why they are described as open corpora in the literature. A typical example of this type of data is the corpus that contains newspaper archives or parliamentary debates. Every year, the number of available data increases. It is for this reason that it is difficult to maintain a perfect balance between the different parts of these corpora, whose representativeness cannot be fully guaranteed. We will return to the problem of representativeness in Chapter 6. On the other hand, these corpora remain up to date. In cases where they comprise a period of a few decades, they make it possible to observe the appearance of certain changes in language.

The second major distinction to be made among existing corpora differentiates general language corpora from specialized language corpora. General language corpora aim to offer a panorama of the whole of a language at a given time. It is evidently impossible to collect a sample of the whole language, but in the same way that a general language dictionary aims to describe the common lexicon of a language, the general corpus seeks to offer a global image, including the main textual genres found in language. These corpora are really valuable when it comes to studying a language as a whole, but they cannot offer precise answers on linguistic phenomena present in certain specific communication means, such as mobile texting, social media, medical reports, etc.

In order to study one of these areas specifically, it is preferable to resort to a specialized corpus. In fact, there are corpora especially devoted to texting, social media, etc. In addition, general corpora include productions by adults who are native speakers of the language represented. Other corpora specialize in representing other population categories, regardless of whether they are monolingual children in the process of acquiring their mother tongue, bilingual children, foreign-language learners, or even children with neuro-developmental disorders influencing language acquisition, such as autism and specific language impairment. Finally, by default, a general corpus includes examples of the variety considered as a language standard, or one of its main varieties. In French, it generally refers to the French language from France and, more precisely, from the Parisian region. In English, general corpora can refer to the English language from the UK or to American English. Conversely, some corpora specialize in the productions of speakers of a certain language variety, such as French from Frenchspeaking Switzerland, Belgium, Canada, etc.

General or specialized language corpora can contain either written language or spoken language samples. For a long time, written language corpora were the norm, but analysis of the spoken language has developed broadly since the 2000s. Corpora of spoken language are typically of smaller size than written language ones, since they require manual transcription. As a matter of fact, it is easy to record voices, but what is difficult is to carry out searches directly on an audio file. At the same time, speech recognition software does not always fully allow reliable automatic transcriptions. It is for this reason that the oral data must be transcribed manually, which often limits the size of the spoken corpora. More recently, audio-visual recording corpora (also called "multimodal" corpora) have been created, in order to facilitate, for instance, the study of gestures and facial expressions as well as their role in communication. These corpora still pose many codification and interpretation challenges. Finally, let us point out that video corpora are also used for the study of sign language.

Another distinction that can be made regarding the types of existing corpora relates to the type of processing carried out on the linguistic data of the corpus. On the one hand, raw corpora contain nothing but language samples. This scenario represents the majority of the French corpora. On the other hand, some annotated corpora contain specific linguistic information, apart from the language samples. The most common type of annotation is the assignment of a grammatical category to each word in the corpus, as we have already mentioned. More rarely, certain corpora contain a syntactic analysis of all of their sentences, as well as other types of information, such as an annotation of the discourse relations (cause, condition, etc.) which interconnect the sentences within the text corpora. Finally, certain corpora, which have been transcribed with the aim of studying phonological phenomena, may end up being transcribed using the International Phonetic Alphabet.

So far, all the types of corpora we have considered are monolingual. Another distinction that we can make is to differentiate these corpora from multilingual corpora. There are two types of multilingual corpora. On the one hand, we have comparable corpora, which contain similar samples produced by native speakers in two or more languages. For example, it is possible to build a comparable corpus of parliamentary debates in France and the UK. Such a corpus would make it possible to compare the ways of speaking in a similar context in two languages and two different cultures. On the other hand, so-called parallel corpora contain texts produced in one language and their translation into one or more other languages. These corpora make it possible to study the linguistic correspondences between languages, as well as the linguistic phenomena linked to the translation process. Parallel corpora can also be annotated with exact matches between sentences. This process is called alignment and gives rise to so-called aligned corpora.

Finally, many corpora are drawn from contemporary written or spoken data. However, there are archives that make it possible to study the history of a language, going back to ancient French, for example. Contemporary corpora are used for studying language in a synchronic way, that is, at a given moment during its evolution, whereas historical corpora make it possible to carry out studies from a diachronic point of view, that is, on the evolution of language.

Conclusion

In this chapter, we have defined corpus linguistics as an empirical discipline, that is, based on the observation of real data. We have also seen that corpus linguistics often resorts to a quantitative methodology, studying a large sample of data which is representative of the phenomenon studied, with the aim of generalizing the observations to the whole of the language or to a language's register. We have shown that the main difference between corpus linguistics and experimental linguistics is the way in which empirical data are collected. In the case of corpus linguistics, data are collected in a natural context and then observed, whereas in the case of experimental linguistics, one or more causes are manipulated within a controlled context in order to observe their effects. Finally, we have seen that corpora can be very diverse in nature, depending on whether they are made up once and for all or incremental, general or specialized, annotated or not, monolingual or multilingual, synchronous or diachronic.

1.9. Revision questions and answer key 1.9.1. Questions 1) Which of the following disciplines traditionally involves a rationalist methodology, and which disciplines are based on an empirical methodology? Can we think of any situation in which a discipline of a rather empirical nature could have recourse to a rationalist methodology and vice versa? chemistry -ethics -medicine -law -anthropology 2) Among Chomsky's objections to corpus linguistics, which of them can also be applied to the experimental methodology?

3) In the research projects mentioned below, which one seems to use corpora as a methodological tool (corpus-based) and which seems to use corpora as a theoretical tool (corpus-driven)? a) Search in a corpus for all passive voice sentences in order to formulate the rules governing the use of this construction in French. b) Search in a corpus for all passive voice sentences in order to determine whether they are used more with state verbs than with activity verbs. 4) Why have computing tools especially devoted to corpus linguistics been developed? What are their main functions? 5) Look for an example of a quantitative study and another qualitative study that could be done so as to determine the most common types of spelling mistakes made by children. Which would be the specific contributions of each of these studies? 6) How could we use a corpus and carry out an experiment to study the question of the different types of spelling errors in a complementary way? 1.9.2. Answer key 1) First of all, let us recall that the rationalist methodology interrogates the knowledge of the researcher by means of introspection and reasoning, whereas the empirical methodology looks for answers by observing or experimenting on data that is external to the researcher. Chemistry is typically an empirical science, which makes extensive use of experimentation and observation. Ethics is a philosophical discipline that involves reflections on moral questions. These reflections are, by nature, introspective and involve a rationalist methodology. Law is a science that studies the rules and laws that govern social relationships. Many aspects of the law involve the interpretation of existing rules or the creation of rules based on reasoning and common sense. Thus, introspection plays a big role. That being said, in certain cases, law also deals with external data. For example, a search can be performed throughout previous decisions (case law), in order to find a similar case that could apply to a certain situation. The role of case law is very different in different legal systems. In Englishspeaking countries, which apply the common law, previous cases play a fundamental role, because they become binding rules for solving the following cases. We can therefore say that in these countries, the part of empiricism when applying the law is also very important. Anthropology is a discipline that studies humanity in its various aspects

2) Chomsky notably criticized corpus linguistics for offering only a partial vision of language, insofar as a corpus includes the productions of a limited number of speakers, at a given situation. This same observation also applies to the experimental methodology, which tests a small number of speakers along a limited number of linguistic stimuli. The main response to such criticism is that these areas are based on the use of quantitative methods (namely inferential statistics), which make it possible to draw conclusions from a sample and to extrapolate them to an entire population. The criticism of the potentially problematic choice of subjects who could be aphasic and not represent the normal use of language also applies to experimental methodology. In theory, though, such subjects could also be recruited for an experiment by mistake. That being said, good practices in corpus linguistics and experimental linguistics require obtaining information about participants beforehand, which can eventually eliminate this type of bias. Typically, researchers verify that the people who contribute to a French corpus are native French speakers. Likewise, they test the language skills of speakers before considering them by default as French-speaking, bilingual, etc.

3) a) This type of research is corpus-driven, because the starting point is not hypotheses which have to be verified throughout the corpus. The starting point for research is the corpus itself, in order to be able to infer usage rules from its content. b) On the other hand, this type of research is corpus-based, because it starts from a hypothesis (e.g. "passive sentences tend to be used more frequently with state verbs"), and seeks to verify it in the corpus, which, in that way, only works as an analysis tool.

4) These tools have been developed for simplifying searches within a corpus. Otherwise, it would be very inconvenient to use the standard tools that are present in a word processor, for example. In particular, concordancers make it very easy to extract all the occurrences of a word or an expression with its left and right context, as well as to determine its main collocations. These tools also help us create a list of all the words in the corpus, sorted by frequency. While one corpus can be compared to another reference corpus, these tools also make it possible to extract a list of keywords that are specific to the corpus studied. In the field of multilingual corpora, aligners make it possible to align parallel corpora sentence by sentence, and then to extract a sentence and its translation by means of a bilingual concordancer.

5) A quantitative study on this question could focus on the creation of categories for classifying spelling mistakes, for example, agreement errors, redoubling of consonants, dumb letters, etc., and then counting all the occurrences of errors belonging to each category. By applying a statistical test, this study would then make it possible to know whether students tend to make certain types of mistakes more often (e.g. grammatical errors) rather than other mistakes (e.g. lexicon errors). A qualitative study on this same question would identify some examples of spelling mistakes and analyze in detail the contexts in which they occur, for example the grammatical category of the words concerned, whether they are rare or frequent words, occurring in a long or a short sentence, what type of phonemes is poorly transcribed, etc. This study would make it possible to identify linguistic contexts that tend to be conducive to spelling mistakes.

6) The results of the quantitative corpus study summarized above, namely the quantification of the different types of spelling mistakes, could be considered as a kick off for an experimental study. For example, the corpus study could help identify one type of common error, and one type of rare mistake. An experiment could then help to determine whether being in a stressful situation or not has a different impact on the two types of error.

7) a)

In order to study a phonological phenomenon like this, a spoken corpus is essential. This corpus should be specific to the population of French-speaking Switzerland. A large type of corpus comprising a large number of different speakers would be desirable. Finally, this corpus should contain a synchronic type of data, corresponding to the current pronunciation, rather than to its diachronic evolution. b) In order to study the evolution of a language, a diachronic corpus is essential. This constraint implies the use of written data, since oral data only go back to the middle of the 20th Century. Finally, the chosen corpus should include productions made by adult native speakers. c) In order to study translation, a parallel corpus is required. This corpus should contain original texts in French and their translation in English. It should be a synchronic corpus, corresponding to current uses of the language.

Further reading

For a discussion regarding the main defining elements of corpus linguistics, the works by

The corpora containing spontaneous interactions in a conversational context have also made it possible to study the various ways in which speakers attune their speech to their conversational partner, as well as the study of prosodic phenomena in various communicative contexts. These studies have shown that, in the context of spontaneous speech, speakers tend to stress those words that are not foreseeable in conversation and to choose a prosodic contour reflecting the syntactic organization of their sentences

Finally, the use of corpora makes it possible to perform a quantitative analysis of word frequency, as well as its influence on their pronunciation. Frequency studies have shown that words that are more frequently used have a greater impact on consonant lenition phenomena. For example, the pronunciation of [t] evolves towards

These phenomena are important for understanding how phonological changes occur in a language over time. In summary, studying phonology using corpus data makes it possible to bring to light the interfaces between syntax and speech. These are not so evident when we work with data drawn from reading words or texts (these are less extensive and not so spontaneous).

The first study that we will introduce in this section concerns the question of speech articulation rhythm in different French varieties.

-Paris and Lyon (France); -Tournai and Liège (Belgium); -Neuchâtel, Nyon and Geneva (Switzerland).

Each person was recorded both during a reading task and throughout a conversation. This study thus made it possible to compare two kinds of speech. Before proceeding with the analysis, the data were transcribed and the texts were aligned with the sound signal. For each speech sample, the authors counted the number of syllables spoken between two pauses. The articulation speed was then calculated in milliseconds per syllable for each segment between two pauses. The data obtained were analyzed using a statistical model which made it possible to test the influence of several variables (the causes we mentioned in Chapter 1; see also Chapter 8 for a more detailed explanation of this notion) on other variables (the observed effect). In this study, the observed effect was the rhythm of speech articulation. The possible causes tested in the model included social variables such as age, geographic origin of the speakers and their gender, as well as speech style (reading or conversation). The results, which we will only partially report here, indicate that the geographic origin of the speakers does have an influence on articulation speed. More specifically, Swiss speakers have a slower speech articulation rhythm than French speakers (particularly Parisian speakers). Belgian speakers also speak more quickly than Swiss speakers, especially those from Neuchâtel and Nyon. This study tends to confirm the idea that Swiss speakers have a slower articulation than other French speakers. Furthermore, speech style may also have an effect on articulation speed, syllable duration being shorter in conversations than in reading. This result confirms the importance of studying not only reading texts but also, more importantly, spontaneous spoken speech excerpts in order to study a phenomenon such as articulation speed.

The second case study that we will discuss in order to illustrate the role of corpora in phonology concerns the phenomenon of liaisons. In French, many liaisons are considered optional, insofar as speakers can choose whether to make them or not. For example, in (1), it is possible to either pronounce the latent-word final consonant [z] as connected to the final -s of the verb allons (let's go) or not.

(1) Nous allons au cinéma.

(We are going to the movies.)

Several studies have used corpus data to study the factors that lead speakers to pronounce the liaison or not. For example,

Results showed that optional liaisons were mostly found in consonants [t], [z] and

Morphology

In order to study the rules for assembling morphemes into words, as well as the productivity of different morphemes (the number of words that a rule makes it possible to create), morphology needs to rely on external data, since frequency data cannot be inferred through introspection. Therefore, morphologists have long resorted to word lists. At present, these lists can be retrieved automatically from computerized dictionaries. Some morphologists also consider these lists as a form of corpus, because they make it possible to gather large amounts of data. Working with word lists can certainly be considered as an empirical method. Nevertheless, these data do not necessarily represent a corpus stricto sensu, since they do not contain extracts of language produced naturally. For some research questions, these lists can be very useful, whereas in other cases, it is necessary to resort to a real corpus containing linguistic productions in their context of use.

The usefulness of word lists has been clearly illustrated by a study by

-the final sounds of words; -their last letters, defined as the spelling reproducing the last vowel and, if applicable, its corresponding coda, for example, -one in the word trombone;

-the suffix, in the cases where there was one.

This study made it possible to show that the grammatical gender of a noun can be predicted in a large number of cases, contrary to the claims of many French theoretical studies in grammar. Furthermore, the spelling's ending seems to be the best of the three predictors. In the French language, certain sequences of letters are strongly associated with a certain grammatical genre, for example, -ette is associated with the feminine form, regardless of whether this is a suffix, as in the word statuette (small statue), or not, as in the word devinette (riddle). For this study, the use of a word list offers major advantages compared to a corpus: the grammatical category of words is already known, which simplifies noun retrieval: every word is already associated with its grammatical gender. These two pieces of information are missing from raw corpora. What is more, since all nouns in French have a grammatical gender (or in rare cases, even two), a dictionary offers an extensive list of examples which help us look for regularities.

The use of natural data from corpora rather than simple word lists is nonetheless essential to answer other research questions in morphology. First, in order to assess the productivity of a morphological rule, or even to assess word formation, that is, whether certain words made up from derivation or morphological composition exist, a corpus can offer much more information than the one found in a dictionary. Indeed, these corpora, and in particular, the large corpora available via the Internet, make it possible to find occurrences even for very rare words, as well as to provide very recent examples of language uses. This is all the more significant at a moment in the history of language when these uses have not yet been listed in dictionaries. The use of corpora also offers the possibility of finding out the context in which a certain word was produced, with the aim of checking, for example, whether the meaning of the derived word in such a context was the one expected. Corpus-based research has also made it possible to show that certain derivations deemed impossible from the point of view of theoretical morphology did nonetheless exist. For example, in theoretical analyses of French morphology, it was deemed impossible to attach the prefix anti-to a morphological pattern such as verb + -able

Second, in their study focusing on the derivation -able,

Finally, the use of corpora makes it possible to identify whether certain morphological derivations are more or less productive in certain language registers, or if they are produced more by certain types of speakers (e.g. within a determined age group) or in geographical areas. However, word lists do not make it possible to answer such questions, since they represent a normative use, corresponding to a standard variety. Corpora also help to trace how the productivity of a morphological rule evolves over several decades by means of monitor corpora, or even over several centuries, by comparing corpora from different periods

The study by

Syntax

The use of corpora in the field of syntax has been controversial for a long time, since this field has relied on an essentially rationalist methodology for several decades, as a result of Chomsky's works (see Chapter 1). However, the use of corpora in the field of syntax has grown considerably. In particular, the use of corpora makes it possible to compare the productions of various speakers, of different language varieties as well as different registers, providing a much more nuanced and realistic vision of the structures underlying language uses, rather than the intuitions of a single speaker. In addition, the syntactic analyses of corpora have made it possible to obtain a fine measurement of the frequency of so-called grammatical sentences, compared to those considered as ungrammatical, and thus making it possible to overcome such binary opposition. This frequency analysis can also be completed by an analysis of other factors (lexical, grammatical, discursive, etc.), making the uses of certain types of constructions more or less likely to occur in various discourse genres. The large amount of data provided by corpora, combined with the use of automatic analysis tools, have also made it possible to uncover trends which could not have been observed with the naked eye on the basis of just a few occurrences.

The study by

(2) Dans la rue, il y a des femmes qui discutent. (In the street, women are talking.)

In the literature, this type of structure is associated with the presentation of new events in discourse. By performing a corpus study on spoken French using the Corpus de français parlé parisien des années 2000, the authors identified all the occurrences of "il y a" or "y a" (both forms meaning "there is") and then manually chose only the cases (98 in total) in which (il) y a was followed by a definite noun phrase and a pseudo-relative. Among these sentences, only 16 occurrences corresponded to cases where a new event was being introduced in the discourse, as the literature claims. Analyzing examples helped the authors identify other types of functions for this structure, notably the introduction of a new entity in discourse, as in (3).

(3) Il y a Sophie qui veut te parler.

(Sophie wants to talk to you.)

These different functions were more easily identified thanks to the large availability of contextual language in corpus data. This study illustrates how a corpus study can combine quantitative elements (the prevalence of different functions for a structure) with qualitative ones (the identification of semantic functions).

Verwimp and Lahousse's study could be carried out on a transcribed spoken corpus in the absence of any form of syntactic notation, since the structure the authors were looking for matched a clearly identified lexical pattern. Nevertheless, the same does not apply to the study of other structures that are not associated with such a pattern. For this type of case, the use of a syntactically annotated corpus becomes compulsory. This is illustrated by the second study that we will discuss in this section.

Another large-scale annotated corpus study was carried out to explore the question of the placement of the attributive adjective, either before or after the noun it modifies. In French, this is considered a complex question, because the factors leading to one placement or the other are linked to phonology, morphology, syntax, semantics and discourse. Among all these factors, the identification of those playing the most important role cannot be carried out without a quantitative analysis of large-scale data.

In conclusion, the analysis of syntactic phenomena using large annotated corpora helps us go beyond traditional qualitative and rationalist analyses. However, these analyses are only possible on corpora that have been parsed syntactically, and these are still rare due to the complexity of applying automatic parsers. Spoken data are particularly difficult to automatically annotate syntactically due to the presence of numerous repetitions, hesitations, etc., which ruins parsing. The results from studies on syntactically parsed corpora cannot therefore be generalized to all language registers for the moment. However, it is likely that these studies will increase in the future insomuch as the quality of syntactic parsers will keep on improving.

Lexicon

Unlike syntax, lexicon is ideally suited for corpus analysis, and it is also the field that has been approached from that perspective for the longest time. In fact, most of the time, the study of lexicon can be done directly on a raw corpus, or in some cases, on a corpus that has been annotated with part-ofspeech tagging.

Corpora make it possible to identify all the words used in a language as thoroughly as possible. It is evidently impossible to list all the words existing in any language, but using large corpora, it has become possible to get a much more realistic idea of the number of words in circulation compared to the lists that could be drawn from dictionary databases, which list only part of it. For example, based on the Google Books corpus, which contains more than 500 billion words (including 361 billion in English), drawn from literary sources from the 16th Century to the present day,

Corpora also have another great advantage for lexicon study: they make it possible to identify the sequences of words that frequently appear together, which are called collocations, for example prendre un douche (to take a shower) or forte pluie (pouring rain). We can thus identify fixed sequences like idioms, as well as determine their degree of fixedness. For example, these studies help us to establish whether it is possible for a certain idiom to be used in the passive form or not, with different verbal tenses, or whether the idiom's topic is fixed, as in j'en mettrais ma main au feu (I would stake my life on it), or free, as in mettre le feu aux poudres (to stir up a hornets' nest).

From the point of view of lexical meaning, corpora are valuable resources for determining the vast array of meanings that a word may take on in different contexts. By means of corpus studies, it is also possible to define lexical fields and to study meaning relations between words, such as synonymy and anonymity. For example, in order to determine whether two words are synonymous or not, the analysis of corpus data makes it possible to determine whether these two words can appear in the same linguistic context or not. Finally, the analysis of corpora from different textual and spoken genres makes it possible to determine the situational contexts in which the words are used, in a much more nuanced way than register indications, such as soutenu (formal) or populaire (informal), found in dictionaries (see Chapter 3, section 3.5).

The first example of a study that we will introduce in this section is an antonym analysis and, more specifically, antonyms which are used together within the same utterance, as in (4), where the use of antonyms in bold serves to reinforce the contrast with another pair of opposites in the utterance, in this case, between students and workers.

(4) The initiative was very popular with the students and very unpopular among workers.

The second lexicon study that we will introduce deals with the question of neologisms and, more specifically, how they get into the language or not. In France, the question of neologisms is a sensitive point, especially when the new words have been borrowed from English. The rejection of Anglicisms gave rise to several decrees, and then to the law concerning the use of the French language (known as the Toubon law), which prohibits the use of foreign terms in public administration texts. One of the consequences of these decrees was the creation of a Commission for the enrichment of the French language (formerly known as the Terminology and Neology Commission), which was responsible for suggesting French terms that could help avoid Anglicisms. In order to study whether these suggestions are actually used by French speakers,

Finally, we will mention another study concerning the French lexicon and its connections with English, which was conducted in the context of Frenchspeaking Canada. In Canada, the question of Anglicisms is also sensitive, insofar as French is a minority language and struggles to keep its importance alongside English.

Discourse analysis

The field of discourse analysis is an empirical field par excellence, insofar as the objective is to study the structure and content of different types of interactions. However, for a long time, these studies were carried out in an almost exclusively qualitative manner, with the aim of analyzing certain particular situations in detail. These studies are very useful for understanding the nature of interactions. For example, Traverso (2019) studied the structure of requests from people asking for help in an office with access to social rights. This study provided a very detailed analysis of the linguistic, paralinguistic and gestural resources used in this particular situation. All these criteria cannot be contemplated simultaneously by a quantitative analysis. Discourse analysis studies also triggered the development of the first multimodal corpora, which make it possible to study interactions in very rich contexts. In addition to these qualitative studies, it is also possible to analyze certain aspects of discourse from a quantitative perspective. We will focus on this type of analysis in the following section.

The discourse elements that are best suited for a corpus quantitative analysis are those easily identified on the basis of raw data. For example, many studies have focused on how discourse markers such as bon, ben and alors (well, so, I mean, etc.) are used in different kinds of speech and with what functions (e.g.

The above-mentioned studies mainly apply to spoken and interactive language. However, other studies have also focused on written discursive genres. In particular, these studies aim to study the way in which discourses are constructed from the point of view of their cohesion, by analyzing discourse connectives such as parce que, donc and quand (because, therefore and when), for example in

Quantitative discourse studies have also sought to study the differences between discursive genres

The study by

(5) Emma est arrivée en retard parce qu'elle a raté son train.

(Emma arrived late because she missed her train).

(6) Emma est très désorganisée, car elle perd tout le temps ses affaires.

(Emma is very disorganized because she loses her stuff all the time).

Simon and Degand further hypothesized that the difference between car and parce que is not stable between the spoken and written modes, due to the fact that the connective car no longer seems to be widely used in spontaneous spoken discourse. In order to verify these hypotheses, they used a written corpus (Le Soir newspaper) and a spoken corpus (Valibel database), both representing the variety of French spoken in Belgium. First, they retrieved all the occurrences of car and parce que. They found that in written language, the two connectives have a very similar frequency (approximately one occurrence every 300 words for parce que and every 250 words for car), whereas their frequency is quite different in spoken language. On the one hand, parce que is 10 times more frequent in spoken language than in written language, whereas it is the opposite for car, which is 10 times less frequent than in written language. In spoken language, parce que is more than 185 times more frequent than car.

The authors then manually classified 50 occurrences of each connective found in the spoken and written modes as either objective or subjective, that is, a total of 200 occurrences, randomly chosen from the corpus. Then, for each connective, they were able to compare the differences in distribution between the types of relations both in the written and spoken modes. The results indicated that in written language, car is a more subjective connective than parce que. However, in spoken language, parce que is used for communicating all types of causal relations, replacing car. This study shows the semantic criteria for making a distinction between two connectives that have a similar meaning and reveals important differences in their use between the spoken and written modes.

The second study we will discuss in detail in this section deals with a particular textual genre, namely the SMS language and its influence on the young generation's command of other written genres.

The authors analyzed the types of spelling mistakes produced by the students. For the dictation exercise, they found that the grammatical errors were the highest (agreement, etc.). In Facebook conversations, spelling alterations to typical written words in social media represented less than one in three words, which contradicts the idea that social media language is entirely different from other language registers. Next, the authors showed that there is no link between the propensity of students to use alterations on social networks and their spelling skills in the two dictations. The authors also investigated whether the formal alterations found in the language of social networks had repercussions in other discursive genres. Unfortunately, only a list of 30 words had occurrences in the different genres, which is insufficient for carrying out a truly quantitative analysis. However, they could see that the altered forms in social network conversations (bcp instead of beaucoup, c instead of c'est, etc.) were not found in the spelling of these words in the other register nor were they misspelled in the dictation exercises. This study thus offers arguments against the misconception that the writing style in social networks degrades young people's spelling skills and makes them unable to differentiate textual genres.

Pragmatics

Pragmatics studies language use in context. This definition brings together a wide range of heterogeneous phenomena such as speech acts, implicatures, politeness phenomena and conversation analysis. Pragmatics has many points of contact with both discourse analysis and sociolinguistics. As in the case of these two disciplines, corpora represent valuable tools in pragmatics, because they make it possible to study the use of language in real communication situations.

Certain pragmatic phenomena such as turn-taking in conversations or the use of discourse markers discussed in the previous section can be studied by looking for certain linguistic forms, for example discourse markers such as bon, ben, voilà (well, so, actually) or certain specific parts of corpora, such as the first utterances in conversations, depending on whose turn it is to speak. However, for other pragmatic phenomena such as speech acts and implicatures, as well as for expressing politeness, there is no systematic relationship between linguistic forms and pragmatic functions.

For these phenomena, it is necessary to use data annotation and this annotation must be done manually on the basis of an observation of the entire corpus. This process may become time-consuming, and annotations are not always easy to carry out, insofar as the speech acts that speakers communicate by means of their utterances in many cases are not communicated transparently. For example, it is possible to ask someone to open a window with a formulation such as Could you open the window? which explicitly mentions the subject of the request. However, this can also be done by means of much more indirect formulations such as It's hot in here! or It's hard to breathe. The role of this type of formulation largely depends on the context and cannot be automatically inferred from the linguistic form employed. As soon as a speech act is associated with a certain type of utterance (e.g. interrogative sentences) or frequently associated with certain words (such as sorry or oops for excuses), a corpus search becomes greatly simplified. This research should nonetheless be subject to manual verification (see

We have already illustrated the use of corpora for the study of discourse markers and connectives in the previous section. In this section, we will introduce a study illustrating the usefulness of corpus data for the study of scalar implicatures. These implicatures are communicated through the use of a weak scalar term which contextually excludes the affirmation of the stronger term. For example, the quantifier certains (some) as in (7) pragmatically excludes the interpretation, although logically possible, according to which all of Laura's friends are nice.

(7) Certains amis de Laura sont sympathiques.

(Some of Laura's friends are nice).

Thus, the use of quantifiers generates a pragmatically enriched interpretation in the form of an implicature: certains mais pas tous (some friends, but not all of them, are nice). Some pragmatists (e.g.

Sociolinguistics

In order to study social variations in language use, sociolinguistics necessarily resorts to external data. The use of corpora has therefore long been a fundamental tool in sociolinguistics. In particular, it makes it possible to compare how different social groups such as women, men and people in the cities, the suburbs or in the countryside use language in natural situations. Another major concern of sociolinguistics is to document regional variations in the use of languages, and for this, corpora are also an essential resource. Moreover, numerous corpora have been developed with the aim of documenting the use of certain regional varieties such as French-speaking Switzerland, Belgium and Canada (see Chapter 4). Sociolinguists also believe that variation is a sign of an ongoing change in language, and in many cases, corpora represent an effective tool for uncovering this kind of variation, particularly in the different registers of the spoken language.

The first study we will introduce deals with the issue of linguistic changes. Gardner-Chloros and Secova (2018) studied the formulation of indirect questions in Parisian French and, more specifically, the existence of indirect questions in which the interrogative word is positioned in situ in embedded structures as in (8), rather than the standard variant (9). (

(literally: I don't know he does what).

(9) Je ne sais pas ce qu'il fait.

(I don't know what he is doing).

In order to study the distribution and prevalence of this type of structure in the French spoken in Parisian suburbs, all the occurrences of indirect questions were collected in a small oral corpus of approximately 350,000 words. The identity of the person who produced the occurrence was then classified into different categories:

-age group; -ethnicity (French parents, mixed origin, two immigrant parents from the same culture); -the diversity of friends' networks (in terms of percentage of friends in the same ethnic group); -degree of bilingualism (French and another language). This coding enabled the authors to count how many occurrences were produced following the different criteria identified and to prove that young people from bilingual backgrounds use the post-verbal structure (8) significantly more when compared to young people from monolingual French-speaking families. The ethnic group also plays an important role, since young people with two immigrant parents also use this structure significantly more than young people from French families. Gender also plays a salient role, since boys tend to use such structures more than girls. Finally, having a culturally diverse network of friends is also correlated with the use of these structures: young people with a group of friends mixed at 80% tend to use them more than young people with an unmixed group, or a group mixed at 20%. The authors also tested which of all the factors mentioned above best predicted the type of indirect question used (pre-verbal as in (8) or post-verbal as in (

The authors finally compared the results of their corpus with the use of these same structures in the Corpus de français parlé parisien des années 2000. They observed that this structure was much less used in this second corpus, with only two occurrences by male speakers of Moroccan origin. Thus, this study showed that the use of post-verbal indirect questions represents a case of language change initiated by the less privileged social strata of the population, rather than a prestige change (as is the case with other sociolinguistic changes). This change seems motivated by the desire to give more weight to the interrogative word by placing it at the end of the utterance, while keeping the same word order in situ in direct questions and in embedded questions.

-age (15-16 or 17-18 years); -gender; -the level of experience in writing text messages, the latter variable being coded in a binary way (experienced or inexperienced), from the number of messages sent each day.

Each of the messages in the corpus was coded according to the three linguistic variables to be studied, namely the message's length, the presence of opening and closing elements and its main function. The results indicated that text messages differ from other text genres in that 73% of them do not contain an opening and/or closing expression. As regards the effect of the authors' gender, the messages written by girls tend to be longer than those of boys, but only among the age group of 15-16 years. The level of experience also has an influence on the type of message produced. Inexperienced writers most frequently produce text messages with opening and closing expressions. Finally, SMS mainly has a relational function more than an informative one (56% of messages), and this trend tends to be more pronounced in the younger group and among girls. This study illustrates the way in which different social factors influence textual production within the context of a particular discursive genre.

Diachronic linguistics

The quantitative study of the linguistic changes that languages experience over the centuries, is based on the study of corpus data. Such data makes it possible to document the different stages of these changes, as well as the different periods in the history of a language in which these changes took place. The main limitation to the use of corpus data for studying linguistic changes from a diachronic perspective lies in the fact that linguistic changes generally first take place in the spoken language. However, the first spoken corpora date from the second half of the 20th Century. For previous periods, data is limited to written records and, more specifically, to certain registers such as literature and legislation. Due to the lack of spoken records, linguists sometimes resort to the speaking attributes of fictional characters in dialogue as a source. While these dialogue excerpts reveal a less formal language than in other written genres, they do not reproduce certain essential characteristics of the spontaneous spoken language, such as hesitations and errors. Certainly, these data offer no indications concerning phonology or prosody, which are nonetheless important elements in the processes of linguistic change.

The first study that we will introduce in this section deals with the emergence of the counterfactual conditional in structures as shown in (10).

(10) Si j'avais su, je ne serais pas venu.

(If I had known, I would not have come).

In order to trace the emergence of this function of the conditional in French,

The results indicated that the first evidence of the past conditional goes back to the 12th Century but that the frequency of this structure was fairly low until the 16th Century, with around 10 occurrences every 100,000 words. The frequency then sharply increased from the 17th Century and reached 70 occurrences every 100,000 words from the 18th Century onwards. The moment when the uses of the past conditional increased in the 17th Century matched a strong progression in its counterfactual uses, which became the most broadly employed function of this construction. Linguistic theories on the evolution of perfect forms predict that evolution can develop from a previous interpretation (e.g. when the process described by the verb, namely the action, the state or the result described, precedes another process in the sentence) to a past interpretation (when the process described by the verb represents a bygone situation at the time of the utterance). The authors therefore coded every occurrence according to the type of process described: previous or past. Their results showed that the past conditional had an essentially previous interpretation until the 17th Century, when the past interpretation began to increase. This change matched a strong growth in the counterfactual uses of the past conditional. The study thus illustrates how corpora make it possible to document the emergence and the later evolution of a certain language trait. Thanks to the annotation performed on data themselves, it also shows that certain frequency changes can be associated with the emergence of new functions.

The second study that we will discuss concerns the pair of causal connectives car and parce que, which we have already mentioned above (section 2.5).

Conclusion

In this chapter, we have shown that the use of corpora can prove to be a valuable tool in all areas of theoretical linguistics. One of the main advantages of corpora is that they contain natural data providing a glimpse of different forms of language use, which can thus be studied while taking into account a rich linguistic context. We have seen that certain studies can be carried out on the basis of raw data, particularly in the field of lexicon, whereas in other fields, such as syntax or discourse analysis, usually data has to be annotated, something which can be done manually or partially automated. We will discuss the topic of manual annotations in Chapter 7. The studies introduced in this chapter have also shown that it is possible to quantitatively analyze different types of observations and data. We will introduce the most accessible methods in detail in Chapter 8. Finally, all the corpus studies illustrated in this chapter were carried out on corpora of very diverse shape and size. In Chapter 5, we will discuss in more depth the data that are available in French to the public for carrying out corpus studies.

Revision questions and answer key

2.10.1. Questions 1) What data are necessary to be able to conduct a corpus study on the way the pronunciation of vowels like the [a] in patte and the

2) Why is the use of word lists not always enough for studying morphological phenomena in a language?

3) Why is syntax a field of study in linguistics where the use of corpora is still limited? 4) Which are the lexicon studies that cannot be fully performed on raw data? 5) What are the areas of discourse analysis properly suited to carry out quantitative studies? 6) What are the constraints posed by the study of pragmatic phenomena such as speech acts and implicatures on a corpus study? 7) Which are the sociological factors suitable for carrying out the study of linguistic phenomena on corpora? What types of data do we need to perform these analyses? 8) What are the limitations to the use of corpora for studying the evolution of languages diachronically? 2.10.2. Answer key 1) In order to study this question, it is necessary to use two spoken corpora respectively representing a variety of French spoken in France, for example in Paris, and that of French-speaking Switzerland. These corpora must also contain diachronic information, for example regarding the evolution of pronunciation from the second half of the 20th Century to the present day. A selection of occurrences of the vowels to be studied, representing the different periods, should then be looked up in the corpus. For each occurrence, the pronunciation should be noted, either by a phonetic analysis of the production of the vowel or by a manual annotation carried out by two native speakers. Then, the various pronunciations should be quantitatively compared between the two corpora and the periods involved.

2) First, when using word lists, morphologists do not have access to the linguistic context in which these words were produced. For a number of research questions, this type of information is crucial, for example, when it comes to determining the meaning of a derived word. Second, by using a word list from a dictionary rather than a corpus, morphologists only have access to a limited subset of the speakers' linguistic productions. In fact, dictionaries provide a list of well-established uses in language, which correspond to the language's standard variety. However, new uses are constantly emerging and these are not listed in dictionaries. Likewise, corpora also represent less formal varieties of language than the written standard found in dictionaries, which makes it possible to diversify the language uses identified in this way. Finally, large corpora make it possible to have access to much more data than word lists, which makes it possible to identify rare phenomena which do not appear in dictionaries.

3) Syntax has been the area of language most influenced by Chomsky's work and has followed for decades the rationalist methodology that this author advocated for linguistics. Indeed, formulating grammaticality judgments is one of the areas in linguistic studies for which native speakers can trust their intuitions, albeit partially. At present, this objection in principle is largely over, but the main remaining barrier is of a methodological nature, since the study of many syntactic phenomena requires the use of syntactically annotated corpora. However, syntactic annotation tools are still imperfect and sometimes too complex to implement or use. As a result, few corpora have been the subject of such annotations and their complexity discourages certain linguists from using them. 4) Unlike syntax, in many cases, the lexicon can be studied on raw data, that is, non-annotated data. However, due to the existence of many polysemic and homonymic words in French, many lexical searches must be done on the basis of morphosyntaxically annotated data in order to avoid identifying parasitic occurrences. For instance, when looking up the word "orange" as a color, it is necessary to separate its adjectival uses from its nominal uses. In addition, research on the metaphorical uses of a word must be subject to manual annotation on the basis of their context, since metaphors have no lexical markers that make it possible to differentiate them from the literal uses of the same words. 5) In general, all the elements that can be identified on the basis of surface lexical features are well-suited for quantitative analysis, since it is possible to identify numerous occurrences by means of a computerized search. We should nonetheless observe that this first identification often requires a later manual sorting of the data. In the case of discourse, the phenomena which are well-suited for quantitative analysis are the lexical cohesion markers. These are the use of anaphoric pronouns (he, she, etc.), discourse connectives indicating coherence relations between discursive units (because, when, if, etc.) and the use of discourse markers (well, I mean, you see, etc.), which index the interpersonal relationships between the speakers and offer clues to the discursive planning of the speakers. Conversely, phenomena such as the structuring of information within discourse and the analysis of global coherence are much more difficult to annotate on a large scale since they require an entirely manual processing of the corpus.

6) The main constraint posed by many pragmatic phenomena such as the study of speech acts and implicatures is related to the fact that the occurrences of these phenomena cannot be identified on the basis of lexical markers which can be automatically searched for in the corpus. For example, an indirect speech act such as a request can take many different superficial forms as a question (could you shut up?) or an assertion (I'd like you to stop talking). The same goes for implicatures, which are not linked to the use of specific words, with the exception of generalized scalar implicatures, which are typically associated with the use of quantifiers such as some and a few, logical connectives like or and some telic verbs such as to start. In order to identify the occurrences of pragmatic phenomena which are not related to words, it is therefore necessary to scan the entire corpus manually. What is more, the speech act that the speaker intended to produce is sometimes ambiguous and even when having access to a linguistic production: it is not always possible to clearly identify the speaker's intention. This annotation therefore involves an interpretation on the part of the researcher, which is, in part, necessarily subjective.

7) When the sociological information about the speakers in a corpus is known, it is very easy to use this for comparing the productions of different categories of speakers such as men and women, people from different age groups, from different socio-economic backgrounds, etc. These data are typically not part of the corpus itself, but are listed in the metadata (see Chapter 6).

8) The main limitation concerns the availability of diachronic data. Spoken corpora in particular do not go back further than the second half of the 20th Century, which considerably limits the generalizations that can be drawn regarding the evolution of languages, since this primarily takes place in speech. In addition, diachronic corpora only offer a list of certain textual genres, such as literature and legal texts, which also limits the generalization of the conclusions that can be drawn from them.

Further reading

The usefulness of corpora for studying lexicon, syntax, pragmatics, sociolinguistics and discourse is introduced with English examples in the book by O'

How to Use Corpora in Applied Linguistics

In this chapter, we will continue to explore the multiple uses of corpora in the different areas of applied linguistics, in order to complement the presentation of theoretical linguistics in Chapter 2. In particular, we will see how corpus data can be used for studying the language of specific groups such as children, individuals with language impairments and foreign language learners. We will then illustrate the role of corpora as a tool for teaching languages, as well as for creating dictionaries. Finally, we will discuss the uses of corpora outside the language sciences, in order to study literary texts, and also within the legal framework.

Language acquisition

In order to study language development in children through observation, the use of empirical methods is essential. It is for this reason that this field has been a pioneer in the development and sharing of corpus data. At the beginning of the 20th Century, several researchers systematically studied the language of their children by means of notebooks, where they recorded their observations (e.g.

Language acquisition corpora show some specificities when compared to other corpora. First, children's language develops mainly during the preschool years. The development of spoken language largely precedes the start of the written language learning process. This is why language acquisition corpora are by nature spoken corpora, which require a written transcript in order to be analyzed. The transcription itself poses certain challenges, since children who acquire language produce mistakes (see Chapter 7 for a discussion regarding the different ways of annotating such mistakes). Studying these mistakes provides valuable clues for understanding the acquisition process. It is therefore advisable not to erase them in the transcriptions, but to keep a log of them so that they can be easily identified (e.g. using categories such as "consonant substitution", or "truncated word"). Finally, in the recordings, children interact within their environment (often at home), with people they know well, most often, their parents. For this reason, language acquisition corpora frequently include language samples produced by children as well as by adults. This configuration makes it possible to analyze the way in which adults respond to children's language, as well as the connections between the language that children hear from their parents and their own productions. This information represents valuable clues for studying acquisition mechanisms, and these are particularly valuable for theoretical frameworks which attribute a key role to social interactions as the source of language acquisition (e.g.

Two types of corpora can be considered when studying children's language. On the one hand, we have so-called longitudinal corpora, in which one or more children are regularly recorded over a period of several years, and on the other hand, we have cross-sectional corpora, in which groups of children of different ages are recorded only once. Each corpus type has its own advantages and disadvantages. The great advantage of longitudinal corpora is that they make it possible to study the evolution of language much more precisely than cross-sectional corpora, since these contain samples collected at close intervals. Their disadvantage is that they include samples from a very limited number of children. Given the importance of individual differences in language acquisition

The main limitation to the use of corpora (whether longitudinal or crosssectional) for studying children's language is that they contain only a small portion of the language that children produce at a given time. Typically, a longitudinal corpus includes a recording of a few hours, gathered every one to three weeks.

A second limitation inherent to the use of corpora is that even very dense corpora can only be used to study language productions. However, it is well known that a comprehension and production of language do not develop in a totally synchronous manner (e.g.

The first study that we present to illustrate the use of corpora for the study of language acquisition concerns the acquisition of the French verbal system for expressing temporal references.

Language impairments

In the same way as corpora make it possible to study language acquisition in children experiencing typical development, they also provide important information about language acquisition in atypical populations such as children with autism spectrum disorder or specific language impairment. The main advantage of using corpora to study such populations is that they provide an overview of the communicative strategies they deploy in order to compensate for certain language and communication difficulties. For example, corpus data make it possible to analyze the way in which a child with limited syntactic skills manages to ask for an object or to ask a question. These also provide an overview of a child's linguistic competence when they find themselves in a familiar situation, offering them better chances of displaying all of their skills. This feature becomes of crucial importance when studying the speech of children with autism spectrum disorders, who suffer from a high level of anxiety when facing unknown situations. Finally, corpora are also useful for comparing the skills of children and patients in different and natural discursive situations (Da Silva Genest and Masson 2019).

Corpora are becoming increasingly used for identifying linguistic markers charactering certain language disorders in adult populations, for example Alzheimer's disease

The main limitation to the use of corpora for studying atypical populations (which is also valid for the study of normally developing children) is that corpora only provide information on spontaneous linguistic productions. However, when a certain linguistic phenomenon is not found in the corpus, it does not necessarily mean that the child or patient recorded in the corpus does not have the competence to produce such types of words or sentences. It only means that they did not employ them during the recorded sessions, either because they did not have the opportunity to produce such an element, or because they deployed an avoidance strategy due to the fact that they could not master such constructions. As we have already seen, this limitation is particularly acute for the study of rare linguistic phenomena. As we will later explore in the case studies, when studying children with atypical development, it may be advisable to analyze a corpus of children with typical development (or suffering from a different pathology which does not involve the same linguistic deficits) in parallel, so as to favor the comparison between different linguistic productions in similar contexts. In the same way, when it comes to studying the language of adult patients, it is necessary to compare it with the language used by healthy adults, produced in similar contexts.

Another limitation to the use of corpora is that the latter do not make it possible to measure the connections between verbal productions and other cognitive skills, such as working memory or non-verbal intelligence. Gathering data about these non-linguistic skills is often important in order to understand the nature and causes of the language deficits observed. It is for this reason that the study of language production in patients is often carried out via constrained production tasks, such as the ability to name images or to repeat non-words or sentences (see, for example, Seiger-Gardner and Almodovar 2017), rather than based on corpus data alone. Performed in an experimental context, these tasks make it possible to control many linguistic parameters that influence production, as well as to limit the impact of avoidance strategies and to test the existence of connections with other individual differences in working memory or non-verbal intelligence.

The first study that we will discuss in this section compared the linguistic productions of six children with autism spectrum disorder (ASD) with those of six children with Down syndrome. The aim was to determine whether the syntactic development of children with ASD followed a different trajectory compared to another population who also suffers from language impairments.

The second study concerns the production of noun phrases by Frenchspeaking children with specific language impairment (SLI).

Second language acquisition

The field of second language acquisition is one of those in which the use of corpora has grown exponentially in recent decades. The creation of numerous learner corpora, as well as the development of new methods and annotation tools, has largely contributed to this evolution. While linguists working on the question of second language acquisition have long used learners' productions as a source to build their theories, these data were limited to very small samples or even to single-person studies. Therefore, the generalization potential linked to these data was highly questionable. This led to the creation of real learner corpora, aiming to provide representative samples of this population.

There are different types of corpora containing language produced by learners. The first learner corpora produced at the end of the 1980s were limited to written productions, and these corpora are still the most numerous today

As is the case with other types of corpora, learner corpora have mainly been compiled in English, but there are also resources for other frequently taught languages such as French (see Chapter 5, section 5.5). Some corpora contain language samples produced by learners of different mother tongues, and this information can be found in the metadata (see Chapter 6, section 6.4). These corpora are valuable tools for measuring the role of different mother tongues in the process of acquiring the same foreign language, as we will see below.

Finally, most learner corpora are cross-sectional corpora, including one sample per participant and representing a given moment during the acquisition process, since most of the time learners included in a corpus have a homogeneous level of competence in the foreign language. These corpora are very useful for determining learner competence at a certain level, but considered individually, they do not make it possible to study different acquisition stages. For this, longitudinal corpora are necessary, but these are rare due to the difficulty of sampling the same learners across several years. One way to get around this problem is to compare several cross-sectional corpora including learners from different levels of competence.

The first study we will discuss focused on whether learners at a very advanced proficiency level keep on improving, which would justify the need to define different development stages for learners beyond the so-called advanced stage of acquisition. To do this, in the spoken corpus InterFra, Forsberg Lundell et al. (2014) defined three groups of French non-native speakers whose mother tongue was Swedish. Each group included 10 speakers. The first group was made up of speakers aged 19 to 34 years who had lived for one to two years in France. The second group included speakers aged 25 to 30 who had lived between 5 and 15 years in France, and the third group included speakers aged from 45 to 60 years who had lived between 15 and 30 years in France. These groups were compared to two groups of 10 native speakers each, who were between 15 and 30 years old and 45 and 60 years old respectively, chosen to match the ages of learners.

The groups of learners were then compared on the basis of five linguistic indicators:

-the number of non-native morphosyntactic forms produced, for example, gender or plural agreement mistakes; -the number of left dislocations, in sentences such as "moi, si tu me demandes, il a tort" (literally: me, if you ask, he is wrong), which are typical of spoken French; -the number of formulaic sequences such as collocations; -lexical richness, calculated based on the number of words with high, middle and low frequency in corpus data that are used; -fluency, measured by articulation speed and utterance length between two pauses.

The results indicated that the group of learners with 5-15 years of residence differed from the 1-2 years of residence group on the following criteria: use of formulaic sequences, lexical richness and fluency. The group having lived longer than 15 years in France did not differ from the 5-15 years of residence group on any of these indicators. However, speakers who had lived the longest in France managed to pass for natives in a listening discrimination test administered to French native speakers, unlike the speakers from the 5-15 years of residence group. This indicates that some form of progression must have taken place between these two groups, but which could not be measured through the tests chosen for this study. Furthermore, all the groups of learners differed from native speakers (but did not differ from each other) on the assessment of morphosyntax, which seems to indicate that this is an area which can remain beyond the reach of even the most advanced learners. This study thus showed that language continues to develop beyond the so-called advanced acquisition stage and that this progression is not uniform among the different dimensions of language. As the lexicon continues to progress, certain aspects of the language system such as morphosyntax remain at a non-native level, even at truly advanced acquisition stages.

The second study that we will introduce compared the use of two English discourse markers, in fact and actually, by learners of two different mother tongues and by native speakers. Buysse (2020) compared the oral productions of French-speaking and Dutch-speaking learners of English in the LINDSEI corpus with the productions of native English speakers in the LOCNEC corpus, a corpus which was compiled to be comparable with the LINSDEI (see Chapter 4 for a definition of the concept of comparability). The interest in comparing French and Dutch speakers is that there are different translation equivalents for the English markers in both languages. While Dutch has two markers which closely resemble those in English (eigenlijk for actually and in feite for in fact), French only has one close marker, which is en fait. So, in French, actually has no translation equivalent of its own. In her study, the author first performed a frequency analysis regarding these two markers in the three sub-corpora. The results indicated that actually is significantly more common than in fact among Dutch native speakers in comparison to French speakers. Conversely, in fact is significantly more common than actually in the speech of French speakers compared to Dutch speakers. On the basis of the literature, she then identified all the possible functions for these markers in English, such as introducing an elaboration or a contrast, and then annotated all the occurrences of these markers according to one of these functions. This analysis allowed her to show that learners use all the possible functions that the markers offer, even if their respective frequency varies a little between French speakers and the other two groups. That being said, the low number of occurrences of the marker actually among French speakers (56 in all) prevents a quantitative analysis of the differences between its different functions. In summary, this study demonstrated the influence of speakers' mother tongue on the use of discourse markers in a foreign language, and in particular, the importance of having a similar marker in L1 to help learners use markers appropriately in a foreign language. Indeed, Dutch speakers, who have two very similar markers in their mother tongue, use in fact and actually in the same way and in the same proportions as natives. On the other hand, French speakers tend to under-use the marker, which has no direct equivalent in their mother tongue (actually) and to overuse the other marker (in fact), to perform the same functions, as they would do in French. This study thus indicates that negative transfer effects occur even among advanced learners.

Language teaching

In addition to collecting learner corpora as we discussed earlier, the area of language teaching currently makes an extensive use of corpora produced by native speakers (see, for example,

In the field of vocabulary in particular, the use of corpora makes it possible to empirically provide lists of the most frequent words in a certain field, which should therefore be taught as a priority. Another key point for mastering a foreign language is to know, apart from the meaning of isolated words, certain elements of phraseology, in other words the typical linguistic sequences in which a word occurs, for example for the word "knowledge", "to acquire knowledge", "knowledge gain" or "prior knowledge". Some researchers even think that these elements should be taught as lexical units

In addition, corpora provide examples of spoken language, which are clearly more realistic than the constructed dialogues contained in most language methods. Given that they include natural interactions, corpora include reformulations, hesitation markers, turn-taking devices, etc., which are not reproduced in artificial dialogues, but which are important for learners to master since they are an integral part of language uses among native speakers. Finally, the creation of learner corpora has also made it possible to bring a new dimension to language teaching, by allowing learners to consult non-native productions and to compare them with native productions. Access to such data enables learners to become aware of the differences between their productions and those of natives. In addition, these corpora often contain an annotation of errors, which favors an explicit learning process and enables learners to become conscious of typical errors and to avoid them.

An important question for language teaching is to determine to what extent the corpora developed for linguistic research can be reused as such in the classroom. On the one hand, there are many advantages to letting learners use corpora by themselves, for example, by teaching them to search for word occurrences using a concordancer. This practice induces active reflection on the language, when it comes to determining what to look for and how to look for it, which enhances learner autonomy and stimulates students to become involved in the learning process

Limitations on the use of corpora created for research also apply to the use of raw corpus data for creating language methods. In order to base a language method on corpus data, it is imperative that the corpus chosen is adapted to the target audience, in particular from the point of view of the variety of the language represented, discourse genres, the age of the speakers, etc. According to

In this section, we will introduce two studies which show the usefulness of corpora for language teaching. Each of them compared corpus data with the presentation of the same phenomenon using different language methods.

-the grammar points discussed; -the order in which they were presented; -the vocabulary used in the examples to introduce such points.

Then, they compared the examples with frequency data drawn from a 20 million word corpus, corresponding to four different language registers.

In each of the three areas studied, significant differences were observed between corpus data and the presentation of the same phenomenon in language methods. For example, in the section introducing the forms that noun phrases may take in English, most of the methods only indicate a pre-nominal modifiers can be an adjective (a nice man), a present participle (an exciting game) or a past participle (stolen goods). However, in written corpora, nouns are also common modifiers of other nouns (e.g. metal seat and tomato sauce) and the relationships they express are diverse and complex. This syntactic pattern should also be included in language methods. In addition, the order of presentation for the different grammatical features does not correspond to the uses observed in corpora, especially in the case of verbal tenses. Most methods strongly emphasize progressive forms and represent them as the default form in conversations. However, corpus analysis shows that the most frequent case in many language registers is, on the contrary, the simple aspect. Finally, the authors observed that the verbs used for illustrating different grammatical properties in language methods are not necessarily the most common verbs in real-life language. Although introducing less frequent verbs may be useful for broadening learner vocabulary, it is nevertheless surprising that the most common words are not used, at least for beginner learners. This study showed that the intuitions of language method designers often do not reflect actual language uses. Corpus data make it possible to produce better-suited educational materials to match the realities encountered by learners.

Racine and Detey (2017) also compared the information given in language methods with corpus data, focusing on the question of liaisons in French. Producing liaisons is a particularly difficult aspect of spoken French for learners. In fact, producing liaisons correctly requires mastering production constraints (such as identifying the consonant involved in the liaison, taking into account possible modifications to the phonological environment, etc.), as well as the syntactic environment making the liaison either required, optional or forbidden. Most methods of French as a foreign language focus only on the compulsory, optional or forbidden nature of the liaison, which they present in a normative and simplified manner

Lexicography

Writing a dictionary requires the use of textual data in order to identify the words that should be included and to illustrate their contexts of use. Since the beginnings of dictionaries, lexicographers have manually collected examples from various sources, mainly literary ones. This focus on literary texts is particularly visible in the case of French, a language for which lexicographers have focused on a formal register of the language. For example, the Trésor de la Langue Française has drawn its 430,000 examples from "two centuries of French literary productions"

Regardless of the stylistic genre targeted, the use of quantitative methods linked to corpus linguistics has led to many advances in lexicography and has been one of its main application areas. Indeed, searching for occurrences of words corpora rather than registering them manually over readings has permitted lexicographers to list examples much more easily and to use the frequency information provided by the corpus data, in order to decide which meanings to include and the order in which to present them in the dictionary entry.

Since the 1970s, the first dictionaries making use of corpus data became available in English. The first large-scale lexicographic project involving the massive use of corpus data was the COBUILD dictionary, dating from 1987. Following this project, many lexicographers quickly decided to use corpus data by insisting that word census and meaning classification based on a qualitative approach were unreliable

To be sure, corpora offer many uses for lexicography (see, in particular,

The first study that we will introduce for illustrative purposes stresses the importance of using a corpus adapted to the target audience of a dictionary, rather than a general language corpus. As we said above, in the Englishspeaking lexicographic tradition, the use of corpora is now the norm, but reference corpora most often refer to the same stylistic genres.

The authors then retrieved certain keywords from the Oxford Children Corpus in order to compare them with those in the Oxford English Corpus, containing texts intended for adults. These lists were retrieved automatically, then manually compared by sorting the keywords into thematic groups for each textual genre. This analysis indicated that the two corpora had different themes. While the authors who write children's fiction talk more about nature and the physical world (including body parts, buildings, objects and time), the authors who write adult fiction mainly deal with politics, religion, work, education, human relations and death. From the point of view of functional words, texts intended for children in the corpus mainly refer to the question of space, whereas the texts intended for adults focus primarily on the temporal dimension. Many differences between corpora are also present in non-fiction genres. In addition, this comparison reveals differences in the way of addressing readers, since children's writers have a more direct and informal style.

The authors argued that these differences should be implemented in children's dictionaries in several ways. First, the differences in themes should lead to a choice of words included in the dictionary based on books written for children, rather than producing dictionaries for children that are only abridged versions of adult dictionaries. In particular, the words chosen should reflect their frequency in the books designed for the age group targeted by the dictionary. A comparative analysis of collocations in the two corpora also indicated that words are used differently in children's literature and that these differences should be reflected in dictionaries. For example, the English word play is frequently used with musical instruments and sports nouns in the two corpora.

By contrast, it is more rarely found in collocation with words indicating a role, as in he played an important role in his failure, in texts destined for children than in texts for adults. Finally, the authors argued that corpora intended for children provide excellent sources of examples, because they use words in a context which is familiar to them and involve books that they partly recognize and often appreciate.

For all the words studied, the results indicated significant differences between the results of the corpus study and the entries of different French dictionaries. For example, one of the words analyzed was mec (guy). The corpus analysis revealed four types of use for this word:

-a man as opposed to a woman (1); -any male individual (2); -a boyfriend (3); -a virile man (4).

However, most dictionaries covered only a part of these meanings. On the other hand, when a meaning was included in the dictionary, it was often done incorrectly. For example, for meaning (3), the Nouveau Petit Robert mentioned "a woman's companion", whereas the corpus showed that this use also extends to homosexual couples. Likewise, the Trésor de la Langue Française indicated that meaning (2) of the word mec listed above is often used in a derogatory way, as in certain collocations like pauvre mec or petit mec (a poor guy). However, corpus analysis has revealed that the predominant collocations are rather neutral, as in jeune mec (a young guy) or positive, as in mec bien (a good guy), beau mec (a handsome guy). Dictionary examples do not properly reflect the most frequent connotation of the word mec in the sense of the male individual. This study showed that the use of spoken corpora makes it possible to offer a more complete and appropriate treatment of frequently spoken words which are hardly ever (or not even) included in dictionaries.

Stylistics

The stylistic study of literary texts is traditionally based on a qualitative analysis of chosen text excerpts. However, the choice of excerpts is often complex, since the identification of interesting characteristics to be studied in a text is not always visible at a first reading. This is why literary scholars working in stylistics have started to acknowledge the interest of incorporating quantitative methods from corpus linguistics into the analysis of literary texts, so as to have a more objective starting point for identifying interesting themes and relevant excerpts, which should be the subject of a book's qualitative analysis. As we will see below, in the field of stylistics, corpus linguistics tools do not seek to replace qualitative analysis, but only aim to guide the choice of themes and excerpts to analyze.

Another advantage of the quantitative analysis of literary texts is that it provides clues for identifying the author of a text when the latter is unknown or controversial. In this case, a series of linguistic indicators are used for measuring the formal similarities between this text and other reference works by different possible authors. The problem then is to find out the best indicators for identifying the similarities between an author's text and its differences with the texts of other authors and to later apply these measures to the controversial text in order to determine which author this text shares more similarities with. These indicators are, for example, words and sentence length, the list of frequently used words, vocabulary richness, frequent collocations, the position of words in sentences and frequent syntactic structures

In order to illustrate the role of quantitative methods in corpus linguistics for the stylistic analysis of literary texts, we will first introduce a study on theme and keyword identification in the novel Casino Royale by Ian Fleming. For this study,

In addition to this keyword analysis, the authors classified the semantic areas of the novel, which WMatrix software can perform automatically. By combining these two methods, the authors were able to identify important words from the novel. Then, they analyzed the occurrences of these words one by one using the concordance file (see Chapter 5, section 5.7). This analysis made it possible to show how the different uses of these words contribute to build upon the themes of the novel, in a much more detailed way than through the analysis of selected excerpts, because all the occurrences involved in the development of a theme could be identified and analyzed. This study revealed the importance of supplementing the analysis of excerpts with automatic analyses covering the whole of the novel.

The second example that we will discuss in this section deals with the analysis of blockbusters, big budget movie film scripts, as a fictional genre.

McIntyre and Walker (2010) built a corpus of 200,000 words from blockbuster film scripts. Literary critics of this cinematographic genre noted that male and female characters are given unequal treatment, with a clear bias in favor of men. Gender distinctions are also reflected in the stereotypical roles occupied by the different characters. In particular, physical prowess is a characteristic trait of the male hero, and most of the time, the latter operates at the margins of society, whose established power he rejects. The authors wanted to verify whether the characteristics identified by literary critics were reflected in the language of film scripts.

The results indicated that male characters have a much longer speaking time than the female characters, more than four times more words in the corpus. Likewise, the topics men and women talk about (identified from the keywords of the corpus) also differ. Words linked to power were dominant in men's discourse, which again confirms one of the gender stereotypes. On the other hand, the analysis did not show a tendency to reject authority on the part of male protagonists, as illustrated by the frequent use of terms of address such as sir. Nevertheless, the authors added that a qualitative analysis would be essential to learn more about this last point. In short, this study confirmed that corpus analysis can help us to study not only the style of an author or a text genre but also the way in which the different characters in a written production are represented.

Legal linguistics

In recent decades, the expertise of linguists has been increasingly sought in the context of legal cases, for questions relating to all language areas. For example, linguists are sometimes required to authenticate a person's voice on a recording or to determine their geographic origin. Linguists also analyze complaints related to the linguistic complexity of certain public information documents, when users argue that they have misused a product or misunderstood their rights due to such excessive difficulty

While some of the questions mentioned above can be answered using theoretical knowledge, as in the case of Gricean maxims, many others require the analysis of corpus data. This is particularly the case of requests concerning the attribution of a text to one or more alleged authors. We have already mentioned the question of textual attribution in the context of literary stylistic analyses. This problem arises somewhat differently in the case of judicial inquiries, in particular because of the type of linguistic material involved. In this context, linguists are most often required to examine threatening or blackmail letters, suicide notes, ransom demands and police statements. However, dealing with this type of material poses a certain number of methodological challenges

The results indicated that the word then appeared once every 930 words in witness statements, as opposed to once every 78 words in police statements. Thus, frequency of this word in Bentley's statement, corresponding to one occurrence every 53 words, was much closer to police language rather than to that of the witnesses. A search for this same connective in the COBUILD corpus of spoken English showed a frequency of one occurrence every 500 words, which matches the use by witnesses rather than the police. Even more strikingly, the structure of the type then I appeared 10 times more frequently in the COBUILD corpus that the I then construction, repeatedly employed in the declaration. This result confirmed that it is very unlikely that Bentley spontaneously could have produced those sentences and that the police certainly added elements to his story. Based on this new analysis, the case was re-evaluated and Bentley was posthumously acquitted in 1998.

Conclusion

In this chapter, we have shown how corpus linguistics can be of use in different areas of applied linguistics. We have seen that language acquisition corpora have helped us to understand the different stages of this process, in particular regarding the study of the associations between the language that children hear in their environment and their own productions. We have seen that corpus analysis makes it possible to better characterize the language specificities of people suffering from language and communication impairments, by studying the different ways in which these patients interact in a natural environment. We then reviewed the multiple applications of learner corpora to better grasp second language acquisition processes and showed how these corpora can be integrated into teaching materials. We also discussed the increasingly widespread use of corpora as a basis for the creation of dictionaries and showed that these data help us to overcome many inherent limitations of a purely qualitative approach to writing dictionaries. Finally, we discussed the different ways in which the corpus linguistics methodology makes it possible to provide valuable tools for the stylistic analysis of texts, as well as for author identification in a legal framework.

3.9.

Revision questions and answer key 3.9.1. Questions 1) In addition to morphosyntax discussed in this chapter, what are the other aspects of language acquisition that are well suited for corpus-based research?

2) What are the methodological aspects that should be considered when carrying out a corpus study with children with autism spectrum disorders?

3) What type of learner corpora should be used for tackling the research questions below? a) Study of the development of negation in French during the process of acquiring French as a foreign language. b) Study of the role of culture in the ability of learners to produce speech acts of requesting in French.

4) Can we use corpora to teach the pragmatic aspects of language, such as politeness, to learners? 5) What are the advantages and disadvantages of using real examples drawn directly from corpora in dictionaries? 6) Imagine a research question in literary stylistics, which could be appropriate for carrying out a quantitative corpus study. 7) Imagine another situation apart from the ones described in this chapter in which corpus analysis would make it possible to detect language uses condemned by the law.

Answer key

1) As is always the case with corpus research, the more easily an element is searchable using surface features of the language like unannotated words and sentences, the easier it is to include in a corpus quantitative analysis using raw data. For example, this type of analysis is quite suitable for studying the vocabulary growth of a young child or, more specifically, the emergence of certain words in their lexicon. If the corpus has been annotated, as is the case of many corpora in the CHILDES database, other analyses become possible. For instance, it is possible to study the type of lexical errors made during various developmental stages or the diversification in the repertoire of speech acts which are available to the child.

2) To begin with, we should bear in mind that the use of corpora for studying the productions of children with atypical development meets the same limitations as for typical children: only the aspects linked to production can be studied and they do not make it possible to ensure that an element which is absent from the corpus actually means an inability to master it. This second point can be particularly problematic in the case of children with atypical development, because they develop compensation strategies that allow them to avoid the confrontation of elements which they find problematic. In the case of children with ASD, it is important to write down the different indications regarding the linguistic profile of the recorded children and to include them in the metadata, since autism represents a broad spectrum of skills and deficits which could lead to comparing children with very different linguistic and cognitive profiles. Above all, it is fundamental that any child recorded in the corpus has been diagnosed with ASD according to the diagnostic tests recognized in the literature, rather than following the mere indication of the pediatrician. Next, information on cognitive skills (non-verbal IQ, working memory) is also important, ideally at different time frames in the case of longitudinal corpora. Finally, it is important that data collection takes place in a context that is familiar to the child and involves as many familiar activities as possible in order to limit any blockages due to anxiety issues.

3) a) In order to study the development of negation in French during the process of acquiring French as a foreign language, it is advisable to use a longitudinal corpus covering different acquisition stages or, else, various cross-sectional corpora of learners at different levels. Negation takes different forms in spoken and in written French (optional use of "ne" in spoken data). This type of study should also compare acquisition processes in spoken and written data. b) In order to study the role of culture on the ability of learners to produce speech acts of requesting in French, it is advisable to use a corpus comprising learners of various languages and cultures. Comparing spoken and written data could also be useful. In order to determine the causes of possible differences between learners and native speakers, the use of comparable corpora produced by native speakers of French would be advisable.

4) Yes, it is possible to teach the use of politeness through corpora. That being said, as we discussed in Chapter 2, it is difficult to identify certain pragmatic uses automatically by means of a corpus search, because these uses are not transparently linked with the linguistic form used. In many cases, it is necessary to analyze the corpus manually so as to find interesting occurrences. However, automatic corpus analysis provides many examples of politeness routines, as the ones related to the opening of a conversation, to its closure or, to speech acts such as apologizing. This could be achieved by searching for specific locations in the interactions (the first or the last lines of exchanges), or through keywords like sorry or excuse me.

5) The main advantage of using real examples is that they appropriately match the language as it is used by speakers. These examples also reveal the phraseological constructions into which words are frequently grouped, making it possible to provide a rich illustration of their uses. On the other hand, these examples are often too long to be inserted into dictionaries without making any changes. Furthermore, the actual uses of words do not always make it possible for their meaning to be inferred, and therefore do not necessarily accomplish their illustrative role as intended in dictionaries. For example, one of the occurrences of the word colline (hill) in the Le Monde corpus (year 2012) is "Sur la colline, tout le monde est allé voter" (On the hill, everyone went voting). This sentence would be a poor example for understanding the meaning of this word. There are many other similar cases in the corpus. Word occurrences should be carefully sorted in order to keep the examples which are sufficiently concise and which offer an illustration making it possible to infer the meaning of the word. 6) Corpus linguistics methodology makes it possible to make contributions on many aspects of literary stylistics, since the search can be based on a keyword analysis of the book in question. For instance, such an analysis could try to identify the important themes in Molière's different plays, as well as to assess whether there are differences in the way in which male and female characters approach such themes. 7) A very good example of the use of corpus linguistics concerns the problem of plagiarism. Today, plagiarism has become increasingly easier to achieve thanks to the wide availability of texts online. By analyzing very large corpora, this crime has now become more easily identifiable by automatic means. Indeed, on a sentence longer than seven to eight words, the probability of producing exactly the same construction as somebody elseby chance -is close to 0

Further reading

The role of corpora for studying language acquisition in children is described in a very accessible way by

How to Use Multilingual Corpora

In this chapter, we will discuss the main characteristics of multilingual corpora, as well as their different uses. First, we will discuss the advantages and disadvantages of two types of multilingual corpora, namely comparable corpora and parallel corpora. We will see that one of the great difficulties inherent in the use of comparable corpora is the need to define a neutral term of comparison, called tertium comparationis, which enables us to measure similarities and differences between languages. We will discuss the different possible terms of comparison, depending on the type of research question being considered. Parallel corpora make it possible to compare texts in their original language, with the corresponding translation into one or more languages. We will discuss the particularities of translations as a text genre and show that, due to these particularities, they cannot be used as if they were original language texts. In the rest of the chapter, we will illustrate the use of multilingual corpora in the fields of contrastive linguistics, translation and bilingual lexicography.

Comparable corpora and parallel corpora

Multilingual studies can be based on two types of corpus data. First of all, comparable corpora contain original texts in different languages. These corpora are built so as to make samples as similar as possible between languages, and to prevent comparison bias. For example, it would be inappropriate to compare French editorials with English dispatches, even though these two types of texts belong to the journalistic genre. Indeed, their many differences in communicational aims and content make them different in nature, and such disparities could mask differences between languages. It is necessary to neutralize the differences in the type of data used in order to bring out the differences between languages. According to

-the time when the texts were written;

-their discursive genre (descriptive, argumentative, etc.); -the type of audience targeted and their field (law, science, etc.).

For example, in order to study the linguistic differences between French and English, one possibility would be to create a comparable corpus of leading articles from journalistic sources with a similar political orientation, published during the same years.

Parallel corpora containing texts in one or more original languages, and their translations into one or more languages, represent the second type of multilingual corpora. It sometimes happens that parallel corpora contain only texts translated into different languages from another language that has not been included in the corpus, or it may occur that the original text cannot be identified among all the texts. As we will see later, the use of corpora in which source languages and target languages remain unidentified poses major problems for contrastive linguistics, due to the special status of translations as a discursive genre (see also

Both comparable and parallel corpora have many advantages, and also some disadvantages, which we will discuss in the rest of this section. First of all, we should point out that the use of these two types of corpora is not mutually exclusive. On the contrary, the disadvantages of one type can often be counterbalanced, at least partly, by the advantages of the other, and vice versa. This is why many authors are in favor of carrying out contrastive studies on the basis of both comparable and parallel data, when available corpora and time allow for it. We will see examples of such studies later in this chapter.

The main advantage of comparable corpora is their great simplicity of access. A priori, it is possible to create a comparable corpus for any language pair, provided that digitized texts of a comparable nature are available in each language. In the case of languages that have already been the subject of numerous corpora, as is the case for European languages,

The major drawback of comparable corpora is that researchers have to find data that are highly similar in different languages, in order to avoid blurring comparisons, as we have already discussed. In addition, usage conventions may vary considerably between languages even when the same text genre exists in both, which makes them difficult to compare.

In addition to the difficulty of identifying suitable corpora, from a linguistic point of view, the main limitation regarding the use of comparable corpora is the need to find a neutral term of comparison, undeformed by the prism of either language. Finally, we should point out that while linguistic features can be identified when comparing words or syntactic structures in different languages, these traits are nonetheless difficult to annotate systematically. Indeed, they require a complex type of linguistic interpretation and analysis on the part of the annotator, which, in many cases, implies that the results of the annotation may differ when performed by several annotators (see

Unlike comparable corpora, the main advantage of parallel corpora is that they guarantee excellent comparability between languages, since the texts they contain are the same. These corpora make it possible to look for equivalences between words, syntactic structures and discursive phenomena, without having to set points of comparison. As a result, comparing languages through the use of parallel corpora is greatly simplified in contrast to comparable corpora because annotators can keep a track of translation equivalents without having to annotate syntactic or semantic features. This method is called translation spotting in the literature

Another practical problem associated with the use of such corpora is their limited availability. In fact, not all languages or discursive genres are regularly translated. In most cases, translations correspond to written genres, often related to the administrative or the literary field

In addition, language pairs that are regularly the subject of direct translations from one into the other are also limited. What is more, these corpora often include a single source language and a single target language, which makes it impossible to generalize results beyond that particular language pair.

In order to overcome certain limitations pertaining to parallel corpora, the ideal would be to work with a bi-directional corpus, where both languages are alternately source and target, since these corpora make it possible to combine the two types of multilingual data discussed above (comparable and parallel). Bi-directional corpora offer the possibility of studying equivalences in both translation directions through the use of parallel corpora.

In addition, these corpora can be used as comparable corpora, produced in very similar situations, when analyzing only the original language portions of the corpus, as illustrated in Figure

Certain corpora fulfill these conditions, such as the Europarl Corpus, a corpus of debates at the European Parliament, where each member employs their own language and whose exchanges are later transcribed and translated (see Chapter 5 for a list of these corpora).

Looking for a tertium comparationis

One of the main difficulties inherent in contrastive studies is to find a suitable point of comparison between languages. The problem is that, by nature, comparing two languages implies comparing systems that are partly incommensurable. Therefore, linguists are confronted with the challenge of finding common elements around which languages are close enough so as to be comparable. In fact, relevant differences between languages can only be observed insofar as the latter are compared on the basis of a similar concept or structure. If the objects compared differ in nature, then the differences observed will not be relevant. Let us take a practical example. Observing that mice are smaller than elephants is irrelevant to understanding the morphology of mice or elephants, since these are different animals. On the other hand, observing the differences in size between a Chihuahua and a Saint Bernard is relevant for understanding the different morphologies of dogs.

Contrastivists call this point of comparison between languages tertium comparationis. Such a point of comparison should be determined in a neutral manner in relation to the functioning of one language or the other, in order not to bias comparisons. For example, comparing the phonological system of French and German using a list of German phonemes as a starting point would provide a biased comparison, since the comparison platform is not neutral but established on the basis of one of the language's categories. It is therefore necessary to choose a point of comparison that can be applied to both languages and, which is, as far as possible, neutral. For example, when trying to compare tense categories between German and English, Gast (2012) selected different time spheres along a time axis, including the Past Tense, the Present Perfect, the Present Tense and the (will) Future, independently from both languages. He then drew a line corresponding to the time interval that each tense category covered in each language. Through this comparison, he showed that the English Past Tense and the German Präteritum seem to cover similar time intervals, whereas the English Present Perfect and the German Perfekt do not have the same function. Thus, while the English Present Perfect only applies to events in the near past, the German Perfekt covers a wider range which also includes the distant past, as illustrated in Figure

The suitable tertium comparationis type for carrying out a study depends on the kind of linguistic elements compared (phonemes, syntactic structures, speech acts, etc.). A distinction can be made between the tertium comparationis based on linguistic forms and those based on linguistic function

A tertium comparationis determined exclusively by formal criteria, however, is not appropriate, not even for comparing structural elements from different languages. As we have seen previously, English and German have two verbal tenses to refer to the past. Thus, from a structural point of view, we could say that these two languages are similar. However, uses between the two languages are quite different. Conversely, a language may lack a certain linguistic form but still express it through other means. For example, in some languages, speakers verbalize the source of information (which they have acquired either directly by their own perception or indirectly by inference or hearsay) by means of a verbal suffix. These languages have what is called an evidential verbal system. This is not the case in French, which does not have such suffixes in its verbal morphology. However, French speakers have other means of indicating sources of information in their statements, in particular by adding phrases such as il paraît que (it seems that), j'en conclus que (I conclude that) or je vois que (I see that). So, to infer from the absence of a suffix that the French language does not make it possible to express belief sources would therefore be wrong. That being said, the fact that languages express certain concepts by different means can, in certain cases, give rise to interesting differences, particularly at the age when these elements are acquired by children and the way in which speakers encode this information. The potential impact of such encoding differences on speaker's cognition is known as linguistic relativism (see

In many cases, a tertium comparationis based on semantic equivalence appears to be preferable to a tertium comparationis based on formal criteria. However,

In summary, in addition to being based on corpora with high comparability, contrastive studies should use neutral points of comparison that make it possible to establish comparisons between linguistic phenomena across languages, which are as relevant and adequate as possible. Depending on the research question, the appropriate equivalence levels will be different.

Translations as a discursive genre

The main question raised by the use of parallel corpora concerns the status of translations and, more specifically, the possibility of using them as language samples. An important amount of research carried out since the 2000s has shown that translations represent a discursive genre in their own right, and that translations do not fully share the same properties as texts written in original language. This discursive genre is also sufficiently stable and different from others so as to be identifiable using machine learning algorithms for automatic text classification

One of the reasons why translations represent a stylistic genre different from original texts is that these keep a certain imprint of the source language. Even if translators are language professionals, their lexical, grammatical and stylistic choices are still influenced by what they have to translate. For example,

In addition to these influences, which vary from one source language to another, some authors have hypothesized that translations are so similar (to the point of making up a stylistic genre of its own) due to certain effects related to the translation process itself. These effects might reflect translation universals rather than the variable effects pertaining to the languages involved

To conclude, in order to limit the bias introduced by the use of translations, it is desirable to use bi-directional corpora as far as possible, as well as to study language equivalences in the two directions of translation. We will see examples of such corpora later in this chapter. We will illustrate the fact that these corpora can help us to work simultaneously on comparable and parallel data, and thus exploit the advantages of each, while limiting their bias.

Multilingual corpora and contrastive linguistics

In its beginnings in the 1950s, contrastive linguistics emerged as a discipline aiming to compare two or more languages with the aim of improving language teaching methods. Indeed, linguists working on language teaching had long observed that mistakes made by learners were often linked to transfers from their mother tongue. This observation justified the systematic study of differences between languages in order to better understand the risk of making mistakes in different learner populations (see, in particular,

These new data led to a relative abandonment of contrastive studies for several decades. The situation has changed a great deal since the 1990s, thanks to the arrival of corpus linguistics, which made it possible to empirically compare linguistic systems. The data provided by these contrastive corpus-based studies are not only useful in theoretical linguistics for understanding how languages work, but may be helpful for other applications, notably for the development of tools such as bilingual dictionaries (see section 4.6). In this section, we will present a sample of studies which illustrate the usefulness of corpora for carrying out contrastive linguistic studies.

The first case study that we will discuss concerns the French-English language pair and, more specifically, how the verbs faire in French and make in English work, both of which can be used in causative constructions such as faire rire or make believe. On an intuitive level, it may seem that these verbs share a similar meaning and perform equivalent functions in both languages. However, by means of an empirical study of both comparable and parallel data,

Gilquin's study is based on the PLECI bi-directional parallel corpus, which contains newspaper articles and fictional texts in English and French. This corpus can be useful both as a comparable corpus and as a parallel corpus, as illustrated in Figure

The results revealed some similarities between the two languages. First, the distribution of occurrences between nominal and pronominal subjects was very similar. Second, the two verbs were mainly complemented by verbs describing concrete actions such as partir rather than existential verbs like exist. Despite these similarities, significant differences were also observed. To begin with, in terms of frequency, the verb faire appeared four times more frequently in texts in original French compared to make in original English texts, and this was a first indicator that the role of each is not the same in both languages. Furthermore, verbs used with make were much more limited than those used with faire. The four most frequent verbs in English (feel, look, work and think) represented 25% of the occurrences. By contrast, in French, 12 different verbs were needed to reach this same proportion of occurrences. Conversely, some of the uses of the verb make seem much more atypical than the verb faire. For example, the verb make was mainly used in relation to inanimate subjects, which was not the case with faire. In sum, although the two verbs have a partly convergent semantic profile, each of them also has frequent uses that are not found in the other language in a similar proportion. These semantic differences indicate that the verb make might not be the best translation choice for faire, and the other way around. In order to empirically determine the percentage of correspondences between two words, a mutual correspondence (MC) value can be calculated. This value takes into account the number of translations by the supposed equivalent word compared to the total number of occurrences, in both directions of translation

As + Bs

In the case of the pair made of faire/make, the MC value was 15.4%. Such a low value tends to confirm that these two words are not equivalent. In most cases, the causative construction faire + infinitif in French is translated by an English verb carrying the notion of causality, also called the synthetic causative. For example, the expression faire taire is often translated using the verb to silence. In the case of the verb make, its most frequent translations are the verb make as well as paraphrases, for English expressions that cannot be literally translated into French. For example, the sentence "it was the very intensity of her devotion that had made her give him a softness of upbringing…" was translated by adding "Par un excès de tendresse, Lady O'Connell l'éleva avec une faiblesse…".

In a nutshell, this study made it possible to show that two words which may seem close, and which are often described as translation equivalents in reference tools such as bilingual dictionaries, are in fact partially different from each other. Furthermore, these differences can only emerge on the basis of a quantitative corpus study, which highlights the differences in frequency and context of use.

The second study we present in this section was devoted to the analysis of the different factors that influence translations in parallel corpora. To do this, Dupont and Zufferey (2017) compared the way in which concessive connectives are often treated as translation equivalents in bilingual dictionaries, namely: however, yet, nevertheless and nonetheless in English and respectively pourtant, toutefois, néanmoins and cependant in French. The authors specifically studied the role of three factors in the observed equivalences: the translation direction (French-English or English-French), the stylistic genre (journalistic texts or parliamentary debates) and the translators' degree of expertise (non-professional volunteers, journalists or qualified translators). For this study, the occurrences of the eight abovementioned connectives were drawn from three parallel corpora (Europarl for the parliamentary debate genre, a corpus of newspaper articles and the TED corpus of online conferences; see Chapter 5 for a description of these corpora). These occurrences were then manually disambiguated in order to remove occurrences which had not been used as a concessive connective, for example when the connective yet was used to indicate a temporal relation.

The results showed that in original texts, the frequency of connectives often vary depending on language register, particularly in English, where the four connectives vary significantly. In French, only the connective pourtant varied significantly between journalistic texts and parliamentary debates. An analysis of translations also showed differences between the two genres. For French connectives, the typical translations in the journalistic genre were either the generic connective but, or there was an outright absence of a connective in the translation. In the parliamentary debate genre, more specific connectives were used: the connective however was a frequent translation for the four French connectives, not to mention yet as the translation of pourtant and nevertheless for néanmoins. Such a tendency to omit connectives in the journalistic genre can also be found in English. This observation can no doubt be explained by the concern for efficacy in this genre, which tends to limit the amount of words used. The other translations were more variable than in the French-English direction.

We can see that the direction of translation is an important factor to take into account when establishing equivalences between languages. Differences between stylistic genres were also visible in the MC values between connectives. Indeed, these values were very low in the journalistic genre, oscillating between 14% and 27%, against 33% and 57% in the parliamentary debate genre, which reflected the above-mentioned more specific translation choices.

The last variation factor analyzed in this study referred to the translator's level of expertise. On the one hand, European Union translators are qualified professionals. On the other hand, the translations provided for TED conferences are carried out by volunteers. Finally, journalistic text translations are generally carried out by journalists, who are language professionals but not translation professionals. For the English-French pair (remember that the TED corpus is unidirectional), these variations enabled the authors to compare the impact of this variable on the translations under scrutiny. The comparison revealed that translation choices were systematically less varied in the TED corpus than in other corpora. The number of zero translations was also significantly lower. This trend reflected the fact that amateur translators are more likely than others to use the source text as a guide and to avoid structural changes as much as possible (see also

In summary, this study showed that the type of equivalences observed between languages can be variable across discourse genres. However, contrary to what happens in monolingual studies, contrastive studies are often performed on data from a single genre -due to the scarcity of multilingual corpora -which does not always make it possible to compare different genres. This study also showed that equivalences between languages should be considered separately for the two translation directions. Finally, the degree of expertise of translators also plays a role in their translation choices, and this factor should therefore be taken into account in the study of parallel corpora.

Parallel corpora and translation studies

Translation studies is the scientific study of the processes at work in translation, as well as the factors that influence their realization. While translation is a practical and applied discipline, translation studies (translatology or traductology) is a theoretical science. As in the case of contrastive studies, translation studies has benefited from the availability of multilingual corpora, as well as theoretical and methodological advances in corpus linguistics. As we will see in this section, the use of large multilingual corpora makes it possible to carry out quantitative studies on different language pairs simultaneously and, therefore, go beyond the isolated observations that can be made on the basis of individual practice. Later, we will see that the use of the empirical methodology ingrained in corpus analysis can also work as a guide for the translator when it comes to making certain translation choices.

The first study that we present in this section looked into the existence of translation universals. As discussed previously, translations differ in several ways from original texts produced in one language. Translation studies specialists have suggested that a portion of these specificities can stem from the existence of translation universals, that is, from phenomena specifically pertaining to the translation process. One of these universals concerns the supposed propensity of translations to be more explicit (explicitation phenomenon), in terms of cohesion markers, than original texts. This hypothesis has been partly confirmed through corpus studies, performed on a single language pair and limited to one translation direction. Due to these limitations, these studies cannot be generalized to all translations.

In order to overcome this limitation,

The explicitation hypothesis relates to the number of cohesion markers present in translations, which is assumed to be higher than in original texts. Among these, Zufferey and Cartoni chose to focus on the category of causal connectives. Indeed, their use is very frequent, and often optional. In other words, they can be omitted without creating comprehension difficulties

Furthermore, the authors were able to observe that the explicitation rate varied significantly depending on the causal connective in question. On the one hand, some connectives like parce que in French and because in English gave rise to very few explicitation cases. On the other hand, causal connectives like puisque in French and given that in English gave rise to many explicitation cases. The authors attributed this gap to the different semantic profiles of connectives. Those that give rise to explicitation are typically used for introducing a cause presented as already known or easily inferred by the interlocutor, unlike the other connectives which are used for announcing a new cause for the interlocutor (see

The second study that we will discuss does not deal with the analysis of translations themselves but with the stylistic analysis of the source text, namely the search for recurring patterns and monitoring how these patterns are translated.

By analyzing the recurring sequences and the keywords they contained, the author was able to show that these repetitions played a particularly important stylistic role in the novel (which also contains many more repeated sequences than other works by the author), and that these repetitions should be maintained in the translation in order to preserve the spirit of the text. In fact, these sequences made reference in part to the titles of other literary works, and helped to grasp certain intertextuality elements. However, an analysis of the translations of these 27 recurring sequences, both by the Finnish translator and by the Czech translator, showed that they were mostly neutralized by stylistic choices avoiding repetitions. The tendency of translators to avoid repetition is also one of the recurring trends identified in translations, and some translation theorists point to various techniques for achieving that result (Ben-Ari 1998). However, as in the case of Irving's novel, the presence of repetitions may be an integral part of the work's style and erasing them would certainly involve a form of stylistic loss.

In this way, we can see how corpus linguistics can provide translators with tools that may help them adapt their translation choices on the basis of a better identification of the recurrent linguistic properties at work in a text.

Parallel corpora and bilingual dictionaries

We have already discussed the importance of corpora for monolingual lexicography in Chapter 3 (section 3.5). In this section, we will refer more specifically to the role of parallel corpora in the creation of bilingual dictionaries. Bilingual dictionaries are essential for foreign-language learners but they are also controversial among language professionals, especially translators. The latter, in particular, criticize bilingual dictionaries for the limited list of equivalences that they provide and the lack of context, which often prevents users from making an appropriate distinction between the different meanings of a word or expression. Finally, as monolingual dictionaries, these dictionaries do not provide any indication regarding the frequency of the different meanings, apart from the order in which they are listed.

To some extent, equivalences between languages obtained through the use of parallel corpora respond to such criticisms. Corpora provide access to a broad context and offer a greater variety of equivalences than dictionaries. What is more, these can be easily classified by frequency, and differentiated according to the textual genres under consideration. In addition, computerized word alignment techniques make it possible to automatically produce bilingual dictionaries (see, for example,

In order to illustrate the importance of corpus data for providing more suitable translation equivalents than those of bilingual dictionaries, in this section, we will discuss a study concerning partially equivalent word pairs in French and in English.

At a second stage of the study, the authors looked for occurrences of these words in French-English comparable corpora. They chose 100 occurrences in each language and annotated them with the different meanings listed in monolingual dictionaries. They confirmed that some of the meanings frequently found in the corpus could not be adequately translated by their "equivalent". For example, 75% of the occurrences of plus au moins in the corpus should have been translated using expressions such as pretty much or somewhat in English, rather than using the expression more or less. The authors concluded that bilingual dictionaries do not provide enough information for helping users access the correct translation equivalents.

Many other studies have compared the translation equivalents provided by bilingual dictionaries with equivalents observed in parallel corpora. These studies invariably highlight a discrepancy between the translation equivalents found in dictionaries and in corpus data. In most cases, the equivalents provided by dictionaries are much more limited than the equivalents found empirically, or vice versa, dictionaries sometimes list equivalents that are completely absent from corpus data. We will work on two examples by way of illustration.

Conclusion

In this chapter, we have discussed the different uses of comparable and parallel multilingual corpora. We have seen that their advantages and disadvantages are often complementary, and that it is useful to combine these two types of resources in contrastive linguistics. The study of translation often relies on parallel corpora, but can also make use of comparable corpora of texts translated into different languages, without considering the source language. In the field of translation studies, one of the major aims of such studies is to analyze the features of the translated language, with the purpose of looking for translation universals. We have also shown that corpus analysis methods can be useful for uncovering recurring patterns in a source text and to better adapt the strategies used for its translation. Finally, we argued that parallel corpora have become indispensable resources for the creation of bilingual dictionaries, since they provide rich lists of translation equivalents accompanied by their contexts of use, as well as information concerning their frequency in various genres. 3) Why can we say that translations are a full-fledged text genre? 4) What are the parameters to take into account in order to carry out a contrastive study on the use of the indefinite pronouns on in French and man in German? 5) How could we test the supposed translation universal according to which translations are simpler than original texts by means of a parallel corpora study? 6) What types of equivalences are most likely to be insufficiently dealt with in bilingual dictionaries?

Answer key

1) a) In order to study the similarities and differences between the causal connectives porque in Spanish, parce que in French and perché in Italian, the use of a parallel corpus offers great advantages. Indeed, such a corpus makes it possible to establish the degree of mutual correspondences between these connectives, by counting the number of times that they can be translated by each other. Nonetheless, the use of this method also involves the risk of having a distorted vision of the functioning of these connectives, due to the translation prism. This study should therefore be supplemented by a semantic and pragmatic analysis on how these connectives work in the original language, by means of comparable corpora. For instance, the use of these connectives could be compared only in the source language section of the parallel corpus. b) Conversely, to study the way in which European elections are reported in the press in Germany, France and England, the use of comparable corpora seems the most judicious choice. Indeed, for this study, it is important to have access to texts that were originally produced in each language, in such a way that they reflect both the linguistic structures of each language and bring out potentially different discourses regarding the same event. A parallel corpus, containing translations, would not be able to meet these two objectives.

2) a) The comparison of the consonant system in German and French can be done on the basis of formal rather than functional equivalences. In particular, consonants can be compared on the basis of their articulatory features.

b) In order to compare speech acts of thanking in French and Chinese, a tertium comparationis based on pragmatic equivalence is necessary. It is not only the words or expressions that should be compared, but also their illocutionary force, that is, the communicative intention of the speaker.

3) Several reasons have been given in the literature for explaining the linguistic specificities of translations. The first type of explanation concerns the influence of the source language, which inevitably leaves traces in translations. Even if translators are language professionals, they are inevitably influenced by the words and linguistic structures they have to translate, which leads them to make different lexical and syntactic choices than those of a speaker writing in their mother tongue. The second category for explaining translation specificities is of a general nature and is based on the supposed existence of translation universals (linguistic phenomena resulting from the very process of translation), regardless of the source and target languages involved. These universals include simplification, explicitation and standardization. All these processes reflect the pedagogical role of translators, who (unconsciously) try to improve the readability of texts.

4) First of all, this study should be carried out by means of a parallel corpus, in order to determine to what extent these two pronouns are translation equivalents or not. More specifically, a bi-directional parallel corpus should be used, since the equivalences are often variable depending on the direction of translation. This analysis of translations should be supplemented by a study on comparable corpora, made up of the two original language sections from the parallel corpus. For this analysis, the important point would be to establish which comparison factors would best highlight their common points and their differences. In this case, the possible factors could be the tense and aspect of the verb following the pronoun, etc. Finally, this study should, wherever possible, include two different discourse genres, in order to measure the extent of the variations between them.

5) The simplification universal implies that translations should be simpler linguistically than the original texts of the same discursive genre. Various lexical and syntactic factors, easily measurable, could contribute to this simplicity. For example, lexical simplicity implies that the number of different words should be smaller than in an original text. This can be measured thanks to the type/token ratio (see Chapter 8). Syntactic simplicity is measured, for example, by the average length of sentences. The number of words used per sentence can also be calculated, even on a corpus that has not been subjected to syntactic annotations.

6) The most problematic equivalence cases for bilingual dictionaries are partial equivalences, just as those we discussed in this chapter for expressions such as plus au moins and more or less. In these cases, the formal proximity and the identification of certain cases in which these expressions are equivalent may suggest that these words are completely equivalent, when actually they are not. Conversely, false cognates, where meanings are completely different between languages despite a formal resemblance, are easier to identify, since their meaning clearly appears to be different.

Further reading

How to Find and Analyze Corpora in French

This chapter has two aims. Firstly, we will introduce the main existing corpora in French. These corpora can be divided into four categories:

-written corpora; -spoken corpora; -corpora devoted to specific demographics, such as children or learners;

-multilingual corpora where one of the languages included is French.

Secondly, we will present a set of concordancers, which are corpus analysis tools, and discuss their main functionalities. Links to websites providing online access to corpora, as well as corpus consultation tools, are listed at the end of the chapter.

Corpora formats and their availability

Thanks to the Internet, in recent decades sharing corpus data has become far simpler. For example, it is very common for research teams to offer the corpus compiled during their research projects as a tool available to the general public, once the project is finished. The rights of access and use of this data may vary depending on the content and project in question. In some cases, corpora available to the public can be downloaded directly from a website. In other cases they are not downloadable, but instead are only available online, via a dedicated search interface. We will discuss the advantages and disadvantages of these different formats in this section. It should be noted that use for commercial purposes may be subject to specific additional conditions.

The main advantage of downloadable corpora is their great flexibility for carrying out word or structure searches using a concordancer (see section 5.6). These corpora can also be annotated manually or automatically. An important element to take into account when downloading a corpus from the Internet is its encoding format (this not to be confused with the compression format, when applicable, that needs to be handled by means of specific archiving tools). When several formats are available, it is important to choose a format which is compatible with the research tools that will be used for the study. For example, the AntConc concordancer that we discuss can only process text format files (or files with XML or HTML tags which can also be treated as text files). Another element to take into consideration before deciding to download an entire corpus is its size. Indeed, some current corpora like the Google Books corpus

Many corpora can only be viewed online using a dedicated research interface. The advantage of this format is its great simplicity of use. In fact, most interfaces offer user-friendly methods specifying the choice criteria, such as gender, type of speaker, time period, etc., as well as fields for typing in the element(s) to be looked for in a full text search, sometimes enabling the use of search patterns (called regular expressions, see section 5.6). The major drawback of these interfaces is that they do not authorize any type of search. Some are limited to a continuous character string, something which prevents the search for compound words, like chemin de fer (railway) in French, which includes three separate strings of characters. If the search patterns are not usable, this further complicates the search. Let us take a look at an example: it is possible to look for all the occurrences of a regular verb like aimer using a single query looking for the root aim, followed by a wildcard replacing an unspecified number of characters, for example aim*. If this type of search is not enabled by the interface, all the verbal forms must be looked up one by one with their exact forms.

Another limitation to the use of online corpora available for consultation is that they only offer limited access to their metadata (see Chapter 6). Thus, important information for certain research questions is regularly missing. For example, the online interface on the OFROM corpus (see section 5.3) only offers the possibility of looking for productions by women or men, but for the moment does not indicate the total number of participants of each genre, or the number of words produced by each of them. This type of information is nonetheless crucial for the quantitative comparison of linguistic productions between the two genders. In the same way, the consultation interface for the CLAPI corpus (see section 5.3 for a description) does not currently provide information about the total number of words included in the portion of the corpus that is available for online consultation. In many cases, this piece of information can be obtained by contacting the corpus creators. However, when this type of information remains unavailable, these gaps lead to serious limitations in data analysis (see Chapter 8). The problem of sources is even more acute in the case of databases grouping different types of corpora, such as the Lextutor database, which contains both spoken and written data, retrieved from different genres, but unevenly distributed. These databases are useful for quickly finding concordance examples but cannot be seriously considered as representative corpora (see Chapter 6).

In addition to the two above-mentioned distribution formats, some corpora that are available online require prior user registration, as well as explicitly stating the research purpose for which the data will be used. For example, this is the case of the Belgian Valibel database of spoken French (see section 5.3) or the SMS corpus in Switzerland's national languages, collected by the universities of Zurich and Neuchâtel (see section 5.5).

Other corpora are still not distributed for free but can be obtained by paying a varying fee, depending on whether the intended use is for research or for commercial purposes. After purchase, Le Monde newspaper corpus (see section 5.2) can be downloaded via corpora distribution sites such as the European Language Resources Association or ELRA, the Linguistic Data Consortium or LDC, or distributed in a CD-Rom format, such as the French SMS corpus collected in Belgium. Other corpora such as the new version of the Frantext literary text corpus (see section 5.2) are accessible via an annual renewable subscription. Many corpora are also available via the Sketch Engine online platform (see Chapter 6), which is free to access for many institutions in European countries. In addition, institutions often finance the purchase of corpora for their members, so it is advisable to check with one's institution before engaging in any individual purchases.

Finally, we can mention that it is possible to build new corpora from websites that distribute royalty-free data, for example, literary texts now available in the public domain, government data such as parliamentary debates, or texts from participative sites like Wikipedia. In all cases, it is necessary to study the rights of use indicated by the respective sites before starting to compile the corpus. We will discuss this issue in more detail in Chapter 6, which is devoted to presenting the basic principles of corpus creation.

To conclude, it is important to emphasize that, regardless of the format in which a corpus can be accessed, reusing corpus data amounts to benefiting from the often long and costly work carried out by other research teams. That is why, when existing corpora are used, their source must be explicitly mentioned. More specifically, when researchers reuse a corpus created by other teams, they must mention in their publication the Internet link where the data were downloaded or retrieved from. Very often, the authors of a corpus provide a bibliographic reference where they describe their corpus or the name of the team who compiled it. These references must be quoted in any paper making use of the data.

Reference corpora

Unlike many European languages, French still does not have a reference corpus, a representative sample of the French language in general, similar to the British National Corpus that exists for British English, one of the pioneers in the genre. For the time being, it is not possible for linguists to observe how the French language works through the study of a single corpus. However, a multi-genre French corpus is currently in development and should offer an open access phase to the general public in the coming years

For the time being, the closest to a reference corpus for French is the corpus of contemporary French created within the framework of the Orféo project

The Sketch Engine corpus management system

In addition to the big generic corpora, for certain types of research, using corpora belonging to a specific type of genre may prove to be a wise choice. The results obtained from different specific corpora can also be combined in order to improve the generalization of results. We will describe these corpora in the following sections.

Written French corpora

In the field of journalism, the most exhaustive resource is undoubtedly the Le Monde corpus, which contains the newspaper's archives for the period 1987-2012, representing a total of nearly 1,200,000 articles, corresponding to almost 20 million words per year. The Le Monde corpus is a valuable tool not only for exploring the French journalistic style but also for studying recent developments in the language, thanks to its data spanning 25 years. Unfortunately, this corpus is not available for free and must be purchased via the ELRA platform. On the other hand, the newspaper's articles for the year 1998 are available for free online consultation via the Lextutor platform.

Another newspaper is also a good reference for the French journalistic style. The Corpus Journalistique issu de l'Est Républicain includes articles from this regional newspaper for the periods

In the more specialized journalistic genre, the Sciences Humaines corpus produced by ATILF (Analyse et Traitement Informatique de la Langue Française [Computer Processing and Analysis of the French Language]) in Nancy includes 125 linguistic articles from the Sciences Humaines journal. Despite its modest size, this corpus makes it possible to study the specificities of the journalistic style when applied to a particular field. The corpus is now available via the Ortolang platform, where it can be downloaded for free.

As for the literary genre, the Frantext corpus brings together many literary texts ranging from ancient to modern French, in a corpus which totals more than 250 million words. Since 2018, a new version has made it possible to consult the corpus by means of an improved interface, facilitating the search for regular expressions. The corpus has been lemmatized and tagged into grammatical categories, which also helps in refining the search criteria. This version of the corpus is available online but requires a paid subscription. A portion of the corpus, including works from the 18th to the 20th Century, can be downloaded for free from the Ortolang website. The site's interface makes it possible to choose works based on different criteria, such as the time period or the author.

The Base du français médiéval (Guillot-Barbance et al. 2017) offers access to different diachronic corpora. The main corpus, BFM 2016, includes 153 texts, corresponding to more than 4 million words. This database also provides access to the Corpus représentatif des premiers texts français or CORPTEF, which brings together texts from the 9th to the 12th Centuries, and to the Passage du latin au français corpus or PALAFRALAT, which aims to document the linguistic transitions between Latin and French. These data are available free of charge and can be viewed through an online interface. Most of the texts can also be downloaded in PDF format.

The Google Books corpus

In the field of new media, the CoMeRe database includes communication corpora mediated by networks, such as SMS/text messages, tweets, blogs, etc. These data are accessible via the Orféo platform. Also in the field of new media, the Belgian sms4science corpus

Finally, the Corpus Français de l'université de Leipzig, which is not actually a corpus stricto sensu as it contains a set of isolated sentences rather than whole texts, brings together different sources such as newspapers and web pages, as well as entries from the participative encyclopedia, Wikipedia. The data collection mode makes this corpus unsuitable for many types of research but provides a very useful interface for lexical searches, offering the possibility of looking for simple or compound words and having access to all the occurrences within the context, with an indication of the source for each occurrence. The interface also automatically generates a list of the most frequent co-occurrences for each word, as well as the most frequent words to the left and to the right of the word in the search. For each request, the interface returns an indication of the frequency rank of the word looked up in the corpus. This piece of information makes it possible to estimate the potential difficulty of a word, for example, in the context of language teaching or for preparing experimental material, by controlling the frequency of the words used in the experimental materials.

Spoken French corpora

Numerous spoken corpora have emerged since the 2010s. Here, we limit our presentation to resources of a general nature, which are at least partly publicly available. However, many other more specific resources can also be downloaded for free from the Ortolang platform.

The corpus of spoken languages in interactions or CLAPI

The Corpus oral de français de Suisse romande or OFROM corpus was collected at the University of Neuchâtel

The Corpus de français parlé parisien dans les années 2000 or CFPP2000

The Corpus de français parlé au Québec or CFPQ was collected at the University of Sherbrooke

The Belgian French Valibel corpus

The Traitement des corpus oraux en français project or TCOF from the ATILF laboratory brings together corpora collected between the 1980s and 1990s, and later enriched in the 2000s. The portion of the corpus available to the public not only includes interactions between adults and children, but also interactions between adults only. It contains 124 transcripts of dialogues, ranging from 5 to 45 minutes aligned with the sound, representing a total duration of 124 hours. The CID corpus

Finally, the Backbone Project contains many videos of interviews with young speakers of different languages, including French. This corpus was designed to document less commonly taught languages or regional varieties of widespread languages. In the case of French, the interviews include young people from the Guadeloupe and Montpellier regions, in particular. This corpus also has an educational purpose in the area of language teaching. This is why it has incorporated grammar, lexicon and language register annotations, which can be looked up through the online interface.

Children and learner corpora

Many language acquisition corpora can be found in the Child Data Exchange System or CHILDES database

A specific section of the CHILDES database is dedicated to French corpora. In 2019, this section amounted to a total of 16 corpora. The majority of them (12 out of 16) are longitudinal corpora, comprising between one and six children. Half of the corpora focus on very early childhood, with speakers between 1 and 3 years of age, a period during which many elements of the spoken language are acquired. The other corpora include children up to the age of 7, and only one of them (VionColas corpus) includes children up to 11 years old. Six corpora are only available as written transcriptions, five others also have access to sound, and five of them include a video recording.

The CHILDES database also offers a section on bilingualism, including five corpora for which one of the languages is French, the other language is Portuguese (Almeida corpus), Dutch (Amsterdam corpus), Russian (Bailleul corpus) and English (GNP and Watkins corpora). These corpora contain recordings from one to seven children, ranging from 1 to 7 years old. Most of them are longitudinal corpora.

A section of the CHILDES database is dedicated to children with atypical language development. Some of the corpora in this section include data in French. The FoudonReboul corpus is a longitudinal corpus of eight children with autism spectrum disorder (ASD), recorded between the ages of 4 and 9 years. The Nadig corpus also includes 28 French-speaking children diagnosed with ASD, aged between 3 and 7 years. Le Normand corpus includes seven children diagnosed with epilepsy and specific language impairment (SLI), recorded between the ages of 4 and 5 years.

Apart from the CHILDES database, some recent corpora have aimed to study the development of written language in older children. This is the case of the EMA écrits scolaires corpus (Boré and Elalouf 2017), a longitudinal corpus bringing together the text productions of primary and middle school children. A portion of the corpus contains narrative texts produced by CP and CE1 students (6-7 years old), while the other section is made up of a series of argumentative texts produced by CE2 and CM1 students (8-9 years old). The corpus includes images of handwritten texts, raw transcriptions in text format and an annotated transcription, also in text format. This corpus can be downloaded from the Ortolang platform.

In the field of written French language acquisition, the Littéracie avancée corpus produced by the Laboratoire linguistique et didactique des langues étrangères et maternelles (LIDILEM) of Grenoble Alpes University is made up of writings by undergraduate and master's degree students, covering the entire span of study. It contains academic writings such as dissertations, book synopses and reports, as well as motivational letters. The corpus is made up of 11 sub-sections containing at least 10 texts each, produced under similar conditions, namely by students of the same level. This corpus can be downloaded from the Ortolang platform.

In the area of French as a foreign language, numerous learner corpora have been collected. Here, we will only discuss those that are at least partly available to the general public. A more exhaustive list of learner corpora in many languages is provided on the Center for English Corpus Linguistics (CECL) website, from UCLouvain in Belgium. Many learner corpora are also available on the TalkBank online database.

The Corpus écrit de français langue étrangère or Lund CEFLE Corpus brings together texts produced by Swedish learners of French, aged between 16 and 19 years with varying skill levels. The texts are compositions of a descriptive or narrative nature, as well as stories created on the basis of images for description. The corpus amounts to approximately 100,000 words but only part is publicly available. This portion of the corpus comprises a longitudinal section, where each learner has produced four texts. Learner levels range from initial to very advanced, with three to four students for each level. The cross-sectional portion of the corpus includes 136 texts written based on the same image description task, by learners from the initial level to the advanced level, and by a control group of native speakers. All of these can be downloaded in text format.

The Dire autrement corpus, created in Canada by Marie-Josée Hamel and Jasmina Milicevic, contains texts mainly produced by English-speaking learners. It totals approximately 50,000 words and gathers material from different textual genres, either of a narrative or an argumentative nature. The corpus is available on request from the authors.

The French Learner Language Spoken Corpora created by Florence Myles and Rosamund Mitchell brings together seven spoken corpora by French language learners. Six of them were collected at English universities and include English-speaking learners of French, who often studied at university level or during high school. The last one (Brussels' project) includes Dutch-speaking learners. Learner levels vary depending on the corpus. Some of the corpora are longitudinal and others are cross-sectional. These corpora can be downloaded or viewed via an online interface. All the corpora have been transcribed in CHAT format and can be explored with the CLAN tool (see section 5.6.3).

The Phonologie du français contemporain corpus

The Interfra corpus created by Inge Bartning and Fanny Forsberg Lundell focuses on Swedish learners of French at different levels. The corpus contains interviews, narrations based on videos, and images. The first part of the corpus includes French learners who have been exposed to the language within the context of schooling. On the one hand, it comprises high school students and, on the other hand, university students from beginner to advanced level. A second portion of the corpus focuses on advanced learners who have all lived in France, for a period ranging from 1-2 years to more than 30 years (see Chapter 3, section 3.3 for a study based on these data). Control groups of native speakers were also recorded. The corpus is fully available to the public and can be viewed free of charge via an online interface.

The University of West Indies Learner Corpus or UWI L2 Corpus created by Hughes Peters includes material spoken by adult French learners (16 in total) who were also speakers of English and Jamaican Creole, and who had studied French at university. The corpus contains conversations during spoken exams and in informal contexts, and amounts to approximately 15,000 words, 9,500 of which were produced by learners. The corpus has been transcribed in CHAT format and can be downloaded or viewed online.

Multilingual corpora including French

Most of the time, comparable corpora are assembled by researchers for the needs of their projects from existing monolingual corpora. However, comparable corpora are sometimes already publicly available. This is the case, for example, for the three corpora of parliamentary debates collected by

The number of parallel corpora available is constantly increasing. The OPUS database includes many free access parallel corpora, including the Europarl corpus, described below, as well as corpora with subtitles and multilingual data collected from the Internet, such as Wikipedia. These data have been automatically annotated with part-of-speech taggers; however, these annotations have not been verified manually so there remain a small percentage of errors.

The Multilingual Corpora for Cooperation (MLCC) project aims to bring together both parallel and comparable corpora. The parallel portion of the corpus, Multilingual Parallel Corpus, contains texts translated into nine European languages: German, English, Danish, Spanish, French, Italian, Greek, Dutch and Portuguese. The data in this corpus have been drawn from two sources:

-the Official Journal of the European Commission C series, Written Questions from 1993, which corresponds to more than 10 million words; -the Official Journal of the European Union, Annex: Debates of the European Parliament 1992-1994 and which amounts to a total of 5-8 million words per language. The comparable portion of the corpus contains articles from financial newspapers of the early 1990s, in six languages: German, English, Spanish, French, Italian and Dutch. This resource is available for free via the ELRA website.

The parallel portion of the MLCC has been rendered somewhat obsolete by the development of the Europarl corpus

However, these corpora include not only the speeches produced originally in each language, but also their translations. Given the fact that the Europarl corpus was mainly compiled to serve as training material for machine translation systems, the difference in status between the original and translated languages is of little importance. Nevertheless, to carry out contrastive studies, it is very important to have access to this information (see Chapter 4, section 4.4). This is why

The Hansard corpus is also made up of parliamentary debates, more specifically, from the Canadian Parliament. Therefore, debates are in French and English, and accompanied by translations into the other language. This corpus includes spontaneous language, prepared speeches and written texts. The version of the corpus which is available free of charge online includes 1,300,000 aligned sentences or fragments, amounting to approximately 2 million words per language.

Representative of another type of language register, TED conferences have recently made it possible to create a large parallel corpus. Indeed, the presentations in English which can be viewed on the TED website have been transcribed and then translated into many languages (for subtitling purposes) by voluntary users. These translations make it possible to study translation equivalents in a completely different register from the legal and institutional style of the Europarl and Hansard corpora. However, these translations are generally not the product of professional translators, as they are made by volunteers. Taking this into account, the presence of errors should not be discarded, nor should the fact that style may not fully reflect that of professional translations (see Chapter 4, section 4.4, for a study comparing the translations from the Europarl and the TED corpora). Another limitation of this corpus is that it is unidirectional. Actually, TED Talks are always made in English; as a result, English is the only source language, contrary to the Europarl corpus, where all languages are alternately source and target. In the TED Talks corpus, the only variations concern the numerous target languages.

Regarding yet another genre, the CRATER corpus

In the literary field, the ParCoGLiJe corpus

In the area of new media, the Swiss portion of the sms4science corpus collected by the universities of Zurich and Neuchâtel

The English-French Cabal2 parallel corpus, produced by Poitiers University, at the laboratory Formes et representation en linguistique et littérature or FORELL includes journalistic texts, most of which have been drawn from Le Monde Diplomatique between 1998 and 2003. The other sources are Courrier International, Time Magazine, National Geographic and some chapters from Jules Verne's novels. In total, this corpus includes 200 articles which correspond to approximately 400,000 words. The corpus can be queried online. The results provide the sentence in which the looked-up word appears, together with its translation. This tool is very useful for quickly finding examples of word translations but it cannot be used to perform a truly quantitative contrastive analysis, as the total number of occurrences of the word is not mentioned, nor is the translation's direction. In addition, the tool is not suitable for complex queries.

Corpus consultation tools

Concordancers are tools specifically designed for corpus analysis. In this section, we will begin by briefly introducing their main features. We will then focus on the freely available AntConc concordancer, and introduce some of its functionalities. Finally, we will discuss briefly the features of the CLAN concordancer which makes it possible to explore data coded in CHAT format, the annotation standard used in the CHILDES database.

Concordancers

Above all, a concordancer is a tool that makes it possible to look up words in their context of use. For instance, in the Littéracie avancée corpus described above, a search for the French word avis (opinion) by means of a concordancer indicates that the students have used this word 61 times. It also helps visualize the sentences in which it was used, aligned per occurrence of the word retrieved, as we can see in the search results reproduced in Figure

In order to analyze the typical environment of a word, concordancers help determine which words co-occur most frequently with the word looked-up. Although the list provided above gives us an approximate idea of the frequent co-occurrences, it does not let us quantify such associations. For example, in the Littéracie avancée corpus, the five most frequent cooccurrences to the right of the word avis are avis sur, which appears 11 times and then avis et (six times), avis de (five times), avis divergent (four times) and avis des (three times). This list also makes it possible to identify other modifiers of the noun avis apart from divergent, on the basis of the observation of the list of occurrences. Frequent modifiers are différent, particulier, défini, général, mitigé, personnel and respectif. The search for co-occurrences to the left indicates that the most frequent elements found in the corpus are: leur avis (18 times), les avis (seven times), d'avis (six times), l'avis (six times) and son avis (five times). Some concordancers can calculate the probabilities of collocations between certain words, rather than simply establishing the list of words which co-occur in the corpus. In the case of the word avis, the most likely collocations calculated by Antconc are: subsistés, respectifs, émettre, défavorable, réponses and divergentes.

Finally, some concordancers can be used to extract a list of keywords in a corpus by comparing them with a reference corpus (see Chapter 6). More specifically, the concordancer determines which of the words are used significantly more (or even less used in the case of negative keywords) in the search corpus compared to the reference corpus. Another possibility is to take a portion of a corpus and compare it to the rest of the corpus, or to compare a corpus with another similar corpus. For example, in the Littéracie avancée corpus, we can identify the keywords which specifically match student reports, compared with other types of academic work such as dissertations, by comparing this sub-section of the corpus to the others. The resulting list of keywords includes common nouns such as réflexivité, résumé, généralisation, portfolio, globalisation, article, stagiaires, etc., as well as proper nouns like Salaün. The presence of proper nouns in the keyword lists is very frequent because these words often specifically refer to a particular person, which is not used equally in different corpora. In the case of the noun Salaün, its presence in the keywords of the corpus can be explained by the fact that Salaün was a general delegate of a road prevention association who had taken part in an interview that students had to discuss in one of their assignments.

Setting up a list of keywords also makes it possible to compare the themes of different works by the same author. For example, if we compare the novel by Jules Verne Le Tour du monde en 80 jours with his other novel Vingt Mille Lieues sous les mers, the specific keywords of the first one are common nouns such as train, gentlemen, voyageurs and paquebot, as well as proper nouns like Fogg, Passepartout, Phileas, Hong Kong, etc. In another register, a comparison of the Le Monde corpus from the year 2011 with that of the year 1987 generates keywords in the form of common nouns such as euros, economy, Internet and international and proper nouns like Sarkozy, Hollande, Obama, Aubry and Merkel. This list is a good reflection of the important topics and personalities discussed in 2011 whom we did not yet talk about in 1987. When we compare the years 1987-1995 of the Le Monde newspaper, keywords change to: serbes, ETA, Bosnie, Sarajevo, Croatie, Jospin, Balladur, etc.

In a nutshell, the keyword list of a corpus is very useful to identify its main topics, provided that the comparison with the reference corpus is appropriate. Indeed, the latter is of paramount importance in establishing the list of keywords. If we compare the reports of university students with a year of the Le Monde newspaper rather than the rest of their academic work, the keyword list includes élèves, activité, formation, réflexivité, écriture, évaluation, résumé, pensée, enseignants, savoir, etc., because the topics covered vary more widely between the two corpora than between the different types of university work. This is why the topics emerging from this second comparison are those related to the field of education in general, rather than those addressed in the reports in particular.

In sum, concordancers make it possible to analyze recurrent properties in a corpus, such as its frequent words, its collocations and its keywords from a quantitative point of view, something which is not possible to infer from simply reading texts. This is why they represent essential tools for grasping the quantitative properties of a corpus.

Focus on the AntConc concordancer

The AntConc concordancer, developed by Laurence Anthony, is available for free online. AntConc can be used to perform all the analyses described above. This concordancer is compatible with the various current operating systems (Linux, MacOS or Windows). In this section, we will describe its basic principles of use. For a start, AntConc can only read text format files. So, to begin with, it is necessary to convert the files included in the corpus to text format. Depending on their origin, this can be rather easy or more difficult: usual word processors generally have a text format saving functionality, but extracting text from a PDF file can be difficult. AntConc can also read XML files, since these contain text which is accompanied by tags. Before opening one or more files for them to be processed with AntConc, we have to make sure that the encoding chosen in AntConc for reading the characters is suitable for reading the file correctly. By default, AntConc uses UTF-8 encoding. However, this encoding does not correspond to text files containing French characters, because of accented characters.

When the text of a file is not displayed correctly in AntConc, the encoding used can be changed to make it match the file's encoding, for example, the ISO 8859-1 format (Latin.1).

AntConc can be used for looking up words in context and sorting their occurrences depending on the words that appear to the left or to the right of the search word. To look up certain words, the use of wildcards can be of great help. All the wildcards recognized by AntConc for defining the search pattern can be easily viewed in the software. These wildcards are mainly used for looking up all the possible endings of a regular verb in a single request, by searching for the radical of the verb followed by any number of characters (through the use of an asterisk), such as donn*. To look up a singular and a plural word in a single query, it is possible to replace zero or one character exactly with another wildcard, for example, homme+.

AntConc also offers the possibility of visualizing the places where various occurrences can be found in a file by means of a graph, as well as going through the entire file until we find where the occurrence originated. This functionality is very useful to obtain the maximum amount of contextual information and thus disambiguate certain occurrences. The Clusters and Collocates tabs help you to identify the collocations spotted in the corpus, as well as the most likely collocations.

AntConc has another feature which offers the possibility of generating a list of all the words in the corpus sorted by frequency via the Word List tab. This same tab also shows the number of word types and word occurrences in the corpus (see Chapter 8, section 8.2, for a discussion of these concepts).

These figures are essential for performing lexical diversity calculations on corpus data, such as the type/token ratio (see Chapter 8). Finally, AntConc makes it possible to create a list of keywords from the corpus based on the comparison with a reference corpus.

Focus on the CLAN concordancer

The CLAN concordancer works on files encoded in CHAT format, which corresponds to all of the data in the CHILDES database, as well as a number of learner corpora. CLAN can be installed on Mac and Windows operating systems. CLAN commands can also be used with the online version of the corpora.

CLAN offers the possibility of formulating queries on the CHILDES corpus for studying many aspects of children's language. Although query syntax may seem complex at first, it is actually easy to master. A request in CLAN should always start by specifying the name of the command to be performed, followed by the search elements, the file or file's name where the information should be retrieved from, and whether it should be related to the whole corpus or only narrowed to some files. Finally, if applicable, the command should specify the type of speaker whose words have to be analyzed. This specification is often very useful, since the interactions in acquisition corpora most often take place between children and one or more adults and it is necessary to analyze the speech produced by each of them separately.

One of the most useful CLAN commands is the combo command, which helps you to look up words or word sequences produced by specific speakers in the corpus. If the corpus has been annotated, this command also makes it possible to search for grammatical categories, speech acts or even errors. In CHAT format, annotations always take the form of an additional line below the transcription, identified as %mor, for example, when referring to a grammatical category or the morphological representation of a word. The coding of speech acts is identified with a line called %spa. Relevant nonverbal actions are coded with a line called %act. Finally, error coding is identified with a line called %err and has a standardized format. For instance, the $LEX reference indicates a lexical error. In the transcription itself, incorrect words are followed by an asterisk in square brackets so that they can be identified. Let us take a look at an utterance from the York corpus (De Cat and Plunkett 2002): *CHI:tu me l'as donné. %mor:pro:subj|tupro:obj|me pro:subj|il$v:aux|avoir&PRES&2spart|donner-PP&m. % act: takes a book Going back to the combo command, to find all the occurrences of the word pourquoi produced by the child, the syntax of the command should be formulated as follows: combo +spourquoi +t*CHI CLAN also helps you to determine the frequency of words in a CHAT file using the freq command. This command makes it possible to obtain the list of words sorted by frequency, in the same way as the list of words generated by AntConc. The command also helps you to calculate the type/token ratio (see Chapter 8), which represents a measure of lexical diversity. The syntax for such a command is as follows: freqFILE NAME +t*CHI +o Finally, the complexity of children's language is often measured at the start of their development by their mean length of utterance (MLU) (see Chapter 3, section 3.1). The MLU can be calculated automatically in CLAN using the MLU command. To do this, the syntax is very similar to the other commands: mluFILE NAME +t*CHI

Conclusion

In this chapter, we have presented the main corpora available in French. We have observed that, despite the absence of a reference corpus, numerous more specific corpora are available, which can be combined to carry out research in many areas of linguistics, as we will see in the subsequent exercises offered.

The main limitation for using these corpora is their availability, which is often limited and requires going through an online interface, in which only some functionalities can be used. When corpora can be downloaded, a concordancer should be used in order to explore them systematically. Finally, we reviewed the main features of concordancers and presented two of them succinctly: AntConc and CLAN.

5.9.

Revision questions and answer key 5.9.1. Questions 1) Using the interface provided on the website of the Corpus français de l'université de Leipzig, what is the frequency order in this corpus of the words maison, chalet, immeuble, bungalow. What are their most frequent collocations? What can you conclude from these observations?

2) Using the AntConc concordancer, find the 10 most frequent content words (defined as nouns, lexical verbs, adjectives and adverbs) used by the undergraduate students in the Littéracie avancée corpus. What can you conclude? In this same corpus, what are the five most frequently observed co-occurrences and the five most probable collocations for the word élève(s)?

3) Using the Google Books corpus online interface, find: a) when the new spelling of the word clé started replacing the old spelling clef; b) which researcher is more popular, Ferdinand de Saussure or Noam Chomsky; c) whether the nominal use of the word orange preceded or followed its adjectival use in the history of French. 4) Use the online interfaces of the OFROM corpus and the CFPQ corpus. How often do men and women use the verb détester in Frenchspeaking Switzerland and Quebec? What remarks can you make about the possibilities offered by these interfaces? 5) In the York language acquisition corpus on the CHILDES database, what is the most frequent word produced by Anne in the first recording at the age of 1 year and 10 months old? What about the last recording, at 3 years and 5 months old? How did its type/token ratio change in these two files, and what is the MLU in both files? Compare with the results for Max in the same corpus. Based on these clues, who seemed to acquire language the fastest? 6) From the TED corpus, identify the different possible translations of the word issue into French.

Answer key

1) The word maison has 282,802 occurrences in the corpus. It is the 474th most frequent word and takes frequency class number 8. It is the most frequent word in the group. The word immeuble is the second most frequent word, with 20,133 occurrences, making it the 6,633th most frequent word in the corpus and corresponding to frequency class number 12. The third most frequent word is chalet, with 8,184 occurrences in the corpus, corresponding to frequency rank number 13,609 and frequency class number 13. The word bungalow is the least frequent word, with 1,100 occurrences in the corpus, corresponding to frequency rank number 53,542 and frequency class number 16. As shown in the graph provided on the website, the main collocations of maison are the nouns mère, retraite, disque, famile and jardin, the adjective familiale and prepositional phrases such as d'édition and d'arrêt. The collocations for the word immeuble are the nouns quartier, logement, appartement, étages, incendie and bureaux, the prepositional phrase d'habitation, the past participle situé and the demonstrative cet. The collocations for the word chalet are the nouns bois, ski, montagne, location, résidence and vacances, the prepositional phrase d'alpage, the past participles situé and assigné, as well as the proper nouns Roman Polanski and Gstaad. Finally, the collocations for the word bungalow are the nouns plage, villa, location, chambre, chalet, vacances, camping, maison, mobil-home, as well as the adjective petit and the prepositional phrase d'accessibilité. We can see from this list that the meaning of the words can, at least in part, be inferred from their collocations. We can also observe that the word maison is the most generic of the four, and the only one that takes figurative meanings as in maison d'édition, maison d'arrêt or maison de disques. When compared with immeuble, we can see that the word maison also has its own attributes, like jardin and famille. Conversely, maison is associated with appartements and étages, which specifies the type of housing in question, as well as bureaux which shows a different use from that of maison. Finally, chalet and bungalow are both associated with vacation homes, but of a different kind. While chalet is associated with ski, montagne and bois, bungalow is associated with plage and camping. The proper nouns associated with chalet in the corpus show one of the limitations of the collocation analysis. Indeed, in the corpus, several articles referred to Roman Polanski's residence, which made these associations very strong, but these words do not obviously collocate in everyday language.

2) In order to answer this question, it is necessary to open AntConc, and there, to open the files in the L2_DOS_SORB and L3_RS_BOCH directories, which correspond to material by undergraduate students. Then, we have to generate the word list under the Word List tab. The 10 most frequent content words are the following: This list illustrates the fact that the most frequent words in a corpus are those belonging to functional categories such as prepositions and determiners. Indeed, the first content word only appears at the 32nd frequency rank! We can also observe that the frequency of words in a corpus decreases rapidly. The most frequent word in the corpus, that is, de, has 4,461 occurrences, whereas the 32nd word has only 499 occurrences, representing almost 10 times fewer occurrences. In addition, from frequency rank number 4,077 onwards, the words in the corpus only have one occurrence. This distribution reflects Zipf's law (see Chapter 6). The five most frequent co-occurrences to the right of the word élève(s) are: de, ont, et, en and avaient. To the left, these are the words: les, des, l', aux and un. The five most likely collocations are: répartie, accompagné, onze, évaluerai and équitablement.

3) a) To answer this question, we have to type "clé, clef" on the online corpus interface. We should also be careful to choose the French corpus and to determine a sufficiently long time period, for example from 1800 to 2000.

The results obtained indicate that the spelling clé became as frequent as clef in 1963 and has made strong progress since then, to the detriment of the old spelling.    4) In the French corpus of French-speaking Switzerland, the verb détester is used five times by men and 15 times by women. This search can be done very easily by looking for the infinitive détester in its lemmatized form, which helps us to find all the inflected forms in a single request. In the corpus of spoken French from Quebec, there is no occurrence of the verb détester produced by men, versus 16 occurrences produced by women. The search is much more complicated in this interface, since the search by means of lemmatized forms is not possible. All verb forms must be looked up separately.

5) The most frequent word produced by Anne at 1 year and 10 months old was no, with 32 occurrences. At this age, her type/token ratio was 0.37. At 3 years and 5 months old, the most frequent word was ça with 54 occurrences, and her type/token ratio was 0.21. In addition, her MLU increased from 1.82 to 4.83 from the first to the last recordings. The most frequent word produced by Max at the start of the corpus at 1 year and 9 months old was also no, with 24 occurrences. At this age, his type/token ratio was 0.38. At 3 years and 2 months old, at the end of the corpus, the most frequent word was I, with 48 occurrences. The type/token ratio was 0.24. His MLU ranged from 1.17 at the start of the corpus to 3.78 at the end. The comparison of MLU between Anne and Max seems to indicate that Anne developed her language faster than Max. The fact that the type/token ratio decreases with age in the two children reflects that the total number of occurrences they produce increases a lot as recordings progress (e.g. ranging from 298 occurrences to 1,092 occurrences for Anne), which implies a poorer lexical diversity in proportion to the total number of words produced. The type/token ratio cannot therefore be considered as a reliable measure of linguistic development. A better appraisal can be obtained by comparing the number of different words, known as word types, produced by each child. At the end of the corpus, Anne produced 230 word types against 182 for Max, which tends to confirm that her language was more advanced.

6) The English noun issue was used 1,957 times in the TED conference corpus. It is therefore difficult to study all of these occurrences. To quickly determine frequent translations, 100 occurrences can be randomly chosen. This observation of translations gave the following French translations for the first 100 occurrences of the word issue: We can observe that the main translation of the noun issue in to French was problème. We can also reason that the word issue does not always have an exact equivalent in French, which might explain the high number of untranslated occurrences, or translations by means of a paraphrase. In particular, this word is used in expressions such as this issue, often translated by pronouns like cela in French. This research project also revealed the difficulties inherent in the observation of translations. In fact, the TED interface does not currently let us specify the search for words or character strings. Thus, the search also generates irrelevant occurrences of words like tissue, as well as many cases of sentences which have not been translated into French. For this search, it was necessary to consider 175 English sentences in order to find 100 translations of the noun issue into French.

Further reading

A list of existing corpora in many different languages can be found in

In this chapter, we will present the best practices for creating a corpus. First, we will discuss some facts that need to be considered before deciding to create a new corpus and highlight the advantages of reusing existing data whenever possible. Then, we will address various important methodological concerns for creating a corpus, in particular questions related to the size and representativeness of samples, and will explain simple methods for data sampling and coding. We will also briefly discuss the challenges posed by the creation of the spoken corpora. We will finally see that the task of creating a corpus carries with it a certain number of ethical and legal issues which must be dealt with.

Before deciding to build a corpus

The first element to check before starting to compile a new corpus is whether existing data can be used for the planned study. As we will see throughout the chapter, creating a corpus is a challenging task and presents many difficulties. It is actually not always easy to find texts available in a digitized format for all text genres, and even when such texts exist, they might not all be usable due to copyright issues. Choosing the right texts to be included in a corpus should also be the object of careful reflection, since any kind of analysis carried out on data that are not representative of the target genre (see section 6.2) could be largely invalid. When it comes to creating a reference corpus, the data collection phase is so time-consuming that it can only be tackled by a group of experts. Becoming involved in a corpus creation project individually is realistic only in the case of specialized corpora, for example, if the task is narrowed to a specific language register or a regional variety, that is, a project of a smaller size. Even for this type of corpus, several months of work are often necessary for collecting the data, and may take even longer if the latter are enriched with linguistic annotations (see Chapter 7).

The problems are even more complex and numerous when it comes to spoken data. These need to be collected in the form of audio files which are later transcribed to become analyzable with corpus searching tools. The transcription process itself is very time-consuming and its complexity depends on the exact type of annotation that is added to the data (prosodic contours, etc.). To get an idea of the magnitude of the task, up to 15 hours of work are necessary to transcribe one hour of recording

In Chapter 5, we saw that many corpora in French have already been created, and that some of them are available free of charge to the public. Some other European languages, not only English but also German, Dutch, Spanish and others, have an even broader choice of corpora than French. So, when formulating an empirical research question, it is advisable to consider whether these resources could not be used for the study. If necessary, existing data can be supplemented with a smaller portion of new data, and thus significantly simplify the data collection phase. For example, an empirical study on the regional differences in the way questions are formulated in spoken French could reuse data collected from different spoken corpora, including France, French-speaking Switzerland, Quebec and Belgium. If the study were to be extended to other regional French varieties, for example, the French spoken in the Caribbean islands, existing data could be supplemented by samples of such variety. In Chapter 4, we also saw that comparable corpora can often be assembled from existing data. For example,

If, after research, it turns out that the existing corpora are not suitable, then the creation of a new corpus might be considered. In this case, it is essential to properly outline the research question that will be studied on the basis of new data, since the latter will have a crucial influence on the whole process, both during the data collection and the annotation phases. In the field of corpus linguistics, it is very common to hear that there are no good or bad corpora, rather there is only corpora which are more or less suitable to address a certain research question. For example, for investigating the expression of subjectivity in journalistic discourse, a corpus entirely made up of editorials would not be appropriate, since this is only a sub-section of the genre, which incidentally is more likely to contain markers of subjectivity than other sub-genres, as dispatches for instance. In this case, two scenarios are possible: either the conclusions of the study will be limited to the editorial style, or the corpus should be diversified in view of including other types of journalistic texts. The problem we have just mentioned involves a key methodological point for corpus studies, which is the representativeness of data. We will discuss this point in the next section.

Establishing the size and representativeness of data

Let us begin by repeating that there is no ideal size for a corpus, in the same way as there are no intrinsically good or bad corpora. Suffice it to say that the characteristics of a corpus may be more or less appropriate for answering a research question. As the technical capacities of computers have evolved, it has become possible to collect ever larger corpora. Currently, some corpora such as the Google Books corpus (see Chapter 5) and the FrenchWeb 2012 corpus (available on Sketch Engine), collected from the Internet, contain several billion words. For a long time, the rule of thumb for collecting a corpus was that it should be as large as possible. The logic behind this principle was that the larger the corpus, the more likely it would contain occurrences of rare linguistic phenomena. Indeed, when the words of a corpus are listed following their frequency order, as in Table

However, more recently, some researchers have defended the idea that maximum size should not always be the goal in the creation of a corpus, since smaller-sized corpora may prove to be adequate for many research questions which do not involve rare words, as we will see in this chapter. In fact, a large corpus is not always suitable for addressing all kinds of research questions. The question of the optimal size for a corpus primarily depends on the nature of the linguistic phenomenon to be studied. The more frequent a linguistic phenomenon, the better it can be studied on the basis of a small corpus. Rarer linguistic phenomena, on the other hand, require larger corpora. This question is also related to the degree of generalization targeted. A corpus representing a specific genre can be relatively small, whereas general corpora need to be much larger.

As we discussed in Chapter 1, a corpus does not simply represent a collection of randomly chosen texts. In fact, a corpus is a collection of texts or recordings specifically chosen in order to be representative of a language, of a certain register or even a language variety. The question of representativeness is therefore essential so that a corpus can be used for answering a research question. In order to fully understand what this notion represents, we will draw an analogy with opinion polls. Let us imagine that we wish to find out which candidate is more likely to be elected in the next presidential elections. In order to find out, it is not possible to ask all the citizens who they intend to vote for. It is therefore necessary to prepare a sample of the population of a more modest size, to whom it might be possible to ask such a question. Later, the results obtained on the basis of this sample can be extrapolated to the entire population. But in order for this technique to work, it is crucial to carefully choose the sample of respondents, in such a way that it represents the whole population. For example, if the sample chosen includes 500 students met at the exit of a university building, the sample obtained will most likely not correspond to the actual result of the election, since this sample is not representative. In fact, students represent only a small portion of the population. In order for the sample to be representative, it should also include people with other types of occupations, different age ranges and from different regions. The same applies to the compilation of a corpus. In order to be a representative, a reference corpus should contain a balanced set of samples covering the main stylistic genres, both in the spoken and written modes. The main issue is to determine the criteria according to which it is advisable to classify the elements included in the corpus to ensure its representativeness. We can be sure about one thing: these criteria should not be related to the linguistic content of the samples, but rather to a classification made on the basis of external criteria, such as text genres and language registers. For example, it would be rather inappropriate to try to study the production of speech acts in the legal context by choosing a corpus exclusively based on a number of performative verbs such as demand, condemn, order that it contains, since this criterion would influence the results found in the analysis afterwards. However, this study would require the assembly of a corpus which tangibly represents the legal language, such as court decisions, because these writings properly match the targeted field of study, that is, the legal language. To sum up,

In the case of reference corpora, sample distribution between different genres is a complex problem in itself, due to the lack of an existing typology of spoken and written genres that is unanimously accepted. To simplify, let us say that written corpora should contain both public texts (published works) and private ones (letters, emails, etc.), collected from different fields such as the press, the sciences or the literature. The spoken section of a corpus should reconcile a variety of choices. It should include both planned and spontaneous spoken speech, monologues and dialogues, drawn from contexts with various degrees of formality. Very often, the creators of new corpora solve the problem of representativeness by following the criteria used in existing reference corpora, such as the British National Corpus, a pioneer of the genre.

In cases where researchers need to compile specialized corpora, the question of representativeness is posed a little differently. To continue developing the analogy with polls, if the goal is to know who students will vote for in the presidential elections, it will be enough to interview a sample of students, since such a sample is representative of that population. In the same way, the question of the representativeness of specialized corpora is clearly simplified, because this can be achieved by choosing texts or recordings belonging to a specific genre. However, we should keep in mind that there may be sub-genres within a genre, such as novels, short stories or children's stories within the literary genre, and that these may vary from each other.

Even when working within a text genre, we should aim to diversify its sources as much as possible. For a literary corpus, for example, works from different authors should be included.

From a lexical perspective, the representativeness of data in specialized corpora, such as corpora devoted to newspaper or legal articles, can be measured using the concept of saturation

As a matter of fact, the representativeness of a corpus cannot be ascertained once and for all. In the case of closed corpora (see Chapter 1), data aging implies that they are no longer representative of the most recent developments in the language. In the case of monitor corpora, while the new data added at regular intervals may improve their representativeness, they nonetheless pose balancing problems between the different portions of the corpus, an aspect we will discuss in the following section.

In summary,

Choosing language samples

To achieve the representativeness aim discussed above, a corpus should include a sampling of different types of texts or recordings. Unless we are working on a very specific corpus like the Bible or the complete works of an author, most of the time, it is actually impossible to include all the texts or all the recordings belonging to the genre to be studied in a corpus. This is why it is necessary to prepare samples which, once assembled, can work as a representative sub-section of the genre to be studied. The preparation of samples to be included in the corpus poses two important methodological questions: on the one hand, the appropriate size for each sample, and, on the other hand, how to balance the portions of the corpus in such a way that the result is truly representative of the genre.

In order to understand the difficulties of corpus balancing, we will give an example. To be representative, a spoken French corpus should include speakers from different regions, different ages, both male and female. If 95% of the corpus is made up of Parisian speakers aged between 20 and 30 years, such a corpus will not be a representative sample of the population, even if this means including the other speakers among the remaining 5%. While it is true that a sample is expected to bring together a rather restricted version of the overall population, it should also reproduce its main features so that the results obtained on this sample can be extended to the entire population. A corpus of spoken French should therefore include a similar proportion of male/female speakers, from different age groups and different regions. Many other criteria could be included in this selection, such as the socio-economic level of the participants, for instance. As with the question of corpus size, the balancing criterion largely depends on the question the corpus will help to study. In general, a corpus should not be used for establishing a contrast between elements which have not been balanced during the corpus compilation phase. For example, a corpus created for studying the pronunciation of vowels in Paris and Marseille and which has not been compiled representing a balanced sample of different age groups cannot be used carelessly for studying the evolution in the pronunciation of vowels between generations.

In the case of written language general corpora, it is important for the chosen samples to represent different genres, including both published and unpublished texts. In the case of the British National Corpus

-the field, that is, the topic explored in the text; -the time when the text was produced; -the distribution mode, depending on whether it was a book, a newspaper or an unpublished text.

The spoken samples were chosen on the basis of demographic criteria such as age, gender, geographic region and social class, as well as contextual criteria. However, the creators of large corpora have agreed that it is very difficult to fulfill all the criteria to achieve a perfectly balanced corpus. One of the major problems is the difficulty of incorporating new data, an aspect which tends to create bias around the choice in favor of more readily available data. Such difficulty is largely due to copyright issues, which we will address in section 6.6. This issue prevents the inclusion of recently published texts in a corpus that have not yet fallen into the public domain. In addition, published texts are generally more easily accessible than unpublished texts, such as emails or personal letters. Finally, texts published on the Internet are much easier to access than texts published on paper. These differences inevitably induce a certain bias towards specific text categories. In the end, balancing a corpus is never a perfect task. As

In concrete terms, balancing the portions of a corpus can be achieved by defining a sampling frame which delimits the population to be sampled and lists its relevant properties. Each of these properties should then be mirrored by the corpus sample proportionally to its prevalence in the population. For example, in order to create a corpus of French, speakers from different regions should be included in the sample. Therefore, geographic region becomes a relevant trait for the sampling frame of a spoken French corpus. The number of French speakers to be included for each region can be determined proportionally to the number of French speakers living in the different regions sampled. However, in many cases, these proportions are difficult to determine accurately. For example, it is difficult to determine exactly what proportion of the texts published every year belong to the fiction genre and how many are non-fiction. In this case, obtaining the exact figures is undoubtedly possible, but highly complex. The problem becomes even more challenging for the categories of unpublished texts, for which there are no existing figures. In these cases, the classification is often based on common sense or on the pragmatism of the corpus designer, depending on the importance of subcategories for addressing the questions that the corpus is supposed to help study. Now, let us move on to the question of which samples to include in the corpus. The first important question is how these samples should be chosen. A first technique involves choosing the samples completely at random, the idea being that out of the total number of samples in the corpus, the most frequent characteristics will eventually stand out on their own. However, we cannot take for granted that this method yields a balanced sampling, especially in the case of a small corpus. As previously mentioned, a better method might be to define a sampling frame and to divide the samples to be collected depending on the important properties of such a frame. For example, if the sampling frame for a corpus of French spoken in Switzerland includes different criteria such as gender, age, socio-economic level or place of residence, an equivalent number of samples should be chosen to match each selected criterion. Within each criterion (e.g. 20-to 30-year-old middleclass men living in the canton of Geneva), participants can be chosen at random. According to

The next question to consider is related to the number of samples required in the corpus, as well as the ideal size for each sample. Once again, the answers to these questions depend on the type of corpus the researcher has in mind. The more generic the corpus, the more samples will be needed, whereas for a more specialized corpus, fewer samples are necessary in order to provide representative data. On the basis of corpus studies on the differences between genres and between language registers, Biber

Finally, another important question concerns sampling units. Should we include whole texts or only excerpts, or even isolated sentences? The answer to this question often depends on the accessibility of data. On the one hand, it is preferable to create a corpus including language samples which represent a coherent whole, rather than isolated sentences. However, this is not always possible due to copyright reasons. The correct size of samples also depends on the type of text considered. For example, if the goal is to reach a sample of 200,000 words per text genre, this number of words can almost be instantly reached by including one or two entire books in the sample. In this case, it would be a better idea to choose excerpts (e.g. chapters) from different books, instead of a longer portion of a single book. For other types of text such as letters, text messages, etc., the units are so small that it makes no sense to not fully include them. In all cases, it is important to systematically avoid including the same portions of text, for example, always the beginnings or the endings. Indeed,

Preparing and coding corpus files

In order to include language samples in a corpus, first we have to obtain them. In the case of spoken corpora, data acquisition first requires them to be transcribed. This is a very complex process, and we will discuss it in detail in the next section. In the case of written corpora, the situation is not always simple, either. The most favorable scenario is clearly the one in which data are readily available in digital format, which is progressively becoming more frequent, especially when it comes to data gathered from the Internet. However, retrieving text from digital formats may have varying degrees of difficulty depending on the original format; for example, it can be very difficult to isolate the texts of different articles in a journal page formatted as a PDF document, or even impossible when the PDF file includes text images. So, even when we are working with data in a digital format, the task of converting the original format into a usable format, based on the text, is still necessary, as we will see below. In some contexts, however, written data are not available in digital format. In this case, we can either work with printed texts available on paper or with handwritten texts, such as student essays or private letters. Printed texts can be scanned and then processed thanks to optical character recognition (OCR) software, but these always require a manual check made by a human in order to provide a completely reliable result. There might be a high number of errors if the original print is of a poor quality. Finally, for handwritten data, there is no solution other than to manually type it on the computer. Data transcription also raises many questions related to the way in which some of their original features might be preserved. For example, student essays often contain spelling mistakes, which should be left untouched, since they can be very informative for many research questions. But in this case, a version without misspellings should also be included so that the words can be found by a concordancer. In Chapter 7, we will see that errors can be systematically annotated in a corpus, in the same way as syntactic or semantic information is provided.

No matter the way of acquiring data, an important point is to save the corpus files into a format which can then be used by a concordancer. As we saw in Chapter 5, most concordancers (like AntConc) only read files in text format, which can include texts tagged in XML format. Therefore, all newly created files for a corpus should be directly saved into text format. Files which have already been scanned are rarely saved in this format, since this format does not make it possible to include formatting marks, and this makes documents difficult to read. In the case of corpus studies, this is not a problem, however, since the files will not be read by humans, but processed by a concordancer. Files processed with OCR software or word processing tools are often saved in proprietary formats (such as Microsoft Word, for example, DOC or RTF). Files saved in these formats can be easily transformed into text files thanks to specific options such as the "save as" command found in word processors.

Web pages are available in HTML format. This format contains many formatting marks in the form of tags, which are interpreted for creating the various graphic effects which are necessary for a browser to display a web page. These later become visible when a file is opened with an editor in pure text format. Despite their lack of linguistic relevance, these tags are interpreted as textual elements by concordancers.

In order to avoid this problem, software should be used for retrieving text from these files and eliminating unnecessary markings (HTML tags). Word processing software often include this feature, simply by choosing the "text format" option from the "save as" command. The only problem with this option is that it is necessary to open every file one after the other in the word processor so as to perform the operation. This might eventually become a problem with a corpus, including thousands of different files. An alternative solution is to use the AntFile Converter, which is a file conversion software developed by Laurence Antony, the creator of AntConc (see the URL at the end of this book for AntFile Converter). This software can be downloaded for free and used for converting any number of XML, HTML or sometimes even PDF files into text format.

The Sketch Engine corpus creation and management platform, discussed in Chapter 5, also automatically transforms the format of files downloaded from the Web. The advantage of this platform is that it offers the possibility of automatically downloading large amounts of data from the Internet in a single operation (web crawling). The corpus created in this way can be directly analyzed using the tools provided by the platform, for example, for retrieving concordances, word lists or keywords. In its recent versions, the WordSmith concordancer also offers a similar function. This type of tool has made the collection of web-based corpora extremely easy. We should nonetheless bear in mind that the texts found on the Internet are of a highly variable quality and are not representative of the whole language.

If the corpus has been created manually rather than through the use of a platform enabling automatic data download, a practical but important question concerns the number of files that should be created. More specifically, should we create a single file for the whole corpus? Or should we create one file per corpus sub-section? What about a file per text included in the corpus? In general, it is preferable to store every language sample in a separate file. In this way, it is later easier to combine them in different ways for creating sub-corpora, rather than having to retrieve text portions from a larger file. For example, if we collect data on the language used by young people in France, we might then want to compare data depending on different criteria such as gender, geographic region or age group. This comparison can be done in a relatively simple way by grouping all the men's files and all the women's files or, for the same purpose, all the Paris files and all of the Marseille files. But if all men are included in a single file and women in another, then the geographic comparison data needs to be reprocessed.

In order to be able to easily group the files into sub-corpora, it is necessary to represent the features of each sample in an easily accessible manner. A practical solution is to code these characteristics directly onto the file names: this is why these names are another important element that should be taken into account when creating the corpus. For example, it is possible to name files only by using a number, each representing a criterion used when compiling the corpus. Going back to the example of young speakers, one possibility would be to identify all the files from Paris with the number "1", those from Marseille with the number "2", etc. Then, the second digit could be used for coding gender, "1" for women and "2" for men, then the third reference could be for coding the age group, for example, "1" for 16-to 19-year-olds, "2" for 19-to 22-year-olds, etc. Finally, several digits can be used for coding the participant's number. Following this procedure, the sample corresponding to the first 18-year-old male participant from Marseille registered in the corpus would be saved in a file called "221001.txt". The disadvantage of this method is that the coding is opaque for a user who does not have a precise vision of the system used.

A more transparent way to achieve the same result is to use abbreviations. For example, the same file could be coded using abbreviations such as Mar for Marseille, h for men (homme), ado for the 16-to 19-year-old group, which would result in a file called "mar_h_ado_001.txt", if we use the underscore symbol as a separator for the abbreviations. If this system is used, abbreviations should be kept short in order to avoid generating excessively long file names, which might not be readable. We should also avoid inserting spaces or other punctuation marks, since these could interfere with the programs used for opening the files on different platforms (typically concordancers). Finally, if word abbreviations are used, it is desirable that each abbreviation of a category contains the same number of characters (e.g. three letters for all the names of cities), in order to make reading in columns of lists of files easier.

We have pointed out that corpus files should contain plain text, in order to facilitate data analysis. However, for a corpus file to be used as a sample representing a certain type of language, metalinguistic information (which is not part of the text or of the dialogue) should be accessible to the researchers who will analyze it. For example, this type of information includes the date of a newspaper article, the place where a conversation was recorded or the characteristics of the speakers taking part in the dialogue. This "piece of extra information concerning the data" included in the corpus is what we call metadata.

For this information to be made available for future users of the corpus without separating it from the rest of each sample portion, it should be possible to include it in the files, but in such a way that it is not taken into account by a concordancer when counting the words of the corpus. A possible solution could be to insert these marking elements inside tags, something that the concordancer will be able to ignore. Most often, these tags are delimited by chevrons (the less-than and greater-than signs < >).

In this way, the metadata of a corpus sample can be added at the beginning of each document as follows: <texttype: newspaper article> <publication: Le Monde> <author: Jean Dupont> <date: 1 April, 2019> <subject: April Fool's Day>

In the AntConc concordancer, discussed in Chapter 5, it is possible to inform the program about the existence of tags and not to consider the information they contain.

In some cases, the abundance of metadata requires the use of a precise syntax for tags, based on the conventions of computer languages for XML or SGML coding. The conventions used can be explained in the corpus documentation or may follow a more widely recognized standard. Indeed, some sophisticated marking formats have already been developed for corpus data. One of the best known is the TEI format (Text Encoding Initiative), which makes it possible to encode many markings in a standardized way and make corpora sharing easier. Large reference corpora such as the British National Corpus are tagged following the TEI conventions. Without going into details, a TEI-tagged document always contains two types of elements:

-the header; -the body of the text.

And these two elements are respectively made up of other elements. The header section contains metadata, a description of the file, the encoding, the text profile, mainly the language, the context or participants, and even a history of its revisions. All these elements, except for the file's description, are optional. The body of the text mainly contains tags, which are destined to delimit text units such as paragraphs or even sentences. Following XML conventions, TEI tags always begin with chevrons < > and close with </ >. As we will see in Chapter 7, TEI tagging can also be used for making more detailed annotations than dividing the text into sentences.

In addition to indicating the metadata by means of tags inside each file, it is also very useful to provide a summary table in the corpus documentation, as illustrated in the simplified Table

Recording and transcribing spoken data

The collection of spoken corpora poses certain additional challenges compared to written corpora. One of the main difficulties stems from the need to transform spoken data into a written format. As we have seen in Chapter 5, for corpus data to be analyzed, they should be in written format, since concordancers cannot search for words or expressions in audio files. In this section, we will briefly discuss some of the problems related to the representation of spoken data, as well as some possible solutions to sort them out.

The first step we can take to work with spoken corpora concerns the mode of acquiring data. Spoken data need to be recorded and the recording process itself requires special preparation. For data to be as representative and informative as possible, it is essential to properly define the research questions that these data will help answer well in advance. In particular, these questions will determine the type of interactions that should be recorded, the constraints regarding the context as well as the information contained in the transcript. If, for example, the aim of a spoken corpus is to study the lexical specificities of a language variety, the prosodic information contained in the interactions will be of little use. If, on the other hand, the research question concerns information structure in discourse, more specifically the introduction of new and given information in different spoken genres, then prosody will play an important role in studying the interface with the utterance structure and therefore requires a transcription.

An important point to establish before carrying out the recordings is the nature and the amount of contextual information that will need to be added to the transcripts. Spoken conversations are naturally more ambiguous and less precise than written communication, since speakers can use the immediate context to make themselves understood. Audio recordings make it almost impossible to grasp this type of information, which later have to be added to the corpus so that the interactions can be understood and analyzed by the experts who will listen to these recordings. In the case of audiovisual recordings, a larger share of contextual information will be captured and should not be explicitly added to the corpus (although some kind of codification may later be required to perform specific analyses).

Due to the difficulty of collecting and transcribing spoken data, the question of the amount of data needed for creating a corpus is even more acute than for written data. But in this case too, there is no ideal size for a spoken corpus. The amount of data required for studying a certain phenomenon primarily depends on how frequently it occurs. For example,

As we have already mentioned, providing metadata details is particularly important in the case of spoken corpora. Among other things, the metadata appearing in the header should offer information about the main features of the participants: degree of relatedness (parents, friends, colleagues, strangers, etc.), the context in which the conversation took place, the manner in which the recording was captured, etc. The importance of this information depends on the questions that the corpus is expected to answer. For studies on how interpersonal relationships influence interactions, it will be necessary to have as much information as possible about the degree of relatedness between participants, whereas for studies on the use of discourse markers such as bon or ben, this type of information is not so relevant.

In the same way, contextual information may be added to the statements inside the transcripts. Let us insist on the fact that the context of an interaction is so rich that it would be illusory to try to account for all the aspects involved in a transcript. Choices will have to be made depending on the importance of this information for the research question. At least, the transcripts should contain enough contextual information for the meaning of the utterances to be reconstructed if this became necessary in the absence of context. For example, if a person passes by and this event invites a comment from the participants, this piece of information should be mentioned in the transcript, indicated between tags so as not to be confused with the transcription itself.

Finally, the last difficulty related to transcription that we will mention concerns the presentation of the transcripts. In a dialogue, the participants do not always speak one after the other as it happens in the dialogues of a novel or a play. On the contrary, there are many overlaps between speaking turns, as well as pauses. The analysis of overlaps and pauses can be important for certain studies, so the question arises on how to best account for these phenomena. If a transcript is presented in a purely linear fashion, one intervention above the other, valuable information might be lost. This is why other types of presentation are often used. For example, in the CLAPI corpus, the contributions from the different participants are presented one below the other. Overlaps are indicated by green square brackets, making it easy to see where and when they occur, as shown in Figure

In summary, the transcription of spoken data requires many decisions to be made concerning the nature and the amount of information to be added, not only to the dialogues themselves, but also on how to communicate such information on the files and visually. These decisions should be made even before the data collection process begins, since an important portion of contextual information could be lost if it is not recorded during the interactions.

Ethical and legal issues

Creating a corpus involves using (or even sharing with other researchers) language samples produced by third parties. Those persons having contributed to a corpus through their language productions have rights that need to be respected. In the case of a spoken corpus in particular, it is essential for participants to know that they are being recorded and that their data will later be used for linguistic analyses. For this, the creators of a corpus must hand a document to their future participants clearly explaining who the data will be accessible to and how it will be used. The participants can then freely decide whether or not to sign a form stating their consent to take part or not in the study. However, such consent to participate does not suffice to share the data with other people afterwards unless this usage has been explicitly mentioned in the form. In fact, a participant may agree with the idea of being recorded by a researcher and then having such data used for research, but not necessarily agree with having their data being shared with a large number of people, perhaps even with web-free access. In order to be able to share corpus data, it is imperative to ask participants both for their authorization to use and to distribute the data before collecting them. If a participant later refuses to have their data distributed, removing them from the corpus may pose many difficulties. In the case of dialogues, all of the data of the events that the participant was involved in will have to be removed.

The right to anonymity of the persons mentioned in the corpus represents another important ethical problem. Often, people interacting in recorded conversations refer to third parties by naming them. These people did not provide their consent to being talked about in public documents, so their names should be removed before publishing such data. This anonymization process is not always easy, however. For example,

In the case of written corpora, the situation is simpler, especially when it comes to published data. It is reasonable to think that the public figures mentioned in the articles agree to waive their right to anonymity. The responsibility of the corpus compiler is involved when it comes to texts with potentially defamatory content. In the case of articles found on the Internet, in particular, source verification is necessary before indiscriminately including texts collected automatically, following the web crawling processes described earlier in this chapter. In general, we should also be aware of the fact that distributing a corpus implicitly amounts to disseminating the ideas contained inside its texts. In some cases, this may pose ethical problems for researchers. For example,

Written corpora containing published texts are confronted with copyright issues. While it is true that laws differ from country to country, it is not possible to distribute the texts of an author during his/her lifetime without demanding some type of compensation. In France, this period is valid until 70 years after the author's death. However, contrary to what many people think, data accessible on the Internet are also subject to copyright. Their use can be softened, providing that they are accompanied by sufficiently permissive user licenses, such as the Creative Common license which concerns the contents of the collaborative encyclopedia Wikipedia. Due to copyright restrictions, in the case of less permissive licenses, corpora creators encounter many restrictions for including data. There are several possible strategies for properly addressing the copyright problem.

First, we can limit our choice to works that have fallen into the public domain and/or coming from websites where data have been declared free of rights. This solution is the safest one from a legal point of view, though it is not the most satisfactory one from a linguistic point of view. As a matter of fact, this selection method hinders the collection of data that truly mirror contemporary language and certain stylistic genres which are poorly represented on the Internet.

A second solution would be to negotiate the right to use data with their owners. This solution can be realistic when creating a corpus drawn from a limited number of sources. Rights holders often agree to authorize a single researcher to use a reasonable amount of their data for research, but this type of corpus often cannot be later redistributed. This limitation poses a problem for research replicability, which is an important scientific element in order to grant its validity.

Another way of dealing with the copyright problem during corpus distribution would be to allow users to search for concordances in the corpus, but not to visualize it in its entirety. This is the case for many corpora that are only available via an online consultation interface. These interfaces only enable occurrence searches for words or expressions within a certain context. This solution effectively preserves copyright, since the works remain inaccessible. For users, these interfaces make it possible to answer a certain number of research questions related to the lexicon. However, they are unsuitable for research questions that require data processing, for example, some kind of annotation or those that have to take into account a large context, in order to identify certain linguistic phenomena such as speech acts or discursive phenomena.

Conclusion

In this chapter, we have discussed the main elements to consider when creating a new corpus. For a start, we mentioned that corpus creation is a long and complicated process. This is why reusing the already existing data should be prioritized as far as possible. Then, we saw that the important methodological trait to be respected when creating a corpus is datarepresentativeness. The latter can only be defined in relation to a specific research question. The representativeness of a corpus also depends on its balance and the choice of samples it contains. We also introduced some basic principles regarding sample collection and balancing. We then addressed some concrete problems, related to data coding and transcription into a corpus, and concluded that these questions needed to be resolved before starting the data collection phase. Finally, we saw that the creation of a corpus poses several ethical and legal questions which should be carefully considered, since distributing data amounts to disseminating information that belongs to and concerns third parties, whose rights must be respected.

Revision questions and answer key

6.8.1. Questions 1) What types of data should be collected to conduct a representative study of how young people use the discourse marker genre in French?

2) How could we balance the different parts of a corpus aiming to study the French literature of the 18th Century?

3) What are the main questions to consider when choosing the samples to be included in a corpus? 4) Using the Sketch Engine, choose five keywords in order to create a corpus on the French cinema. What are the characteristics of the corpus thus created? Which are these keywords? 5) What transcription information would it be important to add to a corpus of spoken conversations to study the language of the suburbs in France? 6) What are the ethical issues to consider in the following cases: a) a collection of texts produced in class by children; b) a recording of spontaneous conversations of a group of friends at a bar; c) a recording of a teacher's course for a spoken corpus. 7) Which of the following actions do you find problematic from a copyright perspective: a) using a digital version of the novel series Harry Potter for compiling a corpus stored exclusively on your computer; b) sharing this corpus with your partners as part of a corpus linguistics course in order to do joint homework; c) distributing this corpus on the Internet; d) including an entire chapter drawn from this corpus in a publication with the aim of illustrating certain linguistic phenomena that you have annotated.

Answer key

1) In order to have representative data for this research question, the corpus chosen should evidently contain language produced by young speakers. This concept would need to be clarified to be operational, for example, by deciding to include an age group ranging from 15 to 25 years. This study does not specify a geographic region. One way of delimiting research for such a project would be to compare young people living in large cities in four French-speaking regions from different countries, for example, Paris, Brussels, Geneva and Quebec. The young speakers included in the corpus should proportionally represent the two genders and have different socio-economic profiles. Finally, the corpus should contain young speakers recorded under similar conditions in order to avoid any context-related bias. Another study could aim to compare these uses across different speech styles, which should then be represented in the corpus in a balanced manner.

2) This research question is fraught with different constraints. First, the corpus should contain French literature, which restricts the literary subject to works written in original French, rather than translations. It should also span a specific period, which could be defined, for example, as works published between 1800 and 1900. The difficult point to assemble this corpus relates to the way of balancing its content between different literary genres. It should therefore contain novels, short stories, plays and poetry. Let us assume that the corpus targets a size of 200,000 words per genre, in order to have a representative sample of each of them. Since novels and plays are long texts, it would be a good idea to include excerpts (e.g. a chapter or an act) from many different works, rather than two or three texts in their entirety. Conversely, since poems are a very short genre and more marginal in terms of the amount of texts published, a possible decision would be to limit the share of poems to a smaller percentage of the corpus, for example, to limit poetry to 50,000 words.

3) The first question to ask is whether a sample is representative of the genre it embodies in the corpus. For example, before deciding to include an interview with a young Brussels resident in a corpus of French spoken in Belgium, we should make sure that this person has not recently moved to Belgium from another country, and that they properly reflect the linguistic specificities that the corpus is supposed to embody. The second important question concerns the size of the sample that will be included in the corpus. As we recalled above, it is not always optimal to include entire texts in a corpus when these are very long. Depending on the target size for each genre, it is necessary to determine the appropriate size for each sample. A third question to consider concerns the way in which the samples are acquired, depending on whether these are digitized texts, texts to be scanned or transcribed. Besides, it is also necessary to determine which metadata will be associated with each corpus sample. All of these decisions need to be made based on the research question being considered. A final and very important point is to ensure that copyright is respected, either because the text is copyright-free or because the author has provided written consent granting permission to include their text in the corpus. Finally, we should make sure that the sample is acceptable from an ethical point of view, in the sense that it respects the right to anonymity of the person involved and that it does not contain inappropriate content. 4) By creating a corpus with keywords such as cinéma, films and acteurs and using the default parameters offered on the site, Sketch Engine produces a corpus of 90,441 word occurrences, including 2,680 word types retrieved from 35 different pages. The most frequent content word is cinéma, at the 20th frequency rank. The keywords in the corpus include proper nouns such as Edison, Funès, Fernandel, Gabin and Reynaud and also content words like cinéphile, cinéma and crédits. Collocations include elements like cinéma français, cinéma muet, art dramatique, histoire du cinéma, grand écran, carrière cinématographique, mise en scène, film français, actrice américaine, etc. These collocations make perfect sense in view of the search terms used for creating the corpus. 5) Transcription information should include both metadata and indications inside the transcripts themselves. The metadata of such a corpus should at least include information of where the recording took place, its date, the context in which it happened, the conversation topic, the number of participants, the gender of the person recorded, his/her age and profession. Within the transcripts, it might be useful to include an annotation of the non-standard words used, for example, in verlan, with their equivalent standard so that they can be found in a concordancer. An indication of pauses, overlaps and certain prosodic phenomena may also be useful. 6) a) Above all, a collection of texts produced in class by children for assembling a corpus requires protecting the children's right to anonymity. No element in the corpus should make it possible to identify any participant. Depending on the nature of the texts, it is necessary for the content to exclude any element making it possible to identify any other person. b) In the case of a recording of spontaneous conversations of a group of friends at the bar, everyone involved should be notified that the conversation is being recorded and that he/she agrees. Depending on the purpose of the corpus, this agreement should also consider data sharing with third parties. Then, depending on the conversation topics, it would be necessary to ensure that the content is neither defamatory nor offensive, and that it does not enable third party identification. c) Finally, in case we decide to record a lesson from one of the professors for a spoken corpus, we should make sure that the professor has been informed about the recording and has given his/her consent, both for the use and for the possible sharing of the data. 7) a) Using a digital version of the Harry Potter series for assembling a corpus to be stored only on your computer and searching for elements in the text is not problematic a priori, insofar as this digital version has been legally acquired, for example, by buying an e-book from an online bookstore. b) However, sharing this corpus with your classmates within the framework of a corpus linguistics course with the aim of carrying out joint homework is a bit more delicate an issue, because the fact of buying a book does not entitle you to duplicate it or to transmit it free of charge to others. This practice may, however, be considered as a tolerated use of the material if the aim is to carry out joint homework on the data, which are not being used in any other way. c) Distributing this corpus on the Internet is completely illegal under copyright rules which can be enforced for decades after the author's death (70 years in France). In this case, the Harry Potter series will still be protected for many years, and any form of distribution is currently prohibited. d) Including an entire chapter of this corpus within a publication to illustrate certain linguistic phenomena that you have annotated can also be problematic from the point of view of copyright. Though it is acceptable to quote portions of a text while indicating its source, these quotations should not exceed a clearly defined size limit. This size varies from country to country, but generally does not exceed a few hundred words. Thus, publishing an entire chapter of a book is not acceptable. 6.9. Further reading

How to Annotate a Corpus

In Chapter 6, we discussed the importance of associating metadata with corpus files. In this chapter, we will explore how to insert other types of information into a corpus as linguistic annotations. To begin with, we will see that annotating a corpus is a way of adding value to it by widening the field of questions that it will make it possible to investigate. We will then review the different types of annotations we can add to a corpus, briefly present some tools for performing some annotations automatically or for making manual annotations easier. We will also discuss best practices to follow when making a manual annotation and present the different ways to assess the reliability of such annotations. Finally, we will present the principles to be respected in order to make annotation sharing easier.

Corpus annotations

Raw data which are collected in a corpus are not always adequate for answering many research questions. Let us imagine, for example, that we wish to know how often French nouns such as ferme and car appear in a corpus. A simple search for their occurrences in a raw data corpus will provide a biased answer, since these words also have other uses apart from being nouns, and these will be included among the search results (ferme is also a conjugated form of the verb fermer and car is also a coordinating conjunction). In order to keep only the relevant occurrences, should we filter all the relevant occurrences by reviewing them one by one? Such manual sorting tasks can actually be avoided if the words in the corpus have been previously annotated into grammatical categories. This annotation makes it possible to exclusively look for the noun occurrences of ferme, for example.

The great advantage of part-of-speech tagging is that it can be done automatically with almost the accuracy of a manual annotation, regardless of the amount of text to be annotated. As a matter of fact, part-of-speech tagging has been performed on the Google Books corpus (see Chapter 5), which contains billions of words from different languages and has made it possible to refine research on language evolution. For example,

Finally, the main objection often raised against annotations is that any form of annotation is necessarily subjective, at least in part. This implies making choices about the categories to be annotated (see section 7.3), which are never completely neutral. Furthermore, the annotation itself is accompanied by a hermeneutic dimension, which reflects the annotators' point of view about the text. This is due to the fact that all the categories include borderline cases, for which different annotations can be justified. In these cases, annotators must make decisions while simultaneously interpreting data.

An example drawn from recent research on language acquisition will better illustrate the impact of annotation choices. Different authors have studied the early production of causal connectives in children, in different languages.

(1) Grand-mère: pourquoi ça? (Grandmother: why that?) Léa: parce que j'en avais envie. (Léa: because I felt like it.) Conversely, Evers-Vermeul and Sanders (2011), as well as

(2) Léa: je prends ma gourde parce que j'ai soif. (Léa: I'm taking my bottle because I'm thirsty.)

(3) Léa: venez parce qu'il est très tard ! (Léa: come because it is getting very late!) This example illustrates the influence of the methodological choices associated with data annotation and the conclusions that can be drawn from a corpus study.

While it is true that annotation processes involve choices that are always partly subjective, many researchers (e.g.

Different types of annotations

All levels of linguistic analysis can be annotated in a corpus. To begin with, some annotations are related to phonology, particularly in the case of transcriptions for spoken corpora. For example, these annotations indicate prosodic phenomena like pauses, hesitations and prosodic phrasing. In this way, they make it possible to look for such phenomena automatically, without having to listen to all the audio files in the corpus again. In addition to the study of prosody, these annotations can be useful for works of a very diverse nature, for example, for studying the notion of fluency or the interface between syntax and discourse, so as to better understand information structure in discourse.

Words are often the linguistic element the most subjected to annotations in a corpus. The most basic of these annotations is the division into words or occurrences (known as tokenization), which includes punctuation identification, elision processing (in French) or the identification of numbers and dates. However, lemmatization refers to the act of associating every word occurrence in a corpus with its basic morphological form. For example, adjectives such as gentil or gentille (in French) are the masculine and feminine variants of the same word (or type) from a morphological point of view. This canonical form of the word is called its lemma. In the case of adjectives, their lemma is by convention the singular masculine form. Similarly, the words mouse and mice are the singular and plural variants of the same lemma (mouse), whereas eat, eaten and eating are the conjugated forms of the lemma eat. Lemmatizing a corpus is an essential process for studying lexicon, since annotation makes it possible to look for occurrences without having to detail all the morphological variants it may adopt. Lemmatization is one of the types of annotations which can be done automatically, with considerable accuracy (see section 7.4).

The notion of the lemma should be differentiated from the concept of the lexeme, which is used for defining a word from a semantic point of view. So, for example, bat is a lemma which can correspond to two different lexemes, as this word is polysemic and may either refer to a flying mamal or an object used to hit a ball. Finally, we should bear in mind that the usefulness of lemmatization may change significantly from language to language. In languages making little use of inflectional morphology such as English, this process is not much useful. It is nonetheless more useful in French, and even more in languages containing an abundance of morphological inflections, such as Finnish or Russian. In addition to lemmatization, words can be annotated into grammatical categories thanks to part-of-speech tagging, as we previously mentioned in relation to word annotations such as ferme and car. We will come back to this later in the chapter.

Finally, words can be annotated into semantic categories, for example, the word tennis can be annotated as a part of the sports category, later making it easier to quickly go through the content of a corpus, and to disambiguate polysemic words in context. Annotation also provides training and testing data for automatic word sense disambiguation. Indeed, this type of annotation is more difficult to perform automatically than part-of-speech tagging, since it requires conceptual knowledge in context and this is still a major challenge for artificial intelligence. For example, depending on the context, the word mouse may belong to the category of computers or animals.

In addition to words, sentences are also regularly annotated in corpora. The most common annotation is syntactic parsing. As soon as a corpus has undergone a part-of-speech tagging process, it is possible to parse it, and thus reveal how grammatical categories can be grouped into smaller phrases within a sentence. In traditional grammar, these syntactic representations often adopt the form of constituent analyses, as illustrated in Figure

(4) The teacher congratulates the student Due to their tree representations, parsed corpora are often called "treebanks". The most famous corpus containing such an annotation is the Penn Treebank corpus (see

(

Apart from indicating constituent structures, some parsing analyses show the dependency relations between words or syntactic constituents. A dependency relation takes place when one word governs another. For example, in a noun phrase, the noun governs the adjective, which depends on it for receiving agreement marks. Dependency analysis was performed automatically on the Google Books corpus

Sentences can also be analyzed from the point of view of semantic relations between their constituents, as well as the thematic role of each of them. These roles include the agent, the patient and the cause. Finally, sentences may contain a pragmatic annotation of the speech act involved (e.g. a question, a request or a confirmation).

At a higher level, some annotations indicate the relations holding between sentences, or between discourse segments. In this case, it is rare to be able to rely on automatic annotation tools. For this reason, discursive annotations are most often the result of human annotators. These annotations include, for example, coreference relations, associating pronouns or noun phrases with their antecedent in discourse, as illustrated in (6). In this example, the pronoun he in the second sentence is coreferential with the noun phrase Fred. In other words, it is used for designating the same person. This annotation is useful since a pronoun like he is not an autonomous referential expression, meaning that it does not by itself make it possible to identify the referent in question if we ignore the context. (

Other discursive annotations can explain discourse relations holding between phrases or propositions within complex sentences, such as causality as in (

(7) Fred was very happy because he had won his tennis match.

In French, the DEDE corpus

Finally, corpora including children's language or productions of foreign language learners may contain an annotation of errors. These annotations provide valuable clues to assess the nature and extent of the problems associated with each acquisition stage. Error annotations often include categories such as: incorrect word or structure, missing element, superfluous element or inflection/derivation mistake (see

Standardization of annotation schemes

For all the annotations described in this section, international standards have emerged in the literature. When starting an annotation project, it is advisable to gather information about the existence of such standards and to consult previous studies to decide on the best way to annotate data. Whenever possible, it is preferable to reuse existing annotation schemes and to adapt them to the needs of the study rather than creating new schemes from scratch, for which comparisons with literature could be difficult to draw. One of the main initiatives, within the formal framework of the International Organization for Standardization (ISO), more specifically in technical committee no. 37 dedicated to language resource management (see the URL at the end of this book), has enabled around 20 standard drafts for linguistic annotation, be it lexical, syntactic, semantic or discursive in nature. For example,

In many cases, standards are established de facto, when a big project is successful, rather than through the creation of a formal framework. This is the case, for example, in the field of discourse relations. The taxonomy of relations developed for annotating the Penn Discourse Treebank

Finally, we should point out that the annotations described in this section are performed on entire corpora. Indeed, each word belongs to a grammatical category and every sentence communicates a speech act. These annotations imply a global processing of the corpus. Conversely, many other annotations made on corpus studies are devoted to a single type of element to be studied. For example,

The stages of the annotation process

In this section, we will detail the different steps which outline the process for annotating corpus data. Before starting the annotation process itself, we first have to define the categories which will be annotated in the corpus. In practice, these will be represented by a list of tags or sometimes, by dependency relations. For example, the annotation of speech acts requires setting up a list of the acts to be annotated, for example, requesting, promising, asserting, threatening, etc. However, a semantic annotation of verb types could differentiate their aspect (state or event verbs). Finally, an annotation of the grammatical categories associated with each word requires establishing a list of these categories. Defining categories is not as simple a process as it may seem at first glance.

One of the problems is the need to find a good balance between tags which are specific enough so as to avoid unwanted amalgams, and categories which are sufficiently generic so that they can be reliably identified by annotators (see section 7.5). For example, should we make a generic category including all the determiners or should we use specific tags for articles (the, a, etc.), on the one hand, and the possessive pronouns (my, his, etc.), on the other hand? Should we make a generic category for conjunctions or a category for coordinating conjunctions and another one for subordinating conjunctions? There is no single answer to these questions, as the right level of granularity of a (more or less precise or generic) annotation depends, to a large extent, on the objectives of the annotation and its feasibility. It is possible that some sub-distinctions are not useful and can therefore be omitted for a certain research question. For example, a corpus that is annotated into grammatical categories with the aim of being used as a reference tool for beginner language learners will have to use a simplified categorization of grammatical categories. Contrary to this, a categorization intended to be used as an entry point for a syntactic parser software should contain sufficiently precise tags in order to avoid incorrect syntactic analyses. Regardless of whether the annotations are made by software or by a human, it may also happen that some useful distinctions cannot be annotated in a reliable way, because they imply that many cases are ambiguous (see section 7.5). In this case, simplification becomes necessary.

While preparing the instructions for the annotation process, it is important for each category to be clearly defined so that the annotators know how to use them, in cases where the annotation is performed by humans and not automatically. For example, for annotating speech acts in a corpus, a tag like question could be interpreted very differently depending on the annotators, if no explanation is added. Some will only annotate interrogative syntactic forms as in (8) with this tag, whereas others will also include indirect interrogatives as in (

(8) Who will come to the party? (9) I wonder who will come to the party.

(10) I am dying to know the guest list.

In order to avoid these problems, a set of tags should be accompanied by a list of criteria specifying in which contexts each tag should be used (see section 7.6). For example, the category question could be defined as follows for an annotation of speech acts: "Any utterance used in context as a request for information, whether a direct or indirect one". This definition would invite annotators to include examples (

In sum, the definition of annotation instructions largely depends on the linguistic phenomenon studied. In general, it is preferable to choose an annotation scheme as neutral as possible from a theoretical point of view and, in any case, to stick to categories clearly identified and widely accepted in the literature. The results of the annotation will be easily understood and it will be possible to reuse it in future work. As far as possible, the chosen tag sets should be based on annotation standards which have been broadly accepted in the literature. Should these standards be lacking, it would be a good idea to consider the annotation schemes used in previous studies. We should not rule out the point where innovations are desirable or even necessary, but these need to be clearly justified in relation to existing schemes.

Once the tag set has been defined, the corpus processing phase can begin. The first step is to identify which occurrences will be annotated in the corpus. For some phenomena such as the annotation of morphosyntactic categories or speech acts, every word or sentence in the corpus will be involved. However, for other phenomena, the annotation will only refer to very precise elements in the corpus. For example, an annotation of discourse markers such as well, you know and I mean will only relate to these elements.

Annotations covering the entire corpus are often made using computing tools, which we will describe in the next section. In the rest of this section, we will focus on a detailed annotation of specific elements in the corpus. For these annotations, a strategy should be deployed to identify relevant elements. In most cases, identification must be done on raw data, sowhenever possible -it is important to find one or more lexical elements that can be automatically identified by a concordancer. For example, discourse markers can be identified by looking for lexical elements encoding them as you know, I mean, etc. An annotation of cleft structures in French can start by looking up structures containing the verb form c'est. We have offered examples of this type of research in Chapter 2. In other cases, such research will benefit from a preliminary part-of-speech tagging or even from a parsing analysis performed automatically. For example, in order to study causal relations in French, part-of-speech tagging makes it possible to only look for occurrences of car working as conjunctions and eliminating those which are nouns (a type of vehicle).

Whatever the strategy used for retrieving data, the results obtained will in most cases require validation and manual sorting in order to eliminate irrelevant occurrences. For example, occurrences of expressions like you know and I mean do not always correspond to a discourse markers. Sometimes, these expressions are also used in a compositional manner, as in (

Once the corpus to be annotated contains only the relevant occurrences, the annotation process itself can then begin. Whatever the annotation considered and the tag set chosen, the annotation of the first occurrences is generally difficult and many problems and borderline cases arise. It is often impossible to anticipate which dubious cases will appear since corpus data are always much more complex and ambiguous than the reference sentences found in dictionaries. Let us take an example to illustrate this difficulty. According to the Lexconn database

(13) Je suis d'accord de le voir, dans la mesure où il vient seul. (I agree to see him, provided that he comes alone.) (

(15) Tel que je le connais -et je le connais bien -je lui fais confiance: ce mouvement ne va certainement pas s'arrêter et, dans la mesure où il ne s'arrêtera pas, il sera conduit avec habilité, M. Pujol et les autres ministres-présidents n'en manquent pas. (As much I know him -and I know him well -I trust him: this movement will certainly not stop and, as long as it does not stop, it will be led with skill, Mr. Pujol and the other Prime Ministers will see to it.

(16) Mes chers collègues, cette énumération résulte de vos propres interventions. Dans la mesure où on a même demandé l'insertion des familles "homosexuelles", je ne vois pas pourquoi vous protestez. (My dear colleagues, this list results from your own speeches. Insofar as we have even asked for the insertion of "homosexual" families, I don't see why you should protest.) (17) Dans la mesure où nous nous dirigeons vers l'adoption d'un nouveau traité, qui plus est constitutionnel, le dessein d'une Europe élargie et sa finalité doivent être mis en débat dans nos sociétés, sans quoi nous ne serions pas à l'abri d'un incident majeur. (Inasmuch as we are moving towards the adoption of a new treaty, which is a constitutional one furthermore, the aim of an enlarged Europe and its purpose must be debated in our societies, without which we would not be immune from major incident.)

These examples illustrate the complexity and the ambiguity of the real corpus data when compared to invented examples. For example, in (17), we may wonder whether the speaker takes it for granted that Europe is moving towards a new treaty (and therefore presents it as a cause) or whether, on the contrary, it is a condition for a treaty to be adopted.

These difficult cases must be resolved by following systematic criteria guiding all decisions. For example, in (

Annotation criteria can only be fully determined on the basis of the cases encountered in the corpus, since an exhaustive list of problems is often difficult to anticipate. This is why it is wise to test the categories to be annotated on a small portion of occurrences, ranging, for example, from 50 to 100 depending on the difficulty of the annotation scheme and the total number of occurrences to annotate, then refine the criteria or even redesign the categories on the basis of this first annotation. For example, if an initial distinction proves impossible to discern sensibly in the corpus by the annotators, it should be abandoned. Conversely, if a category seems to cover too many disparate cases, it will have to be refined. Annotation should therefore start from theoretical criteria, but will be progressively modified depending on the nature of the data to be annotated, as illustrated in Figure

Annotation tools

We can classify annotation tools depending on the automatic or manual processing that they bring into play. On the one hand, there are the tools making it possible to carry out annotations in an automatic way, for example, by means of part-of-speech tagging or parsing. On the other hand, there are tools that facilitate the process of manual annotation of data by providing an interface to perform the annotation, as well as a format for representing such annotations. In this section, we will briefly introduce some of these tools. We should beware that there are a very large number of them and that their more or less suitable character depends on the type of annotation the researcher has in mind. It will therefore be up to each researcher to identify the tool best suited to his corpus and his/her research question. Very often, a good starting point is to identify which tools have been used in similar studies in the literature.

The usefulness of part-of-speech tagging is such for corpus studies that this annotation is now directly embedded into some corpus creation tools. The Sketch Engine platform, which we presented in Chapter 6, automatically performs this tagging process when a new corpus is created and this can be done for several languages (see below). Since this platform enables you to create new corpora both from the Internet and from manually inserted files, it is very convenient to use it for carrying out part-of-speech tagging. The set of part-of-speech tags used by this program is presented in the documentation provided on the site. Let us also note that systems for annotating part-of-speech tags (POS taggers) should be carefully developed for each language and that the sets of tags corresponding to grammatical categories often vary from one language to another (in concrete terms, their notation may vary even more). However, there are also initiatives there to build up sets of grammatical categories, which if not identical, are at least compatible between languages in terms of fundamental categories (see, for example, the Universal Dependencies project). The tags used may also vary depending on the corpora consulted in different languages, since the annotated corpora available on the Sketch Engine have not all been annotated in the same way. The list of tags used for a corpus is presented on the site. Let us also remember that due to these differences between languages, some of the tools offered online may only work for English. However, Sketch Engine makes it possible to tag a corpus in French as well as in many other languages. If necessary, this annotation can be corrected manually.

Once annotated, the elements of the corpus can be searched by their POS tags by means of a concordancer. For example, Sketch Engine provides the option to search by lemma or by grammatical category. In order to combine search criteria, the CQL query option (Cassandra Query Language) must be chosen. For example, to look for all the adjectives that are used with a form of the word acteur, the CQL query should take the following form: [lemma = "acteur"] [tag = "ADJ"]. This research specifies that all forms of the lemma acteur will be retrieved from the corpus when a word tagged as an adjective appears to its right. To find the adjectives used to the left of this word, the element order of the query must be reversed [tag = "ADJ"] [lemma = "acteur"]. The CQL syntax needed for formulating queries is documented in the search interface.

The annotation of syntactic dependencies can also be done automatically using computer tools, but so far these have not generally been included in the interfaces for creating corpora, and their use requires natural language processing (NLP) skills that go beyond this book. Interested readers will be able to use toolbox components designed for NLP such as GATE, NLTK or spaCy to make these annotations (see URLs at the end of the chapter).

Manual annotations can be done using tools that help processing and reusing annotations, without replacing the effort of human reflection when categorizing each occurrence. For example, Brat is an online tool that makes it possible to annotate not only entities in a corpus, but also the relations between them. We can thus annotate events described in a corpus, as well as the links between the various participants in these events. For example, a verb like invite in (

(18) Max invited Lili to his house for a meal.

All of these are part of the event (invite) and can be linked to it by an annotation describing the named entities involved and the semantic links between them. Being able to annotate relations is also essential for associating anaphoric relations in a text. The annotations made with Brat can later be used for research in the Sketch Engine.

Another example is the EXMARaLDA tool, which has been specifically developed to assist in the transcription and annotation of spoken corpora, and later be able to manage them. This tool makes it possible to perform a time-aligned annotation with audio or video files, to insert metadata and to have access to the results of the annotation in different formats.

Many tools, among which we can mention EXMARaLDA, use the XML format, which is a coding language (eXtensible Markup Language) for defining, and then storing and transmitting structured data. In this language, the elements of a text are marked up using named tags including one or more attributes. These elements can be embedded into each other depending on the needs of the structure.

For example, an element like word can be embedded into a sentence type of element. Taggers in XML thus provide a good way for associating annotations with corpus data. However, in order to be interpreted, a document marked up in XML coding must follow certain syntax, in the form of a document type definition (DTD), which specifies the list of tags with their potential attributes, as well as the possible order of appearance for tags. The most widely used XML schema for coding corpus data is the one provided by the TEI (Text Encoding Initiative). XML can also be used for encoding the metadata of a corpus (see Chapter 6, section 6.4). In those cases standards like the Dublin Core can be used. Tools have been developed in order to make data annotation in XML format easier, such as the Nite XML tool, for example.

For annotations of a specific linguistic phenomenon (in the situation presented at the end of section 7.2), one solution would be to retrieve the relevant data from the corpus, and then to annotate them separately. For example, we can retrieve all the discourse markers using a concordancer, and then annotate their function in a file. Concordancers generally make it possible to export the data retrieved in text format. However, it is afterwards essential to import the data into spreadsheet software rather than to annotate them in a simple word processor. Actually, a spreadsheet makes it possible to view the data as one occurrence per row in a single column. The following columns will be used for inserting the annotations. Thanks to the functionalities offered by the spreadsheet, the different annotations can then be counted, compared by means of a crosstab and whenever necessary, they can be inserted into statistical analysis software (see Chapter 8, section 8.6).

Measuring the quality and reliability of an annotation

The notion of quality of an annotation is particularly useful for annotations made automatically, such as part-of-speech tagging. In this case, the quality of the automatic annotation is measured in comparison with a reference annotation produced by human annotators. This annotation is presumed to be completely correct, that is, it matches what has been deemed appropriate in the annotation scheme. It was by means of this type of comparison that the annotations offered for the Google Books corpus in section 7.2 were considered as suitable tagging and parsing systems. This definition of the accuracy of an annotation is often subdivided into two separate criteria. On the one hand, the recall measures the number of occurrences of each category correctly found by the system. For example, we can report the number of noun occurrences in the reference annotation that the system correctly identified as such, weighing the total number of nouns in the reference. On the other hand, precision measures the number of occurrences properly tagged as nouns, from among all the ones tagged by the system as such. For every category, we can combine the recall and precision scores by calculating their harmonic mean (called F1 score), with identical or different weights. Finally, we can express the overall quality of the automatic annotation by calculating a mean of F1 scores per category, by weighing every category according to its number of occurrences in the reference (micro versus macro mean).

The reason for considering recall and precision at the same time (e.g. with the F1 score) is as follows, illustrated with the example of noun tagging. If a system had a strategy for annotating all ambiguous words as nouns (e.g. rain, break or close), its recall would certainly be excellent, but its precision would be very low, since the tag noun would probably be mistakenly assigned in many cases. Conversely, if a system annotates as noun only the elements that are preceded by a determiner, its precision would be much better this time, since the number of wrong taggers would be lower. However, its recall would be weak because it would fail to correctly tag all the proper nouns. Overall, part-of-speech taggers currently produce results with more than 95% accuracy in the most studied languages such as English and French. Residual errors are either corrected manually or in the case of very large corpora such as the Google Books corpus, they are left as such, because their prevalence is low enough not to significantly bias the results of a research.

While calibrating the performance of automatic tools is ultimately based on a reference human annotation, the question arises whether the annotations made by a human are necessarily 100% accurate. Most of the time, this is not the case. As we have seen, annotation always requires a certain type of interpretation and this may vary slightly from one annotator to another. This is why it is crucial to have annotations carried out by several annotators and to calculate the degree of agreement among them, as we will see below. In addition, humans can also make mistakes due to fatigue, lack of attention or a poor understanding of the annotation instructions. Finally, some examples are really ambiguous because the boundaries between certain categories may be blurred. To understand the ambiguity of such examples, one possibility is to enable the use of two tags simultaneously. The examples annotated in that way can later be treated separately in data analysis and can offer new insights on the nature of the categories thus defined.

In order to improve the reliability of annotations made by humans and to help define clear categories, a commonly used method is to have the same annotation made by two different annotators. The more convergent their annotation, the more it can be considered reliable and the divergent case can be resolved through a discussion a posteriori. Furthermore, this double annotation process will make it possible to indicate whether the categories have been poorly defined. As a matter of fact, if a certain annotation turns out to be impossible to achieve in a convergent manner for human annotators, this indicates that the phenomenon may be poorly defined or that the categories have been poorly delimited.

In order to measure the agreement between two annotators, a first solution is to count the number of times they choose the same tag and to deduce a percentage of convergent annotations. To do this, the two annotations must be compared as in Table

As an example, this table summarizes the annotation of the two possible connective functions for dans la mesure où we mentioned above, carried out by two annotators. Numbers indicate that the two annotators agree that the connective communicates a condition relation in 20 out of the 100 sentences they examined, and a causal relation for 60 other sentences. Conversely, there are 14 sentences where annotator 1 considered that the connective indicated a cause, whereas annotator 2 thought it indicated a condition, and 6 sentences in the opposite case (thus, the total of the sentences in the four situations is 100). In total, the annotators agreed in 80% of the cases. The problem with this estimate is that it does not indicate whether this agreement should be judged as satisfactory or not. What is more, it does not take into account the probability of an agreement which may be the result of chance in some cases. To illustrate this problem, let us imagine that a first annotator always chooses the tag condition to annotate data that actually has 50% condition relations and 50% causal relations. His agreement with a second annotator who would actually analyze the sentences and annotate them correctly would still be 50%. If, however, the real proportion of condition relations in the corpus is 80%, the agreement obtained by chance would be 80%! A better approach is to use the kappa coefficient

In this formula, P(O) corresponds to the proportion of agreement observed during the classification task and P(E) corresponds to the statistical expectation of agreement. The value of P(O) is obtained by dividing the number of matching responses (classifications) by the total number of responses. The value of P(E) is obtained by calculating probabilities, by estimating the average of concordant classifications when the proportion of each class is fixed to the value observed for each annotator. Thus, if each annotator divides their responses into two classes of comparable sizes, P(E) will be close to 50%, whereas if the sizes are unbalanced in the two annotators, P(E) will also increase, since they will have more chances of annotating the most frequent class at the same time

The kappa coefficient may have a value oscillating between -1 and 1. A coefficient equal to zero means that agreement between the two annotators does not exceed the one obtained by chance. A negative value indicates an opposite correlation, obtained when annotators disagree the most. The level of agreement is then lower than the one which could be obtained by chance and the value of P(O) is close to 0. Conversely, the maximum value K = 1 indicates that annotators always agree, the value of P(O) then being 1. So, in practice (for a task that is somewhat coherent), the kappa coefficient generally varies between 0 and 1, that is, between results that can be obtained by chance and total agreement. This does not mean, however, that any coefficient above zero is a good result. According to

The kappa coefficient can be calculated automatically on online statistical calculation sites like VassarStats, which we will discuss in further detail in Chapter 8. In the case we presented in Table

Finally, we should point out that for some research projects, it is not possible to engage two annotators for the whole annotation task. Several solutions are then possible. First, to double annotate a small part of the data to verify that the scheme can be applied convergently, and then to have the rest of the data annotated by one person. Another method is to test the agreement of an annotator with themselves over time. For this, it is possible to re-annotate the same portion of data after a certain time interval, in order to measure the degree of convergence between the two annotations.

Sharing your annotations

Given the importance of the effort invested in an annotation process, many researchers choose to share their annotations, which may benefit other researchers. As we have seen in this chapter, in order to make sharing and reusing of annotations easier, it is important to use categories that are as theoretically neutral as possible. It is also important to be able to export annotations in a standardized format, based on the XML language, for example.

Sharing annotated data also implies that the annotation process and the categories used are clearly documented in an annotation manual, which will be provided to future users together with the data. This manual should allow them to understand and to reuse annotations. For this to be possible, such a manual should include two types of information. First, a list of the tags used with their definition, in the way of a mini-glossary. For example, such a list for an annotation of speech acts could take the form illustrated in (

The annotation manual should also list the rules that have been applied to certain borderline or ambiguous cases in the corpus in order to deal with them systematically. It should also provide information on the corpus processing that preceded the annotation process. For example, it should specify the manner in which the corpus has been segmented into words, sentences, utterances or discourse segments. A form of segmentation is actually a necessary step for any analysis relating to a certain linguistic level and such a segmentation process also poses difficulties which must be resolved. For example, does segmentation into words also include compound words? Does the definition of discourse segments include sentences without verbs? As we saw at the beginning of the chapter, these decisions influence the annotation process and must be clearly documented. Finally, when automatic processing tools have been used for preparing and (helping to) annotating the data in the corpus, they must be clearly indicated. Likewise, revisions that have been made at different stages of the annotation process must be documented, as well as successive versions of the corpus that have been produced, if applicable. Many examples of annotation manuals can be found online and are provided with all the corpora made available to the public.

Conclusion

In this chapter, we introduced the notion of linguistic annotations and discussed the different ways in which these annotations can be incorporated into a corpus. We first reviewed the different types of annotations, covering very diverse elements ranging from phonemes to discourse relations. We then detailed the different stages that make up an annotation process and stressed the importance of good methodological practices, so that the annotation is as valid and reusable whenever possible. We then briefly presented some tools that let you to automatically make annotations or guide manual annotations. We have also seen that these tools often work in standardized formats like XML, which makes it easy to share annotations. We have also addressed the problem of the quality and reliability of annotations and argued that in the case of human annotations, it is difficult to define a quality standard, since each annotator partially interprets the data while annotating them. However, an annotation can be tested from the point of view of its reliability. An annotation is reliable if two annotators produce convergent annotations or if the same annotator produces convergent annotations at two different times. Finally, we presented some recommendations for the creation of an annotation manual, which documents both the content and the annotation process itself in order to enable other researchers to reuse it. 7.9. Revision questions and answer key 7.9.1. Questions 1) Explain three advantages of annotating corpus data.

2) Imagine three examples of corpus studies for which part-of-speech tagging is necessary.

3) In the Frantext corpus powered by Sketch Engine, look for the nouns orange and nage, using annotation into grammatical categories. How many unwanted occurrences (adjectives and verbs) does this method allow you to eliminate? 4) Go back to the French cinema corpus that you created in Chapter 6 using Sketch Engine. Use the CQL option to search for adjectives that are used with the words actrice and film. 5) Using the AntConc, retrieve the occurrences of the connective "dans la mesure où" in the Advanced Literacy corpus available on Ortolang (see Chapter 5). Export the first 20 occurrences to a spreadsheet and annotate them with the "cause" or "condition" tags. Ask a second person to make this annotation and calculate the percentage of agreement and the kappa coefficient. If there is no second annotator, annotate occurrences a second time a few days later. 6) Create a mini manual to document the annotations made in 5. What should it contain?

Answer key

1) The first advantage of annotated corpora is that they make it possible to answer many more research questions than raw corpora. Indeed, whenever the words to be looked up are ambiguous or the phenomenon to be investigated cannot be approached, by surface features such as words, raw corpora reach their limits. Annotated corpora, however, make it possible to look for elements in the corpus related to syntactic or semantic annotations, for example. The second advantage of annotated corpora is that annotations can be reused later and thus add value to a corpus. This advantage is all the more evident inasmuch the same annotation can often be used for answering different research questions. Finally, the annotation of a corpus represents a transparent process when it is well documented, which enables an objective evaluation of the data by the scientific community, contrary to other methodological choices which are often less transparent.

2) Part-of-speech tagging is useful for almost all corpus studies, be they lexicon, syntax, semantics, pragmatics questions, etc. A first example in relation to lexicon concerns the study of all polysemic words, for which part-of-speech tagging makes it possible to sort a good part of the irrelevant occurrences (e.g. to differentiate the adjectival uses of past participles). Since most of the words in the non-specialized lexicon are polysemic, this annotation is essential. In the field of syntax, every analysis should start at classifying words into grammatical categories. It is precisely from these categories that syntactic trees are created. Finally, a search for speech acts communicated by performative verbs such as ask, request and promise can be made easier through the use of POS taggers (as well as lemmatization), since it makes it possible to perform searches for first-person occurrences of the present, which are used for communicating performatives.

3) In Frantext, looking for the noun orange yields a result of 139 occurrences, whereas the same search but without specifying the grammatical category results in 174 occurrences. Searching by category therefore makes it possible to eliminate 35 adjectival occurrences, or 20% of errors. The noun nage results in 154 occurrences, compared to 252 occurrences if classification into categories is not performed. This research therefore eliminates the 98 verbal occurrences, corresponding to 39% of errors.

4) In order to look for the adjectives that are used with the word actrice, it is necessary to make a search for occurrences to the right and another one to the left. The syntax for these queries is: [word = "actrice"] [tag = "ADJ"]. This research focuses on the word rather than on the lemma in order to exclude the masculine occurrences, acteur. The adjectives to the right mainly include nationalities such as française, américaine, italienne, australienne and allemande. The search for adjectives to the left of the word actrice must use the following syntax [tag = "ADJ"] [word = "actrice"]. These adjectives are narrower and list modifiers like meilleure, grande and pire. For the word film, the search can focus on the lemma rather than on the word, in order to identify singulars and plurals. The syntax is therefore: [lemma = "film"] [tag = "ADJ"]. To the right, the results are varied and include adjectives such as super, romantique, dramatique, important, érotique and indépendent. The search for adjectives to the left, with the syntax [tag = "ADJ"] [lemma = "film"] results in the adjectives meilleur, prochain, nouveau and seul. 5) Here in the table below is a list of 20 occurrences drawn from the corpus with two possible annotations. This writing is original insofar as the child owns the character of Perla, the blue macaw of the animated film Rio, as well as one of the places of the film, Rio's forests.

Cause Cause

La consigne est respectée, mais Adam se démarque de ses camarades et de l'auteur dans la mesure où l'enfant et l'animal ne restent ni ensemble ni amis à la fin.

The instructions were followed, but Adam stood out from his comrades and the author insofar as the child and the animal did not stay together or friends at the end.

Cause Cause

Cet exercice semble donc favoriser le statut d'auteur de l'élève, dans la mesure où celui-ci est libre de choisir ses personnages et les événements de son récit.

This exercise seems to favor the student's author status, insofar as the latter is free to choose his characters and the events of his story.

Cause Condition

Il est nécessaire d'en passer par ce genre d'exercice, dans la mesure où celui-ci est important dans l'apprentissage littéraire des élèves.

It is necessary to go through this kind of exercise, insofar as it is important for students' literary learning.

Condition Cause

Le travail de groupe est un choix pédagogique difficile dans la mesure où l'enseignante a dû faire attention à ce qu'aucun élève ne soit laissé pour compte au sein du groupe.

Group work is a difficult pedagogical choice insofar as the teacher had to take care that no student was left behind in the group.

Cause Cause

La contrainte peut certes freiner dans certains cas l'imagination, mais elle est utile dans la mesure où elle permet une progression, puisqu'elle place le travail de l'écriture au premier plan, avant l'inspiration réelle.

In some cases, the constraint can certainly slow down the imagination, but it is useful insofar as it favors progression, since it sets the work of writing in the foreground, before actual inspiration.

Condition Cause

En effet, l'imagination créatrice apparaît comme synonyme d'audace ou de tentation, dans la mesure où elle embellit le quotidien et entretient l'espérance.

Indeed, the creative imagination appears as synonymous with daring or temptation, insofar as it embellishes everyday life and maintains hope.

Cause Cause

Il y a une impossibilité de l'approfondissement de l'imaginaire, dans la mesure où l'école attend de l'élève qu'il évolue dans une certaine direction, au niveau du langage tout autant qu'au niveau psychique.

There is an impossibility of deepening of the imaginary, insofar as the school expects the student to evolve in a certain direction, at the language level as well as at the psychic level.

Cause Cause

Le travail de groupe constitue un choix pédagogique difficile dans la mesure où il est compliqué de faire en sorte que chaque élève soit auteur.

Group work is a difficult pedagogical choice since it is complicated to ensure that each student is an author.

Cause Condition

En conclusion, nous pouvons dire qu'il est possible de considérer que l'élève est auteur dans la mesure où les écrits l'impliquent en tant qu'enfant, faisant ainsi appel à ses sentiments, son vécu, mais aussi à des choix d'écriture et à son expérience personnelle de l'écrit.

In conclusion, we can say that it is possible to consider that the student is an author insofar as the writings engage him as a child, thus appealing to his feelings, his experience, but also to writing choices and his personal writing experience. Writing in young students can also be a source of reading insofar as the response to a writing project generates new targeted readings.

Condition Condition

In order to be able to measure agreement, we must create a crosstab with the two annotations. In the Excel spreadsheet, this can be done automatically by choosing the data and choosing the pivot table option. In this table, annotator 1 should be recorded on the rows and annotator 2 in the columns. For the annotation above, this table looks as follows: The two annotators converge on 13 of the 20 sentences, that is, there is 65% of agreement. This agreement corresponds to a kappa coefficient of 0.2. This value is very low and shows that the annotation is not reliable and should be revised. 6) A manual documenting of this annotation should contain the list of tags as Condition and Cause, together with their definition. It should also contain the rules to follow in ambiguous cases, for example: "The condition relation will only be chosen if the verbal tense used is the conditional tense". This manual should also indicate how the annotation was carried out (by one or two people, throughout how many sessions) and whether successive versions were produced.

Further reading

McEnery

How to Analyze Corpus Data

Quantitative corpus studies produce numerical results such as word frequency or the number of occurrences of a given syntactic structure across different text genres. By using statistical tests, it is possible to analyze these numerical data and reveal tendencies that are not always visible to the naked eye, and to evaluate whether or not the observed trends are statistically significant. The purpose of this chapter is to introduce some simple statistical methods that are commonly used for processing corpus data. To begin with, we will introduce the concept of descriptive statistics. Then, we will present two types of descriptive statistics that, respectively, make it possible to measure lexical diversity (the type/token ratio), and to calculate lexical dispersion in a corpus. We will later describe the principles underlying inferential statistics. In addition, we will see that data from corpora correspond to different types of variables, and that such categorization is important in view of deciding which statistical test should be used. Finally, we will illustrate the use of inferential statistics by presenting a commonly used test in corpus linguistics, namely the chi-square test, which determines whether frequency differences between categories are significant.

Descriptive statistics for corpus data

The first step for analyzing quantitative data is to describe the content of a corpus. For example, this step could involve calculating the number of sentences in the passive voice used in a corpus of journal articles. Imagine that, in this study, the passive sentences were retrieved from 10 texts from different newspapers, resulting in the figures reported in Table

homogeneity. The standard deviation can be calculated automatically thanks to a dedicated function in spreadsheet software (e.g. Excel). This measurement can also be automatically calculated in software devoted to statistical tests (see URLs at the end of this book). Another measure of central tendency is the median, placed at the middle of the different values found in the corpus, so that the data set is divided into the lower half and the upper half. In the case of our example, which includes 10 values ranked in ascending order, the median is equidistant from the fifth and the sixth value, that is, between 78 and 89. Its value is 83.5, the mean of the two intermediate values. When the number of values is an odd number, the median is simply the value in the middle. What is interesting here is that even if the value of 120 in text no. 10 was replaced by 1,200, the median would be the same as before. We can therefore see that the median is more appropriate than the mean for summarizing data in the presence of extreme values.

Before performing descriptive statistics on the data obtained in corpus studies, it is necessary to make the number of occurrences comparable, and this can be achieved through the use of different sources. For example, in order to compare the number of passive sentences in the above-mentioned 10 texts, it would have been necessary to ensure that all the texts had a comparable number of words. Indeed, we can observe that text no. 10 contains almost three times more passive sentences than text no. 1. If text no. 10 were roughly three times longer than text no. 1, the two texts would be equivalent in terms of the proportion of passive sentences. If this were not the case, then the two texts would be different. As a result, when the sources are of variable length, which is generally the case, it is necessary to transform the number of occurrences into relative frequencies, so that they can be easily compared. Let us take another example to illustrate this point. Imagine a study aiming to compare the use of passive sentences in spoken and written modes, and finding 67 occurrences of passive sentences in the spoken corpus versus 3,372 in the written corpus. If these figures come from corpora of different sizes, for example a written corpus of 3,179,546 words and a spoken corpus of 573,484 words, they cannot be compared directly. As a matter of fact, a larger corpus increases the probability of finding more occurrences of a phenomenon. Thus, in order to determine whether passive sentences are used more frequently in the written than in the spoken form, it is necessary to transform the data according to the same base of normalization, for example the number of occurrences per 10,000 words, per 100,000 words or even per million words. To turn a number of occurrences into a relative frequency, we need to apply a rule of three, by dividing the number of occurrences found in the corpus by the total number of words in the corpus, then multiplying by the base of normalization, as shown in the example below, which has a base of normalization equal to 10,000. This (fictitious) comparison indicates that passive sentences are in fact almost 10 times more frequent in written than in spoken discourse: Normalization by means of relative frequencies makes it possible to compare data from different corpora. The choice of the appropriate base of normalization depends on the size of the two corpora. For small corpora of a few tens of thousands of words, a base of normalization set at 1 million words would not be appropriate. If a word appears five times in a corpus of 15,000 words, it would not be wise to conclude that this word would have a frequency of 500 occurrences in a corpus of 1,500,000 words. In fact, the behavior of rare words varies a great deal according to the genre, and even from author to author. For this reason, extrapolating a frequency, obtained on the basis of a small corpus, to a much larger scale does not offer a correct representation of what was actually observed. In general, it is appropriate to choose a base of normalization close to the size of the smallest corpus examined. However, even if it is necessary to normalize data in order to be able to compare corpora, such normalization cannot completely replace raw data, which should also be reported in order to communicate what has actually been observed. For example, raw data can be presented in brackets in a table next to the relative frequencies, as shown in In the text of an article, the two frequency elements can be reported as follows: "There were 94 occurrences of parce que in the corpus, corresponding to a relative frequency of 3.61 occurrences every 10,000 words".

Finally, we should observe that, in other cases, normalizing data may involve other methods, such as transformation into percentages. For example,

Measuring the lexical richness of a corpus

A useful application of descriptive statistics for corpus data is to calculate the lexical richness of a corpus. In Chapter 7, we saw that the notion of word encompasses different realities, depending on whether we adopt the point of view of morphology (grouping up lemmas) or semantics (which focuses on lexemes). There is yet another distinction that needs to be applied to the concept of word, since it is fundamental for corpus linguistics. This notion differentiates word types from word occurrences. For example, think of a teacher asking you to hand in an assignment of "10,000 words". To complete this task, you will count each character string one after the other to verify that you have reached the total requested. For example, according to this definition of "word", there are 12 words in the French sentence in (1). This definition corresponds to what we call word occurrences.

(1) La mère de Jacques est plus jeune que la mère de Pierre. (Jacques' mother is younger than Pierre's mother.)

Other times, however, we refer to a word not to designate the total number of character strings, but the number of different words that a corpus contains. For example, if your teacher informs you that you have to learn a vocabulary of 500 words in a foreign language by the end of the year, these are obviously different words. So, according to this second definition of word, there are only nine different words (or word types) in sentence (1), namely: la, mère, de, Jacques, est, plus, jeune, que, Pierre.

Notions such as word type and word occurrence are both very important in corpus linguistics. The size of a corpus is calculated in word occurrences, whereas the number of word types indicates the diversity of the vocabulary used in the corpus. In order to measure the lexical richness of a text, it is customary to calculate the ratio between the number of word types and the number of word occurrences, according to the following formula: number of word types in the corpus type/token ratio = number of word occurrences in the corpus

The greater the type/token ratio, the more lexically diverse the text is. Generally speaking, written genres have a higher ratio than spoken genres. For example, in the spoken corpus of French from French-speaking Switzerland OFROM (see Chapter 5), the type/token ratio is 0.02, whereas in the French section of the children's literature corpus ByCoGliJe, it is equal to 0.04.

The problem with the type/token ratio is that it is very sensitive to the size of a corpus (for this reason, the type/token value 9/12 = 0.25 obtained for sentence (1) is in no way representative). The comparison between the two corpora that we have just presented could be reliably established since they are similar in size (around a million words), but this ratio may offer biased results for corpora of very different sizes. Indeed, the larger the corpus, the more the words end up repeating themselves and, therefore, the ratio decreases all the more. For example, the type/token ratio in the Sciences Humaines written corpus that contains around 260,000 word occurrences is 0.06, but it drops to 0.007 for the 2012 installment of the Le Monde corpus, which has over 26 million word occurrences. We must therefore avoid using this type of measurement on corpora of different sizes. An alternative solution would be to divide the corpora into segments of equivalent size (e.g. 1,000 words), then to measure the ratio for each segment and, after that, the mean corresponding to the different values.

Measuring lexical dispersion in a corpus

So far, we have discussed how to calculate and report word frequency in a corpus. However, in order to estimate the importance of a word in a corpus, frequency is not the only element to take into account. For example, in a corpus containing scientific articles, the word linguistics may appear relatively frequently, due to the fact that a portion of the corpus is devoted to this field. However, this word will probably never be used in other texts in the corpus covering different fields, unlike words such as analysis, hypothesis or conclusion, which will probably be used in all scientific fields. Let us consider another example. In a small corpus of texts produced by 20 French-speaking students, the word nonobstant appears more frequently than in other language registers such as journalistic texts, but this higher frequency is due to the propensity of a particular student to use this word very frequently. In this case, it would be inappropriate to conclude that the word nonobstant is used more frequently in student texts than in newspaper articles, as this high frequency does not reflect a common practice among students. In order to avoid this type of bias, in addition to word frequency, it can be useful to calculate lexical dispersion in a corpus. This parameter reflects the way in which word occurrences are distributed across the different portions of the corpus.

There are different ways to calculate lexical dispersion in a corpus. One of the simplest ones is to count the number of portions of the corpus in which the word is present. For example, for a corpus made up of texts written by 20 students, if the word nonobstant is used by three of them, we can say that its dispersion is 3/20, or that this word is found in 15% of the corpus. Although this measure may be useful in some cases, it is not entirely satisfactory, since it does not reflect the proportion in which this word has been used throughout the three portions of the corpus. If, in this case, one of the students produces 80% of the occurrences and the other two students produce 10% each, it would be inappropriate to conclude that the distribution is homogeneous in the 15% of the corpus where this word appears. For this reason, other, more sophisticated, measures have been developed. Here, we will discuss only one of them, known as deviation of proportions (DP), which was proposed by

In order to determine the expected proportion, we must count the total number of word occurrences in each portion of the corpus separately, and then divide this number by the total number of word occurrences of the corpus. The proportions observed are calculated by taking into account the total number of occurrences of the word, whose distribution we are trying to determine in each portion of the corpus, and dividing it by the total number of occurrences of the word in the corpus.

In order to find the difference in proportions, we then have to subtract the expected proportion from the observed proportion for each portion of the corpus. All of the figures obtained for each portion of the corpus are then added, and divided by 2. The procedure that we have just described is summarized in the following formula, where  refers to the sum:

values observed proportions proportions Deviation of proportions

Let us work with an example to illustrate this procedure. Suppose we want to know whether the connective car was used consistently by the 10 students who compiled a dossier in the Littéracie avancée corpus (L2_DOS_SORB sub-corpus). In order to find out, first, it will be necessary to start by counting the number of word occurrences produced by each of the 10 students separately, as in the column "Number of word occurrences" in Table

Student

Number of word occurrences

Expected proportion of car

Absolute frequency of car

Observed proportion of car

Difference in proportions

However, the dispersion measure only becomes truly informative when it is compared to the one obtained for other words. For example, the same calculation indicates that puisque has a deviation of proportions equal to 0.45 and parce que a deviation of proportions equal to 0.43. Car seems to be the connective that is used most consistently by students. The homogeneity of puisque and parce que is also very similar.

In summary, in addition to frequency indications, it is important to provide information about lexical dispersion in a corpus, either by simply calculating the percentage of texts in which a word is used, or by reporting a dispersion measurement, such as the deviation of proportions we have just described.

Basics of inferential statistics

So far, we have seen that corpora compile linguistic data collected from texts or recordings. These corpora do not contain all possible linguistic data but represent their samples. The primary goal of research in corpus linguistics is to be able to draw conclusions in relation to a linguistic phenomenon, not only at the level of the observed data sample, but also to generalize it to all data of the same type. To do this, it is necessary to use inferential statistics, a tool that makes it possible to know whether it is correct to generalize the results found, at the level of a sample, to a population.

Before going further in the description of the logic of inferential statistics, we should focus for a moment on the notion of hypothesis, which is the basis for statistics. There are different ways of making hypotheses. We could imagine the following hypotheses in [1] and [2]:

[1] French is a more beautiful language than English.

[2] Children produce few passive sentences.

These hypotheses are interesting but cannot be tested empirically, since it is impossible to collect data that would allow them to be refuted. In fact, hypothesis no. [1] is a subjective value judgment, whereas hypothesis no. [2] has been formulated too vaguely to be empirically testable.

Empirically testable hypotheses must clearly define the variables observed, the relationship between these variables, as well as the measures used for describing them. For example, hypothesis no. [2] could be transformed into no.

[3] H 1 : Children aged between 5 and 8 years produce fewer passive sentences than those aged between 9 and 12 years.

In an inferential statistics test, it is not research hypotheses like [3] that are tested but alternative assumptions called null hypotheses. A null hypothesis focuses on the opposite fact to the one we wish to prove. The null hypothesis for [3], written as H 0 , is

[4] H 0 : Passive sentences are used with a similar frequency by children aged between 5 and 8 years and those aged between 9 and 12 years.

The reason why statistical tests assess the null hypothesis and not the research hypothesis itself arises from the philosophical argument that it is not possible to prove that a hypothesis is true in all cases (be they observed or not), whereas it is possible to prove that it is false by presenting a single case when this exists (counterexample).

An inferential statistics test provides two important values:

-the numerical result of the test, such as the chi-square value (see section 8.5); -the probability value associated with the result, denoted as p. The value of p can oscillate between 0 and 1. It expresses the probability of the sample data being observed if the null hypothesis were true in the population.

Let us go back to the example of the translations of néanmoins and toutefois, presented in Table

The null hypothesis can be rejected when the value of p is smaller than a certain value. In some fields such as medicine, only a margin of error of 1% is tolerated, and p is only considered significant when it is smaller than 0.01. In other fields, some researchers choose a higher confidence threshold and accept a 10% margin of error (which corresponds to a p value equal to 0.1). In this chapter, we will stick to the 5% (0.05) value, which is the most frequently used in the social sciences.

In summary, the result of a test is said to be significant when p is smaller than 0.05. This amounts to saying that in the case where the null hypothesis is correct, the observed difference (or one greater) might be obtained less than 5% of the time. In the case where the value of p is greater than 0.05, the results obtained do not make it possible to confidently reject this conclusion and the null hypothesis. However, this does not mean that the data prove the null hypothesis (following the logic that which has not been proven to be false must necessarily be true). An insignificant test does not mean that the research hypothesis is false, which in itself would be an interesting result. The only adequate conclusion for an insignificant result is that there are not sufficient elements present in the data to be able to reject the null hypothesis.

Finally, an important distinction that must be made in order to understand inferential statistical tests is the difference between one and two-tailed tests. Two-tailed tests are used when the direction of the difference between two groups or two categories has not been specified by the research hypothesis. For example, if the research hypothesis tested is that the connectives néanmoins and toutefois are not used with the same frequency in two text genres, but we do not know which type of text is supposed to contain more connectives than the other, then a two-tailed test should be carried out. However, if the way in which the groups differ is specified in the research hypothesis, as in no. [3], stating that older children use more passive sentences than younger children, then a one-tailed test is preferable.

Typical variables in corpus studies

There are different inferential statistical tests, whose application depends on the type of variable observed. Before describing these tests in depth, we should succinctly refresh the concept of variable and the different forms a variable may take. A variable simply designates something that can display different values. For example, the number of adjectives found in different corpora is a variable. Likewise, the gender of participants in a corpus, or their geographical origin, are also variables. In the literature, types of variable may receive different names. Here, we will follow the approach proposed by

Linguistic variables are related to frequency elements measured within the corpus, such as the number of adjectives or the number of speech acts like requests or orders, or the number of negative prefixes such as un-or disfound in a corpus, to mention just a few examples.

Explanatory variables are related to the context in which linguistic data are produced. These variables can include gender and geographical region, as we have already seen, and also the textual genre or form (spoken or written discourse), the language in the case of comparable corpora, and the age or language proficiency level in the case of children and learner corpora. In short, these variables gather all the necessary elements for analysis, which are not the linguistic data themselves.

The two categories of variable we have just introduced can be measured through different scales:

-nominal; -ordinal; -scalar.

Nominal variables involve values corresponding to different categories, which have no numerical value. For example, a word's grammatical category is a nominal variable that may acquire values such as noun or adjective, among others. The function we assigned to the connective dans la mesure où in Chapter 7 is another example of a nominal variable having two possible values, namely cause and condition. In these examples, nominal variables fall into the category of linguistic variables. Another example of a nominal variable, this time an explanatory one, could be the region of origin of the speakers of a spoken corpus, for example, Paris, Geneva, Brussels, Montreal. In some cases, nominal variables are coded using numbers, for the sake of simplicity, for example 1 for Paris, 2 for Geneva. However, these numbers are simply category codes and their numerical values have no particular meaning.

Ordinal variables are also used for grouping values into different categories, but they have a natural order. For example, if the participants in a corpus are classified into age groups such as group 1 (18-25 years), group 2 (26-40 years) and group 3 (41-60 years) for sociolinguistic analyses, the age variable is an ordinal one. So, here we can see that it is possible to order the different groups, since the participants in group 3 are older than those in group 2, who in turn are older than those in group 1.

In the previous example, while age groups can be ordered as such, it is not possible to do so with the different participants within each group. The variables that can be ordered in this way are called scalar variables, measured on a digital scale, whose points are distributed at equal distance and with a zero point. The property of these variables is that operations such as addition, subtraction, multiplication and division can be performed on them, since they represent measurable quantities, rather than a simple ordered list, as ordinal variables are. For example, if measured continuously rather than in groups, the age of participants can be considered a scalar variable, since the distance between 12 and 13 is the same as between 13 and 14. We can also say that a 28-year-old is four times older than a 7-year-old child. Linguistic variables involving the relative frequency of some kind of phenomenon in a corpus are also scalar variables.

Since different types of variables have different properties, they cannot be processed in the same way. For example, let us imagine that we have collected information concerning the nominal variable mother tongue from the speakers in a corpus, and that we have 36 French-speaking people and 40 German speakers. As such, the only type of information we could report is the frequency with which every variable condition appeared in the data. However, if we had 20 speakers aged 30 years and 20 speakers aged 32 years, we could say, more than on average, the participants are 31 years old.

A detailed description of the suitability of the different types of variables for specific statistical tests is beyond the scope of this chapter, but it is important to know that statistical tests can be classified into two main categories:

-parametric tests; -non-parametric tests.

Parametric tests can be used when the linguistic variable under consideration is measured on a scale, and the explanatory variable is measured following a nominal or an ordinal criterion. These tests include T-tests, used when the explanatory variable has two categories, and ANOVA -Analysis of Variance -used when the explanatory variable has more than two categories. For example, this type of test can be used in studies seeking to find out whether advanced learners produce significantly more discourse connectives than intermediate learners, or whether speakers produce more fallacious arguments when speaking at political party meetings or at electoral campaigns. Such tests will not be discussed in this chapter since they are rarely used in corpus linguistics, but you can refer to

When the linguistic variable is measured on a nominal or ordinal scale, it is necessary to turn to non-parametric tests, such as the chi-square test (or chi 2 ). For example, in order to determine whether type of speech acts varies between two text genres, it is necessary to carry out a chi-square test. It is this test that we will present and illustrate in the rest of this chapter.

Scalar

Relative frequency of passive sentences in a corpus Age of speakers (measured along a continuum)

Measuring the differences between categories

In this section, we will focus on how to report frequency differences between categories in order to determine whether these differences are significant, and then to establish a link between two nominal or ordinal variables, for example, whether the value of a nominal or an ordinal variable depends on another nominal or ordinal variable.

Let us imagine that we are conducting research on the use of regionalisms in French. For example, it is widely known that some French-speaking regions such as Switzerland and Belgium use different words, from those used in France, for designating numbers such as 70 and 90, in this case septante and nonante. The situation seems a little more complicated in the case of 80, which is expressed as huitante in some Swiss cantons and quatrevingts in others. According to a recent linguistic atlas of regional French

Different questions can be studied concerning this situation. First, we should check with corpus data whether the use of huitante varies from one group of cantons to another. To do this, we have to look for the number of occurrences of the word huitante in each of these six cantons by means of the OFROM corpus, which compiles the French spoken in Switzerland (see Chapter 5). Then, we have to group the results in a table such as Table

In order to test whether this difference is significant, we use a statistical test called the chi-square goodness-of-fit test, represented by the symbol  2 due to the notation denoted by the Greek letter that gave it its name.

The  2 is actually a value calculated according to the following formula, where  denotes the sum of all categories considered: This value makes it possible to know the probability of such results actually being observed, if the occurrences were randomly distributed along the variable categories (which would then represent the null hypothesis). The higher the value of  2 , the more it suggests that the distribution observed moves away from the expected one if the distribution is uniform. The value obtained should be compared with a critical value of  2 , which in this case depends not only on the field (which establishes the significance threshold) but also on the number of degrees of freedom (often indicated as "df"). The number of degrees of freedom is calculated by subtracting 1 from the number of categories in the explanatory variable. For a variable with two categories, as we have here, the  2 test is significant with a threshold placed at 0.05 if the value of  2 is greater than 3.84. Thanks to the corpus data considered, we can deduce that (at 5%) the use of huitante is almost certainly associated with a group of cantons, which corroborates the assertion made on the atlas we quoted.

To apply the  2 test, we have to calculate the  2 coefficient value, which can be done manually, as we did here, or by using statistical software. The value can also be calculated online using on the VassarStats site, by choosing the option "Chi square goodness of fit" accessible in the "Frequency data" tab. To do this, in this example, the frequencies observed (Table

The  2 can also be calculated for variables with more than two categories. For example, we might want to determine whether the use of huitante varies between the three cantons that are supposed to use this word, namely Vaud, Valais and Freiburg. This time, we should type the observed values (Table

However, through the observation of the data described above, we cannot yet know whether the cantons vary in their frequency of use of the two regional words huitante and quatre-vingts. To find out, we should draw a comparison between the use of huitante and quatre-vingts in the two groups of cantons. To do this, we have to count the occurrences of huitante and quatre-vingts and relate these to the two cantonal categories in a crosstab.

To build a crosstab, we only have to place one of the variables in columns (often the explanatory variable) and the other variable in rows (the linguistic variable). For the question we are focusing on here, Table

A representation of percentages in relation to the grand total implies a total transformation of the data, in which each cell is divided by the grand total (132). The data thus transformed are not intrinsically more informative than those in Table

The expected value for a cell in the table is calculated by multiplying the total of its row and that of its column, and then dividing the result by the grand total. For example, the expected value for the first cell in Table

As in the  2 goodness-of-fit test, this value should be compared to a critical  2 for a threshold conventionally fixed at 0.05, and the appropriate number of degrees of freedom. In the case of the crosstab, the number of degrees of freedom is calculated according to the (L-1) (C-1) formula, where L corresponds to the number of rows of the table and C corresponds to the number of columns.

In this example, the number of degrees of freedom is equal to (2-1) (2-1), that is, 1. The critical value is 3.84 once again, so the result of the test is significant. This indicates that the distribution of the words quatre-vingts and huitante between the two groups of cantons is not random, but clearly reflects a real difference in linguistic practices between them.

The value of the  2 test of independence can also be calculated using statistical software, or directly online using the same site mentioned previously. This time, on VassarStats, choose the option "Chi square, Cramer's V and Lambda" which is accessible on the "Frequency data" tab, enter the values observed in the table and choose the option "Calculate".

Here is what the VassarStats site retrieves for the values in Table

The indication "Cramer's V" is a measurement of the size of the observed effect. This is an indication of the degree of relation between two variables. The result of the  2 test provides only one answer to the existence of a link between two variables, but not to the importance of this association. For a crosstab with one degree of freedom (2x2), we consider that the effect is small when the value of Cramer's V reaches 0.1, moderate when the value reaches 0.3 and large when the value is 0.5 or more

Let us then imagine that we wish to know whether the use of the words huitante and quatre-vingts varies specifically between the three cantons that use the word huitante significantly less, namely Neuchâtel, Jura and Geneva. In order to find this out, if we enter the figures in Table

The suitable test is called Fisher's exact test, which can be calculated using the same online statistical toolboxes as the  2 test. This test offers only a probability and not a statistical value because it is an exact test. In this case, the value of Fisher's exact test is p = 0.29, which indicates that the distribution between these two words does not vary significantly between these three cantons. For this test, the sentence reporting this result could be as follows: "The speakers of the cantons of Neuchâtel, Jura and Geneva do not vary significantly in their use of the words huitante and quatre-vingts (two-sided Fisher's exact test, p = 0.29)".

Finally, note that the chi-square test of independence also makes it possible to know whether all the cantons differ from one another or not. To carry out this test, let us set aside the values of the canton of Geneva, which is under-represented in the corpus, with only 75,598 words (against more than 100,000 in all the other cantons). This poses problems for carrying out an  2 test, as we have just seen. The result of the  2 test of independence for the five cantons of Vaud, Valais, Freiburg, Neuchâtel and Jura is as follows:  2 (4) = 19.63, p = 0.001. We can see that the difference in the distribution of the two words between the cantons is significant.

However, this result is not entirely informative. Indeed, the only thing that this test lets us conclude is that the distribution between cantons differs significantly in some way, or, in other words, that it is unlikely to observe such a biased distribution under the assumption that huitante and quatrevingts are used uniformly across the five cantons. Nevertheless, the test does not tell us whether all the cantons differ from one another or if only some of them are different from the others, nor whether this difference is linked to the use of the two words or only to one of them.

To determine where a significant difference stems from when an  2 test comprises variables from more than two categories, we have to observe standardized residual values of the test. A standardized residual is a ratio calculated from the observed frequency and the expected frequency for each category, according to the following formula: observed frequency expected frequency standardized residual = expected frequency



The more the residual moves away from 0, the more it means that the category contributes to the significant test result. According to a rule of thumb, we consider that any value of a standardized residual greater than +2 or smaller than -2 shows a significant difference compared to the mean, because this figure means that the value obtained deviates by more than two standard deviations from the mean. A positive value reflects an overuse compared to the other categories, whereas a negative value reflects an underuse. In VassarStats, standardized residuals are calculated automatically and reported below the test results table. In the case of the test that we have to perform, the standardized residuals obtained are shown in Table

Conclusion

In this chapter, we have discussed some simple procedures that help to report and analyze the numerical values obtained in a corpus study. We have made an important distinction between descriptive statistics (which relate only to the sample tested) and inferential statistics (which aim to generalize the results to an entire population). We have also described the different forms that variables in a corpus may adopt, and highlighted the fact that statistical tests must be adapted to the nature of the variables. In terms of descriptive statistics, we started by presenting the advantages and disadvantages of the mean and the median as a way of synthesizing data and concluded that the use of the mean also requires indicating the dispersion of the values around it (notion of standard deviation). We then tackled the question of data normalization, which makes it possible to compare values from different corpora, transforming them into relative frequencies through the use of bases of normalization. We have emphasized that frequency is not the only indicator of the prevalence of a word in a corpus and that its dispersion across the different portions of the corpus is also very important. We therefore introduced a simple and a more sophisticated way (deviation of proportions) to calculate this dispersion. We also showed how to measure the lexical richness of a corpus using the type/token ratio, and presented the limitations of such a measurement. To conclude, we gave two variants of the chi-square test, an inferential statistics test which makes it possible to determine whether the differences observed between the distributions of several categories are significant.

In the case of the chi-square test of independence, we have shown how to represent data using a crosstab. We introduced the notion of standardized residual, which makes it possible to determine where the significant result of the test comes from, and introduced Fisher's exact test as an alternative to the chi-square, when the conditions posed by the latter are not met.

Revision questions and answer key

8.8.1. Questions 1) The tables below show the number of occurrences of the (lemmatized) words bateau and je, as well as their English equivalent boat and I, in the bilingual corpus of children's literature ParCoGLiJe (see Chapter 5). This corpus includes four original works in French and their translation into English, as well as four original works in English and their translation into French. Raw data (number of occurrences of the words bateau, je, boat, I) drawn from these works are presented in the four tables below, as well as number of word types and word occurrences. Normalize the data so as to be able to compare the frequency of these words throughout the texts in each table, and then between the sub-corpora (original texts vs. translated texts, French texts vs. English texts). 2) On the basis of the four above-mentioned sub-corpora, what hypotheses could be formulated concerning:

Original texts in

-the difference in distribution between the words bateau and je in French; -the difference between original texts and translated texts; -the difference between French and English.

What are the corresponding null hypotheses? What are the types of variables corresponding to:

-the number of occurrences of bateau, je, boat, I; -the type of text: translated or original; -the novel: Le jardin secret, L'île au trésor, Oliver Twist, Le livre de la jungle;

-the number of word types in each corpus; -the language: French or English.

3) Calculate the type/token ratio for each text. In which case is it methodologically correct to apply this measure? 4) Calculate lexical dispersion for the words boat, I, bateau and je in each of the four sub-corpora using the deviation of proportions method. What can you notice? 5) Determine whether the distribution of different translations of néanmoins and toutefois found by

Relative frequency every 10,000 words of lemmas bateau and je in French texts (originals and translations) The normalized data enables us to observe that the pronouns je and I have higher frequency than the nouns bateau and boat. This result is widely expected, insofar as closed-class words usually have higher frequencies than open-class words, as is the case of nouns. We can also observe that the use of je and I is highly variable between novels. This gap reflects the different narrative perspectives used. First-person novels make much more use of it than third-person novels. There are also great differences between certain texts and their translation, especially in the case of Mémoires d'un âne, where I appears three times less frequently in the English translation than in the French. Finally, it is interesting to note that the frequency of the words bateau and boat also varies between the original texts and their translation. This seems to indicate that the semantic fields of these words are not the same at all. For example, in French, the word sous-marin appears regularly in the novel Vingt mille lieues sous les mers and this word is translated as underwater boat in English. So, in English, the word boat is used for qualifying this means of transport, but this is not the case in French. A more in-depth analysis would reveal other interesting differences in the use of these words between languages, as we argued in Chapter 4.

2) Here are some examples of hypotheses, but many others could be possible: H1: the word je has a higher frequency than bateau since it is a function word; H0: there is no difference in frequency between the words je and bateau; H1: translated texts are lexically less diversified than original texts due to the simplification universal; H0: translated texts and original texts have the same level of lexical diversity; H1: French texts are longer (have more word occurrences) than English texts; H0: texts in French and English have a similar length.

3) a) The number of occurrences of the words bateau and je is a scalartype linguistic variable.

b) The type of text, either translated or original, is a nominal-type explanatory variable.

c) The novel is a nominal-type explanatory variable.

d) The number of word types in each corpus is a scalar-type linguistic variable. e) Language is a nominal-type explanatory variable. The type/token ratio can only be used for comparing texts of similar length. For example, it is suitable for comparing Lettres de mon moulin and Mémoires d'un âne, but not Lettres de mon moulin and Les trois mousquetaires, since the latter is more than four times longer. In fact, this ratio decreases as the texts lengthen, which is true for the case of these two novels. The type/token ratio can also be used for comparing texts and their translations, only in cases of similar length.

Proportional differences for the words bateau and boat: We can observe that the word bateau in the original texts of the four novels in French and in their English translations is used more homogeneously (0.44 and 0.33 respectively) for the word boat than in the original English texts and their French translations (0.73 and 0.61 respectively). This difference is partly due to the fact that the word bateau is found in the four novels written in French, whereas it is only found in three of the four novels written in English.

Proportional differences for the words je and I: We can see that the dispersion of pronouns je and I is much more homogeneous than that of the noun bateau, as we could expect, given the high frequency of this word. The great homogeneity of this distribution in the texts written in French can also be found in the translations of these texts into English. However, while the texts written in English make homogeneous use of I, their translation into French presents a much less homogeneous use of je.

5) The application of a chi-square test of independence indicates that the distribution of the different translations is not homogeneous between the two connectives ( 2 (4) = 14.06, p <0.01). The standardized residuals also indicate that this difference is due to an overuse of nevertheless/nonetheless when it comes to translating néanmoins (+ 2.41). The other differences are not significant. The effect size is moderate (Cramer's V = 0.22). 8.9. Further reading

Conclusion The Stages for Carrying Out a Corpus Study

Throughout this work, we have presented the different facets of corpus linguistics, both from the point of view of the theoretical questions to which this discipline provides answers and of its methodological foundations. By way of conclusion, we would like to offer a list of ordered stages, making it possible to implement the concepts discussed in this book step by step, and to carry out a corpus study.

C.1. Stage 0: wanting to know more

Before starting any project, something that is important is the desire to learn more about it. It is actually this initial curiosity that gives birth to the best research ideas. Before starting to work on a project, it is essential to try to find a question you are interested in, or at least which arouses your curiosity. Often, this first idea is intuitive and rather vague.

For example, some are fascinated by the question of the differences between men and women and how they are reflected in language. Others are enthusiastic about children and everything related to them. Still others like politics, history or sport. The great advantage of linguistics is that the study of language has interfaces with very many disciplines, and that it is possible to find study subjects in very varied fields. Take the time to question your interests and listen to your intuitions, even if the latter do not (yet) look like a research question that can be studied empirically.

C.2. Stage 1: identify relevant literature

Once you have found the subject you are interested in, the first real research step will be to find relevant documentation. Indeed, corpus-based studies aim to test empirical hypotheses that are often formulated from the results of other studies, either more theoretical or empirical in nature, which have already been published in the scientific literature.

Thus, the first step in a corpus study is to identify relevant sources that have so far explored the research subject under consideration. Most of these sources can be found in scientific journals. For example, journals such as Corpus Linguistics and Linguistic Theory, International Journal of Corpus Linguistics and Corpora are all three specialized in the publication of corpus studies, whereas the Journal of Language Resources and Evaluation publishes articles on methodological aspects related to the compiling and processing of corpus data. In addition, many journals specializing in different areas of linguistics, such as Journal of Pragmatics, Journal of Sociolinguistics and Journal of French Language Studies, regularly publish corpus studies. We have offered many examples of such studies in Chapters 2-4. The names of the journals or books in which these studies have been published can easily be found by consulting the reference list at the end of this book.

The journal sites mentioned above will help you to search for articles using keywords. Another way to identify relevant literature is to use the Google Scholar search engine, which indexes most of the available scientific articles. Other databases that are available for a fee, such as the Scopus database, make it possible to search for articles from many different journals using a single query.

In scientific journals, in most cases, access to articles is not free, although the alternative model called Open Access is gaining popularity. University libraries generally offer access to many online journals and should be checked for availability via the university computer network. When an article comes from a journal that is not freely available, it can still be accessible in certain cases. In fact, more and more researchers are making a version of their articles available to the public on sites such as ResearchGate and Academia, or on the site of their institution or on personal web pages. It is therefore useful to look for the article title directly in a search engine to find out whether such a version is available online.

C.3. Stage 2: formulating research hypotheses

Once the relevant studies have been identified in the scientific literature, the second stage in a corpus study is to formulate specific hypotheses to study the research question under consideration. These hypotheses emerge from the results of previous studies, and also aim to supplement them, for example, by testing new variables.

Let us recall that the hypotheses must be empirically testable (see Chapter 8, section 8.4). In other words, they should clearly identify variables and suggest relations between them. For every research hypothesis, it is also important to formulate the corresponding null hypothesis, since it is the latter that will be evaluated by means of statistical tests.

C.4. Stage 3: operationalizing your hypotheses and choosing data

The operationalization phase of research hypotheses is crucial for the success of an empirical study. This phase consists of determining how the variables will be measured. For example, if the question to be investigated is the assertion that women talk more about their emotions than men, the operationalization of this question will require building or obtaining a lexicon of words related to emotions that can be searched in a corpus, chosen to make it possible to determine whether women actually use these words more than men.

As from the operationalization phase, it is also important to choose the corpus on which the study will be carried out. If the study makes use of existing corpora, the possibility of investigating a certain question or not, or the manner in which it can be operationalized, depends on the characteristics of the corpus.

For example, to be able to compare the language of women and that of men, this variable must be clearly identifiable in the metadata of a corpus. Likewise, the stylistic genre of the corpus should be compatible with the question under investigation. People are more likely to talk about their emotions in spontaneous conversations or when they relate a memory than when they are giving a scientific presentation, for example. The operationalization phase must therefore lead to the decision to use an existing resource, for example among those described in Chapter 5 or, to create a new corpus, according to the principles introduced in Chapter 6.

C.5. Stage 4: extracting and annotating corpus data

Once the corpus has been identified or created, the next step is to determine the best strategy for retrieving the relevant data. In order to automatically search for elements in a corpus, a surface feature such as a word or list of words should be associated with it. In the case of the research question mentioned above, it would be the vocabulary of emotions.

In many cases, the automatic extraction includes data which are irrelevant for analysis and which require manual sorting. For example, not all the uses of the verb like correspond to the expression of an emotion, because in certain cases this verb is used with a modal value (I would like you to eat your ice cream). Data extraction can also be based on a prior automatic analysis of the chosen corpus, such as lemmatization or part-of-speech tagging (see Chapter 7, section 7.4).

For many questions, the raw data retrieved from a corpus will not be sufficient. It will therefore be necessary to annotate data before obtaining answers to these questions. The annotation process requires the prior identification of clear categories (see Chapter 7, section 7.3). A first pilot annotation phase should be used for testing these categories, as well as for identifying problematic cases and leading to the preparation of a more or less detailed annotation manual. In order to be reliable, an annotation should ideally be carried out by several annotators independently, and their measured agreement should be placed above a certain threshold (see Chapter 7, section 7.5).

C.6. Stage 5: analyzing data

The data analysis stage must begin with any transformations in order to make the data comparable across corpora. This stage notably involves the transformation of raw figures corresponding to the number of occurrences observed in the corpus into figures reporting relative frequencies, following a base of normalization. Then, different descriptive statistics should be carried out. In order to be able to draw quantitative conclusions, it is necessary to carry out inferential statistical tests, which will indicate whether the results obtained on the corpus sample can be generalized to the entire population.

We should recall that the possibility of using different tests depends on the way in which the hypotheses have been operationalized and, more specifically, on the types of variables involved (see section 8.4).

C.7. Stage 6: presenting your study in a report or an article

Corpus studies examine empirical data in order to draw quantitative conclusions, which are to be interpreted in the light of the hypotheses formulated in stage 2. The reports or articles presenting this type of results generally follow a very precise structure. In the introduction to the document, the research question is briefly introduced and contextualized. The second section contains a review of relevant previous studies (also called the state of the art), which served as a theoretical basis for the study, or which are inspired on other corpus studies that the current research project will supplement or sometimes call into question. The third section contains a presentation of the hypotheses resulting from it and which will be the subject of the empirical study. The rest of the document aims to present the results of the study. The detailed presentation of the study begins by describing the corpus used and explaining how data was retrieved from it, as well as their annotation. Then, the results are presented in detail, as well as the statistical analyses that were carried out, with the results of the statistical tests reported in accordance with a very precise format (see section 8.6). These results should finally be discussed in a critical manner, indicating, in particular, the extent to which the initial hypothesis is verified. A brief conclusion can summarize the main results obtained, or even provide perspectives for further studies. This order of presentation can be summarized in a diagram as follows:

1) introduction;

C.8. Conclusion

Once all these steps have been completed, you will have the satisfaction of having contributed to research by providing the scientific community with new empirical data, which in turn can serve as a starting point for other research. Your results could be replicated and reassessed by other researchers. Very often, the results of empirical studies also serve to modify and improve existing theories, and thus contribute to make linguistics a scientific study of language. This is one of the key objectives of the empirical approach that we presented in Chapter 1: to provide a scientific perspective on language, using a rigorous methodological approach based on the quantitative analysis of linguistic data.
