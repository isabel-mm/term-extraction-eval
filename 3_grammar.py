import os
import re
import nltk
from collections import Counter

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

GRAMMAR = r"""
    NP: 
        {<JJ>*<NN.*><NN.*>+}    
        {<JJ>*<NN.*><IN><NN.*>+} 
        {<NN.*><NN.*>} 
        {<NN.*><IN><NN.*>} 
"""

CORPUS_FILE = "corpus_completo_procesado.txt"
OUTPUT_FILE = "terminos_extraidos_filtrados.txt"

if not os.path.exists(CORPUS_FILE):
    print(f"âŒ El archivo {CORPUS_FILE} no se encuentra en el directorio.")
    exit()

print("ðŸ“‚ Cargando el corpus...")
with open(CORPUS_FILE, 'r', encoding='utf-8') as f:
    text = f.read().lower()

print("ðŸ” Tokenizando y etiquetando el texto...")
sentences = nltk.sent_tokenize(text)
tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]
tagged_sentences = nltk.pos_tag_sents(tokenized_sentences)

print("ðŸŒ± Extrayendo candidatos a tÃ©rminos...")
chunk_parser = nltk.RegexpParser(GRAMMAR)
term_candidates = set()

for sent in tagged_sentences:
    tree = chunk_parser.parse(sent)
    for subtree in tree.subtrees():
        if subtree.label() == 'NP':  
            term = " ".join(word for word, tag in subtree.leaves())
            if len(term.split()) > 1:  # Filtra tÃ©rminos de una sola palabra
                term_candidates.add(term)

print(f"âœ… Se han extraÃ­do {len(term_candidates)} tÃ©rminos Ãºnicos de mÃ¡s de una palabra.")

print("ðŸ“Š Contando frecuencia de tÃ©rminos en el corpus...")
term_freq = Counter(re.findall(r'\b(?:' + '|'.join(map(re.escape, term_candidates)) + r')\b', text))

filtered_terms = {term: freq for term, freq in term_freq.items() if freq > 2}
print(f"âœ… Se han filtrado {len(filtered_terms)} tÃ©rminos con frecuencia > 2.")

print(f"ðŸ’¾ Guardando tÃ©rminos en {OUTPUT_FILE}...")
with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
    for term, freq in sorted(filtered_terms.items(), key=lambda x: x[1], reverse=True):
        f.write(f"{term}\t{freq}\n")

print(f"âœ… Proceso finalizado: {len(filtered_terms)} tÃ©rminos guardados en {OUTPUT_FILE}.")
