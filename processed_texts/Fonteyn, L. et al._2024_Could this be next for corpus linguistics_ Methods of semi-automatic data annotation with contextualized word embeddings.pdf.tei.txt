introduction corpus linguistics   collection annotation of datum commonly involves relatively balanced combination of computer aided manual labour   common practice   instance   retrieve datum representing particular linguistic phenomenon electronic corpus   e g   mean of concordancer tool query script   subsequently manually categorize collected example different functional semantic group   e g   animate inanimate   literal figurative   agent patient instrument         range of research question linguist aim address mean of corpus datum expanded diversity complexity   researcher started resort complex   multivariate   statistical analysis address question   longer practically feasible continue working way   given labour intensive manual datum annotation   difficult meet growing need annotate larger sample robust statistical research     important practical challenge corpus linguistics determine datum annotation practice evolve need of researcher   e g   paper contributes tackling challenge exploring corpus datum annotation   semi   automatic mean of machine learning   specifically   home use of   contextualized word embedding    e   vectorized representation of meaning of word token based sentential context appear   extracted large language model   llms   e   machine learning architecture large number of adjustable parameter   designed exploit large amount of pre training text datum    natural language processing   nlp    contextualized word embedding generated llms shown perform impressively   downstream task    like of speech tagging   dependency parsing   named entity recognition   e g   method method assess paper takes starting position researcher access pre annotated   e g   of speech tagged   lesser extent semantically annotated   corpora range of   semi   automatic annotation tool   e g   customizable procedure demonstrated case study   classification implemented different classification approach belonging broad type   type   solely extracted contextualized embedding of target word   feature training traditional shelf classification algorithm   particular   resorted k nearest neighbour   knn   support vector machine   svm   algorithm   implemented scikit learn software package   knn   model prediction of category unseen test item belongs decided based k nearest neighbour training set     neighbourhood determined distance embedding of unseen test item training test item   svm algorithm   contrast   uses training datum infer plane space spanned input feature   e   input word embedding   maximally separates instance according class   case of binary classification problem   fitted plane divides space region test item assigned label considering of plane feature representation lays   second type of approach   use original pre trained llm   e   macberth gysbert    finetune parameter order perform classification task hand   apply kind of fine tuning      detailed information model trained   corpus linguistics   common parametric fine tuning procedure incorporates additional parameter tuned order produce probability distribution category example assigned   refer method simply   fine tuning   rely implementation of transformer software package evaluation evaluate approach fold cross validation   cv   procedure   divide available datum non overlapping section   split    test performance of classification approach of split   training material classification algorithm   rely split testing iteration   cv iterative evaluation procedure   yields statistically solid comparison different classification approach   enables assess variance performance of model   e   fluctuation performance difference training test datum    finally   cross validated result allow employ powerful model comparison method helps determine method worth deploying future automatic annotation setting   word sense disambiguation llm generate contextualized embedding   bert   bidirectional encoder representation transformer   fire metaphor demonstrate llm employed partially automate annotation of datum term of word sense category   focus use of set of lemma related conceptual domain of fire   fire   flame   ardent   blaze   burn   popular lexical domain metaphorical extension   charteris black         word related fire frequently occur literal sense   figurative use   e g   describe positive negative emotion    line tsunami case study de toy example   test hypothesis great fire of london         triggered frequencydriven change semantic structure of fire word   purpose   test data annotation study semi automatically   end   random sample of      instance lemma extracted emma corpus   contains text written     prominent author born seventeenth century belonged london based elite    comparing f   score of approach   find   variation individual lemmas   generalization   f   score lowest knn algorithm classify example   exception lemma ardent   knn performs slightly better svm   worst performance of knn found burn fire   poor recall results f   score of             respectively   contrast   best result achieved metric fine tuning   equalled fine tuning ardent   notably   f   score metric fine tuning drop        standard deviation reveal stable performance trial   mass weight far     appears   macberth   embedding fine tuning end end metric learning approach serve highly reliable robust tool automated word sense classification   approach continues perform strongly data taken corpus of specialized language annotated finer level of granularity   e   subtler sense distinction    consider case study of terminological overlap scientific language          isaac newton differentiated concept of weight mass   referred mean of word weight   investigate   instance   long took scientific community adjust usage of word weight newton proposal   newton terminological renewal diffused scientific community   e g   author network   discipline   etc     use of word mass weight affected   conscious effort   improve scientific terminology   large scale specialized corpus of scientific writing royal society corpus source model precision recall f   previous case study   approach sense disambiguation task individual lemma   training testing dataset lemma grouped   result of classifier approach fine grained sense disambiguation of mass weight presented f   score table corpus linguistics   category equal weight   macro average of f   computed   sense class individually average taken class   micro f    contrast   sense class treated separately   means small sense category   challenging label correctly fewer example of training datum   important calculation of f   score   starting micro f   accompanying standard deviation   find relatively high score of classification task lemma individually grouped set     previous case study   accurate   stable   classification approach metric fine tuning   evident macro f   score   difference metric fine tuning second best approach   case svm   large   fact   macro f   ranges             metric fine tuning   of approach score higher        result indicate fine grained wsd task   imbalance low number of training example certain category   classification algorithm knn   svm     regular   fine tuning perform poorly category   appears particularly true fine tuning   noticeably struggles low frequency category   e g   met lemma mass   f            semantic role labelling   scent term agent   object   patient wsd   computational study explored performance of llms semantic role labelling case study   continues theme of olfactionthat    sense smell   odor   processed experienced         scent term reuk geur presented agent of action   action experienced person     den geur van u schepsel heeft oock bedroghen mijnen reuck         dbnl    scent of creature misled sense of smell   b   want gelijck een lieflicke reuk den mensche seer vermaeckt            dbnl    like gentle smell pleases people      c   en hoe lieflijk wierd ik door haaren reuk verkwikt          edbo    gently smell invigorated           scent term reuk geur presented object given thing   person      het geeft een lieffelyke reuk die iets verkwiklyke heeft         edbo    gives lovely smell invigorating   b   blaas   lentewind blaas uw geur door bosschen beemden   hoven         edbo    blow   spring wind blow scent forest   field   yard   c   die   oog met kleur vermaakt   en   hert met geur bewaassemt         dbnl    pleases eye colour   fogs heart scent        p   scent term reuk geur presented patient undergoer of action   person      de heere rook dien lieffelyke reuk         edbo    lord smelled lovely smell   b   de wandelaar juicht haar toe   daar hy haar geur geniet         edbo    hiker cheers   enjoys smell   c   nochtans verneem ik geenen viesen reuk         dbnl    perceive foul smell   table model comparison section   summarize evidence gathered case study   classification method expected strongest result similar semi automatic annotation setup   end   use bayesian model comparison method presented use null hypothesis testing   include bayesian model comparison helps overcome problem occurs frequentist method   estimated effect size of observed difference model entangled underlying sample size   chosen bayesian comparison method jointly analyses cross validated result obtained different classification approach multiple dataset     input use classification accuracythat   proportion of correctly annotated itemsof classification approach   e   knn   svm   fine tuning   metric fine tuning   obtained of fold of outlined case study   output of comparison method consists of estimated probability particular classification approach performs differently similarly of   result of comparison presented table conclusion drawn table fact combining contextualized word embedding llms metric fine tuning appears reliable approach automatically annotating linguistic datum   procedure adaptable corpus linguist annotation need   furthermore   procedure presented added benefit model annotated datum shared   replicate datum annotation scheme corroboration follow study     given robustness   high reliability   flexibility   potential reusability replicability   worth considering   semi   automated datum annotation procedure corpus linguistic methodology       explore llm fully integrated corpus linguistic research   wish note   instance   fact llms bert based macberth gysbert manipulated distinguish literal figurative use of word correctly label semantic role of target word way comparable human annotator necessarily mean llm said   understand   concept of metaphor agency   state information needed successfully distinction outlined human annotator encoded embedding generated llm   work pursuing similar question sort of information encoded contextualized embedding llms   e g